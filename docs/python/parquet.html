<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Reading and Writing the Apache Parquet Format &#8212; pyarrow  documentation</title>
    <link rel="stylesheet" href="_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="_static/bootstrap-3.3.6/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="_static/bootstrap-sphinx.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Building C++ and Cython Extensions using pyarrow" href="extending.html" />
    <link rel="prev" title="Using PyArrow with pandas" href="pandas.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head>
  <body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="index.html">
          pyarrow</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><p class="caption"><span class="caption-text">Getting Started</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="install.html">Install PyArrow</a></li>
<li class="toctree-l1"><a class="reference internal" href="development.html">Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="memory.html">Memory and IO Interfaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">In-Memory Data Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="ipc.html">Streaming, Serialization, and IPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="filesystems.html">File System Interfaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="plasma.html">The Plasma In-Memory Object Store</a></li>
<li class="toctree-l1"><a class="reference internal" href="pandas.html">Using PyArrow with pandas</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Reading and Writing the Apache Parquet Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="extending.html">Building C++ and Cython Extensions using pyarrow</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting_involved.html">Getting Involved</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Reading and Writing the Apache Parquet Format</a><ul>
<li><a class="reference internal" href="#obtaining-pyarrow-with-parquet-support">Obtaining PyArrow with Parquet Support</a></li>
<li><a class="reference internal" href="#reading-and-writing-single-files">Reading and Writing Single Files</a></li>
<li><a class="reference internal" href="#finer-grained-reading-and-writing">Finer-grained Reading and Writing</a></li>
<li><a class="reference internal" href="#compression-encoding-and-file-compatibility">Compression, Encoding, and File Compatibility</a></li>
<li><a class="reference internal" href="#reading-multiples-files-and-partitioned-datasets">Reading Multiples Files and Partitioned Datasets</a></li>
<li><a class="reference internal" href="#using-with-spark">Using with Spark</a></li>
<li><a class="reference internal" href="#multithreaded-reads">Multithreaded Reads</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
              
                
  <li>
    <a href="pandas.html" title="Previous Chapter: Using PyArrow with pandas"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm hidden-tablet">&laquo; Using PyArrow...</span>
    </a>
  </li>
  <li>
    <a href="extending.html" title="Next Chapter: Building C++ and Cython Extensions using pyarrow"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm hidden-tablet">Building C++ ... &raquo;</span>
    </a>
  </li>
              
            
            
            
            
              <li class="hidden-sm">
<div id="sourcelink">
  <a href="_sources/parquet.rst.txt"
     rel="nofollow">Source</a>
</div></li>
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="col-md-12 content">
      
  <div class="section" id="reading-and-writing-the-apache-parquet-format">
<span id="parquet"></span><h1>Reading and Writing the Apache Parquet Format<a class="headerlink" href="#reading-and-writing-the-apache-parquet-format" title="Permalink to this headline">¶</a></h1>
<p>The <a class="reference external" href="http://parquet.apache.org/">Apache Parquet</a> project provides a
standardized open-source columnar storage format for use in data analysis
systems. It was created originally for use in <a class="reference external" href="http://hadoop.apache.org/">Apache Hadoop</a> with systems like <a class="reference external" href="http://drill.apache.org">Apache Drill</a>, <a class="reference external" href="http://hive.apache.org">Apache Hive</a>, <a class="reference external" href="http://impala.apache.org">Apache
Impala (incubating)</a>, and <a class="reference external" href="http://spark.apache.org">Apache Spark</a> adopting it as a shared standard for high
performance data IO.</p>
<p>Apache Arrow is an ideal in-memory transport layer for data that is being read
or written with Parquet files. We have been concurrently developing the <a class="reference external" href="http://github.com/apache/parquet-cpp">C++
implementation of Apache Parquet</a>,
which includes a native, multithreaded C++ adapter to and from in-memory Arrow
data. PyArrow includes Python bindings to this code, which thus enables reading
and writing Parquet files with pandas as well.</p>
<div class="section" id="obtaining-pyarrow-with-parquet-support">
<h2>Obtaining PyArrow with Parquet Support<a class="headerlink" href="#obtaining-pyarrow-with-parquet-support" title="Permalink to this headline">¶</a></h2>
<p>If you installed <code class="docutils literal"><span class="pre">pyarrow</span></code> with pip or conda, it should be built with Parquet
support bundled:</p>
<div class="highlight-ipython"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">import</span> <span class="nn">pyarrow.parquet</span> <span class="kn">as</span> <span class="nn">pq</span>
</pre></div>
</div>
<p>If you are building <code class="docutils literal"><span class="pre">pyarrow</span></code> from source, you must also build <a class="reference external" href="http://github.com/apache/parquet-cpp">parquet-cpp</a> and enable the Parquet extensions when
building <code class="docutils literal"><span class="pre">pyarrow</span></code>. See the <a class="reference internal" href="development.html#development"><span class="std std-ref">Development</span></a> page for more
details.</p>
</div>
<div class="section" id="reading-and-writing-single-files">
<h2>Reading and Writing Single Files<a class="headerlink" href="#reading-and-writing-single-files" title="Permalink to this headline">¶</a></h2>
<p>The functions <a class="reference internal" href="generated/pyarrow.parquet.read_table.html#pyarrow.parquet.read_table" title="pyarrow.parquet.read_table"><code class="xref py py-func docutils literal"><span class="pre">read_table()</span></code></a> and <a class="reference internal" href="generated/pyarrow.parquet.write_table.html#pyarrow.parquet.write_table" title="pyarrow.parquet.write_table"><code class="xref py py-func docutils literal"><span class="pre">write_table()</span></code></a>
read and write the <a class="reference internal" href="data.html#data-table"><span class="std std-ref">pyarrow.Table</span></a> objects, respectively.</p>
<p>Let’s look at a simple table:</p>
<div class="highlight-ipython"><div class="highlight"><pre><span></span><span class="gp">In [2]: </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="gp">In [3]: </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="gp">In [4]: </span><span class="kn">import</span> <span class="nn">pyarrow</span> <span class="kn">as</span> <span class="nn">pa</span>

<span class="gp">In [5]: </span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;one&#39;</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">],</span>
<span class="gp">   ...: </span>                   <span class="s1">&#39;two&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;foo&#39;</span><span class="p">,</span> <span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="s1">&#39;baz&#39;</span><span class="p">],</span>
<span class="gp">   ...: </span>                   <span class="s1">&#39;three&#39;</span><span class="p">:</span> <span class="p">[</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">,</span> <span class="bp">True</span><span class="p">]})</span>
<span class="gp">   ...: </span>

<span class="gp">In [6]: </span><span class="n">table</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">Table</span><span class="o">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
<p>We write this to Parquet format with <code class="docutils literal"><span class="pre">write_table</span></code>:</p>
<div class="highlight-ipython"><div class="highlight"><pre><span></span><span class="gp">In [7]: </span><span class="kn">import</span> <span class="nn">pyarrow.parquet</span> <span class="kn">as</span> <span class="nn">pq</span>

<span class="gp">In [8]: </span><span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="s1">&#39;example.parquet&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>This creates a single Parquet file. In practice, a Parquet dataset may consist
of many files in many directories. We can read a single file back with
<code class="docutils literal"><span class="pre">read_table</span></code>:</p>
<div class="highlight-ipython"><div class="highlight"><pre><span></span><span class="gp">In [9]: </span><span class="n">table2</span> <span class="o">=</span> <span class="n">pq</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s1">&#39;example.parquet&#39;</span><span class="p">)</span>

<span class="gp">In [10]: </span><span class="n">table2</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">()</span>
<span class="gh">Out[10]: </span><span class="go"></span>
<span class="go">   one  three  two</span>
<span class="go">0 -1.0   True  foo</span>
<span class="go">1  NaN  False  bar</span>
<span class="go">2  2.5   True  baz</span>
</pre></div>
</div>
<p>You can pass a subset of columns to read, which can be much faster than reading
the whole file (due to the columnar layout):</p>
<div class="highlight-ipython"><div class="highlight"><pre><span></span><span class="gp">In [11]: </span><span class="n">pq</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s1">&#39;example.parquet&#39;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;one&#39;</span><span class="p">,</span> <span class="s1">&#39;three&#39;</span><span class="p">])</span>
<span class="gh">Out[11]: </span><span class="go"></span>
<span class="go">pyarrow.Table</span>
<span class="go">one: double</span>
<span class="go">three: bool</span>
<span class="go">metadata</span>
<span class="gt">--------</span>
<span class="p">{</span><span class="sa">b</span><span class="s1">&#39;pandas&#39;</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;{&quot;index_columns&quot;: [&quot;__index_level_0__&quot;], &quot;column_indexes&quot;: [{&quot;na&#39;</span>
            <span class="sa">b</span><span class="s1">&#39;me&quot;: null, &quot;field_name&quot;: null, &quot;pandas_type&quot;: &quot;unicode&quot;, &quot;numpy_&#39;</span>
            <span class="sa">b</span><span class="s1">&#39;type&quot;: &quot;object&quot;, &quot;metadata&quot;: {&quot;encoding&quot;: &quot;UTF-8&quot;}}], &quot;columns&quot;:&#39;</span>
            <span class="sa">b</span><span class="s1">&#39; [{&quot;name&quot;: &quot;one&quot;, &quot;field_name&quot;: &quot;one&quot;, &quot;pandas_type&quot;: &quot;float64&quot;,&#39;</span>
            <span class="sa">b</span><span class="s1">&#39; &quot;numpy_type&quot;: &quot;float64&quot;, &quot;metadata&quot;: null}, {&quot;name&quot;: &quot;three&quot;, &quot;&#39;</span>
            <span class="sa">b</span><span class="s1">&#39;field_name&quot;: &quot;three&quot;, &quot;pandas_type&quot;: &quot;bool&quot;, &quot;numpy_type&quot;: &quot;bool&#39;</span>
            <span class="sa">b</span><span class="s1">&#39;&quot;, &quot;metadata&quot;: null}, {&quot;name&quot;: &quot;two&quot;, &quot;field_name&quot;: &quot;two&quot;, &quot;pand&#39;</span>
            <span class="sa">b</span><span class="s1">&#39;as_type&quot;: &quot;unicode&quot;, &quot;numpy_type&quot;: &quot;object&quot;, &quot;metadata&quot;: null}, &#39;</span>
            <span class="sa">b</span><span class="s1">&#39;{&quot;name&quot;: null, &quot;field_name&quot;: &quot;__index_level_0__&quot;, &quot;pandas_type&quot;:&#39;</span>
            <span class="sa">b</span><span class="s1">&#39; &quot;int64&quot;, &quot;numpy_type&quot;: &quot;int64&quot;, &quot;metadata&quot;: null}], &quot;pandas_ver&#39;</span>
            <span class="sa">b</span><span class="s1">&#39;sion&quot;: &quot;0.22.0&quot;}&#39;</span><span class="p">}</span>
</pre></div>
</div>
<p>We need not use a string to specify the origin of the file. It can be any of:</p>
<ul class="simple">
<li>A file path as a string</li>
<li>A <a class="reference internal" href="memory.html#io-native-file"><span class="std std-ref">NativeFile</span></a> from PyArrow</li>
<li>A Python file object</li>
</ul>
<p>In general, a Python file object will have the worst read performance, while a
string file path or an instance of <code class="xref py py-class docutils literal"><span class="pre">NativeFIle</span></code> (especially memory
maps) will perform the best.</p>
</div>
<div class="section" id="finer-grained-reading-and-writing">
<h2>Finer-grained Reading and Writing<a class="headerlink" href="#finer-grained-reading-and-writing" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal"><span class="pre">read_table</span></code> uses the <a class="reference internal" href="generated/pyarrow.parquet.ParquetFile.html#pyarrow.parquet.ParquetFile" title="pyarrow.parquet.ParquetFile"><code class="xref py py-class docutils literal"><span class="pre">ParquetFile</span></code></a> class, which has other features:</p>
<div class="highlight-ipython"><div class="highlight"><pre><span></span><span class="gp">In [12]: </span><span class="n">parquet_file</span> <span class="o">=</span> <span class="n">pq</span><span class="o">.</span><span class="n">ParquetFile</span><span class="p">(</span><span class="s1">&#39;example.parquet&#39;</span><span class="p">)</span>

<span class="gp">In [13]: </span><span class="n">parquet_file</span><span class="o">.</span><span class="n">metadata</span>
<span class="gh">Out[13]: </span><span class="go"></span>
<span class="go">&lt;pyarrow._parquet.FileMetaData object at 0x7fe5d97c24a8&gt;</span>
<span class="go">  created_by: parquet-cpp version 1.3.2-SNAPSHOT</span>
<span class="go">  num_columns: 4</span>
<span class="go">  num_rows: 3</span>
<span class="go">  num_row_groups: 1</span>
<span class="go">  format_version: 1.0</span>
<span class="go">  serialized_size: 1068</span>

<span class="gp">In [14]: </span><span class="n">parquet_file</span><span class="o">.</span><span class="n">schema</span>
<span class="go">                                                                                                                                                                                                                     Out[14]: </span>
<span class="go">&lt;pyarrow._parquet.ParquetSchema object at 0x7fe5d97cde88&gt;</span>
<span class="go">one: DOUBLE</span>
<span class="go">three: BOOLEAN</span>
<span class="go">two: BYTE_ARRAY UTF8</span>
<span class="go">__index_level_0__: INT64</span>
</pre></div>
</div>
<p>As you can learn more in the <a class="reference external" href="https://github.com/apache/parquet-format">Apache Parquet format</a>, a Parquet file consists of
multiple row groups. <code class="docutils literal"><span class="pre">read_table</span></code> will read all of the row groups and
concatenate them into a single table. You can read individual row groups with
<code class="docutils literal"><span class="pre">read_row_group</span></code>:</p>
<div class="highlight-ipython"><div class="highlight"><pre><span></span><span class="gp">In [15]: </span><span class="n">parquet_file</span><span class="o">.</span><span class="n">num_row_groups</span>
<span class="gh">Out[15]: </span><span class="go">1</span>

<span class="gp">In [16]: </span><span class="n">parquet_file</span><span class="o">.</span><span class="n">read_row_group</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="go">           Out[16]: </span>
<span class="go">pyarrow.Table</span>
<span class="go">one: double</span>
<span class="go">three: bool</span>
<span class="go">two: string</span>
<span class="go">__index_level_0__: int64</span>
<span class="go">metadata</span>
<span class="gt">--------</span>
<span class="p">{</span><span class="sa">b</span><span class="s1">&#39;pandas&#39;</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;{&quot;index_columns&quot;: [&quot;__index_level_0__&quot;], &quot;column_indexes&quot;: [{&quot;na&#39;</span>
            <span class="sa">b</span><span class="s1">&#39;me&quot;: null, &quot;field_name&quot;: null, &quot;pandas_type&quot;: &quot;unicode&quot;, &quot;numpy_&#39;</span>
            <span class="sa">b</span><span class="s1">&#39;type&quot;: &quot;object&quot;, &quot;metadata&quot;: {&quot;encoding&quot;: &quot;UTF-8&quot;}}], &quot;columns&quot;:&#39;</span>
            <span class="sa">b</span><span class="s1">&#39; [{&quot;name&quot;: &quot;one&quot;, &quot;field_name&quot;: &quot;one&quot;, &quot;pandas_type&quot;: &quot;float64&quot;,&#39;</span>
            <span class="sa">b</span><span class="s1">&#39; &quot;numpy_type&quot;: &quot;float64&quot;, &quot;metadata&quot;: null}, {&quot;name&quot;: &quot;three&quot;, &quot;&#39;</span>
            <span class="sa">b</span><span class="s1">&#39;field_name&quot;: &quot;three&quot;, &quot;pandas_type&quot;: &quot;bool&quot;, &quot;numpy_type&quot;: &quot;bool&#39;</span>
            <span class="sa">b</span><span class="s1">&#39;&quot;, &quot;metadata&quot;: null}, {&quot;name&quot;: &quot;two&quot;, &quot;field_name&quot;: &quot;two&quot;, &quot;pand&#39;</span>
            <span class="sa">b</span><span class="s1">&#39;as_type&quot;: &quot;unicode&quot;, &quot;numpy_type&quot;: &quot;object&quot;, &quot;metadata&quot;: null}, &#39;</span>
            <span class="sa">b</span><span class="s1">&#39;{&quot;name&quot;: null, &quot;field_name&quot;: &quot;__index_level_0__&quot;, &quot;pandas_type&quot;:&#39;</span>
            <span class="sa">b</span><span class="s1">&#39; &quot;int64&quot;, &quot;numpy_type&quot;: &quot;int64&quot;, &quot;metadata&quot;: null}], &quot;pandas_ver&#39;</span>
            <span class="sa">b</span><span class="s1">&#39;sion&quot;: &quot;0.22.0&quot;}&#39;</span><span class="p">}</span>
</pre></div>
</div>
<p>We can similarly write a Parquet file with multiple row groups by using
<code class="docutils literal"><span class="pre">ParquetWriter</span></code>:</p>
<div class="highlight-ipython"><div class="highlight"><pre><span></span><span class="gp">In [17]: </span><span class="n">writer</span> <span class="o">=</span> <span class="n">pq</span><span class="o">.</span><span class="n">ParquetWriter</span><span class="p">(</span><span class="s1">&#39;example2.parquet&#39;</span><span class="p">,</span> <span class="n">table</span><span class="o">.</span><span class="n">schema</span><span class="p">)</span>

<span class="gp">In [18]: </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
<span class="gp">   ....: </span>    <span class="n">writer</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">)</span>
<span class="gp">   ....: </span>

<span class="gp">In [19]: </span><span class="n">writer</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="gp">In [20]: </span><span class="n">pf2</span> <span class="o">=</span> <span class="n">pq</span><span class="o">.</span><span class="n">ParquetFile</span><span class="p">(</span><span class="s1">&#39;example2.parquet&#39;</span><span class="p">)</span>

<span class="gp">In [21]: </span><span class="n">pf2</span><span class="o">.</span><span class="n">num_row_groups</span>
<span class="gh">Out[21]: </span><span class="go">3</span>
</pre></div>
</div>
</div>
<div class="section" id="compression-encoding-and-file-compatibility">
<h2>Compression, Encoding, and File Compatibility<a class="headerlink" href="#compression-encoding-and-file-compatibility" title="Permalink to this headline">¶</a></h2>
<p>The most commonly used Parquet implementations use dictionary encoding when
writing files; if the dictionaries grow too large, then they “fall back” to
plain encoding. Whether dictionary encoding is used can be toggled using the
<code class="docutils literal"><span class="pre">use_dictionary</span></code> option:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">use_dictionary</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>
</div>
<p>The data pages within a column in a row group can be compressed after the
encoding passes (dictionary, RLE encoding). In PyArrow we use Snappy
compression by default, but Brotli, Gzip, and uncompressed are also supported:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;snappy&#39;</span><span class="p">)</span>
<span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;gzip&#39;</span><span class="p">)</span>
<span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;brotli&#39;</span><span class="p">)</span>
<span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Snappy generally results in better performance, while Gzip may yield smaller
files.</p>
<p>These settings can also be set on a per-column basis:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;foo&#39;</span><span class="p">:</span> <span class="s1">&#39;snappy&#39;</span><span class="p">,</span> <span class="s1">&#39;bar&#39;</span><span class="p">:</span> <span class="s1">&#39;gzip&#39;</span><span class="p">},</span>
               <span class="n">use_dictionary</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;foo&#39;</span><span class="p">,</span> <span class="s1">&#39;bar&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="section" id="reading-multiples-files-and-partitioned-datasets">
<h2>Reading Multiples Files and Partitioned Datasets<a class="headerlink" href="#reading-multiples-files-and-partitioned-datasets" title="Permalink to this headline">¶</a></h2>
<p>Multiple Parquet files constitute a Parquet <em>dataset</em>. These may present in a
number of ways:</p>
<ul class="simple">
<li>A list of Parquet absolute file paths</li>
<li>A directory name containing nested directories defining a partitioned dataset</li>
</ul>
<p>A dataset partitioned by year and month may look like on disk:</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>dataset_name/
  year=2007/
    month=01/
       0.parq
       1.parq
       ...
    month=02/
       0.parq
       1.parq
       ...
    month=03/
    ...
  year=2008/
    month=01/
    ...
  ...
</pre></div>
</div>
<p>The <a class="reference internal" href="generated/pyarrow.parquet.ParquetDataset.html#pyarrow.parquet.ParquetDataset" title="pyarrow.parquet.ParquetDataset"><code class="xref py py-class docutils literal"><span class="pre">ParquetDataset</span></code></a> class accepts either a directory name or a list
or file paths, and can discover and infer some common partition structures,
such as those produced by Hive:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">pq</span><span class="o">.</span><span class="n">ParquetDataset</span><span class="p">(</span><span class="s1">&#39;dataset_name/&#39;</span><span class="p">)</span>
<span class="n">table</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="using-with-spark">
<h2>Using with Spark<a class="headerlink" href="#using-with-spark" title="Permalink to this headline">¶</a></h2>
<p>Spark places some constraints on the types of Parquet files it will read. The
option <code class="docutils literal"><span class="pre">flavor='spark'</span></code> will set these options automatically and also
sanitize field characters unsupported by Spark SQL.</p>
</div>
<div class="section" id="multithreaded-reads">
<h2>Multithreaded Reads<a class="headerlink" href="#multithreaded-reads" title="Permalink to this headline">¶</a></h2>
<p>Each of the reading functions have an <code class="docutils literal"><span class="pre">nthreads</span></code> argument which will read
columns with the indicated level of parallelism. Depending on the speed of IO
and how expensive it is to decode the columns in a particular file
(particularly with GZIP compression), this can yield significantly higher data
throughput:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">pq</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="n">where</span><span class="p">,</span> <span class="n">nthreads</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">pq</span><span class="o">.</span><span class="n">ParquetDataset</span><span class="p">(</span><span class="n">where</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">nthreads</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2016-2017 Apache Software Foundation.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.6.6.<br/>
    </p>
  </div>
</footer>
  </body>
</html>