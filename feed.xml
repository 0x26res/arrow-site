<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.4.3">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2017-06-16T04:40:53-04:00</updated><id>/</id><entry><title type="html">Connecting Relational Databases to the Apache Arrow World with turbodbc</title><link href="/blog/2017/06/16/turbodbc-arrow/" rel="alternate" type="text/html" title="Connecting Relational Databases to the Apache Arrow World with turbodbc" /><published>2017-06-16T04:00:00-04:00</published><updated>2017-06-16T04:00:00-04:00</updated><id>/blog/2017/06/16/turbodbc-arrow</id><content type="html" xml:base="/blog/2017/06/16/turbodbc-arrow/">&lt;!--

--&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://github.com/mathmagique&quot;&gt;Michael König&lt;/a&gt; is the lead developer of the &lt;a href=&quot;https://github.com/blue-yonder/turbodbc&quot;&gt;turbodbc project&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://arrow.apache.org/&quot;&gt;Apache Arrow&lt;/a&gt; project set out to become the universal data layer for
column-oriented data processing systems without incurring serialization costs
or compromising on performance on a more general level. While relational
databases still lag behind in Apache Arrow adoption, the Python database module
&lt;a href=&quot;https://github.com/blue-yonder/turbodbc&quot;&gt;turbodbc&lt;/a&gt; brings Apache Arrow support to these databases using a much
older, more specialized data exchange layer: &lt;a href=&quot;https://en.wikipedia.org/wiki/Open_Database_Connectivity&quot;&gt;ODBC&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;ODBC is a database interface that offers developers the option to transfer data
either in row-wise or column-wise fashion. Previous Python ODBC modules typically
use the row-wise approach, and often trade repeated database roundtrips for simplified
buffer handling. This makes them less suited for data-intensive applications,
particularly when interfacing with modern columnar analytical databases.&lt;/p&gt;

&lt;p&gt;In contrast, turbodbc was designed to leverage columnar data processing from day
one. Naturally, this implies using the columnar portion of the ODBC API. Equally
important, however, is to find new ways of providing columnar data to Python users
that exceed the capabilities of the row-wise API mandated by Python’s &lt;a href=&quot;https://www.python.org/dev/peps/pep-0249/&quot;&gt;PEP 249&lt;/a&gt;.
Turbodbc has adopted Apache Arrow for this very task with the recently released
version 2.0.0:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;turbodbc&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connect&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dsn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;My columnar database&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;SELECT some_integers, some_strings FROM my_table&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fetchallarrow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pyarrow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Table&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;some_integers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;int64&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;some_strings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;With this new addition, the data flow for a result set of a typical SELECT query
is like this:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The database prepares the result set and exposes it to the ODBC driver using
either row-wise or column-wise storage.&lt;/li&gt;
  &lt;li&gt;Turbodbc has the ODBC driver write chunks of the result set into columnar buffers.&lt;/li&gt;
  &lt;li&gt;These buffers are exposed to turbodbc’s Apache Arrow frontend. This frontend
will create an Arrow table and fill in the buffered values.&lt;/li&gt;
  &lt;li&gt;The previous steps are repeated until the entire result set is retrieved.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/turbodbc_arrow.png&quot; alt=&quot;Data flow from relational databases to Python with turbodbc and the Apache Arrow frontend&quot; class=&quot;img-responsive&quot; width=&quot;75%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In practice, it is possible to achieve the following ideal situation: A 64-bit integer
column is stored as one contiguous block of memory in a columnar database. A huge chunk
of 64-bit integers is transferred over the network and the ODBC driver directly writes
it to a turbodbc buffer of 64-bit integers. The Arrow frontend accumulates these values
by copying the entire 64-bit buffer into a free portion of an Arrow table’s 64-bit
integer column.&lt;/p&gt;

&lt;p&gt;Moving data from the database to an Arrow table and, thus, providing it to the Python
user can be as simple as copying memory blocks around, megabytes equivalent to hundred
thousands of rows at a time. The absence of serialization and conversion logic renders
the process extremely efficient.&lt;/p&gt;

&lt;p&gt;Once the data is stored in an Arrow table, Python users can continue to do some
actual work. They can convert it into a &lt;a href=&quot;https://arrow.apache.org/docs/python/pandas.html&quot;&gt;Pandas DataFrame&lt;/a&gt; for data analysis
(using a quick &lt;code class=&quot;highlighter-rouge&quot;&gt;table.to_pandas()&lt;/code&gt;), pass it on to other data processing
systems such as &lt;a href=&quot;http://spark.apache.org/&quot;&gt;Apache Spark&lt;/a&gt; or &lt;a href=&quot;http://impala.apache.org/&quot;&gt;Apache Impala (incubating)&lt;/a&gt;, or store
it in the &lt;a href=&quot;http://parquet.apache.org/&quot;&gt;Apache Parquet&lt;/a&gt; file format. This way, non-Python systems are
efficiently connected with relational databases.&lt;/p&gt;

&lt;p&gt;In the future, turbodbc’s Arrow support will be extended to use more
sophisticated features such as &lt;a href=&quot;https://arrow.apache.org/docs/memory_layout.html#dictionary-encoding&quot;&gt;dictionary-encoded&lt;/a&gt; string fields. We also
plan to pick smaller than 64-bit &lt;a href=&quot;https://arrow.apache.org/docs/metadata.html#integers&quot;&gt;data types&lt;/a&gt; where possible. Last but not
least, Arrow support will be extended to cover the reverse direction of data
flow, so that Python users can quickly insert Arrow tables into relational
databases.&lt;/p&gt;

&lt;p&gt;If you would like to learn more about turbodbc, check out the &lt;a href=&quot;https://github.com/blue-yonder/turbodbc&quot;&gt;GitHub project&lt;/a&gt; and the
&lt;a href=&quot;http://turbodbc.readthedocs.io/&quot;&gt;project documentation&lt;/a&gt;. If you want to learn more about how turbodbc implements the
nitty-gritty details, check out parts &lt;a href=&quot;https://tech.blue-yonder.com/making-of-turbodbc-part-1-wrestling-with-the-side-effects-of-a-c-api/&quot;&gt;one&lt;/a&gt; and &lt;a href=&quot;https://tech.blue-yonder.com/making-of-turbodbc-part-2-c-to-python/&quot;&gt;two&lt;/a&gt; of the
&lt;a href=&quot;https://tech.blue-yonder.com/making-of-turbodbc-part-1-wrestling-with-the-side-effects-of-a-c-api/&quot;&gt;“Making of turbodbc”&lt;/a&gt; series at &lt;a href=&quot;https://tech.blue-yonder.com/&quot;&gt;Blue Yonder’s technology blog&lt;/a&gt;.&lt;/p&gt;</content><author><name>MathMagique</name></author></entry><entry><title type="html">Apache Arrow 0.4.1 Release</title><link href="/blog/2017/06/14/0.4.1-release/" rel="alternate" type="text/html" title="Apache Arrow 0.4.1 Release" /><published>2017-06-14T10:00:00-04:00</published><updated>2017-06-14T10:00:00-04:00</updated><id>/blog/2017/06/14/0.4.1-release</id><content type="html" xml:base="/blog/2017/06/14/0.4.1-release/">&lt;!--

--&gt;

&lt;p&gt;The Apache Arrow team is pleased to announce the 0.4.1 release of the
project. This is a bug fix release that addresses a regression with Decimal
types in the Java implementation introduced in 0.4.0 (see
&lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-1091&quot;&gt;ARROW-1091&lt;/a&gt;). There were a total of &lt;a href=&quot;https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20%3D%200.4.1&quot;&gt;31 resolved JIRAs&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;See the &lt;a href=&quot;http://arrow.apache.org/install&quot;&gt;Install Page&lt;/a&gt; to learn how to get the libraries for your platform.&lt;/p&gt;

&lt;h3 id=&quot;python-wheel-installers-for-windows&quot;&gt;Python Wheel Installers for Windows&lt;/h3&gt;

&lt;p&gt;Max Risuhin contributed fixes to enable binary wheel installers to be generated
for Python 3.5 and 3.6. Thus, 0.4.1 is the first Arrow release for which
PyArrow including bundled &lt;a href=&quot;http://parquet.apache.org&quot;&gt;Apache Parquet&lt;/a&gt; support that can be installed
with either conda or pip across the 3 major platforms: Linux, macOS, and
Windows. Use one of:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install pyarrow
conda install pyarrow -c conda-forge
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;turbodbc-200-with-apache-arrow-support&quot;&gt;Turbodbc 2.0.0 with Apache Arrow Support&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://turbodbc.readthedocs.io/&quot;&gt;Turbodbc&lt;/a&gt;, a fast C++ ODBC interface with Python bindings, released
version 2.0.0 including reading SQL result sets as Arrow record batches. The
team used the PyArrow C++ API introduced in version 0.4.0 to construct
&lt;code class=&quot;highlighter-rouge&quot;&gt;pyarrow.Table&lt;/code&gt; objects inside the &lt;code class=&quot;highlighter-rouge&quot;&gt;turbodbc&lt;/code&gt; library. Learn more in their
&lt;a href=&quot;http://turbodbc.readthedocs.io/en/latest/pages/advanced_usage.html#apache-arrow-support&quot;&gt;documentation&lt;/a&gt; and install with one of:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install turbodbc
conda install turbodbc -c conda-forge
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;</content><author><name>wesm</name></author></entry><entry><title type="html">Apache Arrow 0.4.0 Release</title><link href="/blog/2017/05/23/0.4.0-release/" rel="alternate" type="text/html" title="Apache Arrow 0.4.0 Release" /><published>2017-05-23T00:00:00-04:00</published><updated>2017-05-23T00:00:00-04:00</updated><id>/blog/2017/05/23/0.4.0-release</id><content type="html" xml:base="/blog/2017/05/23/0.4.0-release/">&lt;!--

--&gt;

&lt;p&gt;The Apache Arrow team is pleased to announce the 0.4.0 release of the
project. While only 17 days since the release, it includes &lt;a href=&quot;https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20%3D%200.4.0&quot;&gt;&lt;strong&gt;77 resolved
JIRAs&lt;/strong&gt;&lt;/a&gt; with some important new features and bug fixes.&lt;/p&gt;

&lt;p&gt;See the &lt;a href=&quot;http://arrow.apache.org/install&quot;&gt;Install Page&lt;/a&gt; to learn how to get the libraries for your platform.&lt;/p&gt;

&lt;h3 id=&quot;expanded-javascript-implementation&quot;&gt;Expanded JavaScript Implementation&lt;/h3&gt;

&lt;p&gt;The TypeScript Arrow implementation has undergone some work since 0.3.0 and can
now read a substantial portion of the Arrow streaming binary format. As this
implementation develops, we will eventually want to include JS in the
integration test suite along with Java and C++ to ensure wire
cross-compatibility.&lt;/p&gt;

&lt;h3 id=&quot;python-support-for-apache-parquet-on-windows&quot;&gt;Python Support for Apache Parquet on Windows&lt;/h3&gt;

&lt;p&gt;With the &lt;a href=&quot;https://github.com/apache/parquet-cpp/releases/tag/apache-parquet-cpp-1.1.0&quot;&gt;1.1.0 C++ release&lt;/a&gt; of &lt;a href=&quot;http://parquet.apache.org&quot;&gt;Apache Parquet&lt;/a&gt;, we have enabled the
&lt;code class=&quot;highlighter-rouge&quot;&gt;pyarrow.parquet&lt;/code&gt; extension on Windows for Python 3.5 and 3.6. This should
appear in conda-forge packages and PyPI in the near future. Developers can
follow the &lt;a href=&quot;http://arrow.apache.org/docs/python/development.html&quot;&gt;source build instructions&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;generalizing-arrow-streams&quot;&gt;Generalizing Arrow Streams&lt;/h3&gt;

&lt;p&gt;In the 0.2.0 release, we defined the first version of the Arrow streaming
binary format for low-cost messaging with columnar data. These streams presume
that the message components are written as a continuous byte stream over a
socket or file.&lt;/p&gt;

&lt;p&gt;We would like to be able to support other other transport protocols, like
&lt;a href=&quot;http://grpc.io/&quot;&gt;gRPC&lt;/a&gt;, for the message components of Arrow streams. To that end, in C++ we
defined an abstract stream reader interface, for which the current contiguous
streaming format is one implementation:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-cpp&quot; data-lang=&quot;cpp&quot;&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;RecordBatchReader&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
 &lt;span class=&quot;k&quot;&gt;public&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;virtual&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shared_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Schema&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;virtual&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Status&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GetNextRecordBatch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shared_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RecordBatch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;It would also be good to define abstract stream reader and writer interfaces in
the Java implementation.&lt;/p&gt;

&lt;p&gt;In an upcoming blog post, we will explain in more depth how Arrow streams work,
but you can learn more about them by reading the &lt;a href=&quot;http://arrow.apache.org/docs/ipc.html&quot;&gt;IPC specification&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;c-and-cython-api-for-python-extensions&quot;&gt;C++ and Cython API for Python Extensions&lt;/h3&gt;

&lt;p&gt;As other Python libraries with C or C++ extensions use Apache Arrow, they will
need to be able to return Python objects wrapping the underlying C++
objects. In this release, we have implemented a prototype C++ API which enables
Python wrapper objects to be constructed from C++ extension code:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-cpp&quot; data-lang=&quot;cpp&quot;&gt;&lt;span class=&quot;cp&quot;&gt;#include &quot;arrow/python/pyarrow.h&quot;
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arrow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;import_pyarrow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// Error
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shared_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arrow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RecordBatch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cpp_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GetData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(...);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;PyObject&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;py_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arrow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wrap_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpp_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This API is intended to be usable from Cython code as well:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-cython&quot; data-lang=&quot;cython&quot;&gt;cimport pyarrow
pyarrow.import_pyarrow()&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;python-wheel-installers-on-macos&quot;&gt;Python Wheel Installers on macOS&lt;/h3&gt;

&lt;p&gt;With this release, &lt;code class=&quot;highlighter-rouge&quot;&gt;pip install pyarrow&lt;/code&gt; works on macOS (OS X) as well as
Linux. We are working on providing binary wheel installers for Windows as well.&lt;/p&gt;</content><author><name>wesm</name></author></entry><entry><title type="html">Apache Arrow 0.3.0 Release</title><link href="/blog/2017/05/08/0.3-release/" rel="alternate" type="text/html" title="Apache Arrow 0.3.0 Release" /><published>2017-05-08T00:00:00-04:00</published><updated>2017-05-08T00:00:00-04:00</updated><id>/blog/2017/05/08/0.3-release</id><content type="html" xml:base="/blog/2017/05/08/0.3-release/">&lt;!--

--&gt;

&lt;p&gt;Translations: &lt;a href=&quot;/blog/2017/05/08/0.3-release-japanese/&quot;&gt;日本語&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The Apache Arrow team is pleased to announce the 0.3.0 release of the
project. It is the product of an intense 10 weeks of development since the
0.2.0 release from this past February. It includes &lt;a href=&quot;https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20%3D%200.3.0&quot;&gt;&lt;strong&gt;306 resolved JIRAs&lt;/strong&gt;&lt;/a&gt;
from &lt;a href=&quot;https://github.com/apache/arrow/graphs/contributors&quot;&gt;&lt;strong&gt;23 contributors&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While we have added many new features to the different Arrow implementations,
one of the major development focuses in 2017 has been hardening the in-memory
format, type metadata, and messaging protocol to provide a &lt;strong&gt;stable,
production-ready foundation&lt;/strong&gt; for big data applications. We are excited to be
collaborating with the &lt;a href=&quot;http://spark.apache.org&quot;&gt;Apache Spark&lt;/a&gt; and &lt;a href=&quot;http://www.geomesa.org/&quot;&gt;GeoMesa&lt;/a&gt; communities on
utilizing Arrow for high performance IO and in-memory data processing.&lt;/p&gt;

&lt;p&gt;See the &lt;a href=&quot;http://arrow.apache.org/install&quot;&gt;Install Page&lt;/a&gt; to learn how to get the libraries for your platform.&lt;/p&gt;

&lt;p&gt;We will be publishing more information about the Apache Arrow roadmap as we
forge ahead with using Arrow to accelerate big data systems.&lt;/p&gt;

&lt;p&gt;We are looking for more contributors from within our existing communities and
from other communities (such as Go, R, or Julia) to get involved in Arrow
development.&lt;/p&gt;

&lt;h3 id=&quot;file-and-streaming-format-hardening&quot;&gt;File and Streaming Format Hardening&lt;/h3&gt;

&lt;p&gt;The 0.2.0 release brought with it the first iterations of the &lt;strong&gt;random access&lt;/strong&gt;
and &lt;strong&gt;streaming&lt;/strong&gt; Arrow wire formats. See the &lt;a href=&quot;http://arrow.apache.org/docs/ipc.html&quot;&gt;IPC specification&lt;/a&gt; for
implementation details and &lt;a href=&quot;http://wesmckinney.com/blog/arrow-streaming-columnar/&quot;&gt;example blog post&lt;/a&gt; with some use cases. These
provide low-overhead, zero-copy access to Arrow record batch payloads.&lt;/p&gt;

&lt;p&gt;In 0.3.0 we have solidified a number of small details with the binary format
and improved our integration and unit testing particularly in the Java, C++,
and Python libraries. Using the &lt;a href=&quot;http://github.com/google/flatbuffers&quot;&gt;Google Flatbuffers&lt;/a&gt; project has helped with
adding new features to our metadata without breaking forward compatibility.&lt;/p&gt;

&lt;p&gt;We are not yet ready to make a firm commitment to strong forward compatibility
(in case we find something needs to change) in the binary format, but we will
make efforts between major releases to not make unnecessary
breakages. Contributions to the website and component user and API
documentation would also be most welcome.&lt;/p&gt;

&lt;h3 id=&quot;dictionary-encoding-support&quot;&gt;Dictionary Encoding Support&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/elahrvivaz&quot;&gt;Emilio Lahr-Vivaz&lt;/a&gt; from the &lt;a href=&quot;http://www.geomesa.org/&quot;&gt;GeoMesa&lt;/a&gt; project contributed Java support
for dictionary-encoded Arrow vectors. We followed up with C++ and Python
support (and &lt;code class=&quot;highlighter-rouge&quot;&gt;pandas.Categorical&lt;/code&gt; integration). We have not yet implemented
full integration tests for dictionaries (for sending this data between C++ and
Java), but hope to achieve this in the 0.4.0 Arrow release.&lt;/p&gt;

&lt;p&gt;This common data representation technique for categorical data allows multiple
record batches to share a common “dictionary”, with the values in the batches
being represented as integers referencing the dictionary. This data is called
“categorical” or “factor” in statistical languages, while in file formats like
Apache Parquet it is strictly used for data compression.&lt;/p&gt;

&lt;h3 id=&quot;expanded-date-time-and-fixed-size-types&quot;&gt;Expanded Date, Time, and Fixed Size Types&lt;/h3&gt;

&lt;p&gt;A notable omission from the 0.2.0 release was complete and integration-tested
support for the gamut of date and time types that occur in the wild. These are
needed for &lt;a href=&quot;http://parquet.apache.org&quot;&gt;Apache Parquet&lt;/a&gt; and Apache Spark integration.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Date&lt;/strong&gt;: 32-bit (days unit) and 64-bit (milliseconds unit)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Time&lt;/strong&gt;: 64-bit integer with unit (second, millisecond, microsecond, nanosecond)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Timestamp&lt;/strong&gt;: 64-bit integer with unit, with or without timezone&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fixed Size Binary&lt;/strong&gt;: Primitive values occupying certain number of bytes&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fixed Size List&lt;/strong&gt;: List values with constant size (no separate offsets vector)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We have additionally added experimental support for exact decimals in C++ using
&lt;a href=&quot;https://github.com/boostorg/multiprecision&quot;&gt;Boost.Multiprecision&lt;/a&gt;, though we have not yet hardened the Decimal memory
format between the Java and C++ implementations.&lt;/p&gt;

&lt;h3 id=&quot;c-and-python-support-on-windows&quot;&gt;C++ and Python Support on Windows&lt;/h3&gt;

&lt;p&gt;We have made many general improvements to development and packaging for general
C++ and Python development. 0.3.0 is the first release to bring full C++ and
Python support for Windows on Visual Studio (MSVC) 2015 and 2017. In addition
to adding Appveyor continuous integration for MSVC, we have also written guides
for building from source on Windows: &lt;a href=&quot;https://github.com/apache/arrow/blob/master/cpp/apidoc/Windows.md&quot;&gt;C++&lt;/a&gt; and &lt;a href=&quot;https://github.com/apache/arrow/blob/master/python/doc/source/development.rst&quot;&gt;Python&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For the first time, you can install the Arrow Python library on Windows from
&lt;a href=&quot;https://conda-forge.github.io&quot;&gt;conda-forge&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda install pyarrow -c conda-forge
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;c-glib-bindings-with-support-for-ruby-lua-and-more&quot;&gt;C (GLib) Bindings, with support for Ruby, Lua, and more&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://github.com/kou&quot;&gt;Kouhei Sutou&lt;/a&gt; is a new Apache Arrow contributor and has contributed GLib C
bindings (to the C++ libraries) for Linux. Using a C middleware framework
called &lt;a href=&quot;https://wiki.gnome.org/Projects/GObjectIntrospection&quot;&gt;GObject Introspection&lt;/a&gt;, it is possible to use these bindings
seamlessly in Ruby, Lua, Go, and &lt;a href=&quot;https://wiki.gnome.org/Projects/GObjectIntrospection/Users&quot;&gt;other programming languages&lt;/a&gt;. We will
probably need to publish some follow up blogs explaining how these bindings
work and how to use them.&lt;/p&gt;

&lt;h3 id=&quot;apache-spark-integration-for-pyspark&quot;&gt;Apache Spark Integration for PySpark&lt;/h3&gt;

&lt;p&gt;We have been collaborating with the Apache Spark community on &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-13534&quot;&gt;SPARK-13534&lt;/a&gt;
to add support for using Arrow to accelerate &lt;code class=&quot;highlighter-rouge&quot;&gt;DataFrame.toPandas&lt;/code&gt; in
PySpark. We have observed over &lt;a href=&quot;https://github.com/apache/spark/pull/15821#issuecomment-282175163&quot;&gt;&lt;strong&gt;40x speedup&lt;/strong&gt;&lt;/a&gt; from the more efficient
data serialization.&lt;/p&gt;

&lt;p&gt;Using Arrow in PySpark opens the door to many other performance optimizations,
particularly around UDF evaluation (e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;map&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;filter&lt;/code&gt; operations with
Python lambda functions).&lt;/p&gt;

&lt;h3 id=&quot;new-python-feature-memory-views-feather-apache-parquet-support&quot;&gt;New Python Feature: Memory Views, Feather, Apache Parquet support&lt;/h3&gt;

&lt;p&gt;Arrow’s Python library &lt;code class=&quot;highlighter-rouge&quot;&gt;pyarrow&lt;/code&gt; is a Cython binding for the &lt;code class=&quot;highlighter-rouge&quot;&gt;libarrow&lt;/code&gt; and
&lt;code class=&quot;highlighter-rouge&quot;&gt;libarrow_python&lt;/code&gt; C++ libraries, which handle inteoperability with NumPy,
&lt;a href=&quot;http://pandas.pydata.org&quot;&gt;pandas&lt;/a&gt;, and the Python standard library.&lt;/p&gt;

&lt;p&gt;At the heart of Arrow’s C++ libraries is the &lt;code class=&quot;highlighter-rouge&quot;&gt;arrow::Buffer&lt;/code&gt; object, which is a
managed memory view supporting zero-copy reads and slices. &lt;a href=&quot;https://github.com/JeffKnupp&quot;&gt;Jeff Knupp&lt;/a&gt;
contributed integration between Arrow buffers and the Python buffer protocol
and memoryviews, so now code like this is possible:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyarrow&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pa&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;buf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pa&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frombuffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'foobarbaz'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;buf&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pyarrow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_io&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Buffer&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt; &lt;span class=&quot;mh&quot;&gt;0x7f6c0a84b538&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;memoryview&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;buf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;memory&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt; &lt;span class=&quot;mh&quot;&gt;0x7f6c0a8c5e88&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;buf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_pybytes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'foobarbaz'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We have significantly expanded &lt;a href=&quot;http://parquet.apache.org&quot;&gt;&lt;strong&gt;Apache Parquet&lt;/strong&gt;&lt;/a&gt; support via the C++
Parquet implementation &lt;a href=&quot;https://github.com/apache/parquet-cpp&quot;&gt;parquet-cpp&lt;/a&gt;. This includes support for partitioned
datasets on disk or in HDFS. We added initial Arrow-powered Parquet support &lt;a href=&quot;https://github.com/dask/dask/commit/68f9e417924a985c1f2e2a587126833c70a2e9f4&quot;&gt;in
the Dask project&lt;/a&gt;, and look forward to more collaborations with the Dask
developers on distributed processing of pandas data.&lt;/p&gt;

&lt;p&gt;With Arrow’s support for pandas maturing, we were able to merge in the
&lt;a href=&quot;https://github.com/wesm/feather&quot;&gt;&lt;strong&gt;Feather format&lt;/strong&gt;&lt;/a&gt; implementation, which is essentially a special case of
the Arrow random access format. We’ll be continuing Feather development within
the Arrow codebase. For example, Feather can now read and write with Python
file objects using Arrow’s Python binding layer.&lt;/p&gt;

&lt;p&gt;We also implemented more robust support for pandas-specific data types, like
&lt;code class=&quot;highlighter-rouge&quot;&gt;DatetimeTZ&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Categorical&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;support-for-tensors-and-beyond-in-c-library&quot;&gt;Support for Tensors and beyond in C++ Library&lt;/h3&gt;

&lt;p&gt;There has been increased interest in using Apache Arrow as a tool for zero-copy
shared memory management for machine learning applications. A flagship example
is the &lt;a href=&quot;https://github.com/ray-project/ray&quot;&gt;Ray project&lt;/a&gt; from the UC Berkeley &lt;a href=&quot;https://rise.cs.berkeley.edu/&quot;&gt;RISELab&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Machine learning deals in additional kinds of data structures beyond what the
Arrow columnar format supports, like multidimensional arrays aka “tensors”. As
such, we implemented the &lt;a href=&quot;http://arrow.apache.org/docs/cpp/classarrow_1_1_tensor.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;arrow::Tensor&lt;/code&gt;&lt;/a&gt; C++ type which can utilize the
rest of Arrow’s zero-copy shared memory machinery (using &lt;code class=&quot;highlighter-rouge&quot;&gt;arrow::Buffer&lt;/code&gt; for
managing memory lifetime). In C++ in particular, we will want to provide for
additional data structures utilizing common IO and memory management tools.&lt;/p&gt;

&lt;h3 id=&quot;start-of-javascript-typescript-implementation&quot;&gt;Start of JavaScript (TypeScript) Implementation&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/TheNeuralBit&quot;&gt;Brian Hulette&lt;/a&gt; started developing an Arrow implementation in
&lt;a href=&quot;https://github.com/apache/arrow/tree/master/js&quot;&gt;TypeScript&lt;/a&gt; for use in NodeJS and browser-side applications. We are
benefitting from Flatbuffers’ first class support for JavaScript.&lt;/p&gt;

&lt;h3 id=&quot;improved-website-and-developer-documentation&quot;&gt;Improved Website and Developer Documentation&lt;/h3&gt;

&lt;p&gt;Since 0.2.0 we have implemented a new website stack for publishing
documentation and blogs based on &lt;a href=&quot;https://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt;. Kouhei Sutou developed a &lt;a href=&quot;https://github.com/red-data-tools/jekyll-jupyter-notebook&quot;&gt;Jekyll
Jupyter Notebook plugin&lt;/a&gt; so that we can use Jupyter to author content for
the Arrow website.&lt;/p&gt;

&lt;p&gt;On the website, we have now published API documentation for the C, C++, Java,
and Python subcomponents. Within these you will find easier-to-follow developer
instructions for getting started.&lt;/p&gt;

&lt;h3 id=&quot;contributors&quot;&gt;Contributors&lt;/h3&gt;

&lt;p&gt;Thanks to all who contributed patches to this release.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ git shortlog -sn apache-arrow-0.2.0..apache-arrow-0.3.0
    119 Wes McKinney
     55 Kouhei Sutou
     18 Uwe L. Korn
     17 Julien Le Dem
      9 Phillip Cloud
      6 Bryan Cutler
      5 Philipp Moritz
      5 Emilio Lahr-Vivaz
      4 Max Risuhin
      4 Johan Mabille
      4 Jeff Knupp
      3 Steven Phillips
      3 Miki Tebeka
      2 Leif Walsh
      2 Jeff Reback
      2 Brian Hulette
      1 Tsuyoshi Ozawa
      1 rvernica
      1 Nong Li
      1 Julien Lafaye
      1 Itai Incze
      1 Holden Karau
      1 Deepak Majeti
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;</content><author><name>wesm</name></author></entry><entry><title type="html">Apache Arrow 0.3.0リリース</title><link href="/blog/2017/05/08/0.3-release-japanese/" rel="alternate" type="text/html" title="Apache Arrow 0.3.0リリース" /><published>2017-05-08T00:00:00-04:00</published><updated>2017-05-08T00:00:00-04:00</updated><id>/blog/2017/05/08/0.3-release-japanese</id><content type="html" xml:base="/blog/2017/05/08/0.3-release-japanese/">&lt;!--

--&gt;

&lt;p&gt;&lt;a href=&quot;/blog/2017/05/08/0.3-release/&quot;&gt;原文（English）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Apache Arrowチームは0.3.0のリリースをアナウンスできてうれしいです。2月にリリースした0.2.0から10週間の活発な開発の結果が今回のリリースです。&lt;a href=&quot;https://github.com/apache/arrow/graphs/contributors&quot;&gt;&lt;strong&gt;23人のコントリビューター&lt;/strong&gt;&lt;/a&gt;が&lt;a href=&quot;https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20in%20(Resolved%2C%20Closed)%20AND%20fixVersion%20%3D%200.3.0&quot;&gt;&lt;strong&gt;306個のJIRAのissueを解決&lt;/strong&gt;&lt;/a&gt;しました。&lt;/p&gt;

&lt;p&gt;複数のArrowの実装にたくさんの新しい機能を追加しています。2017年、特に注力して開発するのは、インメモリー用のフォーマット、型のメタデータ、メッセージング用のプロトコルです。これは、ビッグデータアプリケーションに&lt;strong&gt;安定していてプロダクションで使える基盤&lt;/strong&gt;を提供するためです。高性能IOとインメモリーデータ処理にArrowを活用するために、&lt;a href=&quot;http://spark.apache.org&quot;&gt;Apache Spark&lt;/a&gt;・&lt;a href=&quot;http://www.geomesa.org/&quot;&gt;GeoMesa&lt;/a&gt;コミュニティーと協力していてとてもエキサイティングです。&lt;/p&gt;

&lt;p&gt;それぞれのプラットフォームでArrowを使う方法は&lt;a href=&quot;http://arrow.apache.org/install&quot;&gt;インストールページ&lt;/a&gt;を見てください。&lt;/p&gt;

&lt;p&gt;Arrowでビッグデータシステムを高速化するケースを増やすために、近いうちにApache Arrowのロードマップを公開する予定です。&lt;/p&gt;

&lt;p&gt;Arrowの開発に参加するコントリビューターを募集しています。すでにArrowの開発に参加しているコミュニティーからのコントリビューターもそうですし、まだ参加していないGo、R、Juliaといったコミュニティーからのコントリビューターも募集しています。&lt;/p&gt;

&lt;h3 id=&quot;ファイルフォーマットとストリーミングフォーマットの強化&quot;&gt;ファイルフォーマットとストリーミングフォーマットの強化&lt;/h3&gt;

&lt;p&gt;0.2.0では&lt;strong&gt;ランダムアクセス&lt;/strong&gt;用と&lt;strong&gt;ストリーミング&lt;/strong&gt;用のArrowのワイヤーフォーマットを導入しました。実装の詳細は&lt;a href=&quot;http://arrow.apache.org/docs/ipc.html&quot;&gt;IPC仕様&lt;/a&gt;を見てください。ユースケースは&lt;a href=&quot;http://wesmckinney.com/blog/arrow-streaming-columnar/&quot;&gt;使用例を紹介したブログ&lt;/a&gt;を見てください。これらのフォーマットを使うと低オーバーヘッド・コピーなしでArrowのレコードバッチのペイロードにアクセスできます。&lt;/p&gt;

&lt;p&gt;0.3.0ではこのバイナリーフォマットの細かい詳細をたくさん固めました。Java、C++、Python間の連携のテストおよびそれぞれ言語での単体テストの整備も進めました。&lt;a href=&quot;http://github.com/google/flatbuffers&quot;&gt;Google Flatbuffers&lt;/a&gt;は、前方互換性を壊さずにメタデータに新しい機能を追加するのに非常に助かりました。&lt;/p&gt;

&lt;p&gt;まだバイナリーフォーマットの前方互換性を必ず壊さないと約束できる状態ではありませんが（もしかしたら変更する必要があるなにかが見つかるかもしれない）、メジャーリリース間では不必要に互換性を壊さないように努力するつもりです。Apache ArrowのWebサイト、各コンポーネントのユーザー向けのドキュメントおよびAPIドキュメントへのコントリビューションを非常に歓迎します。&lt;/p&gt;

&lt;h3 id=&quot;辞書エンコーディングのサポート&quot;&gt;辞書エンコーディングのサポート&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://www.geomesa.org/&quot;&gt;GeoMesa&lt;/a&gt;プロジェクトの&lt;a href=&quot;https://github.com/elahrvivaz&quot;&gt;Emilio Lahr-Vivaz&lt;/a&gt;はJavaのArrow実装に辞書エンコード対応ベクターをコントリビュートしました。これを受けて、C++とPythonでもサポートしました。（&lt;code class=&quot;highlighter-rouge&quot;&gt;pandas.Categorical&lt;/code&gt;とも連携できます。）辞書エンコーディング用のインテグレーションテスト（C++とJava間でこのデータを送受信するテスト）はまだ完成していませんが、0.4.0までには完成させたいです。&lt;/p&gt;

&lt;p&gt;これはカテゴリーデータ用の一般的なデータ表現テクニックです。これを使うと、複数のレコードバッチで共通の「辞書」を共有し、各レコードバッチの値はこの辞書を参照する整数になります。このデータは統計的言語（statistical language）の分野では「カテゴリー（categorical）」や「因子（factor）」と呼ばれています。Apache Parquetのようなファイルフォーマットの分野ではデータ圧縮のためだけに使われています。&lt;/p&gt;

&lt;h3 id=&quot;日付時刻固定長型の拡張&quot;&gt;日付、時刻、固定長型の拡張&lt;/h3&gt;

&lt;p&gt;0.2.0では現実に使われている日付・時刻型をインテグレーションテスト付きで完全にサポートすることを諦めました。これらは&lt;a href=&quot;http://parquet.apache.org&quot;&gt;Apache Parquet&lt;/a&gt;とApache Sparkとの連携に必要な機能です。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;日付&lt;/strong&gt;: 32-bit（日単位）と64-bit（ミリ秒単位）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;時刻&lt;/strong&gt;: 単位付き64-bit整数（単位：秒、ミリ秒、マイクロ秒、ナノ秒）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;タイムスタンプ（UNIXエポックからの経過時間）&lt;/strong&gt;: 単位付き64-bit整数のタイムゾーン付きとタイムゾーンなし&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;固定長バイナリー&lt;/strong&gt;: 決まったバイト数のプリミティブな値&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;固定長リスト&lt;/strong&gt;: 各要素が同じサイズのリスト（要素のベクターとは別にオフセットのベクターを持つ必要がない）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;C++のArrow実装では、&lt;a href=&quot;https://github.com/boostorg/multiprecision&quot;&gt;Boost.Multiprecision&lt;/a&gt;を使ったexactな小数のサポートを実験的に追加しました。ただし、Java実装とC++実装間での小数のメモリーフォーマットはまだ固まっていません。&lt;/p&gt;

&lt;h3 id=&quot;cとpythonのwindowsサポート&quot;&gt;C++とPythonのWindowsサポート&lt;/h3&gt;

&lt;p&gt;一般的なC++とPythonでの開発用に、パッケージ周りの改良も多数入っています。0.3.0はVisual Studio（MSVC）2015と2017を使ってWindowsを完全にサポートした最初のバージョンです。AppveyorでMSVC用のCIを実行しています。Windows上でソースからビルドするためのガイドも書きました。&lt;a href=&quot;https://github.com/apache/arrow/blob/master/cpp/apidoc/Windows.md&quot;&gt;C++&lt;/a&gt;用と&lt;a href=&quot;https://github.com/apache/arrow/blob/master/python/doc/source/development.rst&quot;&gt;Python&lt;/a&gt;用。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://conda-forge.github.io&quot;&gt;conda-forge&lt;/a&gt;からWindows用のArrowのPythonライブラリーをインストールできます。&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda install pyarrow -c conda-forge
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;cglibバインディングとrubylua他のサポート&quot;&gt;C（GLib）バインディングとRuby・Lua・他のサポート&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://github.com/kou&quot;&gt;Kouhei Sutou&lt;/a&gt;は新しいApache Arrowのコントリビューターです。Linux用の（ArrowのC++実装の）GLibを使ったCバインディングをコントリビュートしました。&lt;a href=&quot;https://wiki.gnome.org/Projects/GObjectIntrospection&quot;&gt;GObject Introspection&lt;/a&gt;というCのミドルウェアを使うことでRuby、Lua、Goや&lt;a href=&quot;https://wiki.gnome.org/Projects/GObjectIntrospection/Users&quot;&gt;他にも様々なプログラミング言語&lt;/a&gt;でシームレスにバインディングを使うことができます。これらのバインディングがどのように動いているか、これらのバインディングをどのように使うかを説明するブログ記事が別途必要な気がします。&lt;/p&gt;

&lt;h3 id=&quot;pysparkを使ったapache-sparkとの連携&quot;&gt;PySparkを使ったApache Sparkとの連携&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-13534&quot;&gt;SPARK-13534&lt;/a&gt;でApache Sparkコミュニティーと協力しています。PySparkでの&lt;code class=&quot;highlighter-rouge&quot;&gt;DataFrame.toPandas&lt;/code&gt;をArrowを使って高速化しようとしています。効率的なデータのシリアライズにより&lt;a href=&quot;https://github.com/apache/spark/pull/15821#issuecomment-282175163&quot;&gt;&lt;strong&gt;40倍以上高速化&lt;/strong&gt;&lt;/a&gt;できるケースがあります。&lt;/p&gt;

&lt;p&gt;PySparkでArrowを使うことでこれまでできなかったパフォーマンス最適化の道が開けました。特に、UDFの評価まわりでいろいろやれることがあるでしょう。（たとえば、Pythonのラムダ関数を使って&lt;code class=&quot;highlighter-rouge&quot;&gt;map&lt;/code&gt;・&lt;code class=&quot;highlighter-rouge&quot;&gt;filter&lt;/code&gt;を実行するケース。）&lt;/p&gt;

&lt;h3 id=&quot;python実装での新しい機能メモリービューfeatherapache-parquetのサポート&quot;&gt;Python実装での新しい機能：メモリービュー、Feather、Apache Parquetのサポート&lt;/h3&gt;

&lt;p&gt;ArrowのPythonライブラリーである&lt;code class=&quot;highlighter-rouge&quot;&gt;pyarrow&lt;/code&gt;は&lt;code class=&quot;highlighter-rouge&quot;&gt;libarrow&lt;/code&gt;と&lt;code class=&quot;highlighter-rouge&quot;&gt;libarrow_python&lt;/code&gt;というC++ライブラリーのCythonバインディングです。&lt;code class=&quot;highlighter-rouge&quot;&gt;pyarrow&lt;/code&gt;はNumPyと&lt;a href=&quot;http://pandas.pydata.org&quot;&gt;pandas&lt;/a&gt;とPythonの標準ライブラリー間のシームレスな連携を実現します。&lt;/p&gt;

&lt;p&gt;ArrowのC++ライブラリーで最も重要なものは&lt;code class=&quot;highlighter-rouge&quot;&gt;arrow::Buffer&lt;/code&gt;オブジェクトです。これはメモリービューを管理します。コピーなしの読み込みとスライスをサポートしている点が重要です。&lt;a href=&quot;https://github.com/JeffKnupp&quot;&gt;Jeff Knupp&lt;/a&gt;はArrowのバッファーとPythonのバッファープロトコルとmemoryviewの連携処理をコントリビュートしました。これにより次のようなことができるようになりました。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyarrow&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pa&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;buf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pa&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frombuffer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'foobarbaz'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;buf&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pyarrow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_io&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Buffer&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt; &lt;span class=&quot;mh&quot;&gt;0x7f6c0a84b538&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;memoryview&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;buf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;memory&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt; &lt;span class=&quot;mh&quot;&gt;0x7f6c0a8c5e88&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;buf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_pybytes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'foobarbaz'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;C++でのParquet実装である&lt;a href=&quot;https://github.com/apache/parquet-cpp&quot;&gt;parquet-cpp&lt;/a&gt;を使うことで大幅に&lt;a href=&quot;http://parquet.apache.org&quot;&gt;&lt;strong&gt;Apache Parquet&lt;/strong&gt;&lt;/a&gt;サポートを改良しました。たとえば、ディスク上にあるかHDFS上にあるか関係なく、パーティションされたデータセットをサポートしました。&lt;a href=&quot;https://github.com/dask/dask/commit/68f9e417924a985c1f2e2a587126833c70a2e9f4&quot;&gt;Daskプロジェクト&lt;/a&gt;はArrowを使ったParquetサポートを実装した最初のプロジェクトです。Dask開発者とはpandsデータを分散処理する文脈でさらに協力できることを楽しみにしています。&lt;/p&gt;

&lt;p&gt;pandasを成熟させるためにArrowを改良することもあり、&lt;a href=&quot;https://github.com/wesm/feather&quot;&gt;&lt;strong&gt;Featherフォーマット&lt;/strong&gt;&lt;/a&gt;の実装をマージしたのもその1つです。Featherフォーマットは本質的にはArrowのランダムアクセスフォーマットの特別なケースの1つです。ArrowのコードベースでFeatherの開発を続けます。たとえば、今のFeatherはArrowのPythonバインディングのレイヤーを使うことでPythonのファイルオブジェクトを読み書きできるようになっています。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;DatetimeTZ&lt;/code&gt;や&lt;code class=&quot;highlighter-rouge&quot;&gt;Categorical&lt;/code&gt;といったpandas固有のデータ型のちゃんとした（robust）サポートも実装しました。&lt;/p&gt;

&lt;h3 id=&quot;cライブラリーでのテンソルサポート&quot;&gt;C++ライブラリーでのテンソルサポート&lt;/h3&gt;

&lt;p&gt;Apache Arrowはコピーなしで共有メモリーを管理するツールという側面があります。機械学習アプリケーションの文脈でこの機能への関心が増えています。UCバークレー校の&lt;a href=&quot;https://rise.cs.berkeley.edu/&quot;&gt;RISELab&lt;/a&gt;の&lt;a href=&quot;https://github.com/ray-project/ray&quot;&gt;Rayプロジェクト&lt;/a&gt;が最初の例です。&lt;/p&gt;

&lt;p&gt;機械学習ではは「テンソル」とも呼ばれる多次元配列というデータ構造を扱います。このようなデータ構造はArrowのカラムフォーマットがサポートしているデータ構造の範囲を超えています。今回のケースでは、&lt;a href=&quot;http://arrow.apache.org/docs/cpp/classarrow_1_1_tensor.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;arrow::Tensor&lt;/code&gt;&lt;/a&gt;というC++の型を追加で実装しました。これはArrowのコピーなしの共有メモリー機能を活用して実装しました。（メモリーの生存期間の管理に&lt;code class=&quot;highlighter-rouge&quot;&gt;arrow::Buffer&lt;/code&gt;を使いました。）C++実装では、これからも、共通のIO・メモリー管理ツールとしてArrowを活用できるようにするため、追加のデータ構造を提供するつもりです。&lt;/p&gt;

&lt;h3 id=&quot;javascripttypescript実装の開始&quot;&gt;JavaScript（TypeScript）実装の開始&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/TheNeuralBit&quot;&gt;Brian Hulette&lt;/a&gt;はNodeJSとWebブラウザー上で動くアプリケーションで使うために&lt;a href=&quot;https://github.com/apache/arrow/tree/master/js&quot;&gt;TypeScript&lt;/a&gt;でのArrowの実装を始めました。FlatBuffersがJavaScriptをファーストクラスでサポートしているので実装が捗ります。&lt;/p&gt;

&lt;h3 id=&quot;webサイトと開発者用ドキュメントの改良&quot;&gt;Webサイトと開発者用ドキュメントの改良&lt;/h3&gt;

&lt;p&gt;0.2.0をリリースしてからドキュメントとブログを公開するためにWebサイトのシステムを&lt;a href=&quot;https://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt;ベースで作りました。Kouhei Sutouは&lt;a href=&quot;https://github.com/red-data-tools/jekyll-jupyter-notebook&quot;&gt;Jekyll Jupyter Notebookプラグイン&lt;/a&gt;を作りました。これによりArrowのWebサイトのコンテンツを作るためにJupyterを使うことができます。&lt;/p&gt;

&lt;p&gt;WebサイトにはC、C++、Java、PythonのAPIドキュメントを公開しました。これらの中にArrowを使い始めるための有益な情報を見つけられるでしょう。&lt;/p&gt;

&lt;h3 id=&quot;コントリビューター&quot;&gt;コントリビューター&lt;/h3&gt;

&lt;p&gt;このリリースにパッチをコントリビュートしたみなさんに感謝します。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ git shortlog -sn apache-arrow-0.2.0..apache-arrow-0.3.0
    119 Wes McKinney
     55 Kouhei Sutou
     18 Uwe L. Korn
     17 Julien Le Dem
      9 Phillip Cloud
      6 Bryan Cutler
      5 Philipp Moritz
      5 Emilio Lahr-Vivaz
      4 Max Risuhin
      4 Johan Mabille
      4 Jeff Knupp
      3 Steven Phillips
      3 Miki Tebeka
      2 Leif Walsh
      2 Jeff Reback
      2 Brian Hulette
      1 Tsuyoshi Ozawa
      1 rvernica
      1 Nong Li
      1 Julien Lafaye
      1 Itai Incze
      1 Holden Karau
      1 Deepak Majeti
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;</content><author><name>wesm</name></author></entry></feed>