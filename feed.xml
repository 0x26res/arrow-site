<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://arrow.apache.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arrow.apache.org/" rel="alternate" type="text/html" /><updated>2024-01-29T11:38:54-05:00</updated><id>https://arrow.apache.org/feed.xml</id><title type="html">Apache Arrow</title><subtitle>Apache Arrow is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, C++, C#, Go, Java, JavaScript, MATLAB, Python, R, Ruby, and Rust.</subtitle><entry><title type="html">Apache Arrow 15.0.0 Release</title><link href="https://arrow.apache.org/blog/2024/01/21/15.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 15.0.0 Release" /><published>2024-01-21T00:00:00-05:00</published><updated>2024-01-21T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2024/01/21/15.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2024/01/21/15.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 15.0.0 release. This covers
over 3 months of development work and includes <a href="https://github.com/apache/arrow/milestone/56?closed=1"><strong>344 resolved issues</strong></a>
on <a href="/release/15.0.0.html#contributors"><strong>536 distinct commits</strong></a> from <a href="/release/15.0.0.html#contributors"><strong>101 distinct contributors</strong></a>.
See the <a href="https://arrow.apache.org/install/">Install Page</a>
to learn how to get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/15.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 14.0.0 release, Curt Hagenlocher, Xuwei Fu, James Duong and Felipe Oliveira Carvalho
have been invited to be committers.
Jonathan Keane and Raúl Cumplido have joined the Project Management Committee (PMC).</p>

<p>As per our tradition of rotating the PMC chair once a year
Andy Grove was elected as the new PMC chair and VP.</p>

<p>Thanks for your contributions and participation in the project!</p>

<h2 id="c-data-interface-notes">C Data Interface notes</h2>

<p>New format strings have been added for ListView, LargeListView, BinaryView and StringView array types.</p>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>

<p>Flight SQL is now considered stable (<a href="https://github.com/apache/arrow/issues/39037">GH-39037</a>). The Flight SQL specification was clarified regarding how the result set schema of a prepared statement is affected by bound parameters (<a href="https://github.com/apache/arrow/issues/37061">GH-37061</a>).</p>

<p>The JDBC Arrow Flight SQL driver now supports mTLS authentication (<a href="https://github.com/apache/arrow/issues/38460">GH-38460</a>) and bind parameters (<a href="https://github.com/apache/arrow/issues/33475">GH-33475</a>), follows the Flight RPC spec when fetching data (<a href="https://github.com/apache/arrow/issues/34532">GH-34532</a>), and can reuse credentials across metadata and data connections (<a href="https://github.com/apache/arrow/issues/38576">GH-38576</a>). On macOS it will also use the system keychain to be consistent with other platforms (<a href="https://github.com/apache/arrow/issues/39014">GH-39014</a>). Applications can also retrieve the underlying Flight RPC metadata from the JDBC driver (GH-38024, GH-38022).</p>

<h2 id="c-notes">C++ notes</h2>

<p>For C++ notes refer to the full changelog.</p>

<h3 id="parquet">Parquet</h3>

<h4 id="new-features">New features:</h4>
<ul>
  <li>Support row group filtering for nested paths for C++ and Parquet (<a href="https://github.com/apache/arrow/issues/39064">GH-39064</a>)</li>
  <li>Implement Parquet Float16 logical type (<a href="https://github.com/apache/arrow/issues/36036">GH-36036</a>)</li>
  <li>Expose sorting_columns in RowGroupMetaData for Parquet files (<a href="https://github.com/apache/arrow/issues/35331">GH-35331</a>)</li>
  <li>Support decompressing concatenated gzip members (stream) (<a href="https://github.com/apache/arrow/issues/38271">GH-38271</a>)</li>
</ul>

<h4 id="api-change">API change:</h4>
<ul>
  <li>Move EstimatedBufferedValueBytes from TypedColumnWriter to ColumnWriter (<a href="https://github.com/apache/arrow/issues/38887">GH-38887</a>)</li>
  <li>Change parquet TypedComparator operation to const methods (<a href="https://github.com/apache/arrow/issues/38874">GH-38874</a>)</li>
  <li>Remove deprecated AppendRowGroup(int64_t num_rows) (<a href="https://github.com/apache/arrow/issues/39208">GH-39208</a>)</li>
  <li>Add api to get RecordReader from RowGroupReader (<a href="https://github.com/apache/arrow/issues/37002">GH-37002</a>)</li>
</ul>

<h4 id="bug-fixes">Bug fixes:</h4>
<ul>
  <li>Add more closed file checks for ParquetFileWriter to Prevent from used-after-close (<a href="https://github.com/apache/arrow/issues/38390">GH-38390</a>)</li>
</ul>

<h4 id="performance-enhancement">Performance enhancement:</h4>
<ul>
  <li>Faster Scalar BYTE_STREAM_SPLIT encoding/decoding (<a href="https://github.com/apache/arrow/issues/38542">GH-38542</a>)</li>
  <li>Faster reading Parquet FLBA (GH-39124, GH-39413)</li>
  <li>Using bloom_filter_length in parquet 2.10 to optimize bloom filter read (<a href="https://github.com/apache/arrow/issues/38860">GH-38860</a>)</li>
</ul>

<h3 id="miscellaneous">Miscellaneous</h3>

<ul>
  <li>Upgrade ORC to 1.9.2 (<a href="https://github.com/apache/arrow/issues/39430">GH-39340</a>)</li>
</ul>

<h2 id="c-notes-1">C# notes</h2>

<p>Removal of build targets:</p>
<ul>
  <li>Remove out-of-support versions of .NET and update C# README <a href="https://github.com/apache/arrow/pull/39165">GH-31579</a></li>
</ul>

<p>New features:</p>
<ul>
  <li>Better support for decimal values which exceed the range of the BCL’s System.Decimal <a href="https://github.com/apache/arrow/pull/38481">GH-38351</a>, <a href="https://github.com/apache/arrow/pull/38508">GH-38483</a></li>
  <li>Expose ArrayDataConcentrator.Concatenate publicly <a href="https://github.com/apache/arrow/pull/38154">GH-38153</a></li>
  <li>Add ToString methods to Arrow classes <a href="https://github.com/apache/arrow/pull/30717">GH-36566</a></li>
  <li>Implement common interfaces for structure arrays and record batches <a href="https://github.com/apache/arrow/pull/38759">GH-38757</a></li>
  <li>Make primitive arrays support IReadOnlyList&lt;T?&gt; <a href="https://github.com/apache/arrow/pull/38680">GH-38348</a>, <a href="https://github.com/apache/arrow/pull/39224">GH-39223</a></li>
  <li>Add ToList to Decimal128Array and Decimal256Array <a href="https://github.com/apache/arrow/pull/37383">GH-37359</a></li>
  <li>Support additional types Interval, Utf8View, BinaryView and ListView <a href="https://github.com/apache/arrow/pull/39043">GH-38316</a>, <a href="https://github.com/apache/arrow/pull/39342">GH-39341</a></li>
  <li>Support creating FlightClient with Grpc.Core.Channel <a href="https://github.com/apache/arrow/pull/39348">GH-39335</a></li>
</ul>

<p>Fixes and improved compatibility:</p>
<ul>
  <li>Make dictionaries in file and memory implementations work correctly and support integration tests <a href="https://github.com/apache/arrow/pull/39146">GH-32662</a></li>
  <li>Support blank column names and enable more integration tests <a href="https://github.com/apache/arrow/pull/39167">GH-36588</a></li>
</ul>

<h2 id="go-notes">Go Notes</h2>

<h3 id="bug-fixes-1">Bug Fixes</h3>

<h4 id="arrow">Arrow</h4>

<ul>
  <li>Ensured reliability of AuthenticateBasicToken behind proxies (<a href="https://github.com/apache/arrow/issues/38198">GH-38198</a>)</li>
  <li>Ensured release callback is properly called on C Data imported arrays/batches (<a href="https://github.com/apache/arrow/issues/38281">GH-38281</a>)</li>
  <li>Fixed rounding errors in decimal256 string functions (<a href="https://github.com/apache/arrow/issues/38395">GH-38395</a>)</li>
  <li>Added <code class="language-plaintext highlighter-rouge">ValueLen</code> to Binary and String array interface (<a href="https://github.com/apache/arrow/issues/38458">GH-38458</a>)</li>
  <li>Fixed Decimal128 rounding issues (<a href="https://github.com/apache/arrow/issues/38477">GH-38477</a>)</li>
  <li>Fixed memory leak in IPC LZ4 decompressor (<a href="https://github.com/apache/arrow/issues/38728">GH-38728</a>)</li>
  <li>Addressed Data race in <code class="language-plaintext highlighter-rouge">GetToTimeFunc</code> for fixed timestamp data types (<a href="https://github.com/apache/arrow/issues/38795">GH-38795</a>)</li>
  <li>Fixed “index out of range” error for empty resultsets of FlightSQL driver (<a href="https://github.com/apache/arrow/issues/39238">GH-39238</a>)</li>
</ul>

<h4 id="parquet-1">Parquet</h4>

<ul>
  <li>Fixed issue with max definition levels when writing a Parquet file under certain circumstances (<a href="https://github.com/apache/arrow/issues/38503">GH-38503</a>)</li>
  <li>File writer now properly tracks the number of rows written beyond the last row group (<a href="https://github.com/apache/arrow/issues/38516">GH-38516</a>)</li>
</ul>

<h3 id="enhancements">Enhancements</h3>

<h4 id="arrow-1">Arrow</h4>

<ul>
  <li>Added an Avro OCF reader for converting Avro files directly to Arrow record batches (<a href="https://github.com/apache/arrow/issues/36760">GH-36760</a>)</li>
  <li>Added support for StringView (<a href="https://github.com/apache/arrow/issues/38718">GH-38718</a>) and C Data ABI StringViews (<a href="https://github.com/apache/arrow/issues/39013">GH-39013</a>)</li>
  <li>GC Checks were enabled for CI running integration tests (<a href="https://github.com/apache/arrow/issues/38824">GH-38824</a>)</li>
</ul>

<h4 id="parquet-2">Parquet</h4>

<ul>
  <li>Implemented Float16 logical type for Parquet files (<a href="https://github.com/apache/arrow/issues/37582">GH-37582</a>)</li>
  <li>Added proper boolean RLE encoding/decoding (<a href="https://github.com/apache/arrow/issues/38462">GH-38462</a>)</li>
</ul>

<h3 id="bug-fixes-2">Bug Fixes</h3>

<h3 id="enhancements-1">Enhancements</h3>

<h2 id="java-notes">Java notes</h2>

<p><strong>We expect a breaking change in the next release, Arrow 16.0.0.</strong> Support for Java 9 modules is coming, but that will require changing the JVM flags used to launch your application (<a href="https://github.com/apache/arrow/issues/38998">GH-38998</a>).  Arrow 15.0.0 is not affected.</p>

<p>A bill-of-materials (BOM) package was added to make it easier to depend on multiple Arrow libraries (<a href="https://github.com/apache/arrow/issues/38264">GH-38264</a>).</p>

<p>The JDBC adapter (separate from the JDBC driver) now supports 256-bit decimals (<a href="https://github.com/apache/arrow/issues/39484">GH-39484</a>) and throws more informative exceptions (<a href="https://github.com/apache/arrow/issues/39355">GH-39355</a>).</p>

<p>Various improvements were made to utilities for working with vectors (GH-38662, GH-38614, GH-38511, GH-38254, GH-38246).</p>

<h2 id="javascript-notes">JavaScript notes</h2>

<p>This release comes with new features and APIs. We also removed <code class="language-plaintext highlighter-rouge">getByteLength</code> to reduce bundle sizes.</p>

<p>New Features with API changes</p>
<ul>
  <li><a href="https://github.com/apache/arrow/pull/39018">GH-39017: [JS] Add typeId as attribute</a></li>
  <li><a href="https://github.com/apache/arrow/pull/39258">GH-39257: [JS] LargeBinary</a></li>
  <li><a href="https://github.com/apache/arrow/pull/35780">GH-15060: [JS] Add LargeUtf8 type</a></li>
  <li><a href="https://github.com/apache/arrow/pull/39260">GH-39259: [JS] Remove getByteLength</a></li>
  <li><a href="https://github.com/apache/arrow/pull/39436">GH-39435: [JS] Add Vector.nullable</a></li>
  <li><a href="https://github.com/apache/arrow/pull/39256">GH-39255: [JS] Allow customization of schema when passing vectors to table constructor</a></li>
  <li><a href="https://github.com/apache/arrow/pull/39254">GH-37983: [JS] Allow nullable fields in table when constructed from vector with nulls</a></li>
</ul>

<p>Package changes</p>
<ul>
  <li><a href="https://github.com/apache/arrow/pull/39475">GH-39289: [JS] Add types to exports</a></li>
</ul>

<h2 id="python-notes">Python notes</h2>

<p>Compatibility notes:</p>
<ul>
  <li>Legacy <code class="language-plaintext highlighter-rouge">ParquetDataset</code> custom implementation has been removed and only the new dataset  API is now in use <a href="https://github.com/apache/arrow/issues/31303">GH-31303</a>.</li>
</ul>

<p>New features:</p>
<ul>
  <li>PyArrow version 14.0.0 included a new specification for Arrow PyCapsules and related dunder methods <a href="https://github.com/apache/arrow/pull/37797">GH-35531</a> and now a public <code class="language-plaintext highlighter-rouge">RecordBatchReader</code> constructor from stream object implementing the PyCapsule Protocol has been added <a href="https://github.com/apache/arrow/issues/39217">GH-[39217](https://github.com/apache/arrow/issues/39217)</a> together with some additional documentation <a href="https://github.com/apache/arrow/issues/39196">GH-[39196](https://github.com/apache/arrow/issues/39196)</a>.</li>
  <li>DLPack protocol support (producer) was added to the Arrow C++ and is exposed in Python through <code class="language-plaintext highlighter-rouge">__dlpack__</code> and <code class="language-plaintext highlighter-rouge">__dlpack_device__</code> dunder methods  <a href="https://github.com/apache/arrow/issues/33984">GH-33984</a>.</li>
  <li>Python now exposes enabling CRC checksum for read and write operations in Paquet <a href="https://github.com/apache/arrow/issues/37242">GH-37242</a>. CRC checksum are optional and can detect data corruption.</li>
  <li><code class="language-plaintext highlighter-rouge">CacheOptions</code> are now configurable from Python as part of the <code class="language-plaintext highlighter-rouge">pyarrow.dataset.ParquetFragmentScanOptions</code> <a href="https://github.com/apache/arrow/issues/36441">GH-36441</a>.</li>
  <li>Parquet metadata to indicate sort order of the data are now exposed in <code class="language-plaintext highlighter-rouge">RowGroupMetaData</code> <a href="https://github.com/apache/arrow/issues/35331">GH-35331</a>.</li>
  <li>Parquet Support write and validate Page CRC (<a href="https://github.com/apache/arrow/issues/37242">GH-37242</a>)</li>
</ul>

<p>Other improvements:</p>
<ul>
  <li>Append parameter from <code class="language-plaintext highlighter-rouge">FileOutputStream</code> is exposed for the <code class="language-plaintext highlighter-rouge">OSFile</code> class <a href="https://github.com/apache/arrow/issues/38857">GH-38857</a>.</li>
  <li>File size can be passed to <code class="language-plaintext highlighter-rouge">make_fragment</code> in the pyarrow datasets (<code class="language-plaintext highlighter-rouge">pyarrow.dataset.FileFormat</code>and <code class="language-plaintext highlighter-rouge">pyarrow.dataset.ParquetFileFormat</code>) <a href="https://github.com/apache/arrow/issues/37857">GH-37857</a>.</li>
  <li>Support for mask parameter is added to <code class="language-plaintext highlighter-rouge">FixedSizeListArray.from_arrays</code> <a href="https://github.com/apache/arrow/issues/34316">GH-34316</a></li>
  <li><code class="language-plaintext highlighter-rouge">to/from_struct_array</code> are added to the <code class="language-plaintext highlighter-rouge">pyarrow.Table</code> class <a href="https://github.com/apache/arrow/issues/33500">GH-33500</a>.</li>
  <li>GIL is released in <code class="language-plaintext highlighter-rouge">.nbytes</code> which is improving performance when calculating the data size <a href="https://github.com/apache/arrow/issues/39096">GH-39096</a>.</li>
  <li>Usage of pandas internals <code class="language-plaintext highlighter-rouge">DatetimeTZBlock</code> has been removed <a href="https://github.com/apache/arrow/issues/38341">GH-38341</a>.</li>
  <li><code class="language-plaintext highlighter-rouge">DataType</code> instance can be passed to <code class="language-plaintext highlighter-rouge">MapType.from_arrays</code> constructor <a href="https://github.com/apache/arrow/issues/39515">GH-39515</a>.</li>
</ul>

<p>Relevant bug fixes:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">S3FileSystem</code> equals <code class="language-plaintext highlighter-rouge">None</code> segfault has been fixed <a href="https://github.com/apache/arrow/issues/38535">GH-38535</a>.</li>
  <li>No-op kernel is added for <code class="language-plaintext highlighter-rouge">dictionary_encode(dictionary)</code> <a href="https://github.com/apache/arrow/issues/34890">GH-34890</a>.</li>
  <li>PrettyPrint for Timestamp type now adds “Z” at the end of the print string when tz is defined in order to add minimum information about the values being stored in UTC <a href="https://github.com/apache/arrow/issues/30117">GH-30117</a>.</li>
</ul>

<h2 id="r-notes">R notes</h2>

<h3 id="new-features-1">New features:</h3>

<ul>
  <li>Bindings for <code class="language-plaintext highlighter-rouge">base::prod</code> have been added so you can now use it in your dplyr pipelines (i.e., <code class="language-plaintext highlighter-rouge">tbl |&gt; summarize(prod(col))</code>) without having to pull the data into R <a href="https://github.com/apache/arrow/pull/38601">GH-38601</a>.</li>
  <li>Calling <code class="language-plaintext highlighter-rouge">dimnames</code> or <code class="language-plaintext highlighter-rouge">colnames</code> on <code class="language-plaintext highlighter-rouge">Dataset</code> objects now returns a useful result rather than just <code class="language-plaintext highlighter-rouge">NULL</code> <a href="https://github.com/apache/arrow/pull/38377">GH-38377</a>.</li>
  <li>The <code class="language-plaintext highlighter-rouge">code()</code> method on Schema objects now takes an optional <code class="language-plaintext highlighter-rouge">namespace</code> argument which, when <code class="language-plaintext highlighter-rouge">TRUE</code>, prefixes names with <code class="language-plaintext highlighter-rouge">arrow::</code> which makes the output more portable <a href="https://github.com/apache/arrow/pull/38144">GH-38144</a>.</li>
</ul>

<h3 id="other-improvements">Other improvements:</h3>

<ul>
  <li>To make debugging problems easier when using arrow with AWS S3 (e..g, <code class="language-plaintext highlighter-rouge">s3_bucket</code>, <code class="language-plaintext highlighter-rouge">S3FileSystem</code>), the debug log level for S3 can be set with the <code class="language-plaintext highlighter-rouge">AWS_S3_LOG_LEVEL</code> environment variable. See <code class="language-plaintext highlighter-rouge">?S3FileSystem</code> for more information. <a href="https://github.com/apache/arrow/pull/38267">GH-38267</a></li>
  <li>An error is now thrown instead of warning and pulling the data into R when any of <code class="language-plaintext highlighter-rouge">sub</code>, <code class="language-plaintext highlighter-rouge">gsub</code>, <code class="language-plaintext highlighter-rouge">stringr::str_replace</code>, <code class="language-plaintext highlighter-rouge">stringr::str_replace_all</code> are passed a length &gt; 1 vector of values in <code class="language-plaintext highlighter-rouge">pattern</code> <a href="https://github.com/apache/arrow/pull/39219">GH-39219</a>.</li>
  <li>Missing documentation was added to <code class="language-plaintext highlighter-rouge">?open_dataset</code> documenting how to use the ND-JSON support added in arrow 13.0.0 <a href="https://github.com/apache/arrow/pull/38258">GH-38258</a>.</li>
  <li>Using arrow with duckdb (i.e., <code class="language-plaintext highlighter-rouge">to_duckdb()</code>) no longer results in warnings when quitting your R session. <a href="https://github.com/apache/arrow/pull/38495">GH-38495</a></li>
</ul>

<p>For more on what’s in the 15.0.0 R package, see the <a href="/docs/r/news/">R changelog</a>.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<h3 id="ruby">Ruby</h3>

<ul>
  <li>Add <code class="language-plaintext highlighter-rouge">Arrow::Table#each_raw_record</code> and <code class="language-plaintext highlighter-rouge">Arrow::RecordBatch#each_raw_record</code> (<a href="https://github.com/apache/arrow/issues/37137">GH-37137</a>, <a href="https://github.com/apache/arrow/issues/37600">GH-37600</a>)</li>
</ul>

<h3 id="c-glib">C GLib</h3>

<ul>
  <li>Follow C++ changes.</li>
</ul>

<h2 id="rust-notes">Rust notes</h2>

<p>The Rust projects have moved to separate repositories outside the
main Arrow monorepo. For notes on the latest release of the Rust
implementation, see the latest <a href="https://github.com/apache/arrow-rs/tags">Arrow Rust changelog</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 15.0.0 release. This covers over 3 months of development work and includes 344 resolved issues on 536 distinct commits from 101 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 14.0.0 release, Curt Hagenlocher, Xuwei Fu, James Duong and Felipe Oliveira Carvalho have been invited to be committers. Jonathan Keane and Raúl Cumplido have joined the Project Management Committee (PMC). As per our tradition of rotating the PMC chair once a year Andy Grove was elected as the new PMC chair and VP. Thanks for your contributions and participation in the project! C Data Interface notes New format strings have been added for ListView, LargeListView, BinaryView and StringView array types. Arrow Flight RPC notes Flight SQL is now considered stable (GH-39037). The Flight SQL specification was clarified regarding how the result set schema of a prepared statement is affected by bound parameters (GH-37061). The JDBC Arrow Flight SQL driver now supports mTLS authentication (GH-38460) and bind parameters (GH-33475), follows the Flight RPC spec when fetching data (GH-34532), and can reuse credentials across metadata and data connections (GH-38576). On macOS it will also use the system keychain to be consistent with other platforms (GH-39014). Applications can also retrieve the underlying Flight RPC metadata from the JDBC driver (GH-38024, GH-38022). C++ notes For C++ notes refer to the full changelog. Parquet New features: Support row group filtering for nested paths for C++ and Parquet (GH-39064) Implement Parquet Float16 logical type (GH-36036) Expose sorting_columns in RowGroupMetaData for Parquet files (GH-35331) Support decompressing concatenated gzip members (stream) (GH-38271) API change: Move EstimatedBufferedValueBytes from TypedColumnWriter to ColumnWriter (GH-38887) Change parquet TypedComparator operation to const methods (GH-38874) Remove deprecated AppendRowGroup(int64_t num_rows) (GH-39208) Add api to get RecordReader from RowGroupReader (GH-37002) Bug fixes: Add more closed file checks for ParquetFileWriter to Prevent from used-after-close (GH-38390) Performance enhancement: Faster Scalar BYTE_STREAM_SPLIT encoding/decoding (GH-38542) Faster reading Parquet FLBA (GH-39124, GH-39413) Using bloom_filter_length in parquet 2.10 to optimize bloom filter read (GH-38860) Miscellaneous Upgrade ORC to 1.9.2 (GH-39340) C# notes Removal of build targets: Remove out-of-support versions of .NET and update C# README GH-31579 New features: Better support for decimal values which exceed the range of the BCL’s System.Decimal GH-38351, GH-38483 Expose ArrayDataConcentrator.Concatenate publicly GH-38153 Add ToString methods to Arrow classes GH-36566 Implement common interfaces for structure arrays and record batches GH-38757 Make primitive arrays support IReadOnlyList&lt;T?&gt; GH-38348, GH-39223 Add ToList to Decimal128Array and Decimal256Array GH-37359 Support additional types Interval, Utf8View, BinaryView and ListView GH-38316, GH-39341 Support creating FlightClient with Grpc.Core.Channel GH-39335 Fixes and improved compatibility: Make dictionaries in file and memory implementations work correctly and support integration tests GH-32662 Support blank column names and enable more integration tests GH-36588 Go Notes Bug Fixes Arrow Ensured reliability of AuthenticateBasicToken behind proxies (GH-38198) Ensured release callback is properly called on C Data imported arrays/batches (GH-38281) Fixed rounding errors in decimal256 string functions (GH-38395) Added ValueLen to Binary and String array interface (GH-38458) Fixed Decimal128 rounding issues (GH-38477) Fixed memory leak in IPC LZ4 decompressor (GH-38728) Addressed Data race in GetToTimeFunc for fixed timestamp data types (GH-38795) Fixed “index out of range” error for empty resultsets of FlightSQL driver (GH-39238) Parquet Fixed issue with max definition levels when writing a Parquet file under certain circumstances (GH-38503) File writer now properly tracks the number of rows written beyond the last row group (GH-38516) Enhancements Arrow Added an Avro OCF reader for converting Avro files directly to Arrow record batches (GH-36760) Added support for StringView (GH-38718) and C Data ABI StringViews (GH-39013) GC Checks were enabled for CI running integration tests (GH-38824) Parquet Implemented Float16 logical type for Parquet files (GH-37582) Added proper boolean RLE encoding/decoding (GH-38462) Bug Fixes Enhancements Java notes We expect a breaking change in the next release, Arrow 16.0.0. Support for Java 9 modules is coming, but that will require changing the JVM flags used to launch your application (GH-38998). Arrow 15.0.0 is not affected. A bill-of-materials (BOM) package was added to make it easier to depend on multiple Arrow libraries (GH-38264). The JDBC adapter (separate from the JDBC driver) now supports 256-bit decimals (GH-39484) and throws more informative exceptions (GH-39355). Various improvements were made to utilities for working with vectors (GH-38662, GH-38614, GH-38511, GH-38254, GH-38246). JavaScript notes This release comes with new features and APIs. We also removed getByteLength to reduce bundle sizes. New Features with API changes GH-39017: [JS] Add typeId as attribute GH-39257: [JS] LargeBinary GH-15060: [JS] Add LargeUtf8 type GH-39259: [JS] Remove getByteLength GH-39435: [JS] Add Vector.nullable GH-39255: [JS] Allow customization of schema when passing vectors to table constructor GH-37983: [JS] Allow nullable fields in table when constructed from vector with nulls Package changes GH-39289: [JS] Add types to exports Python notes Compatibility notes: Legacy ParquetDataset custom implementation has been removed and only the new dataset API is now in use GH-31303. New features: PyArrow version 14.0.0 included a new specification for Arrow PyCapsules and related dunder methods GH-35531 and now a public RecordBatchReader constructor from stream object implementing the PyCapsule Protocol has been added GH-[39217](https://github.com/apache/arrow/issues/39217) together with some additional documentation GH-[39196](https://github.com/apache/arrow/issues/39196). DLPack protocol support (producer) was added to the Arrow C++ and is exposed in Python through __dlpack__ and __dlpack_device__ dunder methods GH-33984. Python now exposes enabling CRC checksum for read and write operations in Paquet GH-37242. CRC checksum are optional and can detect data corruption. CacheOptions are now configurable from Python as part of the pyarrow.dataset.ParquetFragmentScanOptions GH-36441. Parquet metadata to indicate sort order of the data are now exposed in RowGroupMetaData GH-35331. Parquet Support write and validate Page CRC (GH-37242) Other improvements: Append parameter from FileOutputStream is exposed for the OSFile class GH-38857. File size can be passed to make_fragment in the pyarrow datasets (pyarrow.dataset.FileFormatand pyarrow.dataset.ParquetFileFormat) GH-37857. Support for mask parameter is added to FixedSizeListArray.from_arrays GH-34316 to/from_struct_array are added to the pyarrow.Table class GH-33500. GIL is released in .nbytes which is improving performance when calculating the data size GH-39096. Usage of pandas internals DatetimeTZBlock has been removed GH-38341. DataType instance can be passed to MapType.from_arrays constructor GH-39515. Relevant bug fixes: S3FileSystem equals None segfault has been fixed GH-38535. No-op kernel is added for dictionary_encode(dictionary) GH-34890. PrettyPrint for Timestamp type now adds “Z” at the end of the print string when tz is defined in order to add minimum information about the values being stored in UTC GH-30117. R notes New features: Bindings for base::prod have been added so you can now use it in your dplyr pipelines (i.e., tbl |&gt; summarize(prod(col))) without having to pull the data into R GH-38601. Calling dimnames or colnames on Dataset objects now returns a useful result rather than just NULL GH-38377. The code() method on Schema objects now takes an optional namespace argument which, when TRUE, prefixes names with arrow:: which makes the output more portable GH-38144. Other improvements: To make debugging problems easier when using arrow with AWS S3 (e..g, s3_bucket, S3FileSystem), the debug log level for S3 can be set with the AWS_S3_LOG_LEVEL environment variable. See ?S3FileSystem for more information. GH-38267 An error is now thrown instead of warning and pulling the data into R when any of sub, gsub, stringr::str_replace, stringr::str_replace_all are passed a length &gt; 1 vector of values in pattern GH-39219. Missing documentation was added to ?open_dataset documenting how to use the ND-JSON support added in arrow 13.0.0 GH-38258. Using arrow with duckdb (i.e., to_duckdb()) no longer results in warnings when quitting your R session. GH-38495 For more on what’s in the 15.0.0 R package, see the R changelog. Ruby and C GLib notes Ruby Add Arrow::Table#each_raw_record and Arrow::RecordBatch#each_raw_record (GH-37137, GH-37600) C GLib Follow C++ changes. Rust notes The Rust projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Rust implementation, see the latest Arrow Rust changelog.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow DataFusion 34.0.0 Released, Looking Forward to 2024</title><link href="https://arrow.apache.org/blog/2024/01/19/datafusion-34.0.0/" rel="alternate" type="text/html" title="Apache Arrow DataFusion 34.0.0 Released, Looking Forward to 2024" /><published>2024-01-19T00:00:00-05:00</published><updated>2024-01-19T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2024/01/19/datafusion-34.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2024/01/19/datafusion-34.0.0/"><![CDATA[<!--

-->

<h2 id="introduction">Introduction</h2>

<p>We recently <a href="https://crates.io/crates/datafusion/34.0.0">released DataFusion 34.0.0</a>. This blog highlights some of the major
improvements since we <a href="https://arrow.apache.org/blog/2023/06/24/datafusion-25.0.0/.">released DataFusion 26.0.0</a> (spoiler alert there are many)
and a preview of where the community plans to focus in the next 6 months.</p>

<p><a href="https://arrow.apache.org/datafusion/">Apache Arrow DataFusion</a> is an extensible query engine, written in <a href="https://www.rust-lang.org/">Rust</a>, that
uses <a href="https://arrow.apache.org">Apache Arrow</a> as its in-memory format. DataFusion is used by developers to
create new, fast data centric systems such as databases, dataframe libraries,
machine learning and streaming applications. While <a href="https://arrow.apache.org/datafusion/user-guide/introduction.html#project-goals">DataFusion’s primary design
goal</a> is to accelerate creating other data centric systems, it has a
reasonable experience directly out of the box as a <a href="https://arrow.apache.org/datafusion-python/">dataframe library</a> and
<a href="https://arrow.apache.org/datafusion/user-guide/cli.html">command line SQL tool</a>.</p>

<p>This may also be our last update on the Apache Arrow Site. Future
updates will likely be on the DataFusion website as we are working to <a href="https://github.com/apache/arrow-datafusion/discussions/6475">graduate
to a top level project</a> (Apache Arrow DataFusion → Apache DataFusion!) which
will help focus governance and project growth. Also exciting, our <a href="https://github.com/apache/arrow-datafusion/discussions/8522">first
DataFusion in person meetup</a> is planned for March 2024.</p>

<p>DataFusion is very much a community endeavor. Our core thesis is that as a
community we can build much more advanced technology than any of us as
individuals or companies could alone. In the last 6 months between <code class="language-plaintext highlighter-rouge">26.0.0</code> and
<code class="language-plaintext highlighter-rouge">34.0.0</code>, community growth has been strong. We accepted and reviewed over a
thousand PRs from 124 different committers, created over 650 issues and closed 517
of them.
You can find a list of all changes in the detailed <a href="https://github.com/apache/arrow-datafusion/blob/main/datafusion/CHANGELOG.md">CHANGELOG</a>.</p>

<!--
$ git log --pretty=oneline 26.0.0..34.0.0 . | wc -l
     1009

$ git shortlog -sn 26.0.0..34.0.0 . | wc -l
      124

https://crates.io/crates/datafusion/26.0.0
DataFusion 26 released June 7, 2023

https://crates.io/crates/datafusion/34.0.0
DataFusion 34 released Dec 17, 2023

Issues created in this time: 214 open, 437 closed
https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+created%3A2023-06-23..2023-12-17

Issues closes: 517
https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+closed%3A2023-06-23..2023-12-17+

PRs merged in this time 908
https://github.com/apache/arrow-datafusion/pulls?q=is%3Apr+merged%3A2023-06-23..2023-12-17
-->

<h1 id="improved-performance-">Improved Performance 🚀</h1>

<p>Performance is a key feature of DataFusion, DataFusion is 
more than 2x faster on <a href="https://benchmark.clickhouse.com/">ClickBench</a> compared to version <code class="language-plaintext highlighter-rouge">25.0.0</code>, as shown below:</p>

<!--
  Scripts: https://github.com/alamb/datafusion-duckdb-benchmark/tree/datafusion-25-34
  Spreadsheet: https://docs.google.com/spreadsheets/d/1FtI3652WIJMC5LmJbLfT3G06w0JQIxEPG4yfMafexh8/edit#gid=1879366976
  Average runtime on 25.0.0: 7.2s (for the queries that actually ran)
  Average runtime on 34.0.0: 3.6s (for the same queries that ran in 25.0.0)
-->

<figure style="text-align: center;">
  <img src="/img/datafusion-34.0.0/compare-new.png" width="100%" class="img-responsive" alt="Fig 1: Adaptive Arrow schema architecture overview." />
  <figcaption>
    <b>Figure 1</b>: Performance improvement between <code>25.0.0</code> and <code>34.0.0</code> on ClickBench. 
    Note that DataFusion <code>25.0.0</code>, could not run several queries due to 
    unsupported SQL (Q9, Q11, Q12, Q14) or memory requirements (Q33).
  </figcaption>
</figure>

<figure style="text-align: center;">
  <img src="/img/datafusion-34.0.0/compare.png" width="100%" class="img-responsive" alt="Fig 1: Adaptive Arrow schema architecture overview." />
  <figcaption>
    <b>Figure 2</b>: Total query runtime for DataFusion <code>34.0.0</code> and DataFusion <code>25.0.0</code>.
  </figcaption>
</figure>

<p>Here are some specific enhancements we have made to improve performance:</p>
<ul>
  <li><a href="https://arrow.apache.org/blog/2023/08/05/datafusion_fast_grouping/">2-3x better aggregation performance with many distinct groups</a></li>
  <li>Partially ordered grouping / streaming grouping</li>
  <li><a href="https://github.com/apache/arrow-datafusion/pull/7721">Specialized operator for “TopK” <code class="language-plaintext highlighter-rouge">ORDER BY LIMIT XXX</code></a></li>
  <li><a href="https://github.com/apache/arrow-datafusion/pull/7192">Specialized operator for <code class="language-plaintext highlighter-rouge">min(col) GROUP BY .. ORDER by min(col) LIMIT XXX</code></a></li>
  <li><a href="https://github.com/apache/arrow-datafusion/pull/8126">Improved join performance</a></li>
  <li>Eliminate redundant sorting with sort order aware optimizers</li>
</ul>

<h1 id="new-features-">New Features ✨</h1>

<h2 id="dml--insert--creating-files">DML / Insert / Creating Files</h2>

<p>DataFusion now supports writing data in parallel, to individual or multiple
files, using <code class="language-plaintext highlighter-rouge">Parquet</code>, <code class="language-plaintext highlighter-rouge">CSV</code>, <code class="language-plaintext highlighter-rouge">JSON</code>, <code class="language-plaintext highlighter-rouge">ARROW</code> and user defined formats.
<a href="https://github.com/apache/arrow-datafusion/pull/7655">Benchmark results</a> show improvements up to 5x in some cases.</p>

<p>Similarly to reading, data can now be written to any <a href="https://docs.rs/object_store/0.9.0/object_store/index.html"><code class="language-plaintext highlighter-rouge">ObjectStore</code></a>
implementation, including AWS S3, Azure Blob Storage, GCP Cloud Storage, local
files, and user defined implementations. While reading from <a href="https://docs.rs/datafusion/latest/datafusion/datasource/listing/struct.ListingTable.html#features">hive style
partitioned tables</a> has long been supported, it is now possible to write to such
tables as well.</p>

<p>For example, to write to a local file:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">❯</span> <span class="k">CREATE</span> <span class="k">EXTERNAL</span> <span class="k">TABLE</span> <span class="n">awesome_table</span><span class="p">(</span><span class="n">x</span> <span class="nb">INT</span><span class="p">)</span> <span class="n">STORED</span> <span class="k">AS</span> <span class="n">PARQUET</span> <span class="k">LOCATION</span> <span class="s1">'/tmp/my_awesome_table'</span><span class="p">;</span>
<span class="mi">0</span> <span class="k">rows</span> <span class="k">in</span> <span class="k">set</span><span class="p">.</span> <span class="n">Query</span> <span class="n">took</span> <span class="mi">0</span><span class="p">.</span><span class="mi">003</span> <span class="n">seconds</span><span class="p">.</span>

<span class="err">❯</span> <span class="k">INSERT</span> <span class="k">INTO</span> <span class="n">awesome_table</span> <span class="k">SELECT</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">10</span> <span class="k">FROM</span> <span class="n">my_source_table</span><span class="p">;</span>
<span class="o">+</span><span class="c1">-------+</span>
<span class="o">|</span> <span class="k">count</span> <span class="o">|</span>
<span class="o">+</span><span class="c1">-------+</span>
<span class="o">|</span> <span class="mi">3</span>     <span class="o">|</span>
<span class="o">+</span><span class="c1">-------+</span>
<span class="mi">1</span> <span class="k">row</span> <span class="k">in</span> <span class="k">set</span><span class="p">.</span> <span class="n">Query</span> <span class="n">took</span> <span class="mi">0</span><span class="p">.</span><span class="mi">024</span> <span class="n">seconds</span><span class="p">.</span>
</code></pre></div></div>

<p>You can also write to files with the <a href="https://arrow.apache.org/datafusion/user-guide/sql/dml.html#copy"><code class="language-plaintext highlighter-rouge">COPY</code></a>, similarly to <a href="https://duckdb.org/docs/sql/statements/copy.html">DuckDB’s <code class="language-plaintext highlighter-rouge">COPY</code></a>:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">❯</span> <span class="k">COPY</span> <span class="p">(</span><span class="k">SELECT</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">FROM</span> <span class="n">my_source_table</span><span class="p">)</span> <span class="k">TO</span> <span class="s1">'/tmp/output.json'</span><span class="p">;</span>
<span class="o">+</span><span class="c1">-------+</span>
<span class="o">|</span> <span class="k">count</span> <span class="o">|</span>
<span class="o">+</span><span class="c1">-------+</span>
<span class="o">|</span> <span class="mi">3</span>     <span class="o">|</span>
<span class="o">+</span><span class="c1">-------+</span>
<span class="mi">1</span> <span class="k">row</span> <span class="k">in</span> <span class="k">set</span><span class="p">.</span> <span class="n">Query</span> <span class="n">took</span> <span class="mi">0</span><span class="p">.</span><span class="mi">014</span> <span class="n">seconds</span><span class="p">.</span>
</code></pre></div></div>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> /tmp/output.json
<span class="o">{</span><span class="s2">"x"</span>:1<span class="o">}</span>
<span class="o">{</span><span class="s2">"x"</span>:2<span class="o">}</span>
<span class="o">{</span><span class="s2">"x"</span>:3<span class="o">}</span>
</code></pre></div></div>

<h2 id="improved-struct-and-array-support">Improved <code class="language-plaintext highlighter-rouge">STRUCT</code> and <code class="language-plaintext highlighter-rouge">ARRAY</code> support</h2>

<p>DataFusion <code class="language-plaintext highlighter-rouge">34.0.0</code> has much improved <code class="language-plaintext highlighter-rouge">STRUCT</code> and <code class="language-plaintext highlighter-rouge">ARRAY</code>
support, including a full range of <a href="https://arrow.apache.org/datafusion/user-guide/sql/scalar_functions.html#struct-functions">struct functions</a> and <a href="https://arrow.apache.org/datafusion/user-guide/sql/scalar_functions.html#array-functions">array functions</a>.</p>

<!--
❯ create table my_table as values ([1,2,3]), ([2]), ([4,5]);
-->

<p>For example, you can now use <code class="language-plaintext highlighter-rouge">[]</code> syntax and <code class="language-plaintext highlighter-rouge">array_length</code> to access and inspect arrays:</p>
<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">❯</span> <span class="k">SELECT</span> <span class="n">column1</span><span class="p">,</span> 
         <span class="n">column1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">AS</span> <span class="n">first_element</span><span class="p">,</span> 
         <span class="n">array_length</span><span class="p">(</span><span class="n">column1</span><span class="p">)</span> <span class="k">AS</span> <span class="n">len</span> 
  <span class="k">FROM</span> <span class="n">my_table</span><span class="p">;</span>
<span class="o">+</span><span class="c1">-----------+---------------+-----+</span>
<span class="o">|</span> <span class="n">column1</span>   <span class="o">|</span> <span class="n">first_element</span> <span class="o">|</span> <span class="n">len</span> <span class="o">|</span>
<span class="o">+</span><span class="c1">-----------+---------------+-----+</span>
<span class="o">|</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">|</span> <span class="mi">1</span>             <span class="o">|</span> <span class="mi">3</span>   <span class="o">|</span>
<span class="o">|</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span>       <span class="o">|</span> <span class="mi">2</span>             <span class="o">|</span> <span class="mi">1</span>   <span class="o">|</span>
<span class="o">|</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>    <span class="o">|</span> <span class="mi">4</span>             <span class="o">|</span> <span class="mi">2</span>   <span class="o">|</span>
<span class="o">+</span><span class="c1">-----------+---------------+-----+</span>
</code></pre></div></div>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">❯</span> <span class="k">SELECT</span> <span class="n">column1</span><span class="p">,</span> <span class="n">column1</span><span class="p">[</span><span class="s1">'c0'</span><span class="p">]</span> <span class="k">FROM</span>  <span class="n">my_table</span><span class="p">;</span>
<span class="o">+</span><span class="c1">------------------+----------------------+</span>
<span class="o">|</span> <span class="n">column1</span>          <span class="o">|</span> <span class="n">my_table</span><span class="p">.</span><span class="n">column1</span><span class="p">[</span><span class="n">c0</span><span class="p">]</span> <span class="o">|</span>
<span class="o">+</span><span class="c1">------------------+----------------------+</span>
<span class="o">|</span> <span class="p">{</span><span class="n">c0</span><span class="p">:</span> <span class="n">foo</span><span class="p">,</span> <span class="n">c1</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span> <span class="o">|</span> <span class="n">foo</span>                  <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">c0</span><span class="p">:</span> <span class="n">bar</span><span class="p">,</span> <span class="n">c1</span><span class="p">:</span> <span class="mi">2</span><span class="p">}</span> <span class="o">|</span> <span class="n">bar</span>                  <span class="o">|</span>
<span class="o">+</span><span class="c1">------------------+----------------------+</span>
<span class="mi">2</span> <span class="k">rows</span> <span class="k">in</span> <span class="k">set</span><span class="p">.</span> <span class="n">Query</span> <span class="n">took</span> <span class="mi">0</span><span class="p">.</span><span class="mi">002</span> <span class="n">seconds</span><span class="p">.</span>
</code></pre></div></div>

<h2 id="other-features">Other Features</h2>
<p>Other notable features include:</p>
<ul>
  <li>Support aggregating datasets that exceed memory size, with <a href="https://github.com/apache/arrow-datafusion/pull/7400">group by spill to disk</a></li>
  <li>All operators now track and limit their memory consumption, including Joins</li>
</ul>

<h1 id="building-systems-is-easier-with-datafusion-️">Building Systems is Easier with DataFusion 🛠️</h1>

<h2 id="documentation">Documentation</h2>
<p>It is easier than ever to get started using DataFusion with the
new <a href="https://arrow.apache.org/datafusion/library-user-guide/index.html">Library Users Guide</a> as well as significantly improved the <a href="https://docs.rs/datafusion/latest/datafusion/index.html">API documentation</a>.</p>

<h2 id="user-defined-window-and-table-functions">User Defined Window and Table Functions</h2>
<p>In addition to DataFusion’s <a href="https://arrow.apache.org/datafusion/library-user-guide/adding-udfs.html#adding-a-scalar-udf">User Defined Scalar Functions</a>, and <a href="https://arrow.apache.org/datafusion/library-user-guide/adding-udfs.html#adding-an-aggregate-udf">User Defined Aggregate Functions</a>, DataFusion now supports <a href="https://arrow.apache.org/datafusion/library-user-guide/adding-udfs.html#adding-a-window-udf">User Defined Window Functions</a> 
 and <a href="https://arrow.apache.org/datafusion/library-user-guide/adding-udfs.html#adding-a-user-defined-table-function">User Defined Table Functions</a>.</p>

<p>For example, <a href="https://arrow.apache.org/datafusion/user-guide/cli.html">the <code class="language-plaintext highlighter-rouge">datafusion-cli</code></a> implements a DuckDB style <a href="https://arrow.apache.org/datafusion/user-guide/cli.html#supported-sql"><code class="language-plaintext highlighter-rouge">parquet_metadata</code></a>
function as a user defined table function (<a href="https://github.com/apache/arrow-datafusion/blob/3f219bc929cfd418b0e3d3501f8eba1d5a2c87ae/datafusion-cli/src/functions.rs#L222-L248">source code here</a>):</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">❯</span> <span class="k">SELECT</span> 
      <span class="n">path_in_schema</span><span class="p">,</span> <span class="n">row_group_id</span><span class="p">,</span> <span class="n">row_group_num_rows</span><span class="p">,</span> <span class="n">stats_min</span><span class="p">,</span> <span class="n">stats_max</span><span class="p">,</span> <span class="n">total_compressed_size</span> 
<span class="k">FROM</span> 
      <span class="n">parquet_metadata</span><span class="p">(</span><span class="s1">'hits.parquet'</span><span class="p">)</span>
<span class="k">WHERE</span> <span class="n">path_in_schema</span> <span class="o">=</span> <span class="s1">'"WatchID"'</span> 
<span class="k">LIMIT</span> <span class="mi">3</span><span class="p">;</span>

<span class="o">+</span><span class="c1">----------------+--------------+--------------------+---------------------+---------------------+-----------------------+</span>
<span class="o">|</span> <span class="n">path_in_schema</span> <span class="o">|</span> <span class="n">row_group_id</span> <span class="o">|</span> <span class="n">row_group_num_rows</span> <span class="o">|</span> <span class="n">stats_min</span>           <span class="o">|</span> <span class="n">stats_max</span>           <span class="o">|</span> <span class="n">total_compressed_size</span> <span class="o">|</span>
<span class="o">+</span><span class="c1">----------------+--------------+--------------------+---------------------+---------------------+-----------------------+</span>
<span class="o">|</span> <span class="nv">"WatchID"</span>      <span class="o">|</span> <span class="mi">0</span>            <span class="o">|</span> <span class="mi">450560</span>             <span class="o">|</span> <span class="mi">4611687214012840539</span> <span class="o">|</span> <span class="mi">9223369186199968220</span> <span class="o">|</span> <span class="mi">3883759</span>               <span class="o">|</span>
<span class="o">|</span> <span class="nv">"WatchID"</span>      <span class="o">|</span> <span class="mi">1</span>            <span class="o">|</span> <span class="mi">612174</span>             <span class="o">|</span> <span class="mi">4611689135232456464</span> <span class="o">|</span> <span class="mi">9223371478009085789</span> <span class="o">|</span> <span class="mi">5176803</span>               <span class="o">|</span>
<span class="o">|</span> <span class="nv">"WatchID"</span>      <span class="o">|</span> <span class="mi">2</span>            <span class="o">|</span> <span class="mi">344064</span>             <span class="o">|</span> <span class="mi">4611692774829951781</span> <span class="o">|</span> <span class="mi">9223363791697310021</span> <span class="o">|</span> <span class="mi">3031680</span>               <span class="o">|</span>
<span class="o">+</span><span class="c1">----------------+--------------+--------------------+---------------------+---------------------+-----------------------+</span>
<span class="mi">3</span> <span class="k">rows</span> <span class="k">in</span> <span class="k">set</span><span class="p">.</span> <span class="n">Query</span> <span class="n">took</span> <span class="mi">0</span><span class="p">.</span><span class="mi">053</span> <span class="n">seconds</span><span class="p">.</span>
</code></pre></div></div>

<h3 id="growth-of-datafusion-">Growth of DataFusion 📈</h3>
<p>DataFusion has been appearing more publically in the wild. For example</p>
<ul>
  <li>New projects built using DataFusion such as <a href="https://lancedb.com/">lancedb</a>, <a href="https://glaredb.com/">GlareDB</a>, <a href="https://www.arroyo.dev/">Arroyo</a>, and <a href="https://github.com/cmu-db/optd">optd</a>.</li>
  <li>Public talks such as <a href="https://www.youtube.com/watch?v=AJU9rdRNk9I">Apache Arrow Datafusion: Vectorized
Execution Framework For Maximum Performance</a> in <a href="https://www.bagevent.com/event/8432178">CommunityOverCode Asia 2023</a></li>
  <li>Blogs posts such as <a href="https://www.synnada.ai/blog/apache-arrow-arrow-datafusion-ai-native-data-infra-an-interview-with-our-ceo-ozan">Apache Arrow, Arrow/DataFusion, AI-native Data Infra</a>,
<a href="https://www.influxdata.com/blog/flight-datafusion-arrow-parquet-fdap-architecture-influxdb/">Flight, DataFusion, Arrow, and Parquet: Using the FDAP Architecture to build InfluxDB 3.0</a>, and 
<a href="https://www.linkedin.com/pulse/guide-user-defined-functions-apache-arrow-datafusion-dade-aderemi/">A Guide to User-Defined Functions in Apache Arrow DataFusion</a></li>
</ul>

<p>We have also <a href="https://github.com/apache/arrow-datafusion/issues/6782">submitted a paper</a> to <a href="https://2024.sigmod.org/">SIGMOD 2024</a>, one of the
premiere database conferences, describing DataFusion in a technically formal
style and making the case that it is possible to create a modular and extensive query engine 
without sacrificing performance. We hope this paper helps people 
evaluating DataFusion for their needs understand it better.</p>

<h1 id="datafusion-in-2024-">DataFusion in 2024 🥳</h1>

<p>Some major initiatives from contributors we know of this year are:</p>

<ol>
  <li>
    <p><em>Modularity</em>: Make DataFusion even more modular, such as <a href="https://github.com/apache/arrow-datafusion/issues/8045">unifying
built in and user functions</a>, making it easier to customize 
DataFusion’s behavior.</p>
  </li>
  <li>
    <p><em>Community Growth</em>: Graduate to our own top level Apache project, and
subsequently add more committers and PMC members to keep pace with project
growth.</p>
  </li>
  <li>
    <p><em>Use case white papers</em>: Write blog posts and videos explaining
how to use DataFusion for real-world use cases.</p>
  </li>
  <li>
    <p><em>Testing</em>: Improve CI infrastructure and test coverage, more fuzz
testing, and better functional and performance regression testing.</p>
  </li>
  <li>
    <p><em>Planning Time</em>: Reduce the time taken to plan queries, both <a href="https://github.com/apache/arrow-datafusion/issues/7698">wide
tables of 1000s of columns</a>, and in <a href="https://github.com/apache/arrow-datafusion/issues/5637">general</a>.</p>
  </li>
  <li>
    <p><em>Aggregate Performance</em>: Improve the speed of <a href="https://github.com/apache/arrow-datafusion/issues/7000">aggregating “high cardinality”</a> data
when there are many (e.g. millions) of distinct groups.</p>
  </li>
  <li>
    <p><em>Statistics</em>: <a href="https://github.com/apache/arrow-datafusion/issues/8227">Improved statistics handling</a> with an eye towards more
sophisticated expression analysis and cost models.</p>
  </li>
</ol>

<h1 id="how-to-get-involved">How to Get Involved</h1>

<p>If you are interested in contributing to DataFusion we would love to have you
join us. You can try out DataFusion on some of your own data and projects and
let us know how it goes, contribute suggestions, documentation, bug reports, or
a PR with documentation, tests or code. A list of open issues
suitable for beginners is <a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a>.</p>

<p>As the community grows, we are also looking to restart biweekly calls /
meetings. Timezones are always a challenge for such meetings, but we hope to
have two calls that can work for most attendees. If you are interested
in helping, or just want to say hi, please drop us a note via one of 
the methods listed in our <a href="https://arrow.apache.org/datafusion/contributor-guide/communication.html">Communication Doc</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[Introduction We recently released DataFusion 34.0.0. This blog highlights some of the major improvements since we released DataFusion 26.0.0 (spoiler alert there are many) and a preview of where the community plans to focus in the next 6 months. Apache Arrow DataFusion is an extensible query engine, written in Rust, that uses Apache Arrow as its in-memory format. DataFusion is used by developers to create new, fast data centric systems such as databases, dataframe libraries, machine learning and streaming applications. While DataFusion’s primary design goal is to accelerate creating other data centric systems, it has a reasonable experience directly out of the box as a dataframe library and command line SQL tool. This may also be our last update on the Apache Arrow Site. Future updates will likely be on the DataFusion website as we are working to graduate to a top level project (Apache Arrow DataFusion → Apache DataFusion!) which will help focus governance and project growth. Also exciting, our first DataFusion in person meetup is planned for March 2024. DataFusion is very much a community endeavor. Our core thesis is that as a community we can build much more advanced technology than any of us as individuals or companies could alone. In the last 6 months between 26.0.0 and 34.0.0, community growth has been strong. We accepted and reviewed over a thousand PRs from 124 different committers, created over 650 issues and closed 517 of them. You can find a list of all changes in the detailed CHANGELOG. Improved Performance 🚀 Performance is a key feature of DataFusion, DataFusion is more than 2x faster on ClickBench compared to version 25.0.0, as shown below: Figure 1: Performance improvement between 25.0.0 and 34.0.0 on ClickBench. Note that DataFusion 25.0.0, could not run several queries due to unsupported SQL (Q9, Q11, Q12, Q14) or memory requirements (Q33). Figure 2: Total query runtime for DataFusion 34.0.0 and DataFusion 25.0.0. Here are some specific enhancements we have made to improve performance: 2-3x better aggregation performance with many distinct groups Partially ordered grouping / streaming grouping Specialized operator for “TopK” ORDER BY LIMIT XXX Specialized operator for min(col) GROUP BY .. ORDER by min(col) LIMIT XXX Improved join performance Eliminate redundant sorting with sort order aware optimizers New Features ✨ DML / Insert / Creating Files DataFusion now supports writing data in parallel, to individual or multiple files, using Parquet, CSV, JSON, ARROW and user defined formats. Benchmark results show improvements up to 5x in some cases. Similarly to reading, data can now be written to any ObjectStore implementation, including AWS S3, Azure Blob Storage, GCP Cloud Storage, local files, and user defined implementations. While reading from hive style partitioned tables has long been supported, it is now possible to write to such tables as well. For example, to write to a local file: ❯ CREATE EXTERNAL TABLE awesome_table(x INT) STORED AS PARQUET LOCATION '/tmp/my_awesome_table'; 0 rows in set. Query took 0.003 seconds. ❯ INSERT INTO awesome_table SELECT x * 10 FROM my_source_table; +-------+ | count | +-------+ | 3 | +-------+ 1 row in set. Query took 0.024 seconds. You can also write to files with the COPY, similarly to DuckDB’s COPY: ❯ COPY (SELECT x + 1 FROM my_source_table) TO '/tmp/output.json'; +-------+ | count | +-------+ | 3 | +-------+ 1 row in set. Query took 0.014 seconds. $ cat /tmp/output.json {"x":1} {"x":2} {"x":3} Improved STRUCT and ARRAY support DataFusion 34.0.0 has much improved STRUCT and ARRAY support, including a full range of struct functions and array functions. For example, you can now use [] syntax and array_length to access and inspect arrays: ❯ SELECT column1, column1[1] AS first_element, array_length(column1) AS len FROM my_table; +-----------+---------------+-----+ | column1 | first_element | len | +-----------+---------------+-----+ | [1, 2, 3] | 1 | 3 | | [2] | 2 | 1 | | [4, 5] | 4 | 2 | +-----------+---------------+-----+ ❯ SELECT column1, column1['c0'] FROM my_table; +------------------+----------------------+ | column1 | my_table.column1[c0] | +------------------+----------------------+ | {c0: foo, c1: 1} | foo | | {c0: bar, c1: 2} | bar | +------------------+----------------------+ 2 rows in set. Query took 0.002 seconds. Other Features Other notable features include: Support aggregating datasets that exceed memory size, with group by spill to disk All operators now track and limit their memory consumption, including Joins Building Systems is Easier with DataFusion 🛠️ Documentation It is easier than ever to get started using DataFusion with the new Library Users Guide as well as significantly improved the API documentation. User Defined Window and Table Functions In addition to DataFusion’s User Defined Scalar Functions, and User Defined Aggregate Functions, DataFusion now supports User Defined Window Functions and User Defined Table Functions. For example, the datafusion-cli implements a DuckDB style parquet_metadata function as a user defined table function (source code here): ❯ SELECT path_in_schema, row_group_id, row_group_num_rows, stats_min, stats_max, total_compressed_size FROM parquet_metadata('hits.parquet') WHERE path_in_schema = '"WatchID"' LIMIT 3; +----------------+--------------+--------------------+---------------------+---------------------+-----------------------+ | path_in_schema | row_group_id | row_group_num_rows | stats_min | stats_max | total_compressed_size | +----------------+--------------+--------------------+---------------------+---------------------+-----------------------+ | "WatchID" | 0 | 450560 | 4611687214012840539 | 9223369186199968220 | 3883759 | | "WatchID" | 1 | 612174 | 4611689135232456464 | 9223371478009085789 | 5176803 | | "WatchID" | 2 | 344064 | 4611692774829951781 | 9223363791697310021 | 3031680 | +----------------+--------------+--------------------+---------------------+---------------------+-----------------------+ 3 rows in set. Query took 0.053 seconds. Growth of DataFusion 📈 DataFusion has been appearing more publically in the wild. For example New projects built using DataFusion such as lancedb, GlareDB, Arroyo, and optd. Public talks such as Apache Arrow Datafusion: Vectorized Execution Framework For Maximum Performance in CommunityOverCode Asia 2023 Blogs posts such as Apache Arrow, Arrow/DataFusion, AI-native Data Infra, Flight, DataFusion, Arrow, and Parquet: Using the FDAP Architecture to build InfluxDB 3.0, and A Guide to User-Defined Functions in Apache Arrow DataFusion We have also submitted a paper to SIGMOD 2024, one of the premiere database conferences, describing DataFusion in a technically formal style and making the case that it is possible to create a modular and extensive query engine without sacrificing performance. We hope this paper helps people evaluating DataFusion for their needs understand it better. DataFusion in 2024 🥳 Some major initiatives from contributors we know of this year are: Modularity: Make DataFusion even more modular, such as unifying built in and user functions, making it easier to customize DataFusion’s behavior. Community Growth: Graduate to our own top level Apache project, and subsequently add more committers and PMC members to keep pace with project growth. Use case white papers: Write blog posts and videos explaining how to use DataFusion for real-world use cases. Testing: Improve CI infrastructure and test coverage, more fuzz testing, and better functional and performance regression testing. Planning Time: Reduce the time taken to plan queries, both wide tables of 1000s of columns, and in general. Aggregate Performance: Improve the speed of aggregating “high cardinality” data when there are many (e.g. millions) of distinct groups. Statistics: Improved statistics handling with an eye towards more sophisticated expression analysis and cost models. How to Get Involved If you are interested in contributing to DataFusion we would love to have you join us. You can try out DataFusion on some of your own data and projects and let us know how it goes, contribute suggestions, documentation, bug reports, or a PR with documentation, tests or code. A list of open issues suitable for beginners is here. As the community grows, we are also looking to restart biweekly calls / meetings. Timezones are always a challenge for such meetings, but we hope to have two calls that can work for most attendees. If you are interested in helping, or just want to say hi, please drop us a note via one of the methods listed in our Communication Doc.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 15.0.0 Release</title><link href="https://arrow.apache.org/blog/2024/01/10/15.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 15.0.0 Release" /><published>2024-01-10T00:00:00-05:00</published><updated>2024-01-10T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2024/01/10/15.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2024/01/10/15.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 15.0.0 release. This covers
over 3 months of development work and includes <a href="https://github.com/apache/arrow/milestone/56?closed=1"><strong>344 resolved issues</strong></a>
on <a href="/release/15.0.0.html#contributors"><strong>536 distinct commits</strong></a> from <a href="/release/15.0.0.html#contributors"><strong>101 distinct contributors</strong></a>.
See the <a href="https://arrow.apache.org/install/">Install Page</a>
to learn how to get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/15.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 14.0.0 release, Curt Hagenlocher, Xuwei Fu, James Duong and Felipe Oliveira Carvalho
have been invited to be committers.
Jonathan Keane and Raúl Cumplido have joined the Project Management Committee (PMC).</p>

<p>As per our tradition of rotating the PMC chair once a year
Andy Grove was elected as the new PMC chair and VP.</p>

<p>Thanks for your contributions and participation in the project!</p>

<h2 id="c-data-interface-notes">C Data Interface notes</h2>

<p>New format strings have been added for ListView, LargeListView, BinaryView and StringView array types.</p>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>

<p>Flight SQL is now considered stable (<a href="https://github.com/apache/arrow/issues/39037">GH-39037</a>). The Flight SQL specification was clarified regarding how the result set schema of a prepared statement is affected by bound parameters (<a href="https://github.com/apache/arrow/issues/37061">GH-37061</a>).</p>

<p>The JDBC Arrow Flight SQL driver now supports mTLS authentication (<a href="https://github.com/apache/arrow/issues/38460">GH-38460</a>) and bind parameters (<a href="https://github.com/apache/arrow/issues/33475">GH-33475</a>), follows the Flight RPC spec when fetching data (<a href="https://github.com/apache/arrow/issues/34532">GH-34532</a>), and can reuse credentials across metadata and data connections (<a href="https://github.com/apache/arrow/issues/38576">GH-38576</a>). On macOS it will also use the system keychain to be consistent with other platforms (<a href="https://github.com/apache/arrow/issues/39014">GH-39014</a>). Applications can also retrieve the underlying Flight RPC metadata from the JDBC driver (GH-38024, GH-38022).</p>

<h2 id="c-notes">C++ notes</h2>

<p>For C++ notes refer to the full changelog.</p>

<h3 id="parquet">Parquet</h3>

<h4 id="new-features">New features:</h4>
<ul>
  <li>Support row group filtering for nested paths for C++ and Parquet (<a href="https://github.com/apache/arrow/issues/39064">GH-39064</a>)</li>
  <li>Implement Parquet Float16 logical type (<a href="https://github.com/apache/arrow/issues/36036">GH-36036</a>)</li>
  <li>Expose sorting_columns in RowGroupMetaData for Parquet files (<a href="https://github.com/apache/arrow/issues/35331">GH-35331</a>)</li>
  <li>Support decompressing concatenated gzip members (stream) (<a href="https://github.com/apache/arrow/issues/38271">GH-38271</a>)</li>
</ul>

<h4 id="api-change">API change:</h4>
<ul>
  <li>Move EstimatedBufferedValueBytes from TypedColumnWriter to ColumnWriter (<a href="https://github.com/apache/arrow/issues/38887">GH-38887</a>)</li>
  <li>Change parquet TypedComparator operation to const methods (<a href="https://github.com/apache/arrow/issues/38874">GH-38874</a>)</li>
  <li>Remove deprecated AppendRowGroup(int64_t num_rows) (<a href="https://github.com/apache/arrow/issues/39208">GH-39208</a>)</li>
  <li>Add api to get RecordReader from RowGroupReader (<a href="https://github.com/apache/arrow/issues/37002">GH-37002</a>)</li>
</ul>

<h4 id="bug-fixes">Bug fixes:</h4>
<ul>
  <li>Add more closed file checks for ParquetFileWriter to Prevent from used-after-close (<a href="https://github.com/apache/arrow/issues/38390">GH-38390</a>)</li>
</ul>

<h4 id="performance-enhancement">Performance enhancement:</h4>
<ul>
  <li>Faster Scalar BYTE_STREAM_SPLIT encoding/decoding (<a href="https://github.com/apache/arrow/issues/38542">GH-38542</a>)</li>
  <li>Faster reading Parquet FLBA (GH-39124, GH-39413)</li>
  <li>Using bloom_filter_length in parquet 2.10 to optimize bloom filter read (<a href="https://github.com/apache/arrow/issues/38860">GH-38860</a>)</li>
</ul>

<h3 id="miscellaneous">Miscellaneous</h3>

<ul>
  <li>Upgrade ORC to 1.9.2 (<a href="https://github.com/apache/arrow/issues/39430">GH-39340</a>)</li>
</ul>

<h2 id="c-notes-1">C# notes</h2>

<p>Removal of build targets:</p>
<ul>
  <li>Remove out-of-support versions of .NET and update C# README <a href="https://github.com/apache/arrow/pull/39165">GH-31579</a></li>
</ul>

<p>New features:</p>
<ul>
  <li>Better support for decimal values which exceed the range of the BCL’s System.Decimal <a href="https://github.com/apache/arrow/pull/38481">GH-38351</a>, <a href="https://github.com/apache/arrow/pull/38508">GH-38483</a></li>
  <li>Expose ArrayDataConcentrator.Concatenate publicly <a href="https://github.com/apache/arrow/pull/38154">GH-38153</a></li>
  <li>Add ToString methods to Arrow classes <a href="https://github.com/apache/arrow/pull/30717">GH-36566</a></li>
  <li>Implement common interfaces for structure arrays and record batches <a href="https://github.com/apache/arrow/pull/38759">GH-38757</a></li>
  <li>Make primitive arrays support IReadOnlyList&lt;T?&gt; <a href="https://github.com/apache/arrow/pull/38680">GH-38348</a>, <a href="https://github.com/apache/arrow/pull/39224">GH-39223</a></li>
  <li>Add ToList to Decimal128Array and Decimal256Array <a href="https://github.com/apache/arrow/pull/37383">GH-37359</a></li>
  <li>Support additional types Interval, Utf8View, BinaryView and ListView <a href="https://github.com/apache/arrow/pull/39043">GH-38316</a>, <a href="https://github.com/apache/arrow/pull/39342">GH-39341</a></li>
  <li>Support creating FlightClient with Grpc.Core.Channel <a href="https://github.com/apache/arrow/pull/39348">GH-39335</a></li>
</ul>

<p>Fixes and improved compatibility:</p>
<ul>
  <li>Make dictionaries in file and memory implementations work correctly and support integration tests <a href="https://github.com/apache/arrow/pull/39146">GH-32662</a></li>
  <li>Support blank column names and enable more integration tests <a href="https://github.com/apache/arrow/pull/39167">GH-36588</a></li>
</ul>

<h2 id="go-notes">Go Notes</h2>

<h3 id="bug-fixes-1">Bug Fixes</h3>

<h4 id="arrow">Arrow</h4>

<ul>
  <li>Ensured reliability of AuthenticateBasicToken behind proxies (<a href="https://github.com/apache/arrow/issues/38198">GH-38198</a>)</li>
  <li>Ensured release callback is properly called on C Data imported arrays/batches (<a href="https://github.com/apache/arrow/issues/38281">GH-38281</a>)</li>
  <li>Fixed rounding errors in decimal256 string functions (<a href="https://github.com/apache/arrow/issues/38395">GH-38395</a>)</li>
  <li>Added <code class="language-plaintext highlighter-rouge">ValueLen</code> to Binary and String array interface (<a href="https://github.com/apache/arrow/issues/38458">GH-38458</a>)</li>
  <li>Fixed Decimal128 rounding issues (<a href="https://github.com/apache/arrow/issues/38477">GH-38477</a>)</li>
  <li>Fixed memory leak in IPC LZ4 decompressor (<a href="https://github.com/apache/arrow/issues/38728">GH-38728</a>)</li>
  <li>Addressed Data race in <code class="language-plaintext highlighter-rouge">GetToTimeFunc</code> for fixed timestamp data types (<a href="https://github.com/apache/arrow/issues/38795">GH-38795</a>)</li>
  <li>Fixed “index out of range” error for empty resultsets of FlightSQL driver (<a href="https://github.com/apache/arrow/issues/39238">GH-39238</a>)</li>
</ul>

<h4 id="parquet-1">Parquet</h4>

<ul>
  <li>Fixed issue with max definition levels when writing a Parquet file under certain circumstances (<a href="https://github.com/apache/arrow/issues/38503">GH-38503</a>)</li>
  <li>File writer now properly tracks the number of rows written beyond the last row group (<a href="https://github.com/apache/arrow/issues/38516">GH-38516</a>)</li>
</ul>

<h3 id="enhancements">Enhancements</h3>

<h4 id="arrow-1">Arrow</h4>

<ul>
  <li>Added an Avro OCF reader for converting Avro files directly to Arrow record batches (<a href="https://github.com/apache/arrow/issues/36760">GH-36760</a>)</li>
  <li>Added support for StringView (<a href="https://github.com/apache/arrow/issues/38718">GH-38718</a>) and C Data ABI StringViews (<a href="https://github.com/apache/arrow/issues/39013">GH-39013</a>)</li>
  <li>GC Checks were enabled for CI running integration tests (<a href="https://github.com/apache/arrow/issues/38824">GH-38824</a>)</li>
</ul>

<h4 id="parquet-2">Parquet</h4>

<ul>
  <li>Implemented Float16 logical type for Parquet files (<a href="https://github.com/apache/arrow/issues/37582">GH-37582</a>)</li>
  <li>Added proper boolean RLE encoding/decoding (<a href="https://github.com/apache/arrow/issues/38462">GH-38462</a>)</li>
</ul>

<h3 id="bug-fixes-2">Bug Fixes</h3>

<h3 id="enhancements-1">Enhancements</h3>

<h2 id="java-notes">Java notes</h2>

<p><strong>We expect a breaking change in the next release, Arrow 16.0.0.</strong> Support for Java 9 modules is coming, but that will require changing the JVM flags used to launch your application (<a href="https://github.com/apache/arrow/issues/38998">GH-38998</a>).  Arrow 15.0.0 is not affected.</p>

<p>A bill-of-materials (BOM) package was added to make it easier to depend on multiple Arrow libraries (<a href="https://github.com/apache/arrow/issues/38264">GH-38264</a>).</p>

<p>The JDBC adapter (separate from the JDBC driver) now supports 256-bit decimals (<a href="https://github.com/apache/arrow/issues/39484">GH-39484</a>) and throws more informative exceptions (<a href="https://github.com/apache/arrow/issues/39355">GH-39355</a>).</p>

<p>Various improvements were made to utilities for working with vectors (GH-38662, GH-38614, GH-38511, GH-38254, GH-38246).</p>

<h2 id="javascript-notes">JavaScript notes</h2>

<p>This release comes with new features and APIs. We also removed <code class="language-plaintext highlighter-rouge">getByteLength</code> to reduce bundle sizes.</p>

<p>New Features with API changes</p>
<ul>
  <li><a href="https://github.com/apache/arrow/pull/39018">GH-39017: [JS] Add typeId as attribute</a></li>
  <li><a href="https://github.com/apache/arrow/pull/39258">GH-39257: [JS] LargeBinary</a></li>
  <li><a href="https://github.com/apache/arrow/pull/35780">GH-15060: [JS] Add LargeUtf8 type</a></li>
  <li><a href="https://github.com/apache/arrow/pull/39260">GH-39259: [JS] Remove getByteLength</a></li>
  <li><a href="https://github.com/apache/arrow/pull/39436">GH-39435: [JS] Add Vector.nullable</a></li>
  <li><a href="https://github.com/apache/arrow/pull/39256">GH-39255: [JS] Allow customization of schema when passing vectors to table constructor</a></li>
  <li><a href="https://github.com/apache/arrow/pull/39254">GH-37983: [JS] Allow nullable fields in table when constructed from vector with nulls</a></li>
</ul>

<p>Package changes</p>
<ul>
  <li><a href="https://github.com/apache/arrow/pull/39475">GH-39289: [JS] Add types to exports</a></li>
</ul>

<h2 id="python-notes">Python notes</h2>

<p>Compatibility notes:</p>
<ul>
  <li>Legacy <code class="language-plaintext highlighter-rouge">ParquetDataset</code> custom implementation has been removed and only the new dataset  API is now in use <a href="https://github.com/apache/arrow/issues/31303">GH-31303</a>.</li>
</ul>

<p>New features:</p>
<ul>
  <li>PyArrow version 14.0.0 included a new specification for Arrow PyCapsules and related dunder methods <a href="https://github.com/apache/arrow/pull/37797">GH-35531</a> and now a public <code class="language-plaintext highlighter-rouge">RecordBatchReader</code> constructor from stream object implementing the PyCapsule Protocol has been added <a href="https://github.com/apache/arrow/issues/39217">GH-[39217](https://github.com/apache/arrow/issues/39217)</a> together with some additional documentation <a href="https://github.com/apache/arrow/issues/39196">GH-[39196](https://github.com/apache/arrow/issues/39196)</a>.</li>
  <li>DLPack protocol support (producer) was added to the Arrow C++ and is exposed in Python through <code class="language-plaintext highlighter-rouge">__dlpack__</code> and <code class="language-plaintext highlighter-rouge">__dlpack_device__</code> dunder methods  <a href="https://github.com/apache/arrow/issues/33984">GH-33984</a>.</li>
  <li>Python now exposes enabling CRC checksum for read and write operations in Paquet <a href="https://github.com/apache/arrow/issues/37242">GH-37242</a>. CRC checksum are optional and can detect data corruption.</li>
  <li><code class="language-plaintext highlighter-rouge">CacheOptions</code> are now configurable from Python as part of the <code class="language-plaintext highlighter-rouge">pyarrow.dataset.ParquetFragmentScanOptions</code> <a href="https://github.com/apache/arrow/issues/36441">GH-36441</a>.</li>
  <li>Parquet metadata to indicate sort order of the data are now exposed in <code class="language-plaintext highlighter-rouge">RowGroupMetaData</code> <a href="https://github.com/apache/arrow/issues/35331">GH-35331</a>.</li>
  <li>Parquet Support write and validate Page CRC (<a href="https://github.com/apache/arrow/issues/37242">GH-37242</a>)</li>
</ul>

<p>Other improvements:</p>
<ul>
  <li>Append parameter from <code class="language-plaintext highlighter-rouge">FileOutputStream</code> is exposed for the <code class="language-plaintext highlighter-rouge">OSFile</code> class <a href="https://github.com/apache/arrow/issues/38857">GH-38857</a>.</li>
  <li>File size can be passed to <code class="language-plaintext highlighter-rouge">make_fragment</code> in the pyarrow datasets (<code class="language-plaintext highlighter-rouge">pyarrow.dataset.FileFormat</code>and <code class="language-plaintext highlighter-rouge">pyarrow.dataset.ParquetFileFormat</code>) <a href="https://github.com/apache/arrow/issues/37857">GH-37857</a>.</li>
  <li>Support for mask parameter is added to <code class="language-plaintext highlighter-rouge">FixedSizeListArray.from_arrays</code> <a href="https://github.com/apache/arrow/issues/34316">GH-34316</a></li>
  <li><code class="language-plaintext highlighter-rouge">to/from_struct_array</code> are added to the <code class="language-plaintext highlighter-rouge">pyarrow.Table</code> class <a href="https://github.com/apache/arrow/issues/33500">GH-33500</a>.</li>
  <li>GIL is released in <code class="language-plaintext highlighter-rouge">.nbytes</code> which is improving performance when calculating the data size <a href="https://github.com/apache/arrow/issues/39096">GH-39096</a>.</li>
  <li>Usage of pandas internals <code class="language-plaintext highlighter-rouge">DatetimeTZBlock</code> has been removed <a href="https://github.com/apache/arrow/issues/38341">GH-38341</a>.</li>
  <li><code class="language-plaintext highlighter-rouge">DataType</code> instance can be passed to <code class="language-plaintext highlighter-rouge">MapType.from_arrays</code> constructor <a href="https://github.com/apache/arrow/issues/39515">GH-39515</a>.</li>
</ul>

<p>Relevant bug fixes:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">S3FileSystem</code> equals <code class="language-plaintext highlighter-rouge">None</code> segfault has been fixed <a href="https://github.com/apache/arrow/issues/38535">GH-38535</a>.</li>
  <li>No-op kernel is added for <code class="language-plaintext highlighter-rouge">dictionary_encode(dictionary)</code> <a href="https://github.com/apache/arrow/issues/34890">GH-34890</a>.</li>
  <li>PrettyPrint for Timestamp type now adds “Z” at the end of the print string when tz is defined in order to add minimum information about the values being stored in UTC <a href="https://github.com/apache/arrow/issues/30117">GH-30117</a>.</li>
</ul>

<h2 id="r-notes">R notes</h2>

<h3 id="new-features-1">New features:</h3>

<ul>
  <li>Bindings for <code class="language-plaintext highlighter-rouge">base::prod</code> have been added so you can now use it in your dplyr pipelines (i.e., <code class="language-plaintext highlighter-rouge">tbl |&gt; summarize(prod(col))</code>) without having to pull the data into R <a href="https://github.com/apache/arrow/pull/38601">GH-38601</a>.</li>
  <li>Calling <code class="language-plaintext highlighter-rouge">dimnames</code> or <code class="language-plaintext highlighter-rouge">colnames</code> on <code class="language-plaintext highlighter-rouge">Dataset</code> objects now returns a useful result rather than just <code class="language-plaintext highlighter-rouge">NULL</code> <a href="https://github.com/apache/arrow/pull/38377">GH-38377</a>.</li>
  <li>The <code class="language-plaintext highlighter-rouge">code()</code> method on Schema objects now takes an optional <code class="language-plaintext highlighter-rouge">namespace</code> argument which, when <code class="language-plaintext highlighter-rouge">TRUE</code>, prefixes names with <code class="language-plaintext highlighter-rouge">arrow::</code> which makes the output more portable <a href="https://github.com/apache/arrow/pull/38144">GH-38144</a>.</li>
</ul>

<h3 id="other-improvements">Other improvements:</h3>

<ul>
  <li>To make debugging problems easier when using arrow with AWS S3 (e..g, <code class="language-plaintext highlighter-rouge">s3_bucket</code>, <code class="language-plaintext highlighter-rouge">S3FileSystem</code>), the debug log level for S3 can be set with the <code class="language-plaintext highlighter-rouge">AWS_S3_LOG_LEVEL</code> environment variable. See <code class="language-plaintext highlighter-rouge">?S3FileSystem</code> for more information. <a href="https://github.com/apache/arrow/pull/38267">GH-38267</a></li>
  <li>An error is now thrown instead of warning and pulling the data into R when any of <code class="language-plaintext highlighter-rouge">sub</code>, <code class="language-plaintext highlighter-rouge">gsub</code>, <code class="language-plaintext highlighter-rouge">stringr::str_replace</code>, <code class="language-plaintext highlighter-rouge">stringr::str_replace_all</code> are passed a length &gt; 1 vector of values in <code class="language-plaintext highlighter-rouge">pattern</code> <a href="https://github.com/apache/arrow/pull/39219">GH-39219</a>.</li>
  <li>Missing documentation was added to <code class="language-plaintext highlighter-rouge">?open_dataset</code> documenting how to use the ND-JSON support added in arrow 13.0.0 <a href="https://github.com/apache/arrow/pull/38258">GH-38258</a>.</li>
  <li>Using arrow with duckdb (i.e., <code class="language-plaintext highlighter-rouge">to_duckdb()</code>) no longer results in warnings when quitting your R session. <a href="https://github.com/apache/arrow/pull/38495">GH-38495</a></li>
</ul>

<p>For more on what’s in the 15.0.0 R package, see the <a href="/docs/r/news/">R changelog</a>.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<h3 id="ruby">Ruby</h3>

<ul>
  <li>Add <code class="language-plaintext highlighter-rouge">Arrow::Table#each_raw_record</code> and <code class="language-plaintext highlighter-rouge">Arrow::RecordBatch#each_raw_record</code> (<a href="https://github.com/apache/arrow/issues/37137">GH-37137</a>, <a href="https://github.com/apache/arrow/issues/37600">GH-37600</a>)</li>
</ul>

<h3 id="c-glib">C GLib</h3>

<ul>
  <li>Follow C++ changes.</li>
</ul>

<h2 id="rust-notes">Rust notes</h2>

<p>The Rust projects have moved to separate repositories outside the
main Arrow monorepo. For notes on the latest release of the Rust
implementation, see the latest <a href="https://github.com/apache/arrow-rs/tags">Arrow Rust changelog</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 15.0.0 release. This covers over 3 months of development work and includes 344 resolved issues on 536 distinct commits from 101 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 14.0.0 release, Curt Hagenlocher, Xuwei Fu, James Duong and Felipe Oliveira Carvalho have been invited to be committers. Jonathan Keane and Raúl Cumplido have joined the Project Management Committee (PMC). As per our tradition of rotating the PMC chair once a year Andy Grove was elected as the new PMC chair and VP. Thanks for your contributions and participation in the project! C Data Interface notes New format strings have been added for ListView, LargeListView, BinaryView and StringView array types. Arrow Flight RPC notes Flight SQL is now considered stable (GH-39037). The Flight SQL specification was clarified regarding how the result set schema of a prepared statement is affected by bound parameters (GH-37061). The JDBC Arrow Flight SQL driver now supports mTLS authentication (GH-38460) and bind parameters (GH-33475), follows the Flight RPC spec when fetching data (GH-34532), and can reuse credentials across metadata and data connections (GH-38576). On macOS it will also use the system keychain to be consistent with other platforms (GH-39014). Applications can also retrieve the underlying Flight RPC metadata from the JDBC driver (GH-38024, GH-38022). C++ notes For C++ notes refer to the full changelog. Parquet New features: Support row group filtering for nested paths for C++ and Parquet (GH-39064) Implement Parquet Float16 logical type (GH-36036) Expose sorting_columns in RowGroupMetaData for Parquet files (GH-35331) Support decompressing concatenated gzip members (stream) (GH-38271) API change: Move EstimatedBufferedValueBytes from TypedColumnWriter to ColumnWriter (GH-38887) Change parquet TypedComparator operation to const methods (GH-38874) Remove deprecated AppendRowGroup(int64_t num_rows) (GH-39208) Add api to get RecordReader from RowGroupReader (GH-37002) Bug fixes: Add more closed file checks for ParquetFileWriter to Prevent from used-after-close (GH-38390) Performance enhancement: Faster Scalar BYTE_STREAM_SPLIT encoding/decoding (GH-38542) Faster reading Parquet FLBA (GH-39124, GH-39413) Using bloom_filter_length in parquet 2.10 to optimize bloom filter read (GH-38860) Miscellaneous Upgrade ORC to 1.9.2 (GH-39340) C# notes Removal of build targets: Remove out-of-support versions of .NET and update C# README GH-31579 New features: Better support for decimal values which exceed the range of the BCL’s System.Decimal GH-38351, GH-38483 Expose ArrayDataConcentrator.Concatenate publicly GH-38153 Add ToString methods to Arrow classes GH-36566 Implement common interfaces for structure arrays and record batches GH-38757 Make primitive arrays support IReadOnlyList&lt;T?&gt; GH-38348, GH-39223 Add ToList to Decimal128Array and Decimal256Array GH-37359 Support additional types Interval, Utf8View, BinaryView and ListView GH-38316, GH-39341 Support creating FlightClient with Grpc.Core.Channel GH-39335 Fixes and improved compatibility: Make dictionaries in file and memory implementations work correctly and support integration tests GH-32662 Support blank column names and enable more integration tests GH-36588 Go Notes Bug Fixes Arrow Ensured reliability of AuthenticateBasicToken behind proxies (GH-38198) Ensured release callback is properly called on C Data imported arrays/batches (GH-38281) Fixed rounding errors in decimal256 string functions (GH-38395) Added ValueLen to Binary and String array interface (GH-38458) Fixed Decimal128 rounding issues (GH-38477) Fixed memory leak in IPC LZ4 decompressor (GH-38728) Addressed Data race in GetToTimeFunc for fixed timestamp data types (GH-38795) Fixed “index out of range” error for empty resultsets of FlightSQL driver (GH-39238) Parquet Fixed issue with max definition levels when writing a Parquet file under certain circumstances (GH-38503) File writer now properly tracks the number of rows written beyond the last row group (GH-38516) Enhancements Arrow Added an Avro OCF reader for converting Avro files directly to Arrow record batches (GH-36760) Added support for StringView (GH-38718) and C Data ABI StringViews (GH-39013) GC Checks were enabled for CI running integration tests (GH-38824) Parquet Implemented Float16 logical type for Parquet files (GH-37582) Added proper boolean RLE encoding/decoding (GH-38462) Bug Fixes Enhancements Java notes We expect a breaking change in the next release, Arrow 16.0.0. Support for Java 9 modules is coming, but that will require changing the JVM flags used to launch your application (GH-38998). Arrow 15.0.0 is not affected. A bill-of-materials (BOM) package was added to make it easier to depend on multiple Arrow libraries (GH-38264). The JDBC adapter (separate from the JDBC driver) now supports 256-bit decimals (GH-39484) and throws more informative exceptions (GH-39355). Various improvements were made to utilities for working with vectors (GH-38662, GH-38614, GH-38511, GH-38254, GH-38246). JavaScript notes This release comes with new features and APIs. We also removed getByteLength to reduce bundle sizes. New Features with API changes GH-39017: [JS] Add typeId as attribute GH-39257: [JS] LargeBinary GH-15060: [JS] Add LargeUtf8 type GH-39259: [JS] Remove getByteLength GH-39435: [JS] Add Vector.nullable GH-39255: [JS] Allow customization of schema when passing vectors to table constructor GH-37983: [JS] Allow nullable fields in table when constructed from vector with nulls Package changes GH-39289: [JS] Add types to exports Python notes Compatibility notes: Legacy ParquetDataset custom implementation has been removed and only the new dataset API is now in use GH-31303. New features: PyArrow version 14.0.0 included a new specification for Arrow PyCapsules and related dunder methods GH-35531 and now a public RecordBatchReader constructor from stream object implementing the PyCapsule Protocol has been added GH-[39217](https://github.com/apache/arrow/issues/39217) together with some additional documentation GH-[39196](https://github.com/apache/arrow/issues/39196). DLPack protocol support (producer) was added to the Arrow C++ and is exposed in Python through __dlpack__ and __dlpack_device__ dunder methods GH-33984. Python now exposes enabling CRC checksum for read and write operations in Paquet GH-37242. CRC checksum are optional and can detect data corruption. CacheOptions are now configurable from Python as part of the pyarrow.dataset.ParquetFragmentScanOptions GH-36441. Parquet metadata to indicate sort order of the data are now exposed in RowGroupMetaData GH-35331. Parquet Support write and validate Page CRC (GH-37242) Other improvements: Append parameter from FileOutputStream is exposed for the OSFile class GH-38857. File size can be passed to make_fragment in the pyarrow datasets (pyarrow.dataset.FileFormatand pyarrow.dataset.ParquetFileFormat) GH-37857. Support for mask parameter is added to FixedSizeListArray.from_arrays GH-34316 to/from_struct_array are added to the pyarrow.Table class GH-33500. GIL is released in .nbytes which is improving performance when calculating the data size GH-39096. Usage of pandas internals DatetimeTZBlock has been removed GH-38341. DataType instance can be passed to MapType.from_arrays constructor GH-39515. Relevant bug fixes: S3FileSystem equals None segfault has been fixed GH-38535. No-op kernel is added for dictionary_encode(dictionary) GH-34890. PrettyPrint for Timestamp type now adds “Z” at the end of the print string when tz is defined in order to add minimum information about the values being stored in UTC GH-30117. R notes New features: Bindings for base::prod have been added so you can now use it in your dplyr pipelines (i.e., tbl |&gt; summarize(prod(col))) without having to pull the data into R GH-38601. Calling dimnames or colnames on Dataset objects now returns a useful result rather than just NULL GH-38377. The code() method on Schema objects now takes an optional namespace argument which, when TRUE, prefixes names with arrow:: which makes the output more portable GH-38144. Other improvements: To make debugging problems easier when using arrow with AWS S3 (e..g, s3_bucket, S3FileSystem), the debug log level for S3 can be set with the AWS_S3_LOG_LEVEL environment variable. See ?S3FileSystem for more information. GH-38267 An error is now thrown instead of warning and pulling the data into R when any of sub, gsub, stringr::str_replace, stringr::str_replace_all are passed a length &gt; 1 vector of values in pattern GH-39219. Missing documentation was added to ?open_dataset documenting how to use the ND-JSON support added in arrow 13.0.0 GH-38258. Using arrow with duckdb (i.e., to_duckdb()) no longer results in warnings when quitting your R session. GH-38495 For more on what’s in the 15.0.0 R package, see the R changelog. Ruby and C GLib notes Ruby Add Arrow::Table#each_raw_record and Arrow::RecordBatch#each_raw_record (GH-37137, GH-37600) C GLib Follow C++ changes. Rust notes The Rust projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Rust implementation, see the latest Arrow Rust changelog.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 0.9.0 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2024/01/08/adbc-0.9.0-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.9.0 (Libraries) Release" /><published>2024-01-08T00:00:00-05:00</published><updated>2024-01-08T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2024/01/08/adbc-0.9.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2024/01/08/adbc-0.9.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.9.0 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/13"><strong>34
resolved issues</strong></a> from <a href="#contributors"><strong>16 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version
0.9.0.  The <strong>API specification</strong> is versioned separately and is
at version 1.1.0.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.9.0/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>The C#/.NET implementation is gearing up for a proper release and has had
various bugfixes in the core libraries, the drivers, and the ADO.NET wrapper.</p>

<p>The Go implementation now supports executing more methods (like
<code class="language-plaintext highlighter-rouge">ConnectionCommit</code>) when importing a native C/C++ driver.</p>

<p>The PostgreSQL driver can now write decimal values and dictionary-encoded
string/bytestring.  Also, <code class="language-plaintext highlighter-rouge">AdbcConnectionGetTableSchema</code> was fixed to handle
the catalog and schema parameters correctly, and the driver now tries to
provide a row count for non-select queries.</p>

<p>The Python bindings now accept a PyArrow Dataset as a data source for bulk
ingestion.  The new <a href="https://arrow.apache.org/docs/format/CDataInterface/PyCapsuleInterface.html">PyCapsule interface</a> is supported.  It is
possible to build the Python packages via CMake, which simplifies handling the
mixed-C++-and-Python builds.</p>

<p>The R bindings reference-count objects to try to detect improper usage and
avoid crashes.</p>

<p>The Snowflake driver properly handles escaping and case sensitivity in various
metadata methods.  It also now supports <code class="language-plaintext highlighter-rouge">StatementExecuteSchema</code>.</p>

<p>The Javadoc API reference is now included in the documentation.</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.8.0..apache-arrow-adbc-0.9.0
    24	David Li
    10	Curt Hagenlocher
    10	Dewey Dunnington
     7	davidhcoe
     4	William Ayd
     3	Ryan Syed
     3	vleslief-ms
     2	Daniel Espinosa
     2	Joel Lubinitsky
     2	Sutou Kouhei
     1	AnithaPanduranganMS
     1	Joris Van den Bossche
     1	Matt Topol
     1	Ruoxuan Wang
     1	William
     1	rtadepalli
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>The next release may increase the minimum required C++ revision to C++17,
which will line up with the mainline Arrow project
(<a href="https://github.com/apache/arrow-adbc/issues/1431">GH-1431</a>).</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.9.0 release of the Apache Arrow ADBC libraries. This covers includes 34 resolved issues from 16 distinct contributors. This is a release of the libraries, which are at version 0.9.0. The API specification is versioned separately and is at version 1.1.0. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights The C#/.NET implementation is gearing up for a proper release and has had various bugfixes in the core libraries, the drivers, and the ADO.NET wrapper. The Go implementation now supports executing more methods (like ConnectionCommit) when importing a native C/C++ driver. The PostgreSQL driver can now write decimal values and dictionary-encoded string/bytestring. Also, AdbcConnectionGetTableSchema was fixed to handle the catalog and schema parameters correctly, and the driver now tries to provide a row count for non-select queries. The Python bindings now accept a PyArrow Dataset as a data source for bulk ingestion. The new PyCapsule interface is supported. It is possible to build the Python packages via CMake, which simplifies handling the mixed-C++-and-Python builds. The R bindings reference-count objects to try to detect improper usage and avoid crashes. The Snowflake driver properly handles escaping and case sensitivity in various metadata methods. It also now supports StatementExecuteSchema. The Javadoc API reference is now included in the documentation. Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.8.0..apache-arrow-adbc-0.9.0 24 David Li 10 Curt Hagenlocher 10 Dewey Dunnington 7 davidhcoe 4 William Ayd 3 Ryan Syed 3 vleslief-ms 2 Daniel Espinosa 2 Joel Lubinitsky 2 Sutou Kouhei 1 AnithaPanduranganMS 1 Joris Van den Bossche 1 Matt Topol 1 Ruoxuan Wang 1 William 1 rtadepalli Roadmap The next release may increase the minimum required C++ revision to C++17, which will line up with the mainline Arrow project (GH-1431). Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 14.0.2 Release</title><link href="https://arrow.apache.org/blog/2023/12/18/14.0.2-release/" rel="alternate" type="text/html" title="Apache Arrow 14.0.2 Release" /><published>2023-12-18T00:00:00-05:00</published><updated>2023-12-18T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/12/18/14.0.2-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/12/18/14.0.2-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 14.0.2 release.
This is mostly a bugfix release that includes <a href="https://github.com/apache/arrow/milestone/58?closed=1"><strong>33 resolved issues</strong></a>
from <a href="/release/14.0.2.html#contributors"><strong>11 distinct contributors</strong></a>. See the Install Page to learn how to
get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Other bugfixes and improvements have been made: we refer
you to the <a href="/release/14.0.2.html#changelog">complete changelog</a>.</p>

<h2 id="c-notes">C++ notes</h2>

<ul>
  <li>S3FileSystem: fix regression in deleting explicitly created sub-directories (<a href="https://github.com/apache/arrow/issues/38618">GH-38618</a>)</li>
</ul>

<h3 id="parquet">Parquet</h3>

<ul>
  <li>Fix performance regression in reading binary/string columns (<a href="https://github.com/apache/arrow/issues/38432">GH-38432</a>)</li>
  <li>Fix regression in reading Parquet files in parallel (<a href="https://github.com/apache/arrow/issues/38432">GH-38432</a>)</li>
  <li>Fix bug in reading valid Parquet files written with older versions (<a href="https://github.com/apache/arrow/issues/38577">GH-38577</a>)</li>
</ul>

<h2 id="python-notes">Python notes</h2>

<ul>
  <li>Delay initializing the S3 filesystem until first usage (instead on import) (<a href="https://github.com/apache/arrow/issues/38364">GH-38364</a>)</li>
  <li>Fix segfault when PyArrow is imported at shutdown (<a href="https://github.com/apache/arrow/issues/38626">GH-38626</a>)</li>
  <li>Fix potential deadlock when CSV reading errors out (<a href="https://github.com/apache/arrow/issues/38676">GH-38676</a>)</li>
</ul>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 14.0.2 release. This is mostly a bugfix release that includes 33 resolved issues from 11 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Other bugfixes and improvements have been made: we refer you to the complete changelog. C++ notes S3FileSystem: fix regression in deleting explicitly created sub-directories (GH-38618) Parquet Fix performance regression in reading binary/string columns (GH-38432) Fix regression in reading Parquet files in parallel (GH-38432) Fix bug in reading valid Parquet files written with older versions (GH-38577) Python notes Delay initializing the S3 filesystem until first usage (instead on import) (GH-38364) Fix segfault when PyArrow is imported at shutdown (GH-38626) Fix potential deadlock when CSV reading errors out (GH-38676)]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 14.0.1 Release</title><link href="https://arrow.apache.org/blog/2023/11/09/14.0.1-release/" rel="alternate" type="text/html" title="Apache Arrow 14.0.1 Release" /><published>2023-11-09T00:00:00-05:00</published><updated>2023-11-09T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/11/09/14.0.1-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/11/09/14.0.1-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 14.0.1 release of Apache Arrow.</p>

<p>This release contains a single security fix for PyArrow. Other implementations
are unchanged.</p>

<p>It is recommended that users of PyArrow upgrade to 14.0.1. Similarly, it is
recommended that downstream libraries upgrade their dependency requirements
to PyArrow 14.0.1 or later. Binary packages are available on PyPI and conda-forge.</p>

<p>If it is not possible to upgrade, we provide a separate package <code class="language-plaintext highlighter-rouge">pyarrow-hotfix</code>
that disables the vulnerability on older PyArrow versions.
See <a href="https://pypi.org/project/pyarrow-hotfix/">usage instructions</a>.</p>

<h2 id="vulnerability-description">Vulnerability description</h2>

<p>Deserialization of untrusted data in IPC and Parquet readers in PyArrow versions
0.14.0 to 14.0.0 allows arbitrary code execution. An application is vulnerable
if it reads Arrow IPC, Feather or Parquet data from untrusted sources
(for example user-supplied input files).</p>

<p>This vulnerability only affects PyArrow, not other Apache Arrow implementations or bindings.</p>

<p>Reference: <a href="https://www.cve.org/CVERecord?id=CVE-2023-47248">CVE-2023-47248</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 14.0.1 release of Apache Arrow. This release contains a single security fix for PyArrow. Other implementations are unchanged. It is recommended that users of PyArrow upgrade to 14.0.1. Similarly, it is recommended that downstream libraries upgrade their dependency requirements to PyArrow 14.0.1 or later. Binary packages are available on PyPI and conda-forge. If it is not possible to upgrade, we provide a separate package pyarrow-hotfix that disables the vulnerability on older PyArrow versions. See usage instructions. Vulnerability description Deserialization of untrusted data in IPC and Parquet readers in PyArrow versions 0.14.0 to 14.0.0 allows arbitrary code execution. An application is vulnerable if it reads Arrow IPC, Feather or Parquet data from untrusted sources (for example user-supplied input files). This vulnerability only affects PyArrow, not other Apache Arrow implementations or bindings. Reference: CVE-2023-47248.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 0.8.0 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/11/09/adbc-0.8.0-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.8.0 (Libraries) Release" /><published>2023-11-09T00:00:00-05:00</published><updated>2023-11-09T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/11/09/adbc-0.8.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/11/09/adbc-0.8.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.8.0 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/12"><strong>43
resolved issues</strong></a> from <a href="#contributors"><strong>12 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version
0.8.0.  The <strong>API specification</strong> is versioned separately and is
at version 1.1.0.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.8.0/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>C#/.NET has added a BigQuery driver. Also, it now can be used through ADO.NET.</p>

<p>The R bindings now use ADBC 1.1.0, and more packages are now available on
CRAN.</p>

<p>The Snowflake driver now has an option to control whether to return decimal
types or attempt to convert to integers/floats.  The PostgreSQL driver now
uses COPY for bulk ingestion, which Pandas has found is <a href="https://github.com/pandas-dev/pandas/pull/53869#issuecomment-1771657930">approximately 35x
faster</a> than their previous method.  The SQLite driver can now
load <a href="https://www.sqlite.org/loadext.html">extensions</a> and supports more data types, including
Arrow binary (SQL BLOB), and supports binding some dictionary-encoded types
(which will be unpacked).</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.7.0..apache-arrow-adbc-0.8.0
    27	David Li
    23	William Ayd
    19	Dewey Dunnington
     4	Matt Topol
     4	Solomon Choe
     4	davidhcoe
     2	vleslief-ms
     1	Aaron Ross
     1	Fredrik Hoem Grelland
     1	Joel Lubinitsky
     1	OleMussmann
     1	Ruoxuan Wang
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>Work continues on improving the existing drivers in terms of supported
Arrow/database types, performance, and so on.</p>

<p>For Java, the minimum JDK version will stay on JDK 8 as long as the Arrow Java
libraries remain on that version.</p>

<p>There are currently no plans for a second API revision. As work progresses
on asynchronous and device-aware APIs in the Arrow ecosystem, ADBC will
eventually be updated to support any new APIs.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.8.0 release of the Apache Arrow ADBC libraries. This covers includes 43 resolved issues from 12 distinct contributors. This is a release of the libraries, which are at version 0.8.0. The API specification is versioned separately and is at version 1.1.0. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights C#/.NET has added a BigQuery driver. Also, it now can be used through ADO.NET. The R bindings now use ADBC 1.1.0, and more packages are now available on CRAN. The Snowflake driver now has an option to control whether to return decimal types or attempt to convert to integers/floats. The PostgreSQL driver now uses COPY for bulk ingestion, which Pandas has found is approximately 35x faster than their previous method. The SQLite driver can now load extensions and supports more data types, including Arrow binary (SQL BLOB), and supports binding some dictionary-encoded types (which will be unpacked). Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.7.0..apache-arrow-adbc-0.8.0 27 David Li 23 William Ayd 19 Dewey Dunnington 4 Matt Topol 4 Solomon Choe 4 davidhcoe 2 vleslief-ms 1 Aaron Ross 1 Fredrik Hoem Grelland 1 Joel Lubinitsky 1 OleMussmann 1 Ruoxuan Wang Roadmap Work continues on improving the existing drivers in terms of supported Arrow/database types, performance, and so on. For Java, the minimum JDK version will stay on JDK 8 as long as the Arrow Java libraries remain on that version. There are currently no plans for a second API revision. As work progresses on asynchronous and device-aware APIs in the Arrow ecosystem, ADBC will eventually be updated to support any new APIs. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 14.0.0 Release</title><link href="https://arrow.apache.org/blog/2023/11/01/14.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 14.0.0 Release" /><published>2023-11-01T00:00:00-04:00</published><updated>2023-11-01T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/11/01/14.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/11/01/14.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 14.0.0 release. This covers
over 3 months of development work and includes <a href="https://github.com/apache/arrow/milestone/55?closed=1"><strong>483 resolved issues</strong></a>
from <a href="/release/14.0.0.html#contributors"><strong>116 distinct contributors</strong></a>. See the <a href="https://arrow.apache.org/install/">Install Page</a>
to learn how to get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/14.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 13.0.0 release, Metehan Yildirim and Oleks V. have been invited to be committers.</p>

<p>Thanks for your contributions and participation in the project!</p>

<h2 id="columnar-format-notes">Columnar Format Notes</h2>

<p>Motivated by recent innovations in DuckDB and Meta’s Velox engine, new “view” data types were added to the Arrow columnar format spec:</p>

<ul>
  <li>16-byte StringView and BinaryView data types which enable better buffer reuse, faster “false” string comparisons (due to maintaining a prefix) and short string inlining (<a href="https://github.com/apache/arrow/issues/35627">GH-35627</a>).</li>
  <li>ListView and LargeListView types for more performant “out-of-order” building and processing of lists and better buffer reuse (<a href="https://github.com/apache/arrow/issues/37876">GH-37876</a>).</li>
</ul>

<p>A <code class="language-plaintext highlighter-rouge">VariableShapeTensorType</code> was added to the Arrow specification as a canonical extension type (<a href="https://github.com/apache/arrow/issues/24868">GH-24868</a>).</p>

<h2 id="c-data-interface-notes">C Data Interface notes</h2>

<p>Integration testing has been added for the C Data Interface accross Arrow implementations,
ensuring mutual compatibility. (<a href="https://github.com/apache/arrow/issues/37537">GH-37537</a>).
The C++, C# and Go implementations are covered, with Arrow Java soon to come.</p>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>

<p>A new RPC method was added to allow polling for completion in long-running queries as an alternative to the blocking GetFlightInfo call (<a href="https://github.com/apache/arrow/issues/36155">GH-36155</a>). Also, <code class="language-plaintext highlighter-rouge">app_metadata</code> was added to <code class="language-plaintext highlighter-rouge">FlightInfo</code> and <code class="language-plaintext highlighter-rouge">FlightEndpoint</code> (<a href="https://github.com/apache/arrow/issues/37635">GH-37635</a>).</p>

<p>In C++ and Python, an experimental asynchronous GetFlightInfo call was added to the client-side API (<a href="https://github.com/apache/arrow/issues/36512">GH-36512</a>). <code class="language-plaintext highlighter-rouge">ServerCallContext</code> now exposes conveniences to send headers/trailers without having to use middleware (<a href="https://github.com/apache/arrow/issues/36952">GH-36952</a>). The implementation was fixed to not reject unknown field tags to enable interoperability with future versions of Flight that could add new fields (<a href="https://github.com/apache/arrow/issues/36975">GH-36975</a>). The CMake configuration was fixed to correctly require linking to Arrow Flight RPC when using Arrow Flight SQL (<a href="https://github.com/apache/arrow/issues/37406">GH-37406</a>).</p>

<p>In Go, the underlying generated Protobuf code is now exposed for easier low-level integrations with Flight (<a href="https://github.com/apache/arrow/issues/36893">GH-36893</a>).</p>

<p>In Java, the stateful “login” authentication APIs using the Handshake RPC are deprecated; it will not be removed, but it should not be used unless you specifically want the old behavior (<a href="https://github.com/apache/arrow/issues/37722">GH-37722</a>). Utilities were added to help implement basic Flight SQL services for unit testing (<a href="https://github.com/apache/arrow/issues/37795">GH-37795</a>).</p>

<h2 id="c-notes">C++ notes</h2>

<p>Experimental APIs for exporting and importing non-CPU arrays using the C Device Data Interface
have been added (<a href="https://github.com/apache/arrow/issues/36488">GH-36488</a>), together with an experimental API for device synchronization
(<a href="https://github.com/apache/arrow/issues/36103">GH-36103</a>).</p>

<p>Initial compatibility with Emscripten without threading support has been added (<a href="https://github.com/apache/arrow/issues/35176">GH-35176</a>).</p>

<h3 id="compute-layer">Compute layer</h3>

<p>New compute functions:</p>
<ul>
  <li>a <code class="language-plaintext highlighter-rouge">cumulative_mean</code> function on numeric data (<a href="https://github.com/apache/arrow/issues/36931">GH-36931</a>);</li>
</ul>

<p>Improved compute functions:</p>
<ul>
  <li>rounding functions now work natively on integer inputs instead of casting them to floats (<a href="https://github.com/apache/arrow/issues/35273">GH-35273</a>);</li>
  <li>the <code class="language-plaintext highlighter-rouge">divide</code> function now supports duration inputs (<a href="https://github.com/apache/arrow/issues/36789">GH-36789</a>);</li>
  <li><code class="language-plaintext highlighter-rouge">take</code> and <code class="language-plaintext highlighter-rouge">filter</code> now support sparse unions in addition to dense unions (<a href="https://github.com/apache/arrow/issues/36905">GH-36905</a>);</li>
  <li><code class="language-plaintext highlighter-rouge">if_else</code>, <code class="language-plaintext highlighter-rouge">coalesce</code>, <code class="language-plaintext highlighter-rouge">choose</code> and <code class="language-plaintext highlighter-rouge">case_when</code> now support duration inputs (<a href="https://github.com/apache/arrow/issues/37028">GH-37028</a>);</li>
  <li>casting between fixed-size lists and variable-size lists is now supported (<a href="https://github.com/apache/arrow/issues/20086">GH-20086</a>);</li>
  <li>casting from strings to dates is now supported (<a href="https://github.com/apache/arrow/issues/37411">GH-37411</a>);</li>
  <li><code class="language-plaintext highlighter-rouge">mean</code> on integer inputs now uses a floating-point representation for its intermediate sum,
avoiding integer overflow on large inputs (<a href="https://github.com/apache/arrow/issues/34909">GH-34909</a>);</li>
</ul>

<h3 id="datasets">Datasets</h3>

<p>Support for writing encrypted Parquet datasets has been added (<a href="https://github.com/apache/arrow/issues/29238">GH-29238</a>).</p>

<h3 id="gandiva">Gandiva</h3>

<p>Gandiva now supports linking dynamically to LLVM on non-Windows platforms (<a href="https://github.com/apache/arrow/issues/37410">GH-37410</a>).
Previously, Gandiva would always link LLVM statically into <code class="language-plaintext highlighter-rouge">libgandiva</code>.</p>

<h3 id="parquet">Parquet</h3>

<p>RLE is used by default when encoding boolean values if v2 data pages are enabled
(<a href="https://github.com/apache/arrow/issues/36882">GH-36882</a>).</p>

<p>Page indexes can now be encrypted as per the specification (<a href="https://github.com/apache/arrow/issues/34950">GH-34950</a>).</p>

<p>A bug in the DELTA_BINARY_PACKED encoder leading to suboptimal column sizes was fixed (<a href="https://github.com/apache/arrow/issues/37939">GH-37939</a>).</p>

<h3 id="substrait">Substrait</h3>

<p>It is now possible to serialize and deserialize individual expressions using Substrait,
not only full query plans (<a href="https://github.com/apache/arrow/issues/33985">GH-33985</a>).</p>

<h3 id="miscellaneous">Miscellaneous</h3>

<p>A new <code class="language-plaintext highlighter-rouge">CodecOptions</code> class allows customizing compression parameters per-codec (<a href="https://github.com/apache/arrow/issues/35287">GH-35287</a>).</p>

<p>The environment variable <code class="language-plaintext highlighter-rouge">AWS_ENDPOINT_URL</code> is now respected when resolving S3 URIs (<a href="https://github.com/apache/arrow/issues/36770">GH-36770</a>).</p>

<p>Recursively listing S3 filesystem trees should now issue less requests,
leading to improved performance (<a href="https://github.com/apache/arrow/issues/34213">GH-34213</a>).</p>

<p>Comparing a <code class="language-plaintext highlighter-rouge">ChunkedArray</code> to itself now behaves correctly with NaN values (<a href="https://github.com/apache/arrow/issues/37515">GH-37515</a>).</p>

<p>The use of BMI2 instructions on x86 was incorrectly guarded. Those instructions
could be executed on platforms without BMI2 support, leading to crashes (<a href="https://github.com/apache/arrow/issues/37017">GH-37017</a>).</p>

<h2 id="c-notes-1">C# notes</h2>

<p>The following features have been added to the C# implementation apart from other minor ones and some fixes.</p>

<ul>
  <li>Support fixed-size lists (<a href="https://github.com/apache/arrow/issues/33032">GH-33032</a>)</li>
  <li>Support DateOnly and TimeOnly on .NET 6.0+ (<a href="https://github.com/apache/arrow/issues/34620">GH-34620</a>)</li>
  <li>Implement MapType (<a href="https://github.com/apache/arrow/issues/35243">GH-35243</a>)</li>
  <li>Flight SQL implementation for C# (<a href="https://github.com/apache/arrow/issues/36078">GH-36078</a>)</li>
  <li>Implement support for dense and sparse unions (<a href="https://github.com/apache/arrow/issues/36795">GH-36795</a>)</li>
</ul>

<h2 id="go-notes">Go Notes</h2>

<ul>
  <li>The minimum version of Go officially supported is now <code class="language-plaintext highlighter-rouge">go1.19</code> instead of <code class="language-plaintext highlighter-rouge">go1.17</code> (<a href="https://github.com/apache/arrow/issues/37636">GH-37636</a>)</li>
</ul>

<h3 id="bug-fixes">Bug Fixes</h3>

<h4 id="arrow">Arrow</h4>

<ul>
  <li>Documentation fixed to correctly state that the default unit for <code class="language-plaintext highlighter-rouge">TimestampType</code> is seconds (<a href="https://github.com/apache/arrow/issues/35770">GH-35770</a>)</li>
  <li>Fixed leak in the <code class="language-plaintext highlighter-rouge">Concatenate</code> function if there is a panic that is recovered (<a href="https://github.com/apache/arrow/issues/36850">GH-36850</a>)</li>
  <li>Ensure Binary dictionary indices are released on panic (<a href="https://github.com/apache/arrow/issues/36858">GH-36858</a>)</li>
  <li>Fix overflow value causing invalid dates for <code class="language-plaintext highlighter-rouge">MarshalJSON</code> on some timestamps (<a href="https://github.com/apache/arrow/issues/36935">GH-36935</a>)</li>
  <li>Fix leaking dictionary allocations in IPC reader (<a href="https://github.com/apache/arrow/issues/36981">GH-36981</a>)</li>
</ul>

<h4 id="parquet-1">Parquet</h4>

<ul>
  <li>Fixed an issue where DeltaLengthByteArray encoding fails on certain null value scenarios (<a href="https://github.com/apache/arrow/issues/36318">GH-36318</a>)</li>
  <li>Correctly propagate internal <code class="language-plaintext highlighter-rouge">writer.sink.Close()</code> errors from <code class="language-plaintext highlighter-rouge">writer.Close()</code> when writing a Parquet file (<a href="https://github.com/apache/arrow/issues/36645">GH-36645</a>)</li>
  <li>Fixed a panic when writing some specific DeltaBitPacked datasets (<a href="https://github.com/apache/arrow/issues/37102">GH-37102</a>)</li>
  <li>Proper support for Decimal256 data type in Parquet lib (<a href="https://github.com/apache/arrow/issues/37419">GH-37419</a>)</li>
  <li>Corrected inconsistent behavior in <code class="language-plaintext highlighter-rouge">pqarrow</code> column chunk reader (<a href="https://github.com/apache/arrow/issues/37845">GH-37845</a>)</li>
  <li>Rewrote and Fixed ARM64 assembly for bitmap bit extractions and integer packing (<a href="https://github.com/apache/arrow/issues/37712">GH-37712</a>)</li>
</ul>

<h3 id="enhancements">Enhancements</h3>

<ul>
  <li>C Data Interface integration testing has been added and implemented (<a href="https://github.com/apache/arrow/issues/37789">GH-37789</a>)</li>
  <li>pkg.go.dev link is fixed in the Readme (<a href="https://github.com/apache/arrow/issues/37779">GH-37779</a>)</li>
</ul>

<h4 id="arrow-1">Arrow</h4>

<ul>
  <li>Added <code class="language-plaintext highlighter-rouge">String()</code> method to <code class="language-plaintext highlighter-rouge">arrow.Table</code> (<a href="https://github.com/apache/arrow/issues/35296">GH-35296</a>)</li>
  <li>Add proper <code class="language-plaintext highlighter-rouge">array.Null</code> type support handling for arrow/csv writing (<a href="https://github.com/apache/arrow/issues/36623">GH-36623</a>)</li>
  <li>Optimized <code class="language-plaintext highlighter-rouge">GetOrInsert</code> function for memo table handling of dictionary builders (<a href="https://github.com/apache/arrow/issues/36671">GH-36671</a>)</li>
  <li>Made it possible to add custom functions in the <code class="language-plaintext highlighter-rouge">compute</code> package (<a href="https://github.com/apache/arrow/issues/36936">GH-36936</a>)</li>
  <li>Improved performance of dictionary unifier (<a href="https://github.com/apache/arrow/issues/37306">GH-37306</a>)</li>
  <li>Added direct access to dictionary builder indices (<a href="https://github.com/apache/arrow/issues/37416">GH-37416</a>)</li>
  <li>Added ability to read back values from Boolean builders (<a href="https://github.com/apache/arrow/issues/37465">GH-37465</a>)</li>
  <li>Add <code class="language-plaintext highlighter-rouge">ValueLen</code> function to string array (<a href="https://github.com/apache/arrow/issues/37584">GH-37584</a>)</li>
  <li>Avoid unnecessary copying in the default go allocator (<a href="https://github.com/apache/arrow/issues/37687">GH-37687</a>)</li>
  <li>Add <code class="language-plaintext highlighter-rouge">SetNull(i int)</code> to array builders (<a href="https://github.com/apache/arrow/issues/37694">GH-37694</a>)</li>
</ul>

<h4 id="parquet-2">Parquet</h4>

<ul>
  <li>Parquet metadata is allowed to write metadata after writing rowgroups using <code class="language-plaintext highlighter-rouge">pqarrow.FileWriter</code> (<a href="https://github.com/apache/arrow/issues/35775">GH-35775</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">MapOf</code> and <code class="language-plaintext highlighter-rouge">ListOf</code> helper functions have been improved to provide clearer error messages and have better documentation (<a href="https://github.com/apache/arrow/issues/36696">GH-36696</a>)</li>
  <li>Struct tag of <code class="language-plaintext highlighter-rouge">parquet:"-"</code> will be allowed to skip fields when converting a struct to a parquet schema (<a href="https://github.com/apache/arrow/issues/36793">GH-36793</a>)</li>
</ul>

<h2 id="java-notes">Java notes</h2>

<p>Java 21 is enabled and validated in CI (<a href="https://github.com/apache/arrow/issues/37914">GH-37914</a>).</p>

<p>The Gandiva module implemented a breaking change by moving <code class="language-plaintext highlighter-rouge">Types.proto</code> into a subfolder (<a href="https://github.com/apache/arrow/issues/37893">GH-37893</a>).</p>

<p><code class="language-plaintext highlighter-rouge">DefaultVectorComparators</code> added support for <code class="language-plaintext highlighter-rouge">LargeVarCharVector</code>, <code class="language-plaintext highlighter-rouge">LargeVarBinaryVector</code> (<a href="https://github.com/apache/arrow/issues/25659">GH-25659</a>) and for <code class="language-plaintext highlighter-rouge">BitVector</code>, <code class="language-plaintext highlighter-rouge">DateDayVector</code>, <code class="language-plaintext highlighter-rouge">DateMilliVector</code>
<code class="language-plaintext highlighter-rouge">Decimal256Vector</code>, <code class="language-plaintext highlighter-rouge">DecimalVector</code>, <code class="language-plaintext highlighter-rouge">DurationVector</code>, <code class="language-plaintext highlighter-rouge">IntervalDayVector</code>, <code class="language-plaintext highlighter-rouge">TimeMicroVector</code>, <code class="language-plaintext highlighter-rouge">TimeMilliVector</code>, <code class="language-plaintext highlighter-rouge">TimeNanoVector</code>, <code class="language-plaintext highlighter-rouge">TimeSecVector</code>, <code class="language-plaintext highlighter-rouge">TimeStampVector</code> (<a href="https://github.com/apache/arrow/issues/37701">GH-37701</a>).</p>

<p>A bug was fixed in <code class="language-plaintext highlighter-rouge">VectorAppender</code> to prevent resizing the data buffer twice when appending variable-length vectors (<a href="https://github.com/apache/arrow/issues/37829">GH-37829</a>).</p>

<p><code class="language-plaintext highlighter-rouge">VarCharWriter</code> added support for writing from <code class="language-plaintext highlighter-rouge">Text</code> and <code class="language-plaintext highlighter-rouge">String</code> (<a href="https://github.com/apache/arrow/issues/37706">GH-37706</a>). <code class="language-plaintext highlighter-rouge">VarBinaryWriter</code> added support for writing from <code class="language-plaintext highlighter-rouge">byte[]</code> and <code class="language-plaintext highlighter-rouge">ByteBuffer</code> (<a href="https://github.com/apache/arrow/issues/37705">GH-37705</a>).</p>

<p>The JDBC driver will now ignore username and password authentication if a token is provided (<a href="https://github.com/apache/arrow/issues/37073">GH-37073</a>).</p>

<p>A bug was fixed in the Java C-Data interface when importing a vector with an empty array (<a href="https://github.com/apache/arrow/issues/37056">GH-37056</a>).</p>

<p>A bug was fixed in the S3 file system implementation when closing the connection (<a href="https://github.com/apache/arrow/issues/36069">GH-36069</a>).</p>

<p>Arrow datasets now support Substrait <code class="language-plaintext highlighter-rouge">ExtendedExpression</code>s as inputs to filter and project operations (<a href="https://github.com/apache/arrow/issues/34252">GH-34252</a>).</p>

<h2 id="javascript-notes">JavaScript notes</h2>

<ul>
  <li>GH-21815: [JS] Add support for Duration type #37341</li>
  <li>GH-31621: [JS] Fix Union null bitmaps #37122</li>
</ul>

<h2 id="python-notes">Python notes</h2>

<p>Compatibility notes:</p>
<ul>
  <li>Support for Python 3.12 was added (<a href="https://github.com/apache/arrow/issues/37880">GH-37880</a>)</li>
  <li>Support for Cython 3 was added (<a href="https://github.com/apache/arrow/issues/37742">GH-37742</a>)</li>
  <li>PyArrow is now compatible with numpy 2.0 (<a href="https://github.com/apache/arrow/issues/37574">GH-37574</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">pyarrow.compute.CumulativeSumOptions</code> has been deprecated, use <code class="language-plaintext highlighter-rouge">pyarrow.compute.CumulativeOptions</code> instead (<a href="https://github.com/apache/arrow/issues/36240">GH-36240</a>)</li>
</ul>

<p>New features:</p>
<ul>
  <li>Allowing type promotion is now possible in <code class="language-plaintext highlighter-rouge">pyarrow.concat_tables</code> (<a href="https://github.com/apache/arrow/issues/36845">GH-36845</a>)</li>
  <li>Support for vector function UDF was added (<a href="https://github.com/apache/arrow/issues/36672">GH-36672</a>)</li>
</ul>

<p>Other improvements:</p>
<ul>
  <li>The default of <code class="language-plaintext highlighter-rouge">pre_buffer</code> is now set to <code class="language-plaintext highlighter-rouge">True</code> for reading Parquet when using <code class="language-plaintext highlighter-rouge">pyarrow.dataset</code> directly. This can give significant speed-up on filesystems like S3 and is now aligned to <code class="language-plaintext highlighter-rouge">pyarrow.parquet.read_table</code> interface (<a href="https://github.com/apache/arrow/issues/36765">GH-36765</a>)</li>
  <li>Path to timezone database can now be set through python API (<a href="https://github.com/apache/arrow/issues/35600">GH-35600</a>, [GH-38145] (https://github.com/apache/arrow/issues/38145))</li>
  <li><code class="language-plaintext highlighter-rouge">pyarrow.MapScalar.as_py</code>can now be called with custom field name (<a href="https://github.com/apache/arrow/issues/36809">GH-36809</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">FixedShapeTensorType</code> string representation now prints the type parameters (<a href="https://github.com/apache/arrow/issues/35623">GH-35623</a>)</li>
</ul>

<p>Relevant bug fixes:</p>
<ul>
  <li>String to date cast kernel was added to fix python scalar cast regression (<a href="https://github.com/apache/arrow/issues/37411">GH-37411</a>)</li>
  <li>Fix conversion from Python to Arrow when chunking large nested structs (<a href="https://github.com/apache/arrow/issues/32439">GH-32439</a>)</li>
  <li>Fix segfault when passing table as argument to <code class="language-plaintext highlighter-rouge">pyarrow.Table.filter</code> (<a href="https://github.com/apache/arrow/issues/37650">GH-37650</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">use_threads</code> keyword was added to the <code class="language-plaintext highlighter-rouge">group_by</code> method on <code class="language-plaintext highlighter-rouge">pyarrow.Table</code> which gets passed through to the <code class="language-plaintext highlighter-rouge">pyarrow.acero.Declaration.to_table</code> call. Specifing <code class="language-plaintext highlighter-rouge">use_threads=False</code>allows to get stable ordering of the output (<a href="https://github.com/apache/arrow/issues/36709">GH-36709</a>)</li>
  <li>Fix printable representation for <code class="language-plaintext highlighter-rouge">pyarrow.TimestampScalar</code> when values are outside datetime range (<a href="https://github.com/apache/arrow/issues/36323">GH-36323</a>)</li>
  <li>Empty dataframes with zero chunks can now be consumed by the Dataframe Interchange Protocol implementation (<a href="https://github.com/apache/arrow/issues/37050">GH-37050</a>)</li>
  <li>Fix dtype information for categorical columns in the Dataframe Interchange Protocol implementation (<a href="https://github.com/apache/arrow/issues/38034">GH-38034</a>)</li>
  <li>Boolean columns with bitsize 1 are now supported in <code class="language-plaintext highlighter-rouge">from_dataframe</code>of the Dataframe Interchange Protocol (<a href="https://github.com/apache/arrow/issues/37145">GH-37145</a>)</li>
</ul>

<p>Further, the Python bindings benefit from improvements in the C++ library
(e.g. new compute functions); see the C++ notes above for additional details.</p>

<p>The Arrow documentation is now built with an updated Pydata Sphinx Theme which includes light/dark theme, new colors from <a href="https://github.com/Quansight-Labs/accessible-pygments">Accessible pygments themes</a>, version switcher dropdown, search button, etc. (<a href="https://github.com/apache/arrow/issues/36590">GH-36590</a>, <a href="https://github.com/apache/arrow/issues/32451">GH-32451</a>)</p>

<h2 id="r-notes">R notes</h2>

<p>This release of the R package features a substantial refactor of the package configuration, build, and installation. This change should be transparent to most users; however, package contributors can take advantage of a substantially simplified development setup: in most cases, package contributors should be able to use a pre-built nightly version of Arrow C++ in place of a local Arrow development setup. Special thanks to Jacob Wujciak-Jens for taking on this incredible refactor!</p>

<p>In addition to a number of bugfixes and improvements, this release includes several new features related to CSV input/output:</p>

<ul>
  <li>Added support for <code class="language-plaintext highlighter-rouge">,</code> or other characters as a decimal point.</li>
  <li>Added <code class="language-plaintext highlighter-rouge">write_csv_dataset()</code> to better document CSV-specific dataset writing options.</li>
  <li>Ensured that the <code class="language-plaintext highlighter-rouge">schema</code> argument can be specified when reading a CSV dataset with partitions.</li>
</ul>

<p>For more on what’s in the 14.0.0 R package, see the <a href="/docs/r/news/">R changelog</a>.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<h3 id="ruby">Ruby</h3>

<ul>
  <li>Add support for prepared INSERT queries (<a href="https://github.com/apache/arrow/issues/37143">GH-37143</a>)</li>
  <li>When a prepared statement is automatically closed upon exiting a block, use the same options as when the statement was prepared (<a href="https://github.com/apache/arrow/issues/37257">GH-37257</a>)</li>
</ul>

<h3 id="c-glib">C GLib</h3>

<ul>
  <li>Support more properties of <code class="language-plaintext highlighter-rouge">ArrowFlight::ClientOptions</code> (<a href="https://github.com/apache/arrow/issues/37141">GH-37141</a>)</li>
  <li>Add support for prepared INSERT queries (<a href="https://github.com/apache/arrow/issues/37143">GH-37143</a>)</li>
</ul>

<h2 id="rust-notes">Rust notes</h2>

<p>The Rust projects have moved to separate repositories outside the
main Arrow monorepo. For notes on the latest release of the Rust
implementation, see the latest <a href="https://github.com/apache/arrow-rs/tags">Arrow Rust changelog</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 14.0.0 release. This covers over 3 months of development work and includes 483 resolved issues from 116 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 13.0.0 release, Metehan Yildirim and Oleks V. have been invited to be committers. Thanks for your contributions and participation in the project! Columnar Format Notes Motivated by recent innovations in DuckDB and Meta’s Velox engine, new “view” data types were added to the Arrow columnar format spec: 16-byte StringView and BinaryView data types which enable better buffer reuse, faster “false” string comparisons (due to maintaining a prefix) and short string inlining (GH-35627). ListView and LargeListView types for more performant “out-of-order” building and processing of lists and better buffer reuse (GH-37876). A VariableShapeTensorType was added to the Arrow specification as a canonical extension type (GH-24868). C Data Interface notes Integration testing has been added for the C Data Interface accross Arrow implementations, ensuring mutual compatibility. (GH-37537). The C++, C# and Go implementations are covered, with Arrow Java soon to come. Arrow Flight RPC notes A new RPC method was added to allow polling for completion in long-running queries as an alternative to the blocking GetFlightInfo call (GH-36155). Also, app_metadata was added to FlightInfo and FlightEndpoint (GH-37635). In C++ and Python, an experimental asynchronous GetFlightInfo call was added to the client-side API (GH-36512). ServerCallContext now exposes conveniences to send headers/trailers without having to use middleware (GH-36952). The implementation was fixed to not reject unknown field tags to enable interoperability with future versions of Flight that could add new fields (GH-36975). The CMake configuration was fixed to correctly require linking to Arrow Flight RPC when using Arrow Flight SQL (GH-37406). In Go, the underlying generated Protobuf code is now exposed for easier low-level integrations with Flight (GH-36893). In Java, the stateful “login” authentication APIs using the Handshake RPC are deprecated; it will not be removed, but it should not be used unless you specifically want the old behavior (GH-37722). Utilities were added to help implement basic Flight SQL services for unit testing (GH-37795). C++ notes Experimental APIs for exporting and importing non-CPU arrays using the C Device Data Interface have been added (GH-36488), together with an experimental API for device synchronization (GH-36103). Initial compatibility with Emscripten without threading support has been added (GH-35176). Compute layer New compute functions: a cumulative_mean function on numeric data (GH-36931); Improved compute functions: rounding functions now work natively on integer inputs instead of casting them to floats (GH-35273); the divide function now supports duration inputs (GH-36789); take and filter now support sparse unions in addition to dense unions (GH-36905); if_else, coalesce, choose and case_when now support duration inputs (GH-37028); casting between fixed-size lists and variable-size lists is now supported (GH-20086); casting from strings to dates is now supported (GH-37411); mean on integer inputs now uses a floating-point representation for its intermediate sum, avoiding integer overflow on large inputs (GH-34909); Datasets Support for writing encrypted Parquet datasets has been added (GH-29238). Gandiva Gandiva now supports linking dynamically to LLVM on non-Windows platforms (GH-37410). Previously, Gandiva would always link LLVM statically into libgandiva. Parquet RLE is used by default when encoding boolean values if v2 data pages are enabled (GH-36882). Page indexes can now be encrypted as per the specification (GH-34950). A bug in the DELTA_BINARY_PACKED encoder leading to suboptimal column sizes was fixed (GH-37939). Substrait It is now possible to serialize and deserialize individual expressions using Substrait, not only full query plans (GH-33985). Miscellaneous A new CodecOptions class allows customizing compression parameters per-codec (GH-35287). The environment variable AWS_ENDPOINT_URL is now respected when resolving S3 URIs (GH-36770). Recursively listing S3 filesystem trees should now issue less requests, leading to improved performance (GH-34213). Comparing a ChunkedArray to itself now behaves correctly with NaN values (GH-37515). The use of BMI2 instructions on x86 was incorrectly guarded. Those instructions could be executed on platforms without BMI2 support, leading to crashes (GH-37017). C# notes The following features have been added to the C# implementation apart from other minor ones and some fixes. Support fixed-size lists (GH-33032) Support DateOnly and TimeOnly on .NET 6.0+ (GH-34620) Implement MapType (GH-35243) Flight SQL implementation for C# (GH-36078) Implement support for dense and sparse unions (GH-36795) Go Notes The minimum version of Go officially supported is now go1.19 instead of go1.17 (GH-37636) Bug Fixes Arrow Documentation fixed to correctly state that the default unit for TimestampType is seconds (GH-35770) Fixed leak in the Concatenate function if there is a panic that is recovered (GH-36850) Ensure Binary dictionary indices are released on panic (GH-36858) Fix overflow value causing invalid dates for MarshalJSON on some timestamps (GH-36935) Fix leaking dictionary allocations in IPC reader (GH-36981) Parquet Fixed an issue where DeltaLengthByteArray encoding fails on certain null value scenarios (GH-36318) Correctly propagate internal writer.sink.Close() errors from writer.Close() when writing a Parquet file (GH-36645) Fixed a panic when writing some specific DeltaBitPacked datasets (GH-37102) Proper support for Decimal256 data type in Parquet lib (GH-37419) Corrected inconsistent behavior in pqarrow column chunk reader (GH-37845) Rewrote and Fixed ARM64 assembly for bitmap bit extractions and integer packing (GH-37712) Enhancements C Data Interface integration testing has been added and implemented (GH-37789) pkg.go.dev link is fixed in the Readme (GH-37779) Arrow Added String() method to arrow.Table (GH-35296) Add proper array.Null type support handling for arrow/csv writing (GH-36623) Optimized GetOrInsert function for memo table handling of dictionary builders (GH-36671) Made it possible to add custom functions in the compute package (GH-36936) Improved performance of dictionary unifier (GH-37306) Added direct access to dictionary builder indices (GH-37416) Added ability to read back values from Boolean builders (GH-37465) Add ValueLen function to string array (GH-37584) Avoid unnecessary copying in the default go allocator (GH-37687) Add SetNull(i int) to array builders (GH-37694) Parquet Parquet metadata is allowed to write metadata after writing rowgroups using pqarrow.FileWriter (GH-35775) MapOf and ListOf helper functions have been improved to provide clearer error messages and have better documentation (GH-36696) Struct tag of parquet:"-" will be allowed to skip fields when converting a struct to a parquet schema (GH-36793) Java notes Java 21 is enabled and validated in CI (GH-37914). The Gandiva module implemented a breaking change by moving Types.proto into a subfolder (GH-37893). DefaultVectorComparators added support for LargeVarCharVector, LargeVarBinaryVector (GH-25659) and for BitVector, DateDayVector, DateMilliVector Decimal256Vector, DecimalVector, DurationVector, IntervalDayVector, TimeMicroVector, TimeMilliVector, TimeNanoVector, TimeSecVector, TimeStampVector (GH-37701). A bug was fixed in VectorAppender to prevent resizing the data buffer twice when appending variable-length vectors (GH-37829). VarCharWriter added support for writing from Text and String (GH-37706). VarBinaryWriter added support for writing from byte[] and ByteBuffer (GH-37705). The JDBC driver will now ignore username and password authentication if a token is provided (GH-37073). A bug was fixed in the Java C-Data interface when importing a vector with an empty array (GH-37056). A bug was fixed in the S3 file system implementation when closing the connection (GH-36069). Arrow datasets now support Substrait ExtendedExpressions as inputs to filter and project operations (GH-34252). JavaScript notes GH-21815: [JS] Add support for Duration type #37341 GH-31621: [JS] Fix Union null bitmaps #37122 Python notes Compatibility notes: Support for Python 3.12 was added (GH-37880) Support for Cython 3 was added (GH-37742) PyArrow is now compatible with numpy 2.0 (GH-37574) pyarrow.compute.CumulativeSumOptions has been deprecated, use pyarrow.compute.CumulativeOptions instead (GH-36240) New features: Allowing type promotion is now possible in pyarrow.concat_tables (GH-36845) Support for vector function UDF was added (GH-36672) Other improvements: The default of pre_buffer is now set to True for reading Parquet when using pyarrow.dataset directly. This can give significant speed-up on filesystems like S3 and is now aligned to pyarrow.parquet.read_table interface (GH-36765) Path to timezone database can now be set through python API (GH-35600, [GH-38145] (https://github.com/apache/arrow/issues/38145)) pyarrow.MapScalar.as_pycan now be called with custom field name (GH-36809) FixedShapeTensorType string representation now prints the type parameters (GH-35623) Relevant bug fixes: String to date cast kernel was added to fix python scalar cast regression (GH-37411) Fix conversion from Python to Arrow when chunking large nested structs (GH-32439) Fix segfault when passing table as argument to pyarrow.Table.filter (GH-37650) use_threads keyword was added to the group_by method on pyarrow.Table which gets passed through to the pyarrow.acero.Declaration.to_table call. Specifing use_threads=Falseallows to get stable ordering of the output (GH-36709) Fix printable representation for pyarrow.TimestampScalar when values are outside datetime range (GH-36323) Empty dataframes with zero chunks can now be consumed by the Dataframe Interchange Protocol implementation (GH-37050) Fix dtype information for categorical columns in the Dataframe Interchange Protocol implementation (GH-38034) Boolean columns with bitsize 1 are now supported in from_dataframeof the Dataframe Interchange Protocol (GH-37145) Further, the Python bindings benefit from improvements in the C++ library (e.g. new compute functions); see the C++ notes above for additional details. The Arrow documentation is now built with an updated Pydata Sphinx Theme which includes light/dark theme, new colors from Accessible pygments themes, version switcher dropdown, search button, etc. (GH-36590, GH-32451) R notes This release of the R package features a substantial refactor of the package configuration, build, and installation. This change should be transparent to most users; however, package contributors can take advantage of a substantially simplified development setup: in most cases, package contributors should be able to use a pre-built nightly version of Arrow C++ in place of a local Arrow development setup. Special thanks to Jacob Wujciak-Jens for taking on this incredible refactor! In addition to a number of bugfixes and improvements, this release includes several new features related to CSV input/output: Added support for , or other characters as a decimal point. Added write_csv_dataset() to better document CSV-specific dataset writing options. Ensured that the schema argument can be specified when reading a CSV dataset with partitions. For more on what’s in the 14.0.0 R package, see the R changelog. Ruby and C GLib notes Ruby Add support for prepared INSERT queries (GH-37143) When a prepared statement is automatically closed upon exiting a block, use the same options as when the statement was prepared (GH-37257) C GLib Support more properties of ArrowFlight::ClientOptions (GH-37141) Add support for prepared INSERT queries (GH-37143) Rust notes The Rust projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Rust implementation, see the latest Arrow Rust changelog.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow nanoarrow 0.3.0 Release</title><link href="https://arrow.apache.org/blog/2023/10/03/nanoarrow-0.3.0-release/" rel="alternate" type="text/html" title="Apache Arrow nanoarrow 0.3.0 Release" /><published>2023-10-03T00:00:00-04:00</published><updated>2023-10-03T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/10/03/nanoarrow-0.3.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/10/03/nanoarrow-0.3.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.3.0 release of
Apache Arrow nanoarrow. This release covers 42 resolved issues from
4 contributors.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>See the
<a href="https://github.com/apache/arrow-nanoarrow/blob/apache-arrow-nanoarrow-0.3.0/CHANGELOG.md">Changelog</a>
for a detailed list of contributions to this release.</p>

<h3 id="c-library">C library</h3>

<p>The nanoarrow 0.3.0 release includes a number of bugfixes and improvements
to the core C library and IPC C extension:</p>

<ul>
  <li>Improved bit packing/unpacking utilities and performance in tandem with an
<a href="https://github.com/pandas-dev/pandas/pull/54506">experiment to use bitmasks in pandas</a></li>
  <li>Added support for building and consuming interval arrays</li>
  <li>Fixed full validation of offset buffers for specific types of corrupted data
that caused overflow</li>
  <li>Fixed crashes that occurred for certain types of corrupted data and improved
error messages that were misleading for other types of corrupted data.</li>
</ul>

<h3 id="r-bindings">R bindings</h3>

<p>The nanoarrow R bindings are distributed as the <code class="language-plaintext highlighter-rouge">nanoarrow</code> package on
<a href="https://cran.r-project.org/">CRAN</a>. The 0.3.0 release of the R bindings includes
improvements in type support and stability. Notably:</p>

<ul>
  <li>Conversion to/from <code class="language-plaintext highlighter-rouge">bit64::integer64()</code> is now supported</li>
  <li>Warnings and errors for conversions that are invalid or
may loose accuracy were improved</li>
  <li>Extension types and extension type registration are now supported</li>
  <li>Conversion of dictionary-encoded arrays to R vectors is now supported</li>
  <li>Conversion of map arrays to R vectors is now supported</li>
  <li>Improved performance of stream conversion to R vectors for streams of
indeterminate length (e.g., database results from ADBC).</li>
</ul>

<h3 id="experimental-non-cpu-support">Experimental non-CPU support</h3>

<p>With the addition of the
<a href="https://arrow.apache.org/docs/format/CDeviceDataInterface.html">Arrow C Device data interface</a>
comes an opportunity for nanoarrow to handle non-CPU (e.g., GPU) data. The
<a href="https://github.com/apache/arrow-nanoarrow/tree/main/extensions/nanoarrow_device">nanoarrow_device</a>
extension was drafted in tandem with the specification and Arrow C++ implementation.
While it is not an official part of the 0.3.0 release, it is available in an
experimental state for those interested in its use and/or development.</p>

<h3 id="contributor-experience">Contributor experience</h3>

<p>The 0.3.0 release features adoption of <code class="language-plaintext highlighter-rouge">pre-commit</code> hooks to improve consistency
and contributor experience with a complex repository. Special thanks to
<a href="https://github.com/WillAyd">@WillAyd</a> for setting this up!</p>

<h2 id="contributors">Contributors</h2>

<p>This release consists of contributions from 4 contributors in addition
to the invaluable advice and support of the Apache Arrow developer mailing list.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>git shortlog <span class="nt">-sn</span> d4f038ce58655ba6e996cdae165f1b33c3919d51..apache-arrow-nanoarrow-0.3.0 | <span class="nb">grep</span> <span class="nt">-v</span> <span class="s2">"GitHub Actions"</span>
<span class="go">    40  Dewey Dunnington
     7  William Ayd
     2  Bryce Mecum
     1  Dane Pitkin
</span></code></pre></div></div>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.3.0 release of Apache Arrow nanoarrow. This release covers 42 resolved issues from 4 contributors. Release Highlights See the Changelog for a detailed list of contributions to this release. C library The nanoarrow 0.3.0 release includes a number of bugfixes and improvements to the core C library and IPC C extension: Improved bit packing/unpacking utilities and performance in tandem with an experiment to use bitmasks in pandas Added support for building and consuming interval arrays Fixed full validation of offset buffers for specific types of corrupted data that caused overflow Fixed crashes that occurred for certain types of corrupted data and improved error messages that were misleading for other types of corrupted data. R bindings The nanoarrow R bindings are distributed as the nanoarrow package on CRAN. The 0.3.0 release of the R bindings includes improvements in type support and stability. Notably: Conversion to/from bit64::integer64() is now supported Warnings and errors for conversions that are invalid or may loose accuracy were improved Extension types and extension type registration are now supported Conversion of dictionary-encoded arrays to R vectors is now supported Conversion of map arrays to R vectors is now supported Improved performance of stream conversion to R vectors for streams of indeterminate length (e.g., database results from ADBC). Experimental non-CPU support With the addition of the Arrow C Device data interface comes an opportunity for nanoarrow to handle non-CPU (e.g., GPU) data. The nanoarrow_device extension was drafted in tandem with the specification and Arrow C++ implementation. While it is not an official part of the 0.3.0 release, it is available in an experimental state for those interested in its use and/or development. Contributor experience The 0.3.0 release features adoption of pre-commit hooks to improve consistency and contributor experience with a complex repository. Special thanks to @WillAyd for setting this up! Contributors This release consists of contributions from 4 contributors in addition to the invaluable advice and support of the Apache Arrow developer mailing list. $ git shortlog -sn d4f038ce58655ba6e996cdae165f1b33c3919d51..apache-arrow-nanoarrow-0.3.0 | grep -v "GitHub Actions" 40 Dewey Dunnington 7 William Ayd 2 Bryce Mecum 1 Dane Pitkin]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 0.7.0 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/09/23/adbc-0.7.0-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.7.0 (Libraries) Release" /><published>2023-09-23T00:00:00-04:00</published><updated>2023-09-23T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/09/23/adbc-0.7.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/09/23/adbc-0.7.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.7.0 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/11"><strong>50
resolved issues</strong></a> from <a href="#contributors"><strong>8 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version
0.7.0.  The <strong>API specification</strong> is versioned separately and is
at version 1.1.0.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.7.0/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>This is the first release to implement version 1.1.0 of the ADBC API
specification.  The <a href="https://arrow.apache.org/adbc/current/format/specification.html#version-1-1-0">documentation</a> has a fuller
overview of new features, but in brief:</p>

<ul>
  <li>More common options and info keys were added, and clients can now
retrieve option values instead of only being able to set them.</li>
  <li>Connections/statements can be cancelled.</li>
  <li>Drivers can add arbitrary metadata to errors.  For example, the
Flight SQL driver exposes gRPC status details, and the PostgreSQL
driver provides PostgreSQL-specific error fields.</li>
  <li>Miscellaneous other APIs to cover specific feature gaps (e.g.
getting/setting the ‘active’ catalog and schema).</li>
</ul>

<p>The PostgreSQL and SQLite drivers support ingesting Arrow data into
temporary tables, and targeting a table in a particular namespace.
Also, they handle quoting of table/column names in bulk ingestion
better.  Both drivers also support more Arrow types when executing
queries and ingesting data.</p>

<p>The FlightSQL driver supports basic logging via configuration in
Go code or via an environment variable when built as a shared library
(for Python/R).</p>

<p>The Snowflake driver properly handles <code class="language-plaintext highlighter-rouge">TIME</code> fields.</p>

<p>The minimum Go version was bumped to 1.19 (in line with the next
release of the Arrow Go libraries).</p>

<p>R drivers expose functions to quote/escape names and strings for the
specific backend.</p>

<p>The Python DBAPI layer wraps Arrow record batch readers to raise
ADBC exceptions if the driver provides error metadata (see above).</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.6.0..apache-arrow-adbc-0.7.0
    26	David Li
     7	Dewey Dunnington
     7	William Ayd
     2	ElenaHenderson
     1	Matt Topol
     1	Solomon Choe
     1	Sutou Kouhei
     1	davidhcoe
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>Work continues on improving the existing drivers in terms of supported
Arrow/database types, performance, and so on.</p>

<p>For Java, the minimum JDK version may be raised from JDK 8 to 11 or 17,
a move that the Arrow Java libraries are also considering.</p>

<p>There are currently no plans for a second API revision. As work progresses
on asynchronous and device-aware APIs in the Arrow ecosystem, ADBC will
eventually be updated to support any new APIs.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.7.0 release of the Apache Arrow ADBC libraries. This covers includes 50 resolved issues from 8 distinct contributors. This is a release of the libraries, which are at version 0.7.0. The API specification is versioned separately and is at version 1.1.0. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights This is the first release to implement version 1.1.0 of the ADBC API specification. The documentation has a fuller overview of new features, but in brief: More common options and info keys were added, and clients can now retrieve option values instead of only being able to set them. Connections/statements can be cancelled. Drivers can add arbitrary metadata to errors. For example, the Flight SQL driver exposes gRPC status details, and the PostgreSQL driver provides PostgreSQL-specific error fields. Miscellaneous other APIs to cover specific feature gaps (e.g. getting/setting the ‘active’ catalog and schema). The PostgreSQL and SQLite drivers support ingesting Arrow data into temporary tables, and targeting a table in a particular namespace. Also, they handle quoting of table/column names in bulk ingestion better. Both drivers also support more Arrow types when executing queries and ingesting data. The FlightSQL driver supports basic logging via configuration in Go code or via an environment variable when built as a shared library (for Python/R). The Snowflake driver properly handles TIME fields. The minimum Go version was bumped to 1.19 (in line with the next release of the Arrow Go libraries). R drivers expose functions to quote/escape names and strings for the specific backend. The Python DBAPI layer wraps Arrow record batch readers to raise ADBC exceptions if the driver provides error metadata (see above). Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.6.0..apache-arrow-adbc-0.7.0 26 David Li 7 Dewey Dunnington 7 William Ayd 2 ElenaHenderson 1 Matt Topol 1 Solomon Choe 1 Sutou Kouhei 1 davidhcoe Roadmap Work continues on improving the existing drivers in terms of supported Arrow/database types, performance, and so on. For Java, the minimum JDK version may be raised from JDK 8 to 11 or 17, a move that the Arrow Java libraries are also considering. There are currently no plans for a second API revision. As work progresses on asynchronous and device-aware APIs in the Arrow ecosystem, ADBC will eventually be updated to support any new APIs. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>