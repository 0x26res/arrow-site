<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://arrow.apache.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arrow.apache.org/" rel="alternate" type="text/html" /><updated>2023-09-08T07:18:00-04:00</updated><id>https://arrow.apache.org/feed.xml</id><title type="html">Apache Arrow</title><subtitle>Apache Arrow is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, C++, C#, Go, Java, JavaScript, MATLAB, Python, R, Ruby, and Rust.</subtitle><entry><title type="html">Apache Arrow ADBC 0.6.0 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/08/28/adbc-0.6.0-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.6.0 (Libraries) Release" /><published>2023-08-28T00:00:00-04:00</published><updated>2023-08-28T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/08/28/adbc-0.6.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/08/28/adbc-0.6.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.6.0 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/10"><strong>46
resolved issues</strong></a> from <a href="#contributors"><strong>9 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version 0.6.0.
The <strong>API specification</strong> is versioned separately and is at version
1.0.0.  (The API version will be updated to 1.1.0 in the coming
release.)</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.6.0/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>The PostgreSQL and SQLite drivers have better type support.  The
PostgreSQL driver will also chunk its output now instead of always
delivering a single record batch.  The Snowflake driver no longer
requires a URI to connect.  The Python driver manager and Flight SQL
driver deliver more concise error messages, with less fluff around the
actual error message delivered by the database.</p>

<p>A critical bugfix for all Go-based drivers (Flight SQL and Snowflake)
is included.  This fixes panics caused by uninitialized memory in C
Data Interface structures appearing to the garbage collector as
(invalid) pointers to Go memory.  For details, see <a href="https://github.com/apache/arrow-adbc/issues/729">issue
729</a>.</p>

<p>While C# is not yet included in this release, the library is now more
compatible with .NET 4.</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.5.1..apache-arrow-adbc-0.6.0
    30	David Li
    15	William Ayd
     9	Dewey Dunnington
     5	Matt Topol
     5	Solomon Choe
     2	davidhcoe
     1	Alexandre Crayssac
     1	Curt Hagenlocher
     1	Diego Fernández Giraldo
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>The 1.1.0 revision of the ADBC API was accepted, but is not included
in this release.  It will be released as ADBC 0.7.0.</p>

<p>There are currently no plans for a second API revision.  As work
progresses on asynchronous and device-aware APIs in the Arrow
ecosystem, ADBC will eventually be updated to support any new APIs.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.6.0 release of the Apache Arrow ADBC libraries. This covers includes 46 resolved issues from 9 distinct contributors. This is a release of the libraries, which are at version 0.6.0. The API specification is versioned separately and is at version 1.0.0. (The API version will be updated to 1.1.0 in the coming release.) The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights The PostgreSQL and SQLite drivers have better type support. The PostgreSQL driver will also chunk its output now instead of always delivering a single record batch. The Snowflake driver no longer requires a URI to connect. The Python driver manager and Flight SQL driver deliver more concise error messages, with less fluff around the actual error message delivered by the database. A critical bugfix for all Go-based drivers (Flight SQL and Snowflake) is included. This fixes panics caused by uninitialized memory in C Data Interface structures appearing to the garbage collector as (invalid) pointers to Go memory. For details, see issue 729. While C# is not yet included in this release, the library is now more compatible with .NET 4. Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.5.1..apache-arrow-adbc-0.6.0 30 David Li 15 William Ayd 9 Dewey Dunnington 5 Matt Topol 5 Solomon Choe 2 davidhcoe 1 Alexandre Crayssac 1 Curt Hagenlocher 1 Diego Fernández Giraldo Roadmap The 1.1.0 revision of the ADBC API was accepted, but is not included in this release. It will be released as ADBC 0.7.0. There are currently no plans for a second API revision. As work progresses on asynchronous and device-aware APIs in the Arrow ecosystem, ADBC will eventually be updated to support any new APIs. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 13.0.0 Release</title><link href="https://arrow.apache.org/blog/2023/08/24/13.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 13.0.0 Release" /><published>2023-08-24T00:00:00-04:00</published><updated>2023-08-24T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/08/24/13.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/08/24/13.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 13.0.0 release. This covers
over 3 months of development work and includes <a href="https://github.com/apache/arrow/milestone/53?closed=1"><strong>456 resolved issues</strong></a>
from <a href="/release/13.0.0.html#contributors"><strong>108 distinct contributors</strong></a>. See the <a href="https://arrow.apache.org/install/">Install Page</a>
to learn how to get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/13.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 12.0.0 release, Marco Neumann, Gang Wu, Mehmet Ozan Kabak and Kevin Gurney
have been invited to be committers.
Matt Topol, Jie Wen, Ben Baumgold and Dewey Dunnington have joined the
Project Management Committee (PMC).</p>

<p>Thanks for your contributions and participation in the project!</p>

<h2 id="columnar-format-notes">Columnar Format Notes</h2>

<p>The <a href="https://arrow.apache.org/docs/dev/format/Columnar.html#run-end-encoded-layout">run-end encoded layout</a>
has been added.  This layout can allow data with long runs of duplicate values to be encoded and processed
efficiently.  Initial support has been added for <a href="https://github.com/apache/arrow/pull/14179">C++</a> and
<a href="https://github.com/apache/arrow/pull/14223">Go</a>.</p>

<h3 id="c-device-data-interface">C Device Data Interface</h3>

<p>An <strong>experimental</strong> new specification, the
<a href="https://arrow.apache.org/docs/dev/format/CDeviceDataInterface.html">C Device Data Interface</a>,
has been accepted for inclusion <a href="https://github.com/apache/arrow/issues/34971">GH-34971</a>. It builds on the existing
C Data Interface to provide a runtime-agnostic zero-copy sharing mechanism
for Arrow data residing on non-CPU devices.</p>

<p>Reference implementations of the C Device Data Interface will progressively
be added to the standard Arrow libraries after the 13.0.0 release.</p>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>

<p>Support for flagging ordered result sets to clients is now added. (<a href="https://github.com/apache/arrow/issues/34852">GH-34852</a>)</p>

<p>gRPC 1.30 is now the minimum supported version in C++/Python/R/etc. (<a href="https://github.com/apache/arrow/issues/36479">GH-34679</a>)</p>

<p>In C++, various methods now receive a full <code class="language-plaintext highlighter-rouge">ServerCallContext</code> (<a href="https://github.com/apache/arrow/issues/35442">GH-35442</a>, <a href="https://github.com/apache/arrow/issues/35377">GH-35377</a>) and the context now exposes headers sent by the client (<a href="https://github.com/apache/arrow/issues/35375">GH-35375</a>).</p>

<h2 id="c-notes">C++ notes</h2>

<h3 id="building">Building</h3>

<p>CMake 3.16 or later is now required for building Arrow C++ <a href="https://github.com/apache/arrow/issues/34921">GH-34921</a>.</p>

<p>Optimizations are not disabled anymore when the <code class="language-plaintext highlighter-rouge">RelWithDebInfo</code> build type
is selected <a href="https://github.com/apache/arrow/issues/35850">GH-35850</a>. Furthermore, compiler flags can now properly be
customized per-build type using <code class="language-plaintext highlighter-rouge">ARROW_C_FLAGS_DEBUG</code>, <code class="language-plaintext highlighter-rouge">ARROW_CXX_FLAGS_DEBUG</code>
and related variables <a href="https://github.com/apache/arrow/issues/35870">GH-35870</a>.</p>

<h3 id="acero">Acero</h3>

<p>Handling of unaligned buffers is input nodes can be configured programmatically
or by setting the environment variable <code class="language-plaintext highlighter-rouge">ACERO_ALIGNMENT_HANDLING</code>. The default
behavior is to warn when an unaligned buffer is detected <a href="https://github.com/apache/arrow/issues/35498">GH-35498</a>.</p>

<h3 id="compute">Compute</h3>

<p>Several new functions have been added:</p>
<ul>
  <li>aggregate functions “first”, “last”, “first_last” <a href="https://github.com/apache/arrow/issues/34911">GH-34911</a>;</li>
  <li>vector functions “cumulative_prod”, “cumulative_min”, “cumulative_max” <a href="https://github.com/apache/arrow/issues/32190">GH-32190</a>;</li>
  <li>vector function “pairwise_diff” <a href="https://github.com/apache/arrow/issues/35786">GH-35786</a>.</li>
</ul>

<p>Sorting now works on dictionary arrays, with a much better performance than
the naive approach of sorting the decoded dictionary <a href="https://github.com/apache/arrow/issues/29887">GH-29887</a>. Sorting also
works on struct arrays, and nested sort keys are supported using <code class="language-plaintext highlighter-rouge">FieldRed</code> <a href="https://github.com/apache/arrow/issues/33206">GH-33206</a>.</p>

<p>The <code class="language-plaintext highlighter-rouge">check_overflow</code> option has been removed from <code class="language-plaintext highlighter-rouge">CumulativeSumOptions</code> as
it was redundant with the availability of two different functions:
“cumulative_sum” and “cumulative_sum_checked” <a href="https://github.com/apache/arrow/issues/35789">GH-35789</a>.</p>

<p>Run-end encoded filters are efficiently supported <a href="https://github.com/apache/arrow/issues/35749">GH-35749</a>.</p>

<p>Duration types are supported with the “is_in” and “index_in” functions <a href="https://github.com/apache/arrow/issues/36047">GH-36047</a>.
They can be multiplied with all integer types <a href="https://github.com/apache/arrow/issues/36128">GH-36128</a>.</p>

<p>“is_in” and “index_in” now cast their inputs more flexibly: they first attempt
to cast the value set to the input type, then in the other direction if the
former fails <a href="https://github.com/apache/arrow/issues/36203">GH-36203</a>.</p>

<p>Multiple bugs have been fixed in “utf8_slice_codeunits” when the <code class="language-plaintext highlighter-rouge">stop</code> option
is omitted <a href="https://github.com/apache/arrow/issues/36311">GH-36311</a>.</p>

<h3 id="dataset">Dataset</h3>

<p>A custom schema can now be passed when writing a dataset <a href="https://github.com/apache/arrow/issues/35730">GH-35730</a>. The custom
schema can alter nullability or metadata information, but is not allowed to
change the datatypes written.</p>

<h3 id="filesystems">Filesystems</h3>

<p>The S3 filesystem now writes files in equal-sized chunks, for compatibility with
Cloudflare’s “R2” Storage <a href="https://github.com/apache/arrow/issues/34363">GH-34363</a>.</p>

<p>A long-standing issue where S3 support could crash at shutdown because of resources
still being alive after S3 finalization has been fixed <a href="https://github.com/apache/arrow/issues/36346">GH-36346</a>. Now, attempts
to use S3 resources (such as making filesystem calls) after S3 finalization should
result in a clean error.</p>

<p>The GCS filesystem accepts a new option to set the project id <a href="https://github.com/apache/arrow/issues/36227">GH-36227</a>.</p>

<h3 id="ipc">IPC</h3>

<p>Nullability and metadata information for sub-fields of map types is now preserved
when deserializing Arrow IPC <a href="https://github.com/apache/arrow/issues/35297">GH-35297</a>.</p>

<h3 id="orc">Orc</h3>

<p>The Orc adapter now maps Arrow field metadata to Orc type attributes when writing,
and vice-versa when reading <a href="https://github.com/apache/arrow/issues/35304">GH-35304</a>.</p>

<h3 id="parquet">Parquet</h3>

<p>It is now possible to write additional metadata while a <code class="language-plaintext highlighter-rouge">ParquetFileWriter</code> is
open <a href="https://github.com/apache/arrow/issues/34888">GH-34888</a>.</p>

<p>Writing a page index can be enabled selectively per-column <a href="https://github.com/apache/arrow/issues/34949">GH-34949</a>.
In addition, page header statistics are not written anymore if the page
index is enabled for the given column <a href="https://github.com/apache/arrow/issues/34375">GH-34375</a>, as the information would
be redundant and less efficiently accessed.</p>

<p>Parquet writer properties allow specifying the sorting columns <a href="https://github.com/apache/arrow/issues/35331">GH-35331</a>.
The user is responsible for ensuring that the data written to the file
actually complies with the given sorting.</p>

<p>CRC computation has been implemented for v2 data pages <a href="https://github.com/apache/arrow/issues/35171">GH-35171</a>.
It was already implemented for v1 data pages.</p>

<p>Writing compliant nested types is now enabled by default <a href="https://github.com/apache/arrow/issues/29781">GH-29781</a>. This
should not have any negative implication.</p>

<p>Attempting to load a subset of an Arrow extension type is now forbidden
<a href="https://github.com/apache/arrow/issues/20385">GH-20385</a>. Previously, if an extension type’s storage is nested (for example
a “Point” extension type backed by a <code class="language-plaintext highlighter-rouge">struct&lt;x: float64, y: float64&gt;</code>),
it was possible to load selectively some of the columns of the storage type.</p>

<h3 id="substrait">Substrait</h3>

<p>Support for various functions has been added: “stddev”, “variance”, “first”,
“last” (GH-35247, GH-35506).</p>

<p>Deserializing sorts is now supported <a href="https://github.com/apache/arrow/issues/32763">GH-32763</a>. However, some features,
such as clustered sort direction or custom sort functions, are not
implemented.</p>

<h3 id="miscellaneous">Miscellaneous</h3>

<p><code class="language-plaintext highlighter-rouge">FieldRef</code> sports additional methods to get a flattened version of nested
fields <a href="https://github.com/apache/arrow/issues/14946">GH-14946</a>. Compared to their non-flattened counterparts,
the methods <code class="language-plaintext highlighter-rouge">GetFlattened</code>, <code class="language-plaintext highlighter-rouge">GetAllFlattened</code>, <code class="language-plaintext highlighter-rouge">GetOneFlattened</code> and
<code class="language-plaintext highlighter-rouge">GetOneOrNoneFlattened</code> combine a child’s null bitmap with its ancestors’
null bitmaps such as to compute the field’s overall logical validity bitmap.</p>

<p>In other words, given the struct array <code class="language-plaintext highlighter-rouge">[null, {'x': null}, {'x': 5}]</code>,
<code class="language-plaintext highlighter-rouge">FieldRef("x")::Get</code> might return <code class="language-plaintext highlighter-rouge">[0, null, 5]</code>
while <code class="language-plaintext highlighter-rouge">FieldRef("y")::GetFlattened</code> will <em>always</em> return <code class="language-plaintext highlighter-rouge">[null, null, 5]</code>.</p>

<p><code class="language-plaintext highlighter-rouge">Scalar::hash()</code> has been fixed for sliced nested arrays <a href="https://github.com/apache/arrow/issues/35360">GH-35360</a>.</p>

<p>A new floating-point to decimal conversion algorithm exhibits much better
precision <a href="https://github.com/apache/arrow/issues/35576">GH-35576</a>.</p>

<p>It is now possible to cast between scalars of different list-like types
<a href="https://github.com/apache/arrow/issues/36309">GH-36309</a>.</p>

<h2 id="c-notes-1">C# notes</h2>

<h3 id="enhancements">Enhancements</h3>

<ul>
  <li>
    <p>The <a href="https://arrow.apache.org/docs/format/CDataInterface.html">C Data Interface</a> is now supported in the .NET Apache.Arrow library. The main entry points are <code class="language-plaintext highlighter-rouge">CArrowArrayImporter.ImportArray</code>, <code class="language-plaintext highlighter-rouge">CArrowArrayExporter.ExportArray</code>, <code class="language-plaintext highlighter-rouge">CArrowArrayStreamImporter.ImportArrayStream</code>, and <code class="language-plaintext highlighter-rouge">CArrowArrayStreamExporter.ExportArrayStream</code> in the <code class="language-plaintext highlighter-rouge">Apache.Arrow.C</code> namespace. (<a href="https://github.com/apache/arrow/issues/33856">GH-33856</a>, <a href="https://github.com/apache/arrow/issues/33857">GH-33857</a>, <a href="https://github.com/apache/arrow/issues/36120">GH-36120</a>, and <a href="https://github.com/apache/arrow/issues/35809">GH-35809</a>).</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">ArrowBuffer.BitmapBuilder</code> adds <code class="language-plaintext highlighter-rouge">Append(ReadOnlySpan&lt;byte&gt; source, int validBits)</code> and <code class="language-plaintext highlighter-rouge">AppendRange(bool value, int length)</code> to improve performance of array concatenation (<a href="https://github.com/apache/arrow/issues/32605">GH-32605</a>)</p>
  </li>
</ul>

<h3 id="bug-fixes">Bug Fixes</h3>

<ul>
  <li>TotalBytes and TotalRecords are now being serialized in FlightInfo (<a href="https://github.com/apache/arrow/issues/35267">GH-35267</a>)
    <h2 id="go-notes">Go notes</h2>
  </li>
</ul>

<h3 id="enhancements-1">Enhancements</h3>

<h4 id="arrow">Arrow</h4>

<ul>
  <li>Compute arithmetic functions are now available for Float16 (<a href="https://github.com/apache/arrow/issues/35162">GH-35162</a>)</li>
  <li>Float16, Large* and Fixed types are all now supported by the CSV reader/writer (<a href="https://github.com/apache/arrow/issues/36105">GH-36105</a> and <a href="https://github.com/apache/arrow/issues/36141">GH-36141</a>)</li>
  <li>CSV Reader uses <code class="language-plaintext highlighter-rouge">AppendValueFromString</code> for extension types and properly reads empty values as null (<a href="https://github.com/apache/arrow/issues/35188">GH-35188</a> and <a href="https://github.com/apache/arrow/issues/35190">GH-35190</a>)</li>
  <li><a href="https://github.com/substrait-io/substrait-go">Substrait</a> expressions can now be executed using the Compute library (<a href="https://github.com/apache/arrow/issues/35652">GH-35652</a>)</li>
  <li>You can now read back values from Dictionary Builders before finishing the array (<a href="https://github.com/apache/arrow/issues/35711">GH-35711</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">MapType.ValueField</code> and <code class="language-plaintext highlighter-rouge">MapType.ValueType</code> are now deprecated in favor of <code class="language-plaintext highlighter-rouge">MapType.Elem().(*StructType)</code> (<a href="https://github.com/apache/arrow/issues/35909">GH-35909</a>)</li>
  <li>Multiple equality functions which have been deprecated since v9 have  now been removed (Such as <code class="language-plaintext highlighter-rouge">array.ArraySliceEqual</code> in favor of <code class="language-plaintext highlighter-rouge">array.SliceEqual</code>) (<a href="https://github.com/apache/arrow/issues/36198">GH-36198</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">ValueStr</code> method on Timestamp arrays now includes the zone in the output (<a href="https://github.com/apache/arrow/issues/36568">GH-36568</a>)</li>
  <li><em>BREAKING CHANGE</em> <code class="language-plaintext highlighter-rouge">FixedSizeListBuilder.AppendNull</code> no longer requires manually appending nulls to the underlying list  (<a href="https://github.com/apache/arrow/issues/35482">GH-35482</a>)</li>
</ul>

<h4 id="flight">Flight</h4>

<ul>
  <li>FlightSQL driver supports non-prepared queries now (<a href="https://github.com/apache/arrow/issues/35136">GH-35136</a>)</li>
</ul>

<h4 id="parquet-1">Parquet</h4>

<ul>
  <li>Error messages in row group writer have been improved (<a href="https://github.com/apache/arrow/issues/36319">GH-36319</a>)</li>
</ul>

<h3 id="bug-fixes-1">Bug Fixes</h3>

<ul>
  <li>Cross architecture build failures with v12.0.1 have been fixed (<a href="https://github.com/apache/arrow/issues/36052">GH-36052</a>)</li>
</ul>

<h4 id="arrow-1">Arrow</h4>

<ul>
  <li>It is now possible to build the Arrow Go lib using tinygo for building smaller WASM binaries (<a href="https://github.com/apache/arrow/issues/32832">GH-32832</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">Fields</code> method for Schema and StructType now returns a copy of the slice to ensure immutability (<a href="https://github.com/apache/arrow/issues/35306">GH-35306</a> and <a href="https://github.com/apache/arrow/issues/35866">GH-35866</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">array.ApproxEqual</code> for Maps now allows entries for a given element to be presented in any order (<a href="https://github.com/apache/arrow/issues/35828">GH-35828</a>)</li>
  <li>Fix issues with decimal256 arrays (<a href="https://github.com/apache/arrow/issues/35911">GH-35911</a>, <a href="https://github.com/apache/arrow/issues/35965">GH-35965</a>, and <a href="https://github.com/apache/arrow/issues/35975">GH-35975</a>)</li>
  <li>StructType now allows duplicate field names correctly (<a href="https://github.com/apache/arrow/issues/36014">GH-36014</a>)</li>
</ul>

<h4 id="flight-1">Flight</h4>

<ul>
  <li>Fix crash in client middleware (<a href="https://github.com/apache/arrow/issues/35240">GH-35240</a>)</li>
</ul>

<h4 id="parquet-2">Parquet</h4>

<ul>
  <li>Various memory leaks addressed in pqarrow package (<a href="https://github.com/apache/arrow/issues/35015">GH-35015</a>)</li>
  <li>Fixed panic for <code class="language-plaintext highlighter-rouge">ListOf(types)</code> if null (<a href="https://github.com/apache/arrow/issues/35684">GH-35684</a>)</li>
</ul>

<h2 id="java-notes">Java notes</h2>

<p>The JNI bindings for Arrow Dataset now support execute <a href="https://substrait.io/">Substrait</a> plans via the <a href="https://arrow.apache.org/docs/dev/cpp/streaming_execution.html">Acero</a> query engine. (<a href="https://github.com/apache/arrow/issues/34223">GH-34223</a>)</p>

<p>Arrow packages that depend on Netty (most notably, <code class="language-plaintext highlighter-rouge">arrow-memory-netty</code>, but also Arrow Flight) now require either Netty &lt; 4.1.94.Final or Netty &gt;= 4.1.96.Final. In Netty versions 4.1.94.Final and 4.1.95.Final, there was a breaking change in an internal API that affected Arrow; this was reverted in 4.1.96.Final (<a href="https://github.com/apache/arrow/issues/36209">GH-36209</a>, <a href="https://github.com/apache/arrow/issues/36928">GH-36928</a>)</p>

<p><code class="language-plaintext highlighter-rouge">VectorSchemaRoot#slice</code> now always makes a copy, including when the slice covers all rows (previously it did not make a copy in this case). This is a potentially-breaking change if your application depended on the old behavior. (<a href="https://github.com/apache/arrow/issues/35275">GH-35275</a>)</p>

<p>Debug info for allocations is no longer automatically enabled when assertions are enabled (e.g. when running unit tests). Instead, support must be explicitly enabled. This is not quite a breaking change, but may be surprising if you are used to using this information while debugging tests. However, performance should be greatly improved while running tests. (<a href="https://github.com/apache/arrow/issues/34338">GH-34338</a>)</p>

<p>Support for the upcoming Java 21 was added, though we do not yet test this in CI (<a href="https://github.com/apache/arrow/issues/35053">GH-5053</a>). The JNI bindings for Arrow Dataset now expose JSON support (<a href="https://github.com/apache/arrow/issues/36421">GH-36421</a>). Dictionary replacement is now supported when writing the IPC stream format (<a href="https://github.com/apache/arrow/issues/18547">GH-18547</a>).</p>

<h2 id="javascript-notes">JavaScript notes</h2>

<ul>
  <li>Updated dependencies: https://github.com/apache/arrow/pull/36032</li>
</ul>

<h2 id="python-notes">Python notes</h2>

<p>Compatibility notes:</p>

<ul>
  <li>The default format version for Parquet has been bumped from 2.4 to 2.6 <a href="https://github.com/apache/arrow/issues/35746">GH-35746</a>. In practice, this means that nanosecond timestamps now preserve its resolution instead of being converted to microseconds.</li>
  <li>Support for Python 3.7 is dropped <a href="https://github.com/apache/arrow/issues/34788">GH-34788</a></li>
</ul>

<p>New features:</p>

<ul>
  <li>Conversion to non-nano datetime64 for pandas &gt;= 2.0 is now supported <a href="https://github.com/apache/arrow/issues/33321">GH-33321</a></li>
  <li>Write page index is now supported <a href="https://github.com/apache/arrow/issues/36284">GH-36284</a></li>
  <li>Bindings for reading JSON format in Dataset are added <a href="https://github.com/apache/arrow/issues/34216">GH-34216</a></li>
  <li><code class="language-plaintext highlighter-rouge">keys_sorted</code> property of MapType is now exposed <a href="https://github.com/apache/arrow/issues/35112">GH-35112</a></li>
</ul>

<p>Other improvements:</p>

<ul>
  <li>Common python functionality between <code class="language-plaintext highlighter-rouge">Table</code> and <code class="language-plaintext highlighter-rouge">RecordBatch</code> classes has been consolidated ( <a href="https://github.com/apache/arrow/issues/36129">GH-36129</a>, <a href="https://github.com/apache/arrow/issues/35415">GH-35415</a>, <a href="https://github.com/apache/arrow/issues/35390">GH-35390</a>, <a href="https://github.com/apache/arrow/issues/34979">GH-34979</a>, <a href="https://github.com/apache/arrow/issues/34868">GH-34868</a>, <a href="https://github.com/apache/arrow/issues/31868">GH-31868</a>)</li>
  <li>Some functionality for <code class="language-plaintext highlighter-rouge">FixedShapeTensorType</code> has been improved (<code class="language-plaintext highlighter-rouge">__reduce__</code> <a href="https://github.com/apache/arrow/issues/36038">GH-36038</a>, picklability <a href="https://github.com/apache/arrow/issues/35599">GH-35599</a>)</li>
  <li>Pyarrow scalars can now be accepted in the <code class="language-plaintext highlighter-rouge">array</code> constructor <a href="https://github.com/apache/arrow/issues/21761">GH-21761</a></li>
  <li>DataFrame Interchange Protocol implementation and usage is now documented <a href="https://github.com/apache/arrow/issues/33980">GH-33980</a></li>
  <li>Conversion between Arrow and Pandas for map/pydict now has enhanced support <a href="https://github.com/apache/arrow/issues/34729">GH-34729</a></li>
  <li>Usability of <code class="language-plaintext highlighter-rouge">pc.map_lookup</code> / <code class="language-plaintext highlighter-rouge">MapLookupOptions</code> is improved <a href="https://github.com/apache/arrow/issues/36045">GH-36045</a></li>
  <li><code class="language-plaintext highlighter-rouge">zero_copy_only</code> keyword can now also be accepted in <code class="language-plaintext highlighter-rouge">ChunkedArray.to_numpy()</code> <a href="https://github.com/apache/arrow/issues/34787">GH-34787</a></li>
  <li>Python C++ codebase now has linter support in Archery and the CI <a href="https://github.com/apache/arrow/issues/35485">GH-35485</a></li>
</ul>

<p>Relevant bug fixes:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">__array__</code> numpy conversion for Table and RecordBatch is now corrected so that <code class="language-plaintext highlighter-rouge">np.asarray(pa.Table)</code> doesn’t return a transposed result <a href="https://github.com/apache/arrow/issues/34886">GH-34886</a></li>
  <li><code class="language-plaintext highlighter-rouge">parquet.write_to_dataset</code> doesn’t create empty files for non-observed dictionary (category) values anymore <a href="https://github.com/apache/arrow/issues/23870">GH-23870</a></li>
  <li>Dataset writer now also correctly follows default Parquet version of 2.6 <a href="https://github.com/apache/arrow/issues/36537">GH-36537</a></li>
  <li>Comparing <code class="language-plaintext highlighter-rouge">pyarrow.dataset.Partitioning</code> with other type is now correctly handled <a href="https://github.com/apache/arrow/issues/36659">GH-36659</a></li>
  <li>Pickling of pyarrow.dataset PartitioningFactory objects is now supported <a href="https://github.com/apache/arrow/issues/34884">GH-34884</a></li>
  <li>None schema is now disallowed in parquet writer <a href="https://github.com/apache/arrow/issues/35858">GH-35858</a></li>
  <li><code class="language-plaintext highlighter-rouge">pa.FixedShapeTensorArray.to_numpy_ndarray</code> is not failing on sliced arrays <a href="https://github.com/apache/arrow/issues/35573">GH-35573</a></li>
  <li>Halffloat type is now supported in the conversion from Arrow list to pandas <a href="https://github.com/apache/arrow/issues/36168">GH-36168</a></li>
  <li><code class="language-plaintext highlighter-rouge">__from_arrow__</code> is now also implemented for <code class="language-plaintext highlighter-rouge">Array.to_pandas</code> for pandas extension data types <a href="https://github.com/apache/arrow/issues/36096">GH-36096</a></li>
</ul>

<h2 id="r-notes">R notes</h2>

<p>New features:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">open_dataset()</code> now works with ND-JSON files <a href="https://github.com/apache/arrow/issues/35055">GH-35055</a></li>
  <li>Calling <code class="language-plaintext highlighter-rouge">schema()</code> on multiple Arrow objects now returns the object’s schema <a href="https://github.com/apache/arrow/issues/35543">GH-35543</a></li>
  <li>dplyr <code class="language-plaintext highlighter-rouge">.by</code>/<code class="language-plaintext highlighter-rouge">by</code> argument now supported in arrow implementation of dplyr verbs  <a href="https://github.com/apache/arrow/issues/35667">GH-35667</a></li>
</ul>

<p>Other improvements:</p>
<ul>
  <li>Convenience function <code class="language-plaintext highlighter-rouge">arrow_array()</code> can be used to create Arrow Arrays <a href="https://github.com/apache/arrow/issues/36381">GH-36381</a></li>
  <li>Convenience function <code class="language-plaintext highlighter-rouge">scalar()</code> can be used to create Arrow Scalars  <a href="https://github.com/apache/arrow/issues/36265">GH-36265</a></li>
  <li>Prevent crashed when passing data between arrow and duckdb by always calling <code class="language-plaintext highlighter-rouge">RecordBatchReader::ReadNext()</code> from DuckDB from the main R thread <a href="https://github.com/apache/arrow/issues/36307">GH-36307</a></li>
  <li>Issue a warning for <code class="language-plaintext highlighter-rouge">set_io_thread_count()</code> with <code class="language-plaintext highlighter-rouge">num_threads</code> &lt; 2 <a href="https://github.com/apache/arrow/issues/36304">GH-36304</a></li>
  <li>Ensure missing grouping variables are added to the beginning of the variable list <a href="https://github.com/apache/arrow/issues/36305">GH-36305</a></li>
  <li>CSV File reader options class objects can print the selected values <a href="https://github.com/apache/arrow/issues/35955">GH-35955</a></li>
  <li>Schema metadata can be set as a named character vector <a href="https://github.com/apache/arrow/issues/35954">GH-35954</a></li>
  <li>Ensure that the RStringViewer helper class does not own any Array references <a href="https://github.com/apache/arrow/issues/35812">GH-35812</a></li>
  <li><code class="language-plaintext highlighter-rouge">strptime()</code> in arrow will return a timezone-aware timestamp if <code class="language-plaintext highlighter-rouge">%z</code> is part of the format string <a href="https://github.com/apache/arrow/issues/35671">GH-35671</a></li>
  <li>Column ordering when combining <code class="language-plaintext highlighter-rouge">group_by()</code> and <code class="language-plaintext highlighter-rouge">across()</code> now matches dplyr <a href="https://github.com/apache/arrow/issues/35473">GH-35473</a></li>
  <li>Link to correct version of OpenSSL when using autobrew <a href="https://github.com/apache/arrow/issues/36551">GH-36551</a></li>
  <li>Require cmake 3.16 in bundled build script <a href="https://github.com/apache/arrow/issues/36321">GH-36321</a></li>
</ul>

<p>For more on what’s in the 13.0.0 R package, see the <a href="/docs/r/news/">R changelog</a>.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<h3 id="ruby">Ruby</h3>

<h4 id="bug-fixes-2">Bug fixes</h4>

<ul>
  <li>Fixed GC-related issue against random segfault in hash join <a href="https://github.com/apache/arrow/issues/35819">GH-35819</a></li>
  <li>Fixed segfault in <code class="language-plaintext highlighter-rouge">CallExpression.new</code> <a href="https://github.com/apache/arrow/issues/35915">GH-35915</a></li>
</ul>

<h4 id="improvements">Improvements</h4>

<ul>
  <li>FlightRPC: Added a convenient wrapper for the authentication method <a href="https://github.com/apache/arrow/issues/35435">GH-35435</a></li>
  <li>Added empty table support in <code class="language-plaintext highlighter-rouge">#select_columns</code> <a href="https://github.com/apache/arrow/issues/35681">GH-35681</a></li>
  <li>Added optional hash support in <code class="language-plaintext highlighter-rouge">Expression.try_convert</code> <a href="https://github.com/apache/arrow/issues/35915">GH-35915</a></li>
  <li>Parquet: Added <code class="language-plaintext highlighter-rouge">Parquet::ArrowFileReader#each_row_group</code> <a href="https://github.com/apache/arrow/issues/36008">GH-36008</a></li>
  <li>Added support of the automatic installation of arrow-c-glib on Conda environment <a href="https://github.com/apache/arrow/issues/36287">GH-36287</a></li>
</ul>

<h3 id="c-glib">C GLib</h3>

<h4 id="bug-fixes-3">Bug fixes</h4>

<ul>
  <li>Parquet: Fixed GC-related bug in metadata dependencies <a href="https://github.com/apache/arrow/issues/35266">GH-35266</a></li>
  <li>Fixed potentially GC-related issue against random segfault in hash join <a href="https://github.com/apache/arrow/issues/35819">GH-35819</a></li>
</ul>

<h4 id="improvements-1">Improvements</h4>

<ul>
  <li>FlightRPC: Added support to pass <code class="language-plaintext highlighter-rouge">GAFlightServerCallContext</code> object in several methods of <code class="language-plaintext highlighter-rouge">GAFlightServerCustomAuthHandler</code> <a href="https://github.com/apache/arrow/issues/35377">GH-35377</a></li>
  <li>FlightSQL: Added support for INSERT/UPDATE/DELETE <a href="https://github.com/apache/arrow/issues/36408">GH-36408</a></li>
  <li>Added <code class="language-plaintext highlighter-rouge">GArrowRunEndEncodedDataType</code> <a href="https://github.com/apache/arrow/issues/35417">GH-35417</a></li>
  <li>Added <code class="language-plaintext highlighter-rouge">GArrowRunEndEncodedArray</code> <a href="https://github.com/apache/arrow/issues/35418">GH-35418</a></li>
</ul>

<h2 id="rust-notes">Rust notes</h2>

<p>The Rust projects have moved to separate repositories outside the
main Arrow monorepo. For notes on the latest release of the Rust
implementation, see the latest <a href="https://github.com/apache/arrow-rs/tags">Arrow Rust changelog</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 13.0.0 release. This covers over 3 months of development work and includes 456 resolved issues from 108 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 12.0.0 release, Marco Neumann, Gang Wu, Mehmet Ozan Kabak and Kevin Gurney have been invited to be committers. Matt Topol, Jie Wen, Ben Baumgold and Dewey Dunnington have joined the Project Management Committee (PMC). Thanks for your contributions and participation in the project! Columnar Format Notes The run-end encoded layout has been added. This layout can allow data with long runs of duplicate values to be encoded and processed efficiently. Initial support has been added for C++ and Go. C Device Data Interface An experimental new specification, the C Device Data Interface, has been accepted for inclusion GH-34971. It builds on the existing C Data Interface to provide a runtime-agnostic zero-copy sharing mechanism for Arrow data residing on non-CPU devices. Reference implementations of the C Device Data Interface will progressively be added to the standard Arrow libraries after the 13.0.0 release. Arrow Flight RPC notes Support for flagging ordered result sets to clients is now added. (GH-34852) gRPC 1.30 is now the minimum supported version in C++/Python/R/etc. (GH-34679) In C++, various methods now receive a full ServerCallContext (GH-35442, GH-35377) and the context now exposes headers sent by the client (GH-35375). C++ notes Building CMake 3.16 or later is now required for building Arrow C++ GH-34921. Optimizations are not disabled anymore when the RelWithDebInfo build type is selected GH-35850. Furthermore, compiler flags can now properly be customized per-build type using ARROW_C_FLAGS_DEBUG, ARROW_CXX_FLAGS_DEBUG and related variables GH-35870. Acero Handling of unaligned buffers is input nodes can be configured programmatically or by setting the environment variable ACERO_ALIGNMENT_HANDLING. The default behavior is to warn when an unaligned buffer is detected GH-35498. Compute Several new functions have been added: aggregate functions “first”, “last”, “first_last” GH-34911; vector functions “cumulative_prod”, “cumulative_min”, “cumulative_max” GH-32190; vector function “pairwise_diff” GH-35786. Sorting now works on dictionary arrays, with a much better performance than the naive approach of sorting the decoded dictionary GH-29887. Sorting also works on struct arrays, and nested sort keys are supported using FieldRed GH-33206. The check_overflow option has been removed from CumulativeSumOptions as it was redundant with the availability of two different functions: “cumulative_sum” and “cumulative_sum_checked” GH-35789. Run-end encoded filters are efficiently supported GH-35749. Duration types are supported with the “is_in” and “index_in” functions GH-36047. They can be multiplied with all integer types GH-36128. “is_in” and “index_in” now cast their inputs more flexibly: they first attempt to cast the value set to the input type, then in the other direction if the former fails GH-36203. Multiple bugs have been fixed in “utf8_slice_codeunits” when the stop option is omitted GH-36311. Dataset A custom schema can now be passed when writing a dataset GH-35730. The custom schema can alter nullability or metadata information, but is not allowed to change the datatypes written. Filesystems The S3 filesystem now writes files in equal-sized chunks, for compatibility with Cloudflare’s “R2” Storage GH-34363. A long-standing issue where S3 support could crash at shutdown because of resources still being alive after S3 finalization has been fixed GH-36346. Now, attempts to use S3 resources (such as making filesystem calls) after S3 finalization should result in a clean error. The GCS filesystem accepts a new option to set the project id GH-36227. IPC Nullability and metadata information for sub-fields of map types is now preserved when deserializing Arrow IPC GH-35297. Orc The Orc adapter now maps Arrow field metadata to Orc type attributes when writing, and vice-versa when reading GH-35304. Parquet It is now possible to write additional metadata while a ParquetFileWriter is open GH-34888. Writing a page index can be enabled selectively per-column GH-34949. In addition, page header statistics are not written anymore if the page index is enabled for the given column GH-34375, as the information would be redundant and less efficiently accessed. Parquet writer properties allow specifying the sorting columns GH-35331. The user is responsible for ensuring that the data written to the file actually complies with the given sorting. CRC computation has been implemented for v2 data pages GH-35171. It was already implemented for v1 data pages. Writing compliant nested types is now enabled by default GH-29781. This should not have any negative implication. Attempting to load a subset of an Arrow extension type is now forbidden GH-20385. Previously, if an extension type’s storage is nested (for example a “Point” extension type backed by a struct&lt;x: float64, y: float64&gt;), it was possible to load selectively some of the columns of the storage type. Substrait Support for various functions has been added: “stddev”, “variance”, “first”, “last” (GH-35247, GH-35506). Deserializing sorts is now supported GH-32763. However, some features, such as clustered sort direction or custom sort functions, are not implemented. Miscellaneous FieldRef sports additional methods to get a flattened version of nested fields GH-14946. Compared to their non-flattened counterparts, the methods GetFlattened, GetAllFlattened, GetOneFlattened and GetOneOrNoneFlattened combine a child’s null bitmap with its ancestors’ null bitmaps such as to compute the field’s overall logical validity bitmap. In other words, given the struct array [null, {'x': null}, {'x': 5}], FieldRef("x")::Get might return [0, null, 5] while FieldRef("y")::GetFlattened will always return [null, null, 5]. Scalar::hash() has been fixed for sliced nested arrays GH-35360. A new floating-point to decimal conversion algorithm exhibits much better precision GH-35576. It is now possible to cast between scalars of different list-like types GH-36309. C# notes Enhancements The C Data Interface is now supported in the .NET Apache.Arrow library. The main entry points are CArrowArrayImporter.ImportArray, CArrowArrayExporter.ExportArray, CArrowArrayStreamImporter.ImportArrayStream, and CArrowArrayStreamExporter.ExportArrayStream in the Apache.Arrow.C namespace. (GH-33856, GH-33857, GH-36120, and GH-35809). ArrowBuffer.BitmapBuilder adds Append(ReadOnlySpan&lt;byte&gt; source, int validBits) and AppendRange(bool value, int length) to improve performance of array concatenation (GH-32605) Bug Fixes TotalBytes and TotalRecords are now being serialized in FlightInfo (GH-35267) Go notes Enhancements Arrow Compute arithmetic functions are now available for Float16 (GH-35162) Float16, Large* and Fixed types are all now supported by the CSV reader/writer (GH-36105 and GH-36141) CSV Reader uses AppendValueFromString for extension types and properly reads empty values as null (GH-35188 and GH-35190) Substrait expressions can now be executed using the Compute library (GH-35652) You can now read back values from Dictionary Builders before finishing the array (GH-35711) MapType.ValueField and MapType.ValueType are now deprecated in favor of MapType.Elem().(*StructType) (GH-35909) Multiple equality functions which have been deprecated since v9 have now been removed (Such as array.ArraySliceEqual in favor of array.SliceEqual) (GH-36198) ValueStr method on Timestamp arrays now includes the zone in the output (GH-36568) BREAKING CHANGE FixedSizeListBuilder.AppendNull no longer requires manually appending nulls to the underlying list (GH-35482) Flight FlightSQL driver supports non-prepared queries now (GH-35136) Parquet Error messages in row group writer have been improved (GH-36319) Bug Fixes Cross architecture build failures with v12.0.1 have been fixed (GH-36052) Arrow It is now possible to build the Arrow Go lib using tinygo for building smaller WASM binaries (GH-32832) Fields method for Schema and StructType now returns a copy of the slice to ensure immutability (GH-35306 and GH-35866) array.ApproxEqual for Maps now allows entries for a given element to be presented in any order (GH-35828) Fix issues with decimal256 arrays (GH-35911, GH-35965, and GH-35975) StructType now allows duplicate field names correctly (GH-36014) Flight Fix crash in client middleware (GH-35240) Parquet Various memory leaks addressed in pqarrow package (GH-35015) Fixed panic for ListOf(types) if null (GH-35684) Java notes The JNI bindings for Arrow Dataset now support execute Substrait plans via the Acero query engine. (GH-34223) Arrow packages that depend on Netty (most notably, arrow-memory-netty, but also Arrow Flight) now require either Netty &lt; 4.1.94.Final or Netty &gt;= 4.1.96.Final. In Netty versions 4.1.94.Final and 4.1.95.Final, there was a breaking change in an internal API that affected Arrow; this was reverted in 4.1.96.Final (GH-36209, GH-36928) VectorSchemaRoot#slice now always makes a copy, including when the slice covers all rows (previously it did not make a copy in this case). This is a potentially-breaking change if your application depended on the old behavior. (GH-35275) Debug info for allocations is no longer automatically enabled when assertions are enabled (e.g. when running unit tests). Instead, support must be explicitly enabled. This is not quite a breaking change, but may be surprising if you are used to using this information while debugging tests. However, performance should be greatly improved while running tests. (GH-34338) Support for the upcoming Java 21 was added, though we do not yet test this in CI (GH-5053). The JNI bindings for Arrow Dataset now expose JSON support (GH-36421). Dictionary replacement is now supported when writing the IPC stream format (GH-18547). JavaScript notes Updated dependencies: https://github.com/apache/arrow/pull/36032 Python notes Compatibility notes: The default format version for Parquet has been bumped from 2.4 to 2.6 GH-35746. In practice, this means that nanosecond timestamps now preserve its resolution instead of being converted to microseconds. Support for Python 3.7 is dropped GH-34788 New features: Conversion to non-nano datetime64 for pandas &gt;= 2.0 is now supported GH-33321 Write page index is now supported GH-36284 Bindings for reading JSON format in Dataset are added GH-34216 keys_sorted property of MapType is now exposed GH-35112 Other improvements: Common python functionality between Table and RecordBatch classes has been consolidated ( GH-36129, GH-35415, GH-35390, GH-34979, GH-34868, GH-31868) Some functionality for FixedShapeTensorType has been improved (__reduce__ GH-36038, picklability GH-35599) Pyarrow scalars can now be accepted in the array constructor GH-21761 DataFrame Interchange Protocol implementation and usage is now documented GH-33980 Conversion between Arrow and Pandas for map/pydict now has enhanced support GH-34729 Usability of pc.map_lookup / MapLookupOptions is improved GH-36045 zero_copy_only keyword can now also be accepted in ChunkedArray.to_numpy() GH-34787 Python C++ codebase now has linter support in Archery and the CI GH-35485 Relevant bug fixes: __array__ numpy conversion for Table and RecordBatch is now corrected so that np.asarray(pa.Table) doesn’t return a transposed result GH-34886 parquet.write_to_dataset doesn’t create empty files for non-observed dictionary (category) values anymore GH-23870 Dataset writer now also correctly follows default Parquet version of 2.6 GH-36537 Comparing pyarrow.dataset.Partitioning with other type is now correctly handled GH-36659 Pickling of pyarrow.dataset PartitioningFactory objects is now supported GH-34884 None schema is now disallowed in parquet writer GH-35858 pa.FixedShapeTensorArray.to_numpy_ndarray is not failing on sliced arrays GH-35573 Halffloat type is now supported in the conversion from Arrow list to pandas GH-36168 __from_arrow__ is now also implemented for Array.to_pandas for pandas extension data types GH-36096 R notes New features: open_dataset() now works with ND-JSON files GH-35055 Calling schema() on multiple Arrow objects now returns the object’s schema GH-35543 dplyr .by/by argument now supported in arrow implementation of dplyr verbs GH-35667 Other improvements: Convenience function arrow_array() can be used to create Arrow Arrays GH-36381 Convenience function scalar() can be used to create Arrow Scalars GH-36265 Prevent crashed when passing data between arrow and duckdb by always calling RecordBatchReader::ReadNext() from DuckDB from the main R thread GH-36307 Issue a warning for set_io_thread_count() with num_threads &lt; 2 GH-36304 Ensure missing grouping variables are added to the beginning of the variable list GH-36305 CSV File reader options class objects can print the selected values GH-35955 Schema metadata can be set as a named character vector GH-35954 Ensure that the RStringViewer helper class does not own any Array references GH-35812 strptime() in arrow will return a timezone-aware timestamp if %z is part of the format string GH-35671 Column ordering when combining group_by() and across() now matches dplyr GH-35473 Link to correct version of OpenSSL when using autobrew GH-36551 Require cmake 3.16 in bundled build script GH-36321 For more on what’s in the 13.0.0 R package, see the R changelog. Ruby and C GLib notes Ruby Bug fixes Fixed GC-related issue against random segfault in hash join GH-35819 Fixed segfault in CallExpression.new GH-35915 Improvements FlightRPC: Added a convenient wrapper for the authentication method GH-35435 Added empty table support in #select_columns GH-35681 Added optional hash support in Expression.try_convert GH-35915 Parquet: Added Parquet::ArrowFileReader#each_row_group GH-36008 Added support of the automatic installation of arrow-c-glib on Conda environment GH-36287 C GLib Bug fixes Parquet: Fixed GC-related bug in metadata dependencies GH-35266 Fixed potentially GC-related issue against random segfault in hash join GH-35819 Improvements FlightRPC: Added support to pass GAFlightServerCallContext object in several methods of GAFlightServerCustomAuthHandler GH-35377 FlightSQL: Added support for INSERT/UPDATE/DELETE GH-36408 Added GArrowRunEndEncodedDataType GH-35417 Added GArrowRunEndEncodedArray GH-35418 Rust notes The Rust projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Rust implementation, see the latest Arrow Rust changelog.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Aggregating Millions of Groups Fast in Apache Arrow DataFusion 28.0.0</title><link href="https://arrow.apache.org/blog/2023/08/05/datafusion_fast_grouping/" rel="alternate" type="text/html" title="Aggregating Millions of Groups Fast in Apache Arrow DataFusion 28.0.0" /><published>2023-08-05T00:00:00-04:00</published><updated>2023-08-05T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/08/05/datafusion_fast_grouping</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/08/05/datafusion_fast_grouping/"><![CDATA[<!--

-->

<!--- Converted from Google Docs using https://www.buymeacoffee.com/docstomarkdown --->

<h2 id="aggregating-millions-of-groups-fast-in-apache-arrow-datafusion">Aggregating Millions of Groups Fast in Apache Arrow DataFusion</h2>

<p>Andrew Lamb, Daniël Heres, Raphael Taylor-Davies,</p>

<p><em>Note: this article was originally published on the <a href="https://www.influxdata.com/blog/aggregating-millions-groups-fast-apache-arrow-datafusion">InfluxData Blog</a></em></p>

<h2 id="tldr">TLDR</h2>

<p>Grouped aggregations are a core part of any analytic tool, creating understandable summaries of huge data volumes. <a href="https://arrow.apache.org/datafusion/">Apache Arrow DataFusion</a>’s parallel aggregation capability is 2-3x faster in the <a href="https://crates.io/crates/datafusion/28.0.0">newly released version <code class="language-plaintext highlighter-rouge">28.0.0</code></a> for queries with a large number (10,000 or more) of groups.</p>

<p>Improving aggregation performance matters to all users of DataFusion. For example, both InfluxDB, a <a href="https://github.com/influxdata/influxdb">time series data platform</a> and Coralogix, a <a href="https://coralogix.com/?utm_source=InfluxDB&amp;utm_medium=Blog&amp;utm_campaign=organic">full-stack observability</a> platform, aggregate vast amounts of raw data to monitor and create insights for our customers. Improving DataFusion’s performance lets us provide better user experiences by generating insights faster with fewer resources. Because DataFusion is open source and released under the permissive <a href="https://github.com/apache/arrow-datafusion/blob/main/LICENSE.txt">Apache 2.0</a> license, the whole DataFusion community benefits as well.</p>

<p>With the new optimizations, DataFusion’s grouping speed is now close to DuckDB, a system that regularly reports <a href="https://duckdblabs.github.io/db-benchmark/">great</a> <a href="https://duckdb.org/2022/03/07/aggregate-hashtable.html#experiments">grouping</a> benchmark performance numbers. Figure 1 contains a representative sample of <a href="https://github.com/ClickHouse/ClickBench/tree/main">ClickBench</a> on a single Parquet file, and the full results are at the end of this article.</p>

<p><img src="/assets/datafusion_fast_grouping/summary.png" width="700" /></p>

<p><strong>Figure 1</strong>: Query performance for ClickBench queries on queries 16, 17, 18 and 19 on a single Parquet file for DataFusion <code class="language-plaintext highlighter-rouge">27.0.0</code>, DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code> and DuckDB <code class="language-plaintext highlighter-rouge">0.8.1</code>.</p>

<h2 id="introduction-to-high-cardinality-grouping">Introduction to high cardinality grouping</h2>

<p>Aggregation is a fancy word for computing summary statistics across many rows that have the same value in one or more columns. We call the rows with the same values <em>groups</em> and “high cardinality” means there are a large number of distinct groups in the dataset. At the time of writing, a “large” number of groups in analytic engines is around 10,000.</p>

<p>For example the <a href="https://github.com/ClickHouse/ClickBench">ClickBench</a> <em>hits</em> dataset contains 100 million anonymized user clicks across a set of websites. ClickBench Query 17 is:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="nv">"UserID"</span><span class="p">,</span> <span class="nv">"SearchPhrase"</span><span class="p">,</span> <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span>
<span class="k">FROM</span> <span class="n">hits</span>
<span class="k">GROUP</span> <span class="k">BY</span> <span class="nv">"UserID"</span><span class="p">,</span> <span class="nv">"SearchPhrase"</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span>
<span class="k">DESC</span> <span class="k">LIMIT</span> <span class="mi">10</span><span class="p">;</span>
</code></pre></div></div>

<p>In English, this query finds “the top ten (user, search phrase) combinations, across all clicks” and produces the following results (there are no search phrases for the top ten users):</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+---------------------+--------------+-----------------+
| UserID              | SearchPhrase | COUNT(UInt8(1)) |
+---------------------+--------------+-----------------+
| 1313338681122956954 |              | 29097           |
| 1907779576417363396 |              | 25333           |
| 2305303682471783379 |              | 10597           |
| 7982623143712728547 |              | 6669            |
| 7280399273658728997 |              | 6408            |
| 1090981537032625727 |              | 6196            |
| 5730251990344211405 |              | 6019            |
| 6018350421959114808 |              | 5990            |
| 835157184735512989  |              | 5209            |
| 770542365400669095  |              | 4906            |
+---------------------+--------------+-----------------+
</code></pre></div></div>

<p>The ClickBench dataset contains</p>

<ul>
  <li>99,997,497 total rows<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></li>
  <li>17,630,976 different users (distinct UserIDs)<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></li>
  <li>6,019,103 different search phrases<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup></li>
  <li>24,070,560 distinct combinations<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup> of (UserID, SearchPhrase)
Thus, to answer the query, DataFusion must map each of the 100M different input rows into one of the <strong>24 million different groups</strong>, and keep count of how many such rows there are in each group.</li>
</ul>

<h2 id="the-solution">The solution</h2>

<p>Like most concepts in databases and other analytic systems, the basic ideas of this algorithm are straightforward and taught in introductory computer science courses. You could compute the query with a program such as this<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>

<span class="c1"># read file
</span><span class="n">hits</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="s">'hits.parquet'</span><span class="p">,</span> <span class="n">engine</span><span class="o">=</span><span class="s">'pyarrow'</span><span class="p">)</span>

<span class="c1"># build groups
</span><span class="n">counts</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">hits</span><span class="p">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">group</span> <span class="o">=</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s">'UserID'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s">'SearchPhrase'</span><span class="p">]);</span>
    <span class="c1"># update the dict entry for the corresponding key
</span>    <span class="n">counts</span><span class="p">[</span><span class="n">group</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># Print the top 10 values
</span><span class="k">print</span> <span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">counts</span><span class="p">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]))</span>
</code></pre></div></div>

<p>This approach, while simple, is both slow and very memory inefficient. It requires over 40 seconds to compute the results for less than 1% of the dataset<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup>. Both DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code> and DuckDB <code class="language-plaintext highlighter-rouge">0.8.1</code> compute results in under 10 seconds for the <em>entire</em> dataset.</p>

<p>To answer this query quickly and efficiently, you have to write your code such that it:</p>

<ol>
  <li>Keeps all cores busy aggregating via parallelized computation</li>
  <li>Updates aggregate values quickly, using vectorizable loops that are easy for compilers to translate into the high performance <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data">SIMD</a> instructions available in modern CPUs.</li>
</ol>

<p>The rest of this article explains how grouping works in DataFusion and the improvements we made in <code class="language-plaintext highlighter-rouge">28.0.0</code>.</p>

<h3 id="two-phase-parallel-partitioned-grouping">Two phase parallel partitioned grouping</h3>

<p>Both DataFusion <code class="language-plaintext highlighter-rouge">27.0.</code> and <code class="language-plaintext highlighter-rouge">28.0.0</code> use state-of-the-art, two phase parallel hash partitioned grouping, similar to other high-performance vectorized engines like <a href="https://duckdb.org/2022/03/07/aggregate-hashtable.html">DuckDB’s Parallel Grouped Aggregates</a>. In pictures this looks like:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            ▲                        ▲
            │                        │
            │                        │
            │                        │
┌───────────────────────┐  ┌───────────────────┐
│        GroupBy        │  │      GroupBy      │      Step 4
│        (Final)        │  │      (Final)      │
└───────────────────────┘  └───────────────────┘
            ▲                        ▲
            │                        │
            └────────────┬───────────┘
                         │
                         │
            ┌─────────────────────────┐
            │       Repartition       │               Step 3
            │         HASH(x)         │
            └─────────────────────────┘
                         ▲
                         │
            ┌────────────┴──────────┐
            │                       │
            │                       │
 ┌────────────────────┐  ┌─────────────────────┐
 │      GroupyBy      │  │       GroupBy       │      Step 2
 │     (Partial)      │  │      (Partial)      │
 └────────────────────┘  └─────────────────────┘
            ▲                       ▲
         ┌──┘                       └─┐
         │                            │
    .─────────.                  .─────────.
 ,─'           '─.            ,─'           '─.
;      Input      :          ;      Input      :      Step 1
:    Stream 1     ;          :    Stream 2     ;
 ╲               ╱            ╲               ╱
  '─.         ,─'              '─.         ,─'
     `───────'                    `───────'
</code></pre></div></div>

<p><strong>Figure 2</strong>: Two phase repartitioned grouping: data flows from bottom (source) to top (results) in two phases. First (Steps 1 and 2), each core reads the data into a core-specific hash table, computing intermediate aggregates without any cross-core coordination. Then (Steps 3 and 4) DataFusion divides the data (“repartitions”) into distinct subsets by group value, and each subset is sent to a specific core which computes the final aggregate.</p>

<p>The two phases are critical for keeping cores busy in a multi-core system. Both phases use the same hash table approach (explained in the next section), but differ in how the groups are distributed and the partial results emitted from the accumulators. The first phase aggregates data as soon as possible after it is produced. However, as shown in Figure 2, the groups can be anywhere in any input, so the same group is often found on many different cores. The second phase uses a hash function to redistribute data evenly across the cores, so each group value is processed by exactly one core which emits the final results for that group.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ┌─────┐    ┌─────┐
    │  1  │    │  3  │
    │  2  │    │  4  │   2. After Repartitioning: each
    └─────┘    └─────┘   group key  appears in exactly
    ┌─────┐    ┌─────┐   one partition
    │  1  │    │  3  │
    │  2  │    │  4  │
    └─────┘    └─────┘

─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─

    ┌─────┐    ┌─────┐
    │  2  │    │  2  │
    │  1  │    │  2  │
    │  3  │    │  3  │
    │  4  │    │  1  │
    └─────┘    └─────┘    1. Input Stream: groups
      ...        ...      values are spread
    ┌─────┐    ┌─────┐    arbitrarily over each input
    │  1  │    │  4  │
    │  4  │    │  3  │
    │  1  │    │  1  │
    │  4  │    │  3  │
    │  3  │    │  2  │
    │  2  │    │  2  │
    │  2  │    └─────┘
    └─────┘

    Core A      Core B

</code></pre></div></div>

<p><strong>Figure 3</strong>: Group value distribution across 2 cores during aggregation phases. In the first phase, every group value <code class="language-plaintext highlighter-rouge">1</code>, <code class="language-plaintext highlighter-rouge">2</code>, <code class="language-plaintext highlighter-rouge">3</code>, <code class="language-plaintext highlighter-rouge">4</code>, is present in the input stream processed by each core. In the second phase, after repartitioning, the group values <code class="language-plaintext highlighter-rouge">1</code> and <code class="language-plaintext highlighter-rouge">2</code> are processed by core A, and values <code class="language-plaintext highlighter-rouge">3</code> and <code class="language-plaintext highlighter-rouge">4</code> are processed only by core B.</p>

<p>There are some additional subtleties in the <a href="https://github.com/apache/arrow-datafusion/blob/main/datafusion/core/src/physical_plan/aggregates/row_hash.rs">DataFusion implementation</a> not mentioned above due to space constraints, such as:</p>

<ol>
  <li>The policy of when to emit data from the first phase’s hash table (e.g. because the data is partially sorted)</li>
  <li>Handling specific filters per aggregate (due to the <code class="language-plaintext highlighter-rouge">FILTER</code> SQL clause)</li>
  <li>Data types of intermediate values (which may not be the same as the final output for some aggregates such as <code class="language-plaintext highlighter-rouge">AVG</code>).</li>
  <li>Action taken when memory use exceeds its budget.</li>
</ol>

<h3 id="hash-grouping">Hash grouping</h3>

<p>DataFusion queries can compute many different aggregate functions for each group, both <a href="https://arrow.apache.org/datafusion/user-guide/sql/aggregate_functions.html">built in</a> and/or user defined <a href="https://docs.rs/datafusion/latest/datafusion/logical_expr/struct.AggregateUDF.html"><code class="language-plaintext highlighter-rouge">AggregateUDFs</code></a>. The state for each aggregate function, called an <em>accumulator</em>, is tracked with a hash table (DataFusion uses the excellent <a href="https://docs.rs/hashbrown/latest/hashbrown/index.html">HashBrown</a> <a href="https://docs.rs/hashbrown/latest/hashbrown/raw/struct.RawTable.html">RawTable API</a>), which logically stores the “index”  identifying the specific group value.</p>

<h3 id="hash-grouping-in-2700">Hash grouping in <code class="language-plaintext highlighter-rouge">27.0.0</code></h3>

<p>As shown in Figure 3, DataFusion <code class="language-plaintext highlighter-rouge">27.0.0</code> stores the data in a <a href="https://github.com/apache/arrow-datafusion/blob/4d93b6a3802151865b68967bdc4c7d7ef425b49a/datafusion/core/src/physical_plan/aggregates/utils.rs#L38-L50"><code class="language-plaintext highlighter-rouge">GroupState</code></a> structure which, unsurprisingly, tracks the state for each group. The state for each group consists of:</p>

<ol>
  <li>The actual value of the group columns, in <a href="https://docs.rs/arrow-row/latest/arrow_row/index.html">Arrow Row</a> format.</li>
  <li>In-progress accumulations (e.g. the running counts for the <code class="language-plaintext highlighter-rouge">COUNT</code> aggregate) for each group, in one of two possible formats (<a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/expr/src/accumulator.rs#L24-L49"><code class="language-plaintext highlighter-rouge">Accumulator</code></a>  or <a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/physical-expr/src/aggregate/row_accumulator.rs#L26-L46"><code class="language-plaintext highlighter-rouge">RowAccumulator</code></a>).</li>
  <li>Scratch space for tracking which rows match each aggregate in each batch.</li>
</ol>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                           ┌──────────────────────────────────────┐
                           │                                      │
                           │                  ...                 │
                           │ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ │
                           │ ┃                                  ┃ │
    ┌─────────┐            │ ┃ ┌──────────────────────────────┐ ┃ │
    │         │            │ ┃ │group values: OwnedRow        │ ┃ │
    │ ┌─────┐ │            │ ┃ └──────────────────────────────┘ ┃ │
    │ │  5  │ │            │ ┃ ┌──────────────────────────────┐ ┃ │
    │ ├─────┤ │            │ ┃ │Row accumulator:              │ ┃ │
    │ │  9  │─┼────┐       │ ┃ │Vec&lt;u8&gt;                       │ ┃ │
    │ ├─────┤ │    │       │ ┃ └──────────────────────────────┘ ┃ │
    │ │ ... │ │    │       │ ┃ ┌──────────────────────┐         ┃ │
    │ ├─────┤ │    │       │ ┃ │┌──────────────┐      │         ┃ │
    │ │  1  │ │    │       │ ┃ ││Accumulator 1 │      │         ┃ │
    │ ├─────┤ │    │       │ ┃ │└──────────────┘      │         ┃ │
    │ │ ... │ │    │       │ ┃ │┌──────────────┐      │         ┃ │
    │ └─────┘ │    │       │ ┃ ││Accumulator 2 │      │         ┃ │
    │         │    │       │ ┃ │└──────────────┘      │         ┃ │
    └─────────┘    │       │ ┃ │ Box&lt;dyn Accumulator&gt; │         ┃ │
    Hash Table     │       │ ┃ └──────────────────────┘         ┃ │
                   │       │ ┃ ┌─────────────────────────┐      ┃ │
                   │       │ ┃ │scratch indices: Vec&lt;u32&gt;│      ┃ │
                   │       │ ┃ └─────────────────────────┘      ┃ │
                   │       │ ┃ GroupState                       ┃ │
                   └─────▶ │ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │
                           │                                      │
  Hash table tracks an     │                 ...                  │
  index into group_states  │                                      │
                           └──────────────────────────────────────┘
                           group_states: Vec&lt;GroupState&gt;

                           There is one GroupState PER GROUP

</code></pre></div></div>

<p><strong>Figure 4</strong>: Hash group operator structure in DataFusion <code class="language-plaintext highlighter-rouge">27.0.0</code>. A hash table maps each group to a GroupState which contains all the per-group states.</p>

<p>To compute the aggregate, DataFusion performs the following steps for each input batch:</p>

<ol>
  <li>Calculate hash using <a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/physical-expr/src/hash_utils.rs#L264-L307">efficient vectorized code</a>, specialized for each data type.</li>
  <li>Determine group indexes for each input row using the hash table (creating new entries for newly seen groups).</li>
  <li><a href="https://github.com/apache/arrow-datafusion/blob/4ab8be57dee3bfa72dd105fbd7b8901b873a4878/datafusion/core/src/physical_plan/aggregates/row_hash.rs#L562-L602">Update Accumulators for each group that had input rows,</a> assembling the rows into a contiguous range for vectorized accumulator if there are a sufficient number of them.</li>
</ol>

<p>DataFusion also stores the hash values in the table to avoid potentially costly hash recomputation when resizing the hash table.</p>

<p>This scheme works very well for a relatively small number of distinct groups: all accumulators are efficiently updated with large contiguous batches of rows.</p>

<p>However, this scheme is not ideal for high cardinality grouping due to:</p>

<ol>
  <li><strong>Multiple allocations per group</strong> for the group value row format, as well as for the <code class="language-plaintext highlighter-rouge">RowAccumulator</code>s and each  <code class="language-plaintext highlighter-rouge">Accumulator</code>. The <code class="language-plaintext highlighter-rouge">Accumulator</code> may have additional allocations within it as well.</li>
  <li><strong>Non-vectorized updates:</strong> Accumulator updates often fall back to a slower non-vectorized form because the number of distinct groups is large (and thus number of values per group is small) in each input batch.</li>
</ol>

<h3 id="hash-grouping-in-2800">Hash grouping in <code class="language-plaintext highlighter-rouge">28.0.0</code></h3>

<p>For <code class="language-plaintext highlighter-rouge">28.0.0</code>, we rewrote the core group by implementation following traditional system optimization principles: fewer allocations, type specialization, and aggressive vectorization.</p>

<p>DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code> uses the same RawTable and still stores group indexes. The major differences, as shown in Figure 4, are:</p>

<ol>
  <li>Group values are stored either
    <ol>
      <li>Inline in the <code class="language-plaintext highlighter-rouge">RawTable</code> (for single columns of primitive types), where the conversion to Row format costs more than its benefit</li>
      <li>In a separate <a href="https://docs.rs/arrow-row/latest/arrow_row/struct.Row.html">Rows</a> structure with a single contiguous allocation for all groups values, rather than an allocation per group. Accumulators manage the state for all the groups internally, so the code to update intermediate values is a tight type specialized loop. The new <a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/physical-expr/src/aggregate/groups_accumulator/mod.rs#L66-L75"><code class="language-plaintext highlighter-rouge">GroupsAccumulator</code></a> interface results in highly efficient type accumulator update loops.</li>
    </ol>
  </li>
</ol>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌───────────────────────────────────┐     ┌───────────────────────┐
│ ┌ ─ ─ ─ ─ ─ ┐  ┌─────────────────┐│     │ ┏━━━━━━━━━━━━━━━━━━━┓ │
│                │                 ││     │ ┃  ┌──────────────┐ ┃ │
│ │           │  │ ┌ ─ ─ ┐┌─────┐  ││     │ ┃  │┌───────────┐ │ ┃ │
│                │    X   │  5  │  ││     │ ┃  ││  value1   │ │ ┃ │
│ │           │  │ ├ ─ ─ ┤├─────┤  ││     │ ┃  │└───────────┘ │ ┃ │
│                │    Q   │  9  │──┼┼──┐  │ ┃  │     ...      │ ┃ │
│ │           │  │ ├ ─ ─ ┤├─────┤  ││  └──┼─╋─▶│              │ ┃ │
│                │   ...  │ ... │  ││     │ ┃  │┌───────────┐ │ ┃ │
│ │           │  │ ├ ─ ─ ┤├─────┤  ││     │ ┃  ││  valueN   │ │ ┃ │
│                │    H   │  1  │  ││     │ ┃  │└───────────┘ │ ┃ │
│ │           │  │ ├ ─ ─ ┤├─────┤  ││     │ ┃  │values: Vec&lt;T&gt;│ ┃ │
│     Rows       │   ...  │ ... │  ││     │ ┃  └──────────────┘ ┃ │
│ │           │  │ └ ─ ─ ┘└─────┘  ││     │ ┃                   ┃ │
│  ─ ─ ─ ─ ─ ─   │                 ││     │ ┃ GroupsAccumulator ┃ │
│                └─────────────────┘│     │ ┗━━━━━━━━━━━━━━━━━━━┛ │
│                  Hash Table       │     │                       │
│                                   │     │          ...          │
└───────────────────────────────────┘     └───────────────────────┘
  GroupState                               Accumulators


Hash table value stores group_indexes     One  GroupsAccumulator
and group values.                         per aggregate. Each
                                          stores the state for
Group values are stored either inline     *ALL* groups, typically
in the hash table or in a single          using a native Vec&lt;T&gt;
allocation using the arrow Row format
</code></pre></div></div>

<p><strong>Figure 5</strong>: Hash group operator structure in DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code>. Group values are stored either directly in the hash table, or in a single allocation using the arrow Row format. The hash table contains group indexes. A single <code class="language-plaintext highlighter-rouge">GroupsAccumulator</code> stores the per-aggregate state for <em>all</em> groups.</p>

<p>This new structure improves performance significantly for high cardinality groups due to:</p>

<ol>
  <li><strong>Reduced allocations</strong>: There are no longer any individual allocations per group.</li>
  <li><strong>Contiguous native accumulator states</strong>: Type-specialized accumulators store the values for all groups in a single contiguous allocation using a <a href="https://doc.rust-lang.org/std/vec/struct.Vec.html">Rust Vec&lt;T&gt;</a> of some native type.</li>
  <li><strong>Vectorized state update</strong>: The inner aggregate update loops, which are type-specialized and in terms of native <code class="language-plaintext highlighter-rouge">Vec</code>s, are well-vectorized by the Rust compiler (thanks <a href="https://llvm.org/">LLVM</a>!).</li>
</ol>

<h3 id="notes">Notes</h3>

<p>Some vectorized grouping implementations store the accumulator state row-wise directly in the hash table, which often uses modern CPU caches efficiently. Managing accumulator state in columnar fashion may sacrifice some cache locality, however it ensures the size of the hash table remains small, even when there are large numbers of groups and aggregates, making it easier for the compiler to vectorize the accumulator update.</p>

<p>Depending on the cost of recomputing hash values, DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code> may or may not store the hash values in the table. This optimizes the tradeoff between the cost of computing the hash value (which is expensive for strings, for example) vs. the cost of storing it in the hash table.</p>

<p>One subtlety that arises from pushing state updates into GroupsAccumulators is that each accumulator must handle similar variations with/without filtering and with/without nulls in the input. DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code> uses a templated <a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/physical-expr/src/aggregate/groups_accumulator/accumulate.rs#L28-L54"><code class="language-plaintext highlighter-rouge">NullState</code></a> which encapsulates these common patterns across accumulators.</p>

<p>The code structure is heavily influenced by the fact DataFusion is implemented using <a href="https://www.rust-lang.org/">Rust</a>, a new(ish) systems programming language focused on speed and safety. Rust heavily discourages many of the traditional pointer casting “tricks” used in C/C++ hash grouping implementations. The DataFusion aggregation code is almost entirely <a href="https://doc.rust-lang.org/nomicon/meet-safe-and-unsafe.html#:~:text=Safe%20Rust%20is%20the%20true,Undefined%20Behavior%20(a.k.a.%20UB)."><code class="language-plaintext highlighter-rouge">safe</code></a>, deviating into <code class="language-plaintext highlighter-rouge">unsafe</code> only when necessary. (Rust is a great choice because it makes DataFusion fast, easy to embed, and prevents many crashes and security issues often associated with multi-threaded C/C++ code).</p>

<h2 id="clickbench-results">ClickBench results</h2>

<p>The full results of running the <a href="https://github.com/ClickHouse/ClickBench/tree/main">ClickBench</a> queries against the single Parquet file with DataFusion <code class="language-plaintext highlighter-rouge">27.0.0</code>, DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code>, and DuckDB <code class="language-plaintext highlighter-rouge">0.8.1</code> are below. These numbers were run on a GCP <code class="language-plaintext highlighter-rouge">e2-standard-8 machine</code> with 8 cores and 32 GB of RAM, using the scripts <a href="https://github.com/alamb/datafusion-duckdb-benchmark">here</a>.</p>

<p>As the industry moves towards data systems assembled from components, it is increasingly important that they exchange data using open standards such as <a href="https://arrow.apache.org/">Apache Arrow</a> and <a href="https://parquet.apache.org/">Parquet</a> rather than custom storage and in-memory formats. Thus, this benchmark uses a single input Parquet file representative of many DataFusion users and aligned with the current trend in analytics of avoiding a costly load/transformation into a custom storage format prior to query.</p>

<p>DataFusion now reaches near-DuckDB-speeds querying Parquet data. While we don’t plan to engage in a benchmarking shootout with a team that literally wrote <a href="https://dl.acm.org/doi/abs/10.1145/3209950.3209955">Fair Benchmarking Considered Difficult</a>, hopefully everyone can agree that DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code> is a significant improvement.</p>

<p><img src="/assets/datafusion_fast_grouping/full.png" width="700" /></p>

<p><strong>Figure 6</strong>: Performance of DataFusion <code class="language-plaintext highlighter-rouge">27.0.0</code>, DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code>, and DuckDB <code class="language-plaintext highlighter-rouge">0.8.1</code> on all 43 ClickBench queries against a single <code class="language-plaintext highlighter-rouge">hits.parquet</code> file. Lower is better.</p>

<h3 id="notes-1">Notes</h3>

<p>DataFusion <code class="language-plaintext highlighter-rouge">27.0.0</code> was not able to run several queries due to either planner bugs (Q9, Q11, Q12, 14) or running out of memory (Q33). DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code> solves those issues.</p>

<p>DataFusion is faster than DuckDB for query 21 and 22, likely due to optimized implementations of string pattern matching.</p>

<h2 id="conclusion-performance-matters">Conclusion: performance matters</h2>

<p>Improving aggregation performance by more than a factor of two allows developers building products and projects with DataFusion to spend more time on value-added domain specific features. We believe building systems with DataFusion is much faster than trying to build something similar from scratch. DataFusion increases productivity because it eliminates the need to rebuild well-understood, but costly to implement, analytic database technology. While we’re pleased with the improvements in DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code>, we are by no means done and are pursuing <a href="https://github.com/apache/arrow-datafusion/issues/7000">(Even More) Aggregation Performance</a>. The future for performance is bright.</p>

<h2 id="acknowledgments">Acknowledgments</h2>

<p>DataFusion is a <a href="https://arrow.apache.org/datafusion/contributor-guide/communication.html">community effort</a> and this work was not possible without contributions from many in the community. A special shout out to <a href="https://github.com/sunchao">sunchao</a>, <a href="https://github.com/jyshen">yjshen</a>, <a href="https://github.com/yahoNanJing">yahoNanJing</a>, <a href="https://github.com/mingmwang">mingmwang</a>, <a href="https://github.com/ozankabak">ozankabak</a>, <a href="https://github.com/mustafasrepo">mustafasrepo</a>, and everyone else who contributed ideas, reviews, and encouragement <a href="https://github.com/apache/arrow-datafusion/pull/6800">during</a> this <a href="https://github.com/apache/arrow-datafusion/pull/6904">work</a>.</p>

<h2 id="about-datafusion">About DataFusion</h2>

<p><a href="https://arrow.apache.org/datafusion/">Apache Arrow DataFusion</a> is an extensible query engine and database toolkit, written in <a href="https://www.rust-lang.org/">Rust</a>, that uses <a href="https://arrow.apache.org/">Apache Arrow</a> as its in-memory format. DataFusion, along with <a href="https://calcite.apache.org/">Apache Calcite</a>, Facebook’s <a href="https://github.com/facebookincubator/velox">Velox</a>, and similar technology are part of the next generation “<a href="https://www.usenix.org/publications/login/winter2018/khurana">Deconstructed Database</a>” architectures, where new systems are built on a foundation of fast, modular components, rather than as a single tightly integrated system.</p>

<!-- Footnotes themselves at the bottom. -->
<h2 id="notes-2">Notes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p><code class="language-plaintext highlighter-rouge">SELECT COUNT(*) FROM 'hits.parquet';</code> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p><code class="language-plaintext highlighter-rouge">SELECT COUNT(DISTINCT "UserID") as num_users FROM 'hits.parquet';</code> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p><code class="language-plaintext highlighter-rouge">SELECT COUNT(DISTINCT "SearchPhrase") as num_phrases FROM 'hits.parquet';</code> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p><code class="language-plaintext highlighter-rouge">SELECT COUNT(*) FROM (SELECT DISTINCT "UserID", "SearchPhrase" FROM 'hits.parquet')</code> <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Full script at <a href="https://github.com/alamb/datafusion-duckdb-benchmark/blob/main/hash.py">hash.py</a> <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p><a href="https://datasets.clickhouse.com/hits_compatible/athena_partitioned/hits_%7B%7D.parquet">hits_0.parquet</a>, one of the files from the partitioned ClickBench dataset, which has <code class="language-plaintext highlighter-rouge">100,000</code> rows and is 117 MB in size. The entire dataset has <code class="language-plaintext highlighter-rouge">100,000,000</code> rows in a single 14 GB Parquet file. The script did not complete on the entire dataset after 40 minutes, and used 212 GB RAM at peak. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>alamb, Dandandan, tustvold</name></author><category term="release" /><summary type="html"><![CDATA[Aggregating Millions of Groups Fast in Apache Arrow DataFusion Andrew Lamb, Daniël Heres, Raphael Taylor-Davies, Note: this article was originally published on the InfluxData Blog TLDR Grouped aggregations are a core part of any analytic tool, creating understandable summaries of huge data volumes. Apache Arrow DataFusion’s parallel aggregation capability is 2-3x faster in the newly released version 28.0.0 for queries with a large number (10,000 or more) of groups. Improving aggregation performance matters to all users of DataFusion. For example, both InfluxDB, a time series data platform and Coralogix, a full-stack observability platform, aggregate vast amounts of raw data to monitor and create insights for our customers. Improving DataFusion’s performance lets us provide better user experiences by generating insights faster with fewer resources. Because DataFusion is open source and released under the permissive Apache 2.0 license, the whole DataFusion community benefits as well. With the new optimizations, DataFusion’s grouping speed is now close to DuckDB, a system that regularly reports great grouping benchmark performance numbers. Figure 1 contains a representative sample of ClickBench on a single Parquet file, and the full results are at the end of this article. Figure 1: Query performance for ClickBench queries on queries 16, 17, 18 and 19 on a single Parquet file for DataFusion 27.0.0, DataFusion 28.0.0 and DuckDB 0.8.1. Introduction to high cardinality grouping Aggregation is a fancy word for computing summary statistics across many rows that have the same value in one or more columns. We call the rows with the same values groups and “high cardinality” means there are a large number of distinct groups in the dataset. At the time of writing, a “large” number of groups in analytic engines is around 10,000. For example the ClickBench hits dataset contains 100 million anonymized user clicks across a set of websites. ClickBench Query 17 is: SELECT "UserID", "SearchPhrase", COUNT(*) FROM hits GROUP BY "UserID", "SearchPhrase" ORDER BY COUNT(*) DESC LIMIT 10; In English, this query finds “the top ten (user, search phrase) combinations, across all clicks” and produces the following results (there are no search phrases for the top ten users): +---------------------+--------------+-----------------+ | UserID | SearchPhrase | COUNT(UInt8(1)) | +---------------------+--------------+-----------------+ | 1313338681122956954 | | 29097 | | 1907779576417363396 | | 25333 | | 2305303682471783379 | | 10597 | | 7982623143712728547 | | 6669 | | 7280399273658728997 | | 6408 | | 1090981537032625727 | | 6196 | | 5730251990344211405 | | 6019 | | 6018350421959114808 | | 5990 | | 835157184735512989 | | 5209 | | 770542365400669095 | | 4906 | +---------------------+--------------+-----------------+ The ClickBench dataset contains 99,997,497 total rows1 17,630,976 different users (distinct UserIDs)2 6,019,103 different search phrases3 24,070,560 distinct combinations4 of (UserID, SearchPhrase) Thus, to answer the query, DataFusion must map each of the 100M different input rows into one of the 24 million different groups, and keep count of how many such rows there are in each group. The solution Like most concepts in databases and other analytic systems, the basic ideas of this algorithm are straightforward and taught in introductory computer science courses. You could compute the query with a program such as this5: import pandas as pd from collections import defaultdict from operator import itemgetter # read file hits = pd.read_parquet('hits.parquet', engine='pyarrow') # build groups counts = defaultdict(int) for index, row in hits.iterrows(): group = (row['UserID'], row['SearchPhrase']); # update the dict entry for the corresponding key counts[group] += 1 # Print the top 10 values print (dict(sorted(counts.items(), key=itemgetter(1), reverse=True)[:10])) This approach, while simple, is both slow and very memory inefficient. It requires over 40 seconds to compute the results for less than 1% of the dataset6. Both DataFusion 28.0.0 and DuckDB 0.8.1 compute results in under 10 seconds for the entire dataset. To answer this query quickly and efficiently, you have to write your code such that it: Keeps all cores busy aggregating via parallelized computation Updates aggregate values quickly, using vectorizable loops that are easy for compilers to translate into the high performance SIMD instructions available in modern CPUs. The rest of this article explains how grouping works in DataFusion and the improvements we made in 28.0.0. Two phase parallel partitioned grouping Both DataFusion 27.0. and 28.0.0 use state-of-the-art, two phase parallel hash partitioned grouping, similar to other high-performance vectorized engines like DuckDB’s Parallel Grouped Aggregates. In pictures this looks like: ▲ ▲ │ │ │ │ │ │ ┌───────────────────────┐ ┌───────────────────┐ │ GroupBy │ │ GroupBy │ Step 4 │ (Final) │ │ (Final) │ └───────────────────────┘ └───────────────────┘ ▲ ▲ │ │ └────────────┬───────────┘ │ │ ┌─────────────────────────┐ │ Repartition │ Step 3 │ HASH(x) │ └─────────────────────────┘ ▲ │ ┌────────────┴──────────┐ │ │ │ │ ┌────────────────────┐ ┌─────────────────────┐ │ GroupyBy │ │ GroupBy │ Step 2 │ (Partial) │ │ (Partial) │ └────────────────────┘ └─────────────────────┘ ▲ ▲ ┌──┘ └─┐ │ │ .─────────. .─────────. ,─' '─. ,─' '─. ; Input : ; Input : Step 1 : Stream 1 ; : Stream 2 ; ╲ ╱ ╲ ╱ '─. ,─' '─. ,─' `───────' `───────' Figure 2: Two phase repartitioned grouping: data flows from bottom (source) to top (results) in two phases. First (Steps 1 and 2), each core reads the data into a core-specific hash table, computing intermediate aggregates without any cross-core coordination. Then (Steps 3 and 4) DataFusion divides the data (“repartitions”) into distinct subsets by group value, and each subset is sent to a specific core which computes the final aggregate. The two phases are critical for keeping cores busy in a multi-core system. Both phases use the same hash table approach (explained in the next section), but differ in how the groups are distributed and the partial results emitted from the accumulators. The first phase aggregates data as soon as possible after it is produced. However, as shown in Figure 2, the groups can be anywhere in any input, so the same group is often found on many different cores. The second phase uses a hash function to redistribute data evenly across the cores, so each group value is processed by exactly one core which emits the final results for that group. ┌─────┐ ┌─────┐ │ 1 │ │ 3 │ │ 2 │ │ 4 │ 2. After Repartitioning: each └─────┘ └─────┘ group key appears in exactly ┌─────┐ ┌─────┐ one partition │ 1 │ │ 3 │ │ 2 │ │ 4 │ └─────┘ └─────┘ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌─────┐ ┌─────┐ │ 2 │ │ 2 │ │ 1 │ │ 2 │ │ 3 │ │ 3 │ │ 4 │ │ 1 │ └─────┘ └─────┘ 1. Input Stream: groups ... ... values are spread ┌─────┐ ┌─────┐ arbitrarily over each input │ 1 │ │ 4 │ │ 4 │ │ 3 │ │ 1 │ │ 1 │ │ 4 │ │ 3 │ │ 3 │ │ 2 │ │ 2 │ │ 2 │ │ 2 │ └─────┘ └─────┘ Core A Core B Figure 3: Group value distribution across 2 cores during aggregation phases. In the first phase, every group value 1, 2, 3, 4, is present in the input stream processed by each core. In the second phase, after repartitioning, the group values 1 and 2 are processed by core A, and values 3 and 4 are processed only by core B. There are some additional subtleties in the DataFusion implementation not mentioned above due to space constraints, such as: The policy of when to emit data from the first phase’s hash table (e.g. because the data is partially sorted) Handling specific filters per aggregate (due to the FILTER SQL clause) Data types of intermediate values (which may not be the same as the final output for some aggregates such as AVG). Action taken when memory use exceeds its budget. Hash grouping DataFusion queries can compute many different aggregate functions for each group, both built in and/or user defined AggregateUDFs. The state for each aggregate function, called an accumulator, is tracked with a hash table (DataFusion uses the excellent HashBrown RawTable API), which logically stores the “index” identifying the specific group value. Hash grouping in 27.0.0 As shown in Figure 3, DataFusion 27.0.0 stores the data in a GroupState structure which, unsurprisingly, tracks the state for each group. The state for each group consists of: The actual value of the group columns, in Arrow Row format. In-progress accumulations (e.g. the running counts for the COUNT aggregate) for each group, in one of two possible formats (Accumulator or RowAccumulator). Scratch space for tracking which rows match each aggregate in each batch. ┌──────────────────────────────────────┐ │ │ │ ... │ │ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ │ │ ┃ ┃ │ ┌─────────┐ │ ┃ ┌──────────────────────────────┐ ┃ │ │ │ │ ┃ │group values: OwnedRow │ ┃ │ │ ┌─────┐ │ │ ┃ └──────────────────────────────┘ ┃ │ │ │ 5 │ │ │ ┃ ┌──────────────────────────────┐ ┃ │ │ ├─────┤ │ │ ┃ │Row accumulator: │ ┃ │ │ │ 9 │─┼────┐ │ ┃ │Vec&lt;u8&gt; │ ┃ │ │ ├─────┤ │ │ │ ┃ └──────────────────────────────┘ ┃ │ │ │ ... │ │ │ │ ┃ ┌──────────────────────┐ ┃ │ │ ├─────┤ │ │ │ ┃ │┌──────────────┐ │ ┃ │ │ │ 1 │ │ │ │ ┃ ││Accumulator 1 │ │ ┃ │ │ ├─────┤ │ │ │ ┃ │└──────────────┘ │ ┃ │ │ │ ... │ │ │ │ ┃ │┌──────────────┐ │ ┃ │ │ └─────┘ │ │ │ ┃ ││Accumulator 2 │ │ ┃ │ │ │ │ │ ┃ │└──────────────┘ │ ┃ │ └─────────┘ │ │ ┃ │ Box&lt;dyn Accumulator&gt; │ ┃ │ Hash Table │ │ ┃ └──────────────────────┘ ┃ │ │ │ ┃ ┌─────────────────────────┐ ┃ │ │ │ ┃ │scratch indices: Vec&lt;u32&gt;│ ┃ │ │ │ ┃ └─────────────────────────┘ ┃ │ │ │ ┃ GroupState ┃ │ └─────▶ │ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │ │ │ Hash table tracks an │ ... │ index into group_states │ │ └──────────────────────────────────────┘ group_states: Vec&lt;GroupState&gt; There is one GroupState PER GROUP Figure 4: Hash group operator structure in DataFusion 27.0.0. A hash table maps each group to a GroupState which contains all the per-group states. To compute the aggregate, DataFusion performs the following steps for each input batch: Calculate hash using efficient vectorized code, specialized for each data type. Determine group indexes for each input row using the hash table (creating new entries for newly seen groups). Update Accumulators for each group that had input rows, assembling the rows into a contiguous range for vectorized accumulator if there are a sufficient number of them. DataFusion also stores the hash values in the table to avoid potentially costly hash recomputation when resizing the hash table. This scheme works very well for a relatively small number of distinct groups: all accumulators are efficiently updated with large contiguous batches of rows. However, this scheme is not ideal for high cardinality grouping due to: Multiple allocations per group for the group value row format, as well as for the RowAccumulators and each Accumulator. The Accumulator may have additional allocations within it as well. Non-vectorized updates: Accumulator updates often fall back to a slower non-vectorized form because the number of distinct groups is large (and thus number of values per group is small) in each input batch. Hash grouping in 28.0.0 For 28.0.0, we rewrote the core group by implementation following traditional system optimization principles: fewer allocations, type specialization, and aggressive vectorization. DataFusion 28.0.0 uses the same RawTable and still stores group indexes. The major differences, as shown in Figure 4, are: Group values are stored either Inline in the RawTable (for single columns of primitive types), where the conversion to Row format costs more than its benefit In a separate Rows structure with a single contiguous allocation for all groups values, rather than an allocation per group. Accumulators manage the state for all the groups internally, so the code to update intermediate values is a tight type specialized loop. The new GroupsAccumulator interface results in highly efficient type accumulator update loops. ┌───────────────────────────────────┐ ┌───────────────────────┐ │ ┌ ─ ─ ─ ─ ─ ┐ ┌─────────────────┐│ │ ┏━━━━━━━━━━━━━━━━━━━┓ │ │ │ ││ │ ┃ ┌──────────────┐ ┃ │ │ │ │ │ ┌ ─ ─ ┐┌─────┐ ││ │ ┃ │┌───────────┐ │ ┃ │ │ │ X │ 5 │ ││ │ ┃ ││ value1 │ │ ┃ │ │ │ │ │ ├ ─ ─ ┤├─────┤ ││ │ ┃ │└───────────┘ │ ┃ │ │ │ Q │ 9 │──┼┼──┐ │ ┃ │ ... │ ┃ │ │ │ │ │ ├ ─ ─ ┤├─────┤ ││ └──┼─╋─▶│ │ ┃ │ │ │ ... │ ... │ ││ │ ┃ │┌───────────┐ │ ┃ │ │ │ │ │ ├ ─ ─ ┤├─────┤ ││ │ ┃ ││ valueN │ │ ┃ │ │ │ H │ 1 │ ││ │ ┃ │└───────────┘ │ ┃ │ │ │ │ │ ├ ─ ─ ┤├─────┤ ││ │ ┃ │values: Vec&lt;T&gt;│ ┃ │ │ Rows │ ... │ ... │ ││ │ ┃ └──────────────┘ ┃ │ │ │ │ │ └ ─ ─ ┘└─────┘ ││ │ ┃ ┃ │ │ ─ ─ ─ ─ ─ ─ │ ││ │ ┃ GroupsAccumulator ┃ │ │ └─────────────────┘│ │ ┗━━━━━━━━━━━━━━━━━━━┛ │ │ Hash Table │ │ │ │ │ │ ... │ └───────────────────────────────────┘ └───────────────────────┘ GroupState Accumulators Hash table value stores group_indexes One GroupsAccumulator and group values. per aggregate. Each stores the state for Group values are stored either inline *ALL* groups, typically in the hash table or in a single using a native Vec&lt;T&gt; allocation using the arrow Row format Figure 5: Hash group operator structure in DataFusion 28.0.0. Group values are stored either directly in the hash table, or in a single allocation using the arrow Row format. The hash table contains group indexes. A single GroupsAccumulator stores the per-aggregate state for all groups. This new structure improves performance significantly for high cardinality groups due to: Reduced allocations: There are no longer any individual allocations per group. Contiguous native accumulator states: Type-specialized accumulators store the values for all groups in a single contiguous allocation using a Rust Vec&lt;T&gt; of some native type. Vectorized state update: The inner aggregate update loops, which are type-specialized and in terms of native Vecs, are well-vectorized by the Rust compiler (thanks LLVM!). Notes Some vectorized grouping implementations store the accumulator state row-wise directly in the hash table, which often uses modern CPU caches efficiently. Managing accumulator state in columnar fashion may sacrifice some cache locality, however it ensures the size of the hash table remains small, even when there are large numbers of groups and aggregates, making it easier for the compiler to vectorize the accumulator update. Depending on the cost of recomputing hash values, DataFusion 28.0.0 may or may not store the hash values in the table. This optimizes the tradeoff between the cost of computing the hash value (which is expensive for strings, for example) vs. the cost of storing it in the hash table. One subtlety that arises from pushing state updates into GroupsAccumulators is that each accumulator must handle similar variations with/without filtering and with/without nulls in the input. DataFusion 28.0.0 uses a templated NullState which encapsulates these common patterns across accumulators. The code structure is heavily influenced by the fact DataFusion is implemented using Rust, a new(ish) systems programming language focused on speed and safety. Rust heavily discourages many of the traditional pointer casting “tricks” used in C/C++ hash grouping implementations. The DataFusion aggregation code is almost entirely safe, deviating into unsafe only when necessary. (Rust is a great choice because it makes DataFusion fast, easy to embed, and prevents many crashes and security issues often associated with multi-threaded C/C++ code). ClickBench results The full results of running the ClickBench queries against the single Parquet file with DataFusion 27.0.0, DataFusion 28.0.0, and DuckDB 0.8.1 are below. These numbers were run on a GCP e2-standard-8 machine with 8 cores and 32 GB of RAM, using the scripts here. As the industry moves towards data systems assembled from components, it is increasingly important that they exchange data using open standards such as Apache Arrow and Parquet rather than custom storage and in-memory formats. Thus, this benchmark uses a single input Parquet file representative of many DataFusion users and aligned with the current trend in analytics of avoiding a costly load/transformation into a custom storage format prior to query. DataFusion now reaches near-DuckDB-speeds querying Parquet data. While we don’t plan to engage in a benchmarking shootout with a team that literally wrote Fair Benchmarking Considered Difficult, hopefully everyone can agree that DataFusion 28.0.0 is a significant improvement. Figure 6: Performance of DataFusion 27.0.0, DataFusion 28.0.0, and DuckDB 0.8.1 on all 43 ClickBench queries against a single hits.parquet file. Lower is better. Notes DataFusion 27.0.0 was not able to run several queries due to either planner bugs (Q9, Q11, Q12, 14) or running out of memory (Q33). DataFusion 28.0.0 solves those issues. DataFusion is faster than DuckDB for query 21 and 22, likely due to optimized implementations of string pattern matching. Conclusion: performance matters Improving aggregation performance by more than a factor of two allows developers building products and projects with DataFusion to spend more time on value-added domain specific features. We believe building systems with DataFusion is much faster than trying to build something similar from scratch. DataFusion increases productivity because it eliminates the need to rebuild well-understood, but costly to implement, analytic database technology. While we’re pleased with the improvements in DataFusion 28.0.0, we are by no means done and are pursuing (Even More) Aggregation Performance. The future for performance is bright. Acknowledgments DataFusion is a community effort and this work was not possible without contributions from many in the community. A special shout out to sunchao, yjshen, yahoNanJing, mingmwang, ozankabak, mustafasrepo, and everyone else who contributed ideas, reviews, and encouragement during this work. About DataFusion Apache Arrow DataFusion is an extensible query engine and database toolkit, written in Rust, that uses Apache Arrow as its in-memory format. DataFusion, along with Apache Calcite, Facebook’s Velox, and similar technology are part of the next generation “Deconstructed Database” architectures, where new systems are built on a foundation of fast, modular components, rather than as a single tightly integrated system. Notes SELECT COUNT(*) FROM 'hits.parquet'; &#8617; SELECT COUNT(DISTINCT "UserID") as num_users FROM 'hits.parquet'; &#8617; SELECT COUNT(DISTINCT "SearchPhrase") as num_phrases FROM 'hits.parquet'; &#8617; SELECT COUNT(*) FROM (SELECT DISTINCT "UserID", "SearchPhrase" FROM 'hits.parquet') &#8617; Full script at hash.py &#8617; hits_0.parquet, one of the files from the partitioned ClickBench dataset, which has 100,000 rows and is 117 MB in size. The entire dataset has 100,000,000 rows in a single 14 GB Parquet file. The script did not complete on the entire dataset after 40 minutes, and used 212 GB RAM at peak. &#8617;]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 0.5.1 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/06/27/adbc-0.5.1-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.5.1 (Libraries) Release" /><published>2023-06-27T00:00:00-04:00</published><updated>2023-06-27T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/06/27/adbc-0.5.1-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/06/27/adbc-0.5.1-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.5.1 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/7"><strong>8
resolved issues</strong></a> from <a href="#contributors"><strong>8 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version
0.5.1.  The <strong>API specification</strong> is versioned separately and is
at version 1.0.0.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.5.1/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>This is a patch release primarily aimed at fixing a <a href="https://github.com/apache/arrow-adbc/commit/f35485a5f3c9597668c0b4a8936621c97c4adc15">deadlock in the Snowflake driver</a> that was discovered post-release.</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.5.0..apache-arrow-adbc-0.5.1
     9	David Li
     5	Dewey Dunnington
     4	Matt Topol
     3	William Ayd
     2	davidhcoe
     1	Curt Hagenlocher
     1	Diego Fernández Giraldo
     1	Sutou Kouhei
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>Work for the proposed <a href="https://github.com/apache/arrow-adbc/milestone/3">1.1.0 API revision</a> is progressing on a <a href="https://github.com/apache/arrow-adbc/tree/spec-1.1.0">branch</a>, with several API proposals merged in and others awaiting review.  They will then receive prototype implementations before being submitted for review/voting.  This is not currently targeting any release of the ADBC libraries, although we hope to wrap up this effort in the next few months.</p>

<p>A milestone has been created to explore <a href="https://github.com/apache/arrow-adbc/milestone/9">asynchronous APIs</a>, which have been requested for a while now.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.5.1 release of the Apache Arrow ADBC libraries. This covers includes 8 resolved issues from 8 distinct contributors. This is a release of the libraries, which are at version 0.5.1. The API specification is versioned separately and is at version 1.0.0. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights This is a patch release primarily aimed at fixing a deadlock in the Snowflake driver that was discovered post-release. Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.5.0..apache-arrow-adbc-0.5.1 9 David Li 5 Dewey Dunnington 4 Matt Topol 3 William Ayd 2 davidhcoe 1 Curt Hagenlocher 1 Diego Fernández Giraldo 1 Sutou Kouhei Roadmap Work for the proposed 1.1.0 API revision is progressing on a branch, with several API proposals merged in and others awaiting review. They will then receive prototype implementations before being submitted for review/voting. This is not currently targeting any release of the ADBC libraries, although we hope to wrap up this effort in the next few months. A milestone has been created to explore asynchronous APIs, which have been requested for a while now. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Our journey at F5 with Apache Arrow (part 2): Adaptive Schemas and Sorting to Optimize Arrow Usage</title><link href="https://arrow.apache.org/blog/2023/06/26/our-journey-at-f5-with-apache-arrow-part-2/" rel="alternate" type="text/html" title="Our journey at F5 with Apache Arrow (part 2): Adaptive Schemas and Sorting to Optimize Arrow Usage" /><published>2023-06-26T00:00:00-04:00</published><updated>2023-06-26T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/06/26/our-journey-at-f5-with-apache-arrow-part-2</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/06/26/our-journey-at-f5-with-apache-arrow-part-2/"><![CDATA[<!--

-->

<p>In the previous <a href="https://arrow.apache.org/blog/2023/04/11/our-journey-at-f5-with-apache-arrow-part-1/">article</a>, we discussed our use of Apache Arrow within the context of the OpenTelemetry project. We investigated various techniques to maximize the efficiency of Apache Arrow, aiming to find the optimal balance between data compression ratio and queryability. The compression results speak for themselves, boasting improvements ranging from 1.5x to 5x better than the original OTLP protocol. In this article, we will delve into three techniques that have enabled us to enhance both the compression ratio and memory usage of Apache Arrow buffers within the current version of the <a href="https://github.com/f5/otel-arrow-adapter">OTel Arrow protocol</a>.</p>

<p>The first technique we’ll discuss aims to optimize schemas in terms of memory usage. As you’ll see, the gains can be substantial, potentially halving memory usage in certain cases. The second section will delve more deeply into the various approaches that can be used to handle recursive schema definitions. Lastly, we’ll emphasize that the design of your schema(s), coupled with the sorts you can apply at the record level, play a pivotal role in maximizing the benefits of Apache Arrow and its columnar representation.</p>

<h2 id="handling-dynamic-and-unknown-data-distributions">Handling dynamic and unknown data distributions</h2>

<p>In certain contexts, the comprehensive definition of an Arrow schema can end up being overly broad and complex in order to cover all possible cases that you intend to represent in columnar form. However, as is often the case with complex schemas, only a subset of this schema will actually be utilized for a specific deployment. Similarly, it’s not always possible to determine the optimal dictionary encoding for one or more fields in advance. Employing a broad and very general schema that covers all cases is usually more memory-intensive. This is because, for most implementations, a column without value still continues to consume memory space. Likewise, a column with dictionary encoding that indexes a uint64 will occupy four times more memory than the same column with a dictionary encoding based on a uint8.</p>

<p>To illustrate this more concretely, let’s consider an OTel collector positioned at the output of a production environment, receiving a telemetry data stream produced by a large and dynamic set of servers. Invariably, the content of this telemetry stream will change in volume and nature over time. It’s challenging to predict the optimal schema in such a scenario, and it’s equally difficult to know in advance the distribution of a particular attribute of the telemetry data passing through this point.</p>

<p>To optimize such scenarios, we have adopted an intermediary approach that we have named <strong>dynamic Arrow schema</strong>, aiming to gradually adapt the schema based on the observed data. The general principle is relatively simple. We start with a general schema defining the maximum envelope of what should be represented. Some fields of this schema will be declared optional, while other fields will be encoded with multiple possible options depending on the observed distribution. In theory, this principle can be applied to other types of transformations (e.g., recursive column creation) but we will let your imagination explore these other options. So if you encounter data streams where certain fields are not utilized, some union variants remain unused, and/or the value distribution of a field cannot be determined a priori, it may be worthwhile to invest time in implementing this model. This can lead to improved efficiency in terms of compression ratio, memory usage, and processing speed.</p>

<p>The following Go Arrow schema definition provides an example of such a schema, instrumented with a collection of annotations. These annotations will be processed by an enhanced Record Builder, equipped with the ability to dynamically adapt the schema. The structure of this system is illustrated in Figure 1.</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">var</span> <span class="p">(</span>
  <span class="c">// Arrow schema for the OTLP Arrow Traces record (without attributes, links, and events).</span>
  <span class="n">TracesSchema</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">.</span><span class="n">NewSchema</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
      <span class="c">// Nullabe:true means the field is optional, in this case of 16 bit unsigned integers</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">ID</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint16</span><span class="p">,</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">Resource</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">ID</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint16</span><span class="p">,</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
        <span class="c">// --- Use dictionary with 8 bit integers initially ----</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">SchemaUrl</span><span class="p">,</span><span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span><span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">DroppedAttributesCount</span><span class="p">,</span><span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint32</span><span class="p">,</span><span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">}</span><span class="o">...</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">Scope</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
          <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">ID</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint16</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">acommon</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">acommon</span><span class="o">.</span><span class="n">DeltaEncoding</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
          <span class="c">// --- Use dictionary with 8 bit integers initially ----</span>
          <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">Name</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">acommon</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">acommon</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
          <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">Version</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">acommon</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">acommon</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
          <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">DroppedAttributesCount</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint32</span><span class="p">,</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">}</span><span class="o">...</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">SchemaUrl</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">StartTimeUnixNano</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FixedWidthTypes</span><span class="o">.</span><span class="n">Timestamp_ns</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">DurationTimeUnixNano</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FixedWidthTypes</span><span class="o">.</span><span class="n">Duration_ms</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">)},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">TraceId</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">FixedSizeBinaryType</span><span class="p">{</span><span class="n">ByteWidth</span><span class="o">:</span> <span class="m">16</span><span class="p">}},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">SpanId</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">FixedSizeBinaryType</span><span class="p">{</span><span class="n">ByteWidth</span><span class="o">:</span> <span class="m">8</span><span class="p">}},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">TraceState</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">ParentSpanId</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">FixedSizeBinaryType</span><span class="p">{</span><span class="n">ByteWidth</span><span class="o">:</span> <span class="m">8</span><span class="p">},</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">Name</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">)},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">KIND</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Int32</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">DroppedAttributesCount</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint32</span><span class="p">,</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">DroppedEventsCount</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint32</span><span class="p">,</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">DroppedLinksCount</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint32</span><span class="p">,</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">Status</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">StatusCode</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Int32</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">StatusMessage</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">}</span><span class="o">...</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
    <span class="p">},</span> <span class="no">nil</span><span class="p">)</span>
  <span class="p">)</span>
</code></pre></div></div>

<p>In this example, Arrow field-level metadata are employed to designate when a field is optional (Nullable: true) or to specify the minimal dictionary encoding applicable to a particular field (Metadata Dictionary8/16/…). Now let’s imagine a scenario utilizing this schema in a straightforward scenario, wherein only a handful of fields are actually in use, and the cardinality of most dictionary-encoded fields is low (i.e., below 2^8). Ideally, we’d want a system capable of dynamically constructing the following simplified schema, which, in essence, is a strict subset of the original schema.</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">var</span> <span class="p">(</span>
  <span class="c">// Simplified schema definition generated by the Arrow Record encoder based on</span>
  <span class="c">// the data observed.</span>
  <span class="n">TracesSchema</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">.</span><span class="n">NewSchema</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">ID</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint16</span><span class="p">,</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">StartTimeUnixNano</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FixedWidthTypes</span><span class="o">.</span><span class="n">Timestamp_ns</span><span class="p">},</span>
    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">TraceId</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">FixedSizeBinaryType</span><span class="p">{</span><span class="n">ByteWidth</span><span class="o">:</span> <span class="m">16</span><span class="p">}},</span>
    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">SpanId</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">FixedSizeBinaryType</span><span class="p">{</span><span class="n">ByteWidth</span><span class="o">:</span> <span class="m">8</span><span class="p">}},</span>
    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">Name</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">DictionaryType</span> <span class="p">{</span>
      <span class="n">IndexType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint8</span><span class="p">,</span>
      <span class="n">ValueType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">}},</span>
    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">KIND</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">DictionaryType</span> <span class="p">{</span>
      <span class="n">IndexType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint8</span><span class="p">,</span>
      <span class="n">ValueType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Int32</span><span class="p">,</span>
    <span class="p">},</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
  <span class="p">},</span> <span class="no">nil</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Additionally, we desire a system capable of automatically adapting the aforementioned schema if it encounters new fields or existing fields with a cardinality exceeding the size of the current dictionary definition in future batches. In extreme scenarios, if the cardinality of a specific field surpasses a certain threshold, we would prefer the system to automatically revert to the non-dictionary representation (mechanism of dictionary overflow). That is precisely what we will elaborate on in the remainder of this section.</p>

<p>An overview of the different components and events used to implement this approach is depicted in figure 1.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/adaptive-schema-architecture.svg" width="100%" class="img-responsive" alt="Fig 1: Adaptive Arrow schema architecture overview." />
  <figcaption>Fig 1: Adaptive Arrow schema architecture overview.</figcaption>
</figure>

<p>The overall Adaptive Arrow schema component takes a data stream segmented into batches and produces one or multiple streams of Arrow Records (one schema per stream). Each of these records is defined with an Arrow schema, which is based both on the annotated Arrow schema and the shape of fields observed in the incoming data.</p>

<p>More specifically, the process of the Adaptive Arrow schema component consists of four main phases</p>

<p><strong>Initialization phase</strong></p>

<p>During the initialization phase, the Arrow Record Encoder reads the annotated Arrow schema (i.e. the reference schema) and generates a collection of transformations. When these transformations are applied to the reference schema, they yield the first minimal Arrow schema that adheres to the constraints depicted by these annotations. In this initial iteration, all optional fields are eliminated, and all dictionary-encoded fields are configured to utilize the smallest encoding as defined by the annotation (only <code class="language-plaintext highlighter-rouge">Dictionary8</code> in the previous example). These transformations form a tree, reflecting the structure of the reference schema.</p>

<p><strong>Feeding phase</strong></p>

<p>Following the initialization is the feeding phase. Here, the Arrow Record Encoder scans the batch and attempts to store all the fields in an Arrow Record Builder, which is defined by the schema created in the prior step. If a field exists in the data but is not included in the schema, the encoder will trigger a <code class="language-plaintext highlighter-rouge">missing field</code> event. This process continues until the current batch is completely processed. An additional internal check is conducted on all dictionary-encoded fields in the Arrow Record builder to ensure there’s no dictionary overflow (i.e. more unique entries than the cardinality of the index permits). <code class="language-plaintext highlighter-rouge">Dictionary overflow</code> events are generated if such a situation is detected. Consequently, by the end, all unknown fields and dictionary overflow would have been detected, or alternatively, no discrepancies would have surfaced if the data aligns perfectly with the schema.</p>

<p><strong>Corrective phase</strong></p>

<p>If at least one event has been generated, a corrective phase will be initiated to fix the schema. This optional stage considers all the events generated in the previous stage and adjusts the transformation tree accordingly to align with the observed data. A <code class="language-plaintext highlighter-rouge">missing field</code> event will remove a NoField transformation for the corresponding field. A <code class="language-plaintext highlighter-rouge">dictionary overflow</code> event will modify the dictionary transformation to mirror the event (e.g. changing the index type from uint8 to uint16, or if the maximum index size has been reached, the transformation will remove the dictionary-encoding and revert to the original non-dictionary-encoded type). The updated transformation tree is subsequently used to create a new schema and a fresh Arrow Record Builder. This Record Builder is then utilized to replay the preceding feeding phase with the batch that wasn’t processed correctly.</p>

<p><strong>Routing phase</strong></p>

<p>Once a Record Builder has been properly fed, an Arrow Record is created, and the system transitions into the routing phase. The router component calculates a schema signature of the record and utilizes this signature to route the record to an existing Arrow stream compatible with the signature, or it initiates a new stream if there is no match.</p>

<p>This four-phase process should gradually adapt and stabilize the schema to a structure and definition that is optimized for a specific data stream. Unused fields will never unnecessarily consume memory. Dictionary-encoded fields will be defined with the most optimal index size based on the observed data cardinality, and fields with a cardinality exceeding a certain threshold (defined by configuration) will automatically revert to their non-dictionary-encoded versions.</p>

<p>To effectively execute this approach, you must ensure that there is a sufficient level of flexibility on the receiver side. It’s crucial that your downstream pipeline remains functional even when some fields are missing in the schema or when various dictionary index configurations are employed. While this may not always be feasible without implementing additional transformations upon reception, it proves worthwhile in certain scenarios.</p>

<p>The following results highlight the significant memory usage reduction achieved through the application of various optimization techniques. These results were gathered using a schema akin to the one previously presented. The considerable memory efficiency underscores the effectiveness of this approach.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/memory-usage-25k-traces.png" width="100%" class="img-responsive" alt="Fig 2: Comparative analysis of memory usage for different schema optimizations." />
  <figcaption>Fig 2: Comparative analysis of memory usage for different schema optimizations.</figcaption>
</figure>

<p>The concept of a transformation tree enables a generalized approach to perform various types of schema optimizations based on the knowledge acquired from the data. This architecture is highly flexible; the current implementation allows for the removal of unused fields, the application of the most specific dictionary encoding, and the optimization of union type variants. In the future, there is potential for introducing additional optimizations that can be expressed as transformations on the initial schema. An implementation of this approach is available <a href="https://github.com/f5/otel-arrow-adapter/tree/main/pkg/otel/common/schema">here</a>.</p>

<h2 id="handling-recursive-schema-definition">Handling recursive schema definition</h2>

<p>Apache Arrow does not support recursive schema definitions, implying that data structures with variable depth cannot be directly represented. Figure 3 exemplifies such a recursive definition where the value of an attribute can either be a simple data type, a list of values, or a map of values. The depth of this definition cannot be predetermined.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/recursive-def-otel-attributes.svg" width="100%" class="img-responsive" alt="Fig 3: Recursive definition of OTel attributes." />
  <figcaption>Fig 3: Recursive definition of OTel attributes.</figcaption>
</figure>

<p>Several strategies can be employed to circumvent this limitation. Technically, the dynamic schema concept we’ve presented could be expanded to dynamically update the schema to include any missing level of recursion. However, for this use case, this method is complex and has the notable downside of not offering any assurance on the maximum size of the schema. This lack of constraint can pose security issues; hence, this approach isn’t elaborated upon.</p>

<p>The second approach consists of breaking the recursion by employing a serialization format that supports the definition of a recursive schema. The result of this serialization can then be integrated into the Arrow record as a binary type column, effectively halting the recursion at a specific level. To fully leverage the advantages of columnar representation, it is crucial to apply this ad-hoc serialization as deeply within the data structure as feasible. In the context of OpenTelemetry, this is performed at the attribute level – more specifically, at the second level of attributes.</p>

<p>A variety of serialization formats, such as protobuf or CBOR, can be employed to encode recursive data. Without particular treatment, these binary columns may not be easily queryable by the existing Arrow query engines. Therefore, it’s crucial to thoughtfully ascertain when and where to apply such a technique. While I’m not aware of any attempts to address this limitation within the Arrow system, it doesn’t seem insurmountable and would constitute a valuable extension. This would help reduce the complexity of integrating Arrow with other systems that rely on such recursive definitions.</p>

<h2 id="importance-of-sorting">Importance of sorting</h2>

<p>In our preceding <a href="https://arrow.apache.org/blog/2023/04/11/our-journey-at-f5-with-apache-arrow-part-1/">article</a>, we explored a variety of strategies to represent hierarchical data models, including nested structures based on struct/list/map/union, denormalization and flattening representations, as well as a multi-record approach. Each method presents its unique advantages and disadvantages. However, in this last section, we’ll delve deeper into the multi-record approach, focusing specifically on its ability to offer versatile sorting options and how these options contribute to an enhanced compression ratio.</p>

<p>In the OTel Arrow protocol, we leverage the multi-record approach to represent metrics, logs, and traces. The following entity-relationship diagram offers a simplified version of various record schemas and illustrates their relationships, specifically those used to represent gauges and sums. A comprehensive description of the Arrow data model employed in OpenTelemetry can be accessed <a href="https://github.com/f5/otel-arrow-adapter/blob/main/docs/data_model.md">here</a>.</p>

<p>These Arrow records, also referred to as tables, form a hierarchy with <code class="language-plaintext highlighter-rouge">METRICS</code> acting as the primary entry point. Each table can be independently sorted according to one or more columns. This sorting strategy facilitates the grouping of duplicated data, thereby improving the compression ratio.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/metric-dp-data-model.png" width="100%" class="img-responsive" alt="Fig 4: A simplified entity-relationship diagram representing OTel sum &amp; gauge metrics." />
  <figcaption>Fig 4: A simplified entity-relationship diagram representing OTel sum &amp; gauge metrics.</figcaption>
</figure>

<p>The relationship between the primary <code class="language-plaintext highlighter-rouge">METRICS</code> table and the secondary <code class="language-plaintext highlighter-rouge">RESOURCE_ATTRS</code>, <code class="language-plaintext highlighter-rouge">SCOPE_ATTRS</code>, and <code class="language-plaintext highlighter-rouge">NUMBER_DATA_POINTS</code> tables is established through a unique <code class="language-plaintext highlighter-rouge">id</code> in the main table and a <code class="language-plaintext highlighter-rouge">parent_id</code> column in each of the secondary tables. This {id,parent_id} pair represents an overhead that should be minimized to the greatest extent possible post-compression.</p>

<p>To achieve this, the ordering process for the different tables adheres to the hierarchy, starting from the main table down to the leaf. The main table is sorted (by one or multiple columns), and then an incremental id is assigned to each row. This numerical id is stored using delta-encoding, which is implemented on top of Arrow.</p>

<p>The secondary tables directly connected to the main table are sorted using the same principle, but the <code class="language-plaintext highlighter-rouge">parent_id</code> column is consistently utilized as the last column in the sort statement. Including the <code class="language-plaintext highlighter-rouge">parent_id</code> column in the sort statement enables the use of a variation of delta encoding. The efficiency of this approach is summarized in the chart below.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/compressed-message-size.png" width="100%" class="img-responsive" alt="Fig 5: Comparative analysis of compression ratios - OTLP Protocol vs. Two variations of the OTel Arrow Protocol with multivariate metrics stream. (lower percentage is better)" />
  <figcaption>Fig 5: Comparative analysis of compression ratios - OTLP Protocol vs. Two variations of the OTel Arrow Protocol with multivariate metrics stream. (lower percentage is better)</figcaption>
</figure>

<p>The second column presents the average size of the OTLP batch both pre- and post-ZSTD compression for batches of varying sizes. This column serves as a reference point for the ensuing two columns. The third column displays results for the OTel Arrow protocol without any sorting applied, while the final column showcases results for the OTel Arrow protocol with sorting enabled.</p>

<p>Before compression, the average batch sizes for the two OTel Arrow configurations are predictably similar. However, post-compression, the benefits of sorting each individual table on the compression ratio become immediately apparent. Without sorting, the OTel Arrow protocol exhibits a compression ratio that’s 1.40 to 1.67 times better than the reference. When sorting is enabled, the OTel Arrow protocol outperforms the reference by a factor ranging from 4.94 to 7.21 times!</p>

<p>The gains in terms of compression obviously depend on your data and the redundancy of information present in your data batches. According to our observations, the choice of a good sort generally improves the compression ratio by a factor of 1.5 to 8.</p>

<p>Decomposing a complex schema into multiple simpler schemas to enhance sorting capabilities, coupled with a targeted approach to efficiently encode the identifiers representing the relationships, emerges as an effective strategy for enhancing overall data compression. This method also eliminates complex Arrow data types, such as lists, maps, and unions. Consequently, it not only improves but also simplifies data query-ability. This simplification proves beneficial for existing query engines, which may struggle to operate on intricate schemas.</p>

<h2 id="conclusion-and-next-steps">Conclusion and next steps</h2>

<p>This article concludes our two-part series on Apache Arrow, wherein we have explored various strategies to maximize the utility of Apache Arrow within specific contexts. The adaptive schema architecture presented in the second part of this series paves the way for future optimization possibilities. We look forward to seeing what the community can add based on this contribution.</p>

<p>Apache Arrow is an exceptional project, continually enhanced by a thriving ecosystem. However, throughout our exploration, we have noticed certain gaps or points of friction that, if addressed, could significantly enrich the overall experience.</p>
<ul>
  <li>Designing an efficient Arrow schema can, in some cases, prove to be a challenging task. Having the <strong>ability to collect statistics</strong> at the record level could facilitate this design phase (data distribution per field, dictionary stats, Arrow array sizes before/after compression, and so on). These statistics would also assist in identifying the most effective columns on which to base the record sorting.</li>
  <li><strong>Native support for recursive schemas</strong> would also increase adoption by simplifying the use of Arrow in complex scenarios. While I’m not aware of any attempts to address this limitation within the Arrow system, it doesn’t seem insurmountable and would constitute a valuable extension. This would help reduce the complexity of integrating Arrow with other systems that rely on such recursive definitions.</li>
  <li><strong>Harmonizing the support for data types as well as IPC stream capabilities</strong> would also be a major benefit. Predominant client libraries support nested and hierarchical schemas, but their use is limited due to a lack of full support across the rest of the ecosystem. For example, list and/or union types are not well supported by query engines or Parquet bridges. Also, the advanced dictionary support within IPC streams is not consistent across different implementations (i.e. delta dictionaries and replacement dictionaries are not supported by all implementations).</li>
  <li><strong>Optimizing the support of complex schemas</strong> in terms of memory consumption and compression rate could be improved by natively integrating the concept of the dynamic schema presented in this article.</li>
  <li><strong>Detecting dictionary overflows</strong> (index level) is not something that is easy to test on the fly. The API could be improved to indicate this overflow as soon as an insertion occurs.</li>
</ul>

<p>Our effort to utilize Apache Arrow in conjunction with OpenTelemetry has produced encouraging results. While this has necessitated considerable investment in terms of development, exploration, and benchmarking, we hope that these articles will aid in accelerating your journey with Apache Arrow. Looking ahead, we envision an end-to-end integration with Apache Arrow and plan to significantly extend our use of the Arrow ecosystem. This extension involves providing a bridge with Parquet and integrating with a query engine such as DataFusion, with the goal of processing telemetry streams within the collector.</p>]]></content><author><name>Laurent Quérel</name></author><category term="application" /><summary type="html"><![CDATA[In the previous article, we discussed our use of Apache Arrow within the context of the OpenTelemetry project. We investigated various techniques to maximize the efficiency of Apache Arrow, aiming to find the optimal balance between data compression ratio and queryability. The compression results speak for themselves, boasting improvements ranging from 1.5x to 5x better than the original OTLP protocol. In this article, we will delve into three techniques that have enabled us to enhance both the compression ratio and memory usage of Apache Arrow buffers within the current version of the OTel Arrow protocol. The first technique we’ll discuss aims to optimize schemas in terms of memory usage. As you’ll see, the gains can be substantial, potentially halving memory usage in certain cases. The second section will delve more deeply into the various approaches that can be used to handle recursive schema definitions. Lastly, we’ll emphasize that the design of your schema(s), coupled with the sorts you can apply at the record level, play a pivotal role in maximizing the benefits of Apache Arrow and its columnar representation. Handling dynamic and unknown data distributions In certain contexts, the comprehensive definition of an Arrow schema can end up being overly broad and complex in order to cover all possible cases that you intend to represent in columnar form. However, as is often the case with complex schemas, only a subset of this schema will actually be utilized for a specific deployment. Similarly, it’s not always possible to determine the optimal dictionary encoding for one or more fields in advance. Employing a broad and very general schema that covers all cases is usually more memory-intensive. This is because, for most implementations, a column without value still continues to consume memory space. Likewise, a column with dictionary encoding that indexes a uint64 will occupy four times more memory than the same column with a dictionary encoding based on a uint8. To illustrate this more concretely, let’s consider an OTel collector positioned at the output of a production environment, receiving a telemetry data stream produced by a large and dynamic set of servers. Invariably, the content of this telemetry stream will change in volume and nature over time. It’s challenging to predict the optimal schema in such a scenario, and it’s equally difficult to know in advance the distribution of a particular attribute of the telemetry data passing through this point. To optimize such scenarios, we have adopted an intermediary approach that we have named dynamic Arrow schema, aiming to gradually adapt the schema based on the observed data. The general principle is relatively simple. We start with a general schema defining the maximum envelope of what should be represented. Some fields of this schema will be declared optional, while other fields will be encoded with multiple possible options depending on the observed distribution. In theory, this principle can be applied to other types of transformations (e.g., recursive column creation) but we will let your imagination explore these other options. So if you encounter data streams where certain fields are not utilized, some union variants remain unused, and/or the value distribution of a field cannot be determined a priori, it may be worthwhile to invest time in implementing this model. This can lead to improved efficiency in terms of compression ratio, memory usage, and processing speed. The following Go Arrow schema definition provides an example of such a schema, instrumented with a collection of annotations. These annotations will be processed by an enhanced Record Builder, equipped with the ability to dynamically adapt the schema. The structure of this system is illustrated in Figure 1. var ( // Arrow schema for the OTLP Arrow Traces record (without attributes, links, and events). TracesSchema = arrow.NewSchema([]arrow.Field{ // Nullabe:true means the field is optional, in this case of 16 bit unsigned integers {Name: constants.ID, Type: arrow.PrimitiveTypes.Uint16, Nullable: true}, {Name: constants.Resource, Type: arrow.StructOf([]arrow.Field{ {Name: constants.ID, Type: arrow.PrimitiveTypes.Uint16, Nullable: true}, // --- Use dictionary with 8 bit integers initially ---- {Name: constants.SchemaUrl,Type: arrow.BinaryTypes.String,Metadata: schema.Metadata(schema.Dictionary8), Nullable: true}, {Name: constants.DroppedAttributesCount,Type: arrow.PrimitiveTypes.Uint32,Nullable: true}, }...), Nullable: true}, {Name: constants.Scope, Type: arrow.StructOf([]arrow.Field{ {Name: constants.ID, Type: arrow.PrimitiveTypes.Uint16, Metadata: acommon.Metadata(acommon.DeltaEncoding), Nullable: true}, // --- Use dictionary with 8 bit integers initially ---- {Name: constants.Name, Type: arrow.BinaryTypes.String, Metadata: acommon.Metadata(acommon.Dictionary8), Nullable: true}, {Name: constants.Version, Type: arrow.BinaryTypes.String, Metadata: acommon.Metadata(acommon.Dictionary8), Nullable: true}, {Name: constants.DroppedAttributesCount, Type: arrow.PrimitiveTypes.Uint32, Nullable: true}, }...), Nullable: true}, {Name: constants.SchemaUrl, Type: arrow.BinaryTypes.String, Metadata: schema.Metadata(schema.Dictionary8), Nullable: true}, {Name: constants.StartTimeUnixNano, Type: arrow.FixedWidthTypes.Timestamp_ns}, {Name: constants.DurationTimeUnixNano, Type: arrow.FixedWidthTypes.Duration_ms, Metadata: schema.Metadata(schema.Dictionary8)}, {Name: constants.TraceId, Type: &amp;arrow.FixedSizeBinaryType{ByteWidth: 16}}, {Name: constants.SpanId, Type: &amp;arrow.FixedSizeBinaryType{ByteWidth: 8}}, {Name: constants.TraceState, Type: arrow.BinaryTypes.String, Metadata: schema.Metadata(schema.Dictionary8), Nullable: true}, {Name: constants.ParentSpanId, Type: &amp;arrow.FixedSizeBinaryType{ByteWidth: 8}, Nullable: true}, {Name: constants.Name, Type: arrow.BinaryTypes.String, Metadata: schema.Metadata(schema.Dictionary8)}, {Name: constants.KIND, Type: arrow.PrimitiveTypes.Int32, Metadata: schema.Metadata(schema.Dictionary8), Nullable: true}, {Name: constants.DroppedAttributesCount, Type: arrow.PrimitiveTypes.Uint32, Nullable: true}, {Name: constants.DroppedEventsCount, Type: arrow.PrimitiveTypes.Uint32, Nullable: true}, {Name: constants.DroppedLinksCount, Type: arrow.PrimitiveTypes.Uint32, Nullable: true}, {Name: constants.Status, Type: arrow.StructOf([]arrow.Field{ {Name: constants.StatusCode, Type: arrow.PrimitiveTypes.Int32, Metadata: schema.Metadata(schema.Dictionary8), Nullable: true}, {Name: constants.StatusMessage, Type: arrow.BinaryTypes.String, Metadata: schema.Metadata(schema.Dictionary8), Nullable: true}, }...), Nullable: true}, }, nil) ) In this example, Arrow field-level metadata are employed to designate when a field is optional (Nullable: true) or to specify the minimal dictionary encoding applicable to a particular field (Metadata Dictionary8/16/…). Now let’s imagine a scenario utilizing this schema in a straightforward scenario, wherein only a handful of fields are actually in use, and the cardinality of most dictionary-encoded fields is low (i.e., below 2^8). Ideally, we’d want a system capable of dynamically constructing the following simplified schema, which, in essence, is a strict subset of the original schema. var ( // Simplified schema definition generated by the Arrow Record encoder based on // the data observed. TracesSchema = arrow.NewSchema([]arrow.Field{ {Name: constants.ID, Type: arrow.PrimitiveTypes.Uint16, Nullable: true}, {Name: constants.StartTimeUnixNano, Type: arrow.FixedWidthTypes.Timestamp_ns}, {Name: constants.TraceId, Type: &amp;arrow.FixedSizeBinaryType{ByteWidth: 16}}, {Name: constants.SpanId, Type: &amp;arrow.FixedSizeBinaryType{ByteWidth: 8}}, {Name: constants.Name, Type: &amp;arrow.DictionaryType { IndexType: arrow.PrimitiveTypes.Uint8, ValueType: arrow.BinaryTypes.String}}, {Name: constants.KIND, Type: &amp;arrow.DictionaryType { IndexType: arrow.PrimitiveTypes.Uint8, ValueType: arrow.PrimitiveTypes.Int32, }, Nullable: true}, }, nil) ) Additionally, we desire a system capable of automatically adapting the aforementioned schema if it encounters new fields or existing fields with a cardinality exceeding the size of the current dictionary definition in future batches. In extreme scenarios, if the cardinality of a specific field surpasses a certain threshold, we would prefer the system to automatically revert to the non-dictionary representation (mechanism of dictionary overflow). That is precisely what we will elaborate on in the remainder of this section. An overview of the different components and events used to implement this approach is depicted in figure 1. Fig 1: Adaptive Arrow schema architecture overview. The overall Adaptive Arrow schema component takes a data stream segmented into batches and produces one or multiple streams of Arrow Records (one schema per stream). Each of these records is defined with an Arrow schema, which is based both on the annotated Arrow schema and the shape of fields observed in the incoming data. More specifically, the process of the Adaptive Arrow schema component consists of four main phases Initialization phase During the initialization phase, the Arrow Record Encoder reads the annotated Arrow schema (i.e. the reference schema) and generates a collection of transformations. When these transformations are applied to the reference schema, they yield the first minimal Arrow schema that adheres to the constraints depicted by these annotations. In this initial iteration, all optional fields are eliminated, and all dictionary-encoded fields are configured to utilize the smallest encoding as defined by the annotation (only Dictionary8 in the previous example). These transformations form a tree, reflecting the structure of the reference schema. Feeding phase Following the initialization is the feeding phase. Here, the Arrow Record Encoder scans the batch and attempts to store all the fields in an Arrow Record Builder, which is defined by the schema created in the prior step. If a field exists in the data but is not included in the schema, the encoder will trigger a missing field event. This process continues until the current batch is completely processed. An additional internal check is conducted on all dictionary-encoded fields in the Arrow Record builder to ensure there’s no dictionary overflow (i.e. more unique entries than the cardinality of the index permits). Dictionary overflow events are generated if such a situation is detected. Consequently, by the end, all unknown fields and dictionary overflow would have been detected, or alternatively, no discrepancies would have surfaced if the data aligns perfectly with the schema. Corrective phase If at least one event has been generated, a corrective phase will be initiated to fix the schema. This optional stage considers all the events generated in the previous stage and adjusts the transformation tree accordingly to align with the observed data. A missing field event will remove a NoField transformation for the corresponding field. A dictionary overflow event will modify the dictionary transformation to mirror the event (e.g. changing the index type from uint8 to uint16, or if the maximum index size has been reached, the transformation will remove the dictionary-encoding and revert to the original non-dictionary-encoded type). The updated transformation tree is subsequently used to create a new schema and a fresh Arrow Record Builder. This Record Builder is then utilized to replay the preceding feeding phase with the batch that wasn’t processed correctly. Routing phase Once a Record Builder has been properly fed, an Arrow Record is created, and the system transitions into the routing phase. The router component calculates a schema signature of the record and utilizes this signature to route the record to an existing Arrow stream compatible with the signature, or it initiates a new stream if there is no match. This four-phase process should gradually adapt and stabilize the schema to a structure and definition that is optimized for a specific data stream. Unused fields will never unnecessarily consume memory. Dictionary-encoded fields will be defined with the most optimal index size based on the observed data cardinality, and fields with a cardinality exceeding a certain threshold (defined by configuration) will automatically revert to their non-dictionary-encoded versions. To effectively execute this approach, you must ensure that there is a sufficient level of flexibility on the receiver side. It’s crucial that your downstream pipeline remains functional even when some fields are missing in the schema or when various dictionary index configurations are employed. While this may not always be feasible without implementing additional transformations upon reception, it proves worthwhile in certain scenarios. The following results highlight the significant memory usage reduction achieved through the application of various optimization techniques. These results were gathered using a schema akin to the one previously presented. The considerable memory efficiency underscores the effectiveness of this approach. Fig 2: Comparative analysis of memory usage for different schema optimizations. The concept of a transformation tree enables a generalized approach to perform various types of schema optimizations based on the knowledge acquired from the data. This architecture is highly flexible; the current implementation allows for the removal of unused fields, the application of the most specific dictionary encoding, and the optimization of union type variants. In the future, there is potential for introducing additional optimizations that can be expressed as transformations on the initial schema. An implementation of this approach is available here. Handling recursive schema definition Apache Arrow does not support recursive schema definitions, implying that data structures with variable depth cannot be directly represented. Figure 3 exemplifies such a recursive definition where the value of an attribute can either be a simple data type, a list of values, or a map of values. The depth of this definition cannot be predetermined. Fig 3: Recursive definition of OTel attributes. Several strategies can be employed to circumvent this limitation. Technically, the dynamic schema concept we’ve presented could be expanded to dynamically update the schema to include any missing level of recursion. However, for this use case, this method is complex and has the notable downside of not offering any assurance on the maximum size of the schema. This lack of constraint can pose security issues; hence, this approach isn’t elaborated upon. The second approach consists of breaking the recursion by employing a serialization format that supports the definition of a recursive schema. The result of this serialization can then be integrated into the Arrow record as a binary type column, effectively halting the recursion at a specific level. To fully leverage the advantages of columnar representation, it is crucial to apply this ad-hoc serialization as deeply within the data structure as feasible. In the context of OpenTelemetry, this is performed at the attribute level – more specifically, at the second level of attributes. A variety of serialization formats, such as protobuf or CBOR, can be employed to encode recursive data. Without particular treatment, these binary columns may not be easily queryable by the existing Arrow query engines. Therefore, it’s crucial to thoughtfully ascertain when and where to apply such a technique. While I’m not aware of any attempts to address this limitation within the Arrow system, it doesn’t seem insurmountable and would constitute a valuable extension. This would help reduce the complexity of integrating Arrow with other systems that rely on such recursive definitions. Importance of sorting In our preceding article, we explored a variety of strategies to represent hierarchical data models, including nested structures based on struct/list/map/union, denormalization and flattening representations, as well as a multi-record approach. Each method presents its unique advantages and disadvantages. However, in this last section, we’ll delve deeper into the multi-record approach, focusing specifically on its ability to offer versatile sorting options and how these options contribute to an enhanced compression ratio. In the OTel Arrow protocol, we leverage the multi-record approach to represent metrics, logs, and traces. The following entity-relationship diagram offers a simplified version of various record schemas and illustrates their relationships, specifically those used to represent gauges and sums. A comprehensive description of the Arrow data model employed in OpenTelemetry can be accessed here. These Arrow records, also referred to as tables, form a hierarchy with METRICS acting as the primary entry point. Each table can be independently sorted according to one or more columns. This sorting strategy facilitates the grouping of duplicated data, thereby improving the compression ratio. Fig 4: A simplified entity-relationship diagram representing OTel sum &amp; gauge metrics. The relationship between the primary METRICS table and the secondary RESOURCE_ATTRS, SCOPE_ATTRS, and NUMBER_DATA_POINTS tables is established through a unique id in the main table and a parent_id column in each of the secondary tables. This {id,parent_id} pair represents an overhead that should be minimized to the greatest extent possible post-compression. To achieve this, the ordering process for the different tables adheres to the hierarchy, starting from the main table down to the leaf. The main table is sorted (by one or multiple columns), and then an incremental id is assigned to each row. This numerical id is stored using delta-encoding, which is implemented on top of Arrow. The secondary tables directly connected to the main table are sorted using the same principle, but the parent_id column is consistently utilized as the last column in the sort statement. Including the parent_id column in the sort statement enables the use of a variation of delta encoding. The efficiency of this approach is summarized in the chart below. Fig 5: Comparative analysis of compression ratios - OTLP Protocol vs. Two variations of the OTel Arrow Protocol with multivariate metrics stream. (lower percentage is better) The second column presents the average size of the OTLP batch both pre- and post-ZSTD compression for batches of varying sizes. This column serves as a reference point for the ensuing two columns. The third column displays results for the OTel Arrow protocol without any sorting applied, while the final column showcases results for the OTel Arrow protocol with sorting enabled. Before compression, the average batch sizes for the two OTel Arrow configurations are predictably similar. However, post-compression, the benefits of sorting each individual table on the compression ratio become immediately apparent. Without sorting, the OTel Arrow protocol exhibits a compression ratio that’s 1.40 to 1.67 times better than the reference. When sorting is enabled, the OTel Arrow protocol outperforms the reference by a factor ranging from 4.94 to 7.21 times! The gains in terms of compression obviously depend on your data and the redundancy of information present in your data batches. According to our observations, the choice of a good sort generally improves the compression ratio by a factor of 1.5 to 8. Decomposing a complex schema into multiple simpler schemas to enhance sorting capabilities, coupled with a targeted approach to efficiently encode the identifiers representing the relationships, emerges as an effective strategy for enhancing overall data compression. This method also eliminates complex Arrow data types, such as lists, maps, and unions. Consequently, it not only improves but also simplifies data query-ability. This simplification proves beneficial for existing query engines, which may struggle to operate on intricate schemas. Conclusion and next steps This article concludes our two-part series on Apache Arrow, wherein we have explored various strategies to maximize the utility of Apache Arrow within specific contexts. The adaptive schema architecture presented in the second part of this series paves the way for future optimization possibilities. We look forward to seeing what the community can add based on this contribution. Apache Arrow is an exceptional project, continually enhanced by a thriving ecosystem. However, throughout our exploration, we have noticed certain gaps or points of friction that, if addressed, could significantly enrich the overall experience. Designing an efficient Arrow schema can, in some cases, prove to be a challenging task. Having the ability to collect statistics at the record level could facilitate this design phase (data distribution per field, dictionary stats, Arrow array sizes before/after compression, and so on). These statistics would also assist in identifying the most effective columns on which to base the record sorting. Native support for recursive schemas would also increase adoption by simplifying the use of Arrow in complex scenarios. While I’m not aware of any attempts to address this limitation within the Arrow system, it doesn’t seem insurmountable and would constitute a valuable extension. This would help reduce the complexity of integrating Arrow with other systems that rely on such recursive definitions. Harmonizing the support for data types as well as IPC stream capabilities would also be a major benefit. Predominant client libraries support nested and hierarchical schemas, but their use is limited due to a lack of full support across the rest of the ecosystem. For example, list and/or union types are not well supported by query engines or Parquet bridges. Also, the advanced dictionary support within IPC streams is not consistent across different implementations (i.e. delta dictionaries and replacement dictionaries are not supported by all implementations). Optimizing the support of complex schemas in terms of memory consumption and compression rate could be improved by natively integrating the concept of the dynamic schema presented in this article. Detecting dictionary overflows (index level) is not something that is easy to test on the fly. The API could be improved to indicate this overflow as soon as an insertion occurs. Our effort to utilize Apache Arrow in conjunction with OpenTelemetry has produced encouraging results. While this has necessitated considerable investment in terms of development, exploration, and benchmarking, we hope that these articles will aid in accelerating your journey with Apache Arrow. Looking ahead, we envision an end-to-end integration with Apache Arrow and plan to significantly extend our use of the Arrow ecosystem. This extension involves providing a bridge with Parquet and integrating with a query engine such as DataFusion, with the goal of processing telemetry streams within the collector.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow DataFusion 26.0.0</title><link href="https://arrow.apache.org/blog/2023/06/24/datafusion-25.0.0/" rel="alternate" type="text/html" title="Apache Arrow DataFusion 26.0.0" /><published>2023-06-24T00:00:00-04:00</published><updated>2023-06-24T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/06/24/datafusion-25.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/06/24/datafusion-25.0.0/"><![CDATA[<!--

-->

<p>It has been a whirlwind 6 months of DataFusion development since <a href="https://arrow.apache.org/blog/2023/01/19/datafusion-16.0.0">our
last update</a>: the community has grown, many features have been added,
performance improved and we are <a href="https://github.com/apache/arrow-datafusion/discussions/6475">discussing</a> branching out to our own
top level Apache Project.</p>

<h2 id="background">Background</h2>

<p><a href="https://arrow.apache.org/datafusion/">Apache Arrow DataFusion</a> is an extensible query engine and database
toolkit, written in <a href="https://www.rust-lang.org/">Rust</a>, that uses <a href="https://arrow.apache.org">Apache Arrow</a> as its in-memory
format.</p>

<p>DataFusion, along with <a href="https://calcite.apache.org">Apache Calcite</a>, Facebook’s <a href="https://github.com/facebookincubator/velox">Velox</a> and
similar technology are part of the next generation “<a href="https://www.usenix.org/publications/login/winter2018/khurana">Deconstructed
Database</a>” architectures, where new systems are built on a foundation
of fast, modular components, rather as a single tightly integrated
system.</p>

<p>While single tightly integrated systems such as <a href="https://spark.apache.org/">Spark</a>, <a href="https://duckdb.org">DuckDB</a> and
<a href="https://www.pola.rs/">Pola.rs</a> are great pieces of technology, our community believes that
anyone developing new data heavy application, such as those common in
machine learning in the next 5 years, will <strong>require</strong> a high
performance, vectorized, query engine to remain relevant. The only
practical way to gain access to such technology without investing many
millions of dollars to build a new tightly integrated engine, is
though open source projects like DataFusion and similar enabling
technologies such as <a href="https://arrow.apache.org">Apache Arrow</a> and <a href="https://www.rust-lang.org/">Rust</a>.</p>

<p>DataFusion is targeted primarily at developers creating other data
intensive analytics, and offers:</p>

<ul>
  <li>High performance, native, parallel streaming execution engine</li>
  <li>Mature <a href="https://arrow.apache.org/datafusion/user-guide/sql/index.html">SQL support</a>, featuring  subqueries, window functions, grouping sets, and more</li>
  <li>Built in support for Parquet, Avro, CSV, JSON and Arrow formats and easy extension for others</li>
  <li>Native DataFrame API and <a href="https://arrow.apache.org/datafusion-python/">python bindings</a></li>
  <li><a href="https://docs.rs/datafusion/latest/datafusion/index.html">Well documented</a> source code and architecture, designed to be customized to suit downstream project needs</li>
  <li>High quality, easy to use code <a href="https://crates.io/crates/datafusion/versions">released every 2 weeks to crates.io</a></li>
  <li>Welcoming, open community, governed by the highly regarded and well understood <a href="https://www.apache.org/">Apache Software Foundation</a></li>
</ul>

<p>The rest of this post highlights some of the improvements we have made
to DataFusion over the last 6 months and a preview of where we are
heading. You can see a list of all changes in the detailed
<a href="https://github.com/apache/arrow-datafusion/blob/main/datafusion/CHANGELOG.md">CHANGELOG</a>.</p>

<h2 id="even-better-performance">(Even) Better Performance</h2>

<p><a href="https://voltrondata.com/resources/speeds-and-feeds-hardware-and-software-matter">Various</a> benchmarks show DataFusion to be quite close or <a href="https://github.com/tustvold/access-log-bench">even
faster</a> to the state of the art in analytic performance (at the moment
this seems to be DuckDB). We continually work on improving performance
(see <a href="https://github.com/apache/arrow-datafusion/issues/5546">#5546</a> for a list) and would love additional help in this area.</p>

<p>DataFusion now reads single large Parquet files significantly faster by
<a href="https://github.com/apache/arrow-datafusion/pull/5057">parallelizing across multiple cores</a>. Native speeds for reading JSON
and CSV files are also up to 2.5x faster thanks to improvements
upstream in arrow-rs <a href="https://github.com/apache/arrow-rs/pull/3479#issuecomment-1384353159">JSON reader</a> and <a href="https://github.com/apache/arrow-rs/pull/3365">CSV reader</a>.</p>

<p>Also, we have integrated the <a href="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1/">arrow-rs Row Format</a> into DataFusion resulting in up to <a href="https://github.com/apache/arrow-datafusion/pull/6163">2-3x faster sorting and merging</a>.</p>

<h2 id="improved-documentation-and-website">Improved Documentation and Website</h2>

<p>Part of growing the DataFusion community is ensuring that DataFusion’s
features are understood and that it is easy to contribute and
participate. To that end the <a href="https://arrow.apache.org/datafusion/">website</a> has been cleaned up, <a href="https://docs.rs/datafusion/latest/datafusion/index.html#architecture">the
architecture guide</a> expanded, the <a href="https://arrow.apache.org/datafusion/contributor-guide/roadmap.html">roadmap</a> updated, and several
overview talks created:</p>

<ul>
  <li>Apr 2023 <em>Query Engine</em>: <a href="https://youtu.be/NVKujPxwSBA">recording</a> and <a href="https://docs.google.com/presentation/d/1D3GDVas-8y0sA4c8EOgdCvEjVND4s2E7I6zfs67Y4j8/edit#slide=id.p">slides</a></li>
  <li>April 2023 <em>Logical Plan and Expressions</em>: <a href="https://youtu.be/EzZTLiSJnhY">recording</a> and <a href="https://docs.google.com/presentation/d/1ypylM3-w60kVDW7Q6S99AHzvlBgciTdjsAfqNP85K30">slides</a></li>
  <li>April 2023 <em>Physical Plan and Execution</em>: <a href="https://youtu.be/2jkWU3_w6z0">recording</a> and <a href="https://docs.google.com/presentation/d/1cA2WQJ2qg6tx6y4Wf8FH2WVSm9JQ5UgmBWATHdik0hg">slides</a></li>
</ul>

<h2 id="new-features">New Features</h2>

<h3 id="more-streaming-less-memory">More Streaming, Less Memory</h3>

<p>We have made significant progress on the <a href="https://github.com/apache/arrow-datafusion/issues/4285">streaming execution roadmap</a>
such as <a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/trait.ExecutionPlan.html#method.unbounded_output">unbounded datasources</a>, <a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/aggregates/enum.GroupByOrderMode.html">streaming group by</a>, sophisticated
<a href="https://docs.rs/datafusion/latest/datafusion/physical_optimizer/global_sort_selection/index.html">sort</a> and <a href="https://docs.rs/datafusion/latest/datafusion/physical_optimizer/repartition/index.html">repartitioning</a> improvements in the optimizer, and support
for <a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/joins/struct.SymmetricHashJoinExec.html">symmetric hash join</a> (read more about that in the great <a href="https://www.synnada.ai/blog/general-purpose-stream-joins-via-pruning-symmetric-hash-joins">Synnada
Blog Post</a> on the topic). Together, these features both 1) make it
easier to build streaming systems using DataFusion that can
incrementally generate output before (or ever) seeing the end of the
input and 2) allow general queries to use less memory and generate their
results faster.</p>

<p>We have also improved the runtime <a href="https://docs.rs/datafusion/latest/datafusion/execution/memory_pool/index.html">memory management</a> system so that
DataFusion now stays within its declared memory budget <a href="https://github.com/apache/arrow-datafusion/issues/3941">generate
runtime errors</a>.</p>

<h3 id="dml-support-insert-delete-update-etc">DML Support (<code class="language-plaintext highlighter-rouge">INSERT</code>, <code class="language-plaintext highlighter-rouge">DELETE</code>, <code class="language-plaintext highlighter-rouge">UPDATE</code>, etc)</h3>

<p>Part of building high performance data systems includes writing data,
and DataFusion supports several features for creating new files:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">INSERT INTO</code> and <code class="language-plaintext highlighter-rouge">SELECT ... INTO </code> support for memory backed and CSV tables</li>
  <li>New <a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/insert/trait.DataSink.html">API for writing data into TableProviders</a></li>
</ul>

<p>We are working on easier to use <a href="https://github.com/apache/arrow-datafusion/issues/5654">COPY INTO</a> syntax, better support
for writing parquet, JSON, and AVRO, and more – see our <a href="https://github.com/apache/arrow-datafusion/issues/6569">tracking epic</a>
for more details.</p>

<h3 id="timestamp-and-intervals">Timestamp and Intervals</h3>

<p>One mark of the maturity of a SQL engine is how it handles the tricky
world of timestamp, date, times and interval arithmetic. DataFusion is
feature complete in this area and behaves as you would expect,
supporting queries such as</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="n">now</span><span class="p">()</span> <span class="o">+</span> <span class="s1">'1 month'</span> <span class="k">FROM</span> <span class="n">my_table</span><span class="p">;</span>
</code></pre></div></div>

<p>We still have a long tail of <a href="https://github.com/apache/arrow-datafusion/issues/3148">date and time improvements</a>, which we are working on as well.</p>

<h3 id="querying-structured-types-list-and-structs">Querying Structured Types (<code class="language-plaintext highlighter-rouge">List</code> and <code class="language-plaintext highlighter-rouge">Struct</code>s)</h3>

<p>Arrow and Parquet <a href="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/">support nested data</a> well and DataFusion lets you
easily query such <code class="language-plaintext highlighter-rouge">Struct</code> and <code class="language-plaintext highlighter-rouge">List</code>. For example, you can use
DataFusion to read and query the <a href="https://data.mendeley.com/datasets/ct8f9skv97">JSON Datasets for Exploratory OLAP -
Mendeley Data</a> like this:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">----------</span>
<span class="c1">-- Explore structured data using SQL</span>
<span class="c1">----------</span>
<span class="k">SELECT</span> <span class="k">delete</span> <span class="k">FROM</span> <span class="s1">'twitter-sample-head-100000.parquet'</span> <span class="k">WHERE</span> <span class="k">delete</span> <span class="k">IS</span> <span class="k">NOT</span> <span class="k">NULL</span> <span class="k">limit</span> <span class="mi">10</span><span class="p">;</span>
<span class="o">+</span><span class="c1">---------------------------------------------------------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="k">delete</span>                                                                                                                    <span class="o">|</span>
<span class="o">+</span><span class="c1">---------------------------------------------------------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">135037425050320896</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">135037425050320896</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">334902461</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">334902461</span><span class="p">}}</span> <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">134703982051463168</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">134703982051463168</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">405383453</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">405383453</span><span class="p">}}</span> <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">134773741740765184</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">134773741740765184</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">64823441</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">64823441</span><span class="p">}}</span>   <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">132543659655704576</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">132543659655704576</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">45917834</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">45917834</span><span class="p">}}</span>   <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">133786431926697984</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">133786431926697984</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">67229952</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">67229952</span><span class="p">}}</span>   <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">134619093570560002</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">134619093570560002</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">182430773</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">182430773</span><span class="p">}}</span> <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">134019857527214080</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">134019857527214080</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">257396311</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">257396311</span><span class="p">}}</span> <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">133931546469076993</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">133931546469076993</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">124539548</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">124539548</span><span class="p">}}</span> <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">134397743350296576</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">134397743350296576</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">139836391</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">139836391</span><span class="p">}}</span> <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">127833661767823360</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">127833661767823360</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">244442687</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">244442687</span><span class="p">}}</span> <span class="o">|</span>
<span class="o">+</span><span class="c1">---------------------------------------------------------------------------------------------------------------------------+</span>

<span class="c1">----------</span>
<span class="c1">-- Select some deeply nested fields</span>
<span class="c1">----------</span>
<span class="k">SELECT</span>
  <span class="k">delete</span><span class="p">[</span><span class="s1">'status'</span><span class="p">][</span><span class="s1">'id'</span><span class="p">][</span><span class="s1">'$numberLong'</span><span class="p">]</span> <span class="k">as</span> <span class="n">delete_id</span><span class="p">,</span>
  <span class="k">delete</span><span class="p">[</span><span class="s1">'status'</span><span class="p">][</span><span class="s1">'user_id'</span><span class="p">]</span> <span class="k">as</span> <span class="n">delete_user_id</span>
<span class="k">FROM</span> <span class="s1">'twitter-sample-head-100000.parquet'</span> <span class="k">WHERE</span> <span class="k">delete</span> <span class="k">IS</span> <span class="k">NOT</span> <span class="k">NULL</span> <span class="k">LIMIT</span> <span class="mi">10</span><span class="p">;</span>

<span class="o">+</span><span class="c1">--------------------+----------------+</span>
<span class="o">|</span> <span class="n">delete_id</span>          <span class="o">|</span> <span class="n">delete_user_id</span> <span class="o">|</span>
<span class="o">+</span><span class="c1">--------------------+----------------+</span>
<span class="o">|</span> <span class="mi">135037425050320896</span> <span class="o">|</span> <span class="mi">334902461</span>      <span class="o">|</span>
<span class="o">|</span> <span class="mi">134703982051463168</span> <span class="o">|</span> <span class="mi">405383453</span>      <span class="o">|</span>
<span class="o">|</span> <span class="mi">134773741740765184</span> <span class="o">|</span> <span class="mi">64823441</span>       <span class="o">|</span>
<span class="o">|</span> <span class="mi">132543659655704576</span> <span class="o">|</span> <span class="mi">45917834</span>       <span class="o">|</span>
<span class="o">|</span> <span class="mi">133786431926697984</span> <span class="o">|</span> <span class="mi">67229952</span>       <span class="o">|</span>
<span class="o">|</span> <span class="mi">134619093570560002</span> <span class="o">|</span> <span class="mi">182430773</span>      <span class="o">|</span>
<span class="o">|</span> <span class="mi">134019857527214080</span> <span class="o">|</span> <span class="mi">257396311</span>      <span class="o">|</span>
<span class="o">|</span> <span class="mi">133931546469076993</span> <span class="o">|</span> <span class="mi">124539548</span>      <span class="o">|</span>
<span class="o">|</span> <span class="mi">134397743350296576</span> <span class="o">|</span> <span class="mi">139836391</span>      <span class="o">|</span>
<span class="o">|</span> <span class="mi">127833661767823360</span> <span class="o">|</span> <span class="mi">244442687</span>      <span class="o">|</span>
<span class="o">+</span><span class="c1">--------------------+----------------+</span>
</code></pre></div></div>

<h3 id="subqueries-all-the-way-down">Subqueries All the Way Down</h3>

<p>DataFusion can run many different subqueries by rewriting them to
joins. It has been able to run the full suite of TPC-H queries for at
least the last year, but recently we have implemented significant
improvements to this logic, sufficient to run almost all queries in
the TPC-DS benchmark as well.</p>

<h2 id="community-and-project-growth">Community and Project Growth</h2>

<p>The six months since <a href="https://arrow.apache.org/blog/2023/01/19/datafusion-16.0.0">our last update</a> saw significant growth in
the DataFusion community. Between versions <code class="language-plaintext highlighter-rouge">17.0.0</code> and <code class="language-plaintext highlighter-rouge">26.0.0</code>,
DataFusion merged 711 PRs from 107 distinct contributors, not
including all the work that goes into our core dependencies such as
<a href="https://crates.io/crates/arrow">arrow</a>,
<a href="https://crates.io/crates/parquet">parquet</a>, and
<a href="https://crates.io/crates/object_store">object_store</a>, that much of
the same community helps support.</p>

<p>In addition, we have added 7 new committers and 1 new PMC member to
the Apache Arrow project, largely focused on DataFusion, and we
learned about some of the cool <a href="https://arrow.apache.org/datafusion/user-guide/introduction.html#known-users">new systems</a> which are using
DataFusion. Given the growth of the community and interest in the
project, we also clarified the <a href="https://github.com/apache/arrow-datafusion/discussions/6441">mission statement</a> and are
<a href="https://github.com/apache/arrow-datafusion/discussions/6475">discussing</a> “graduate”ing DataFusion to a new top level
Apache Software Foundation project.</p>

<!--
$ git log --pretty=oneline 17.0.0..26.0.0 . | wc -l
     711

$ git shortlog -sn 17.0.0..26.0.0 . | wc -l
      107
-->

<h1 id="how-to-get-involved">How to Get Involved</h1>

<p>Kudos to everyone in the community who has contributed ideas,
discussions, bug reports, documentation and code. It is exciting to be
innovating on the next generation of database architectures together!</p>

<p>If you are interested in contributing to DataFusion, we would love to
have you join us. You can try out DataFusion on some of your own
data and projects and let us know how it goes or contribute a PR with
documentation, tests or code. A list of open issues suitable for
beginners is <a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a>.</p>

<p>Check out our <a href="https://arrow.apache.org/datafusion/contributor-guide/communication.html">Communication Doc</a> for more ways to engage with the
community.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[It has been a whirlwind 6 months of DataFusion development since our last update: the community has grown, many features have been added, performance improved and we are discussing branching out to our own top level Apache Project. Background Apache Arrow DataFusion is an extensible query engine and database toolkit, written in Rust, that uses Apache Arrow as its in-memory format. DataFusion, along with Apache Calcite, Facebook’s Velox and similar technology are part of the next generation “Deconstructed Database” architectures, where new systems are built on a foundation of fast, modular components, rather as a single tightly integrated system. While single tightly integrated systems such as Spark, DuckDB and Pola.rs are great pieces of technology, our community believes that anyone developing new data heavy application, such as those common in machine learning in the next 5 years, will require a high performance, vectorized, query engine to remain relevant. The only practical way to gain access to such technology without investing many millions of dollars to build a new tightly integrated engine, is though open source projects like DataFusion and similar enabling technologies such as Apache Arrow and Rust. DataFusion is targeted primarily at developers creating other data intensive analytics, and offers: High performance, native, parallel streaming execution engine Mature SQL support, featuring subqueries, window functions, grouping sets, and more Built in support for Parquet, Avro, CSV, JSON and Arrow formats and easy extension for others Native DataFrame API and python bindings Well documented source code and architecture, designed to be customized to suit downstream project needs High quality, easy to use code released every 2 weeks to crates.io Welcoming, open community, governed by the highly regarded and well understood Apache Software Foundation The rest of this post highlights some of the improvements we have made to DataFusion over the last 6 months and a preview of where we are heading. You can see a list of all changes in the detailed CHANGELOG. (Even) Better Performance Various benchmarks show DataFusion to be quite close or even faster to the state of the art in analytic performance (at the moment this seems to be DuckDB). We continually work on improving performance (see #5546 for a list) and would love additional help in this area. DataFusion now reads single large Parquet files significantly faster by parallelizing across multiple cores. Native speeds for reading JSON and CSV files are also up to 2.5x faster thanks to improvements upstream in arrow-rs JSON reader and CSV reader. Also, we have integrated the arrow-rs Row Format into DataFusion resulting in up to 2-3x faster sorting and merging. Improved Documentation and Website Part of growing the DataFusion community is ensuring that DataFusion’s features are understood and that it is easy to contribute and participate. To that end the website has been cleaned up, the architecture guide expanded, the roadmap updated, and several overview talks created: Apr 2023 Query Engine: recording and slides April 2023 Logical Plan and Expressions: recording and slides April 2023 Physical Plan and Execution: recording and slides New Features More Streaming, Less Memory We have made significant progress on the streaming execution roadmap such as unbounded datasources, streaming group by, sophisticated sort and repartitioning improvements in the optimizer, and support for symmetric hash join (read more about that in the great Synnada Blog Post on the topic). Together, these features both 1) make it easier to build streaming systems using DataFusion that can incrementally generate output before (or ever) seeing the end of the input and 2) allow general queries to use less memory and generate their results faster. We have also improved the runtime memory management system so that DataFusion now stays within its declared memory budget generate runtime errors. DML Support (INSERT, DELETE, UPDATE, etc) Part of building high performance data systems includes writing data, and DataFusion supports several features for creating new files: INSERT INTO and SELECT ... INTO support for memory backed and CSV tables New API for writing data into TableProviders We are working on easier to use COPY INTO syntax, better support for writing parquet, JSON, and AVRO, and more – see our tracking epic for more details. Timestamp and Intervals One mark of the maturity of a SQL engine is how it handles the tricky world of timestamp, date, times and interval arithmetic. DataFusion is feature complete in this area and behaves as you would expect, supporting queries such as SELECT now() + '1 month' FROM my_table; We still have a long tail of date and time improvements, which we are working on as well. Querying Structured Types (List and Structs) Arrow and Parquet support nested data well and DataFusion lets you easily query such Struct and List. For example, you can use DataFusion to read and query the JSON Datasets for Exploratory OLAP - Mendeley Data like this: ---------- -- Explore structured data using SQL ---------- SELECT delete FROM 'twitter-sample-head-100000.parquet' WHERE delete IS NOT NULL limit 10; +---------------------------------------------------------------------------------------------------------------------------+ | delete | +---------------------------------------------------------------------------------------------------------------------------+ | {status: {id: {$numberLong: 135037425050320896}, id_str: 135037425050320896, user_id: 334902461, user_id_str: 334902461}} | | {status: {id: {$numberLong: 134703982051463168}, id_str: 134703982051463168, user_id: 405383453, user_id_str: 405383453}} | | {status: {id: {$numberLong: 134773741740765184}, id_str: 134773741740765184, user_id: 64823441, user_id_str: 64823441}} | | {status: {id: {$numberLong: 132543659655704576}, id_str: 132543659655704576, user_id: 45917834, user_id_str: 45917834}} | | {status: {id: {$numberLong: 133786431926697984}, id_str: 133786431926697984, user_id: 67229952, user_id_str: 67229952}} | | {status: {id: {$numberLong: 134619093570560002}, id_str: 134619093570560002, user_id: 182430773, user_id_str: 182430773}} | | {status: {id: {$numberLong: 134019857527214080}, id_str: 134019857527214080, user_id: 257396311, user_id_str: 257396311}} | | {status: {id: {$numberLong: 133931546469076993}, id_str: 133931546469076993, user_id: 124539548, user_id_str: 124539548}} | | {status: {id: {$numberLong: 134397743350296576}, id_str: 134397743350296576, user_id: 139836391, user_id_str: 139836391}} | | {status: {id: {$numberLong: 127833661767823360}, id_str: 127833661767823360, user_id: 244442687, user_id_str: 244442687}} | +---------------------------------------------------------------------------------------------------------------------------+ ---------- -- Select some deeply nested fields ---------- SELECT delete['status']['id']['$numberLong'] as delete_id, delete['status']['user_id'] as delete_user_id FROM 'twitter-sample-head-100000.parquet' WHERE delete IS NOT NULL LIMIT 10; +--------------------+----------------+ | delete_id | delete_user_id | +--------------------+----------------+ | 135037425050320896 | 334902461 | | 134703982051463168 | 405383453 | | 134773741740765184 | 64823441 | | 132543659655704576 | 45917834 | | 133786431926697984 | 67229952 | | 134619093570560002 | 182430773 | | 134019857527214080 | 257396311 | | 133931546469076993 | 124539548 | | 134397743350296576 | 139836391 | | 127833661767823360 | 244442687 | +--------------------+----------------+ Subqueries All the Way Down DataFusion can run many different subqueries by rewriting them to joins. It has been able to run the full suite of TPC-H queries for at least the last year, but recently we have implemented significant improvements to this logic, sufficient to run almost all queries in the TPC-DS benchmark as well. Community and Project Growth The six months since our last update saw significant growth in the DataFusion community. Between versions 17.0.0 and 26.0.0, DataFusion merged 711 PRs from 107 distinct contributors, not including all the work that goes into our core dependencies such as arrow, parquet, and object_store, that much of the same community helps support. In addition, we have added 7 new committers and 1 new PMC member to the Apache Arrow project, largely focused on DataFusion, and we learned about some of the cool new systems which are using DataFusion. Given the growth of the community and interest in the project, we also clarified the mission statement and are discussing “graduate”ing DataFusion to a new top level Apache Software Foundation project. How to Get Involved Kudos to everyone in the community who has contributed ideas, discussions, bug reports, documentation and code. It is exciting to be innovating on the next generation of database architectures together! If you are interested in contributing to DataFusion, we would love to have you join us. You can try out DataFusion on some of your own data and projects and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable for beginners is here. Check out our Communication Doc for more ways to engage with the community.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow nanoarrow 0.2 Release</title><link href="https://arrow.apache.org/blog/2023/06/22/nanoarrow-0.120-release/" rel="alternate" type="text/html" title="Apache Arrow nanoarrow 0.2 Release" /><published>2023-06-22T00:00:00-04:00</published><updated>2023-06-22T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/06/22/nanoarrow-0.120-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/06/22/nanoarrow-0.120-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.2.0 release of
Apache Arrow nanoarrow. This initial release covers 19 resolved issues from
6 contributors.</p>

<h2 id="release-highlights">Release Highlights</h2>

<ul>
  <li>Addition of the Arrow <a href="#ipc-stream-support">IPC stream reader extension</a></li>
  <li>Addition of the <a href="#getting-started-with-nanoarrow">Getting Started with nanoarrow</a>
tutorial</li>
  <li>Improvements in reliability and platform test coverage of the <a href="#c-library">C library</a></li>
  <li>Improvements in reliability and type support in the <a href="#r-bindings">R bindings</a></li>
</ul>

<p>See the
<a href="https://github.com/apache/arrow-nanoarrow/blob/apache-arrow-nanoarrow-0.2.0/CHANGELOG.md">Changelog</a>
for a detailed list of contributions to this release.</p>

<h2 id="ipc-stream-support">IPC stream support</h2>

<p>This release includes support for reading schemas and record batches serialized
using the
<a href="https://arrow.apache.org/docs/format/Columnar.html#serialization-and-interprocess-communication-ipc">Arrow IPC format</a>. Based on the
<a href="https://github.com/dvidelabs/flatcc">flatcc</a>
flatbuffers implementation, the nanoarrow IPC read support is implemented as
an optional extension to the core nanoarrow library. The easiest way to get
started is with the <code class="language-plaintext highlighter-rouge">ArrowArrayStream</code> provider using one of the built-in
<code class="language-plaintext highlighter-rouge">ArrowIpcInputStream</code> implementations:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;stdbool.h&gt;</span><span class="cp">
</span>
<span class="cp">#include</span> <span class="cpf">"nanoarrow_ipc.h"</span><span class="cp">
</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">*</span> <span class="n">argv</span><span class="p">[])</span> <span class="p">{</span>
  <span class="kt">FILE</span><span class="o">*</span> <span class="n">file_ptr</span> <span class="o">=</span> <span class="n">freopen</span><span class="p">(</span><span class="nb">NULL</span><span class="p">,</span> <span class="s">"rb"</span><span class="p">,</span> <span class="n">stdin</span><span class="p">);</span>

  <span class="k">struct</span> <span class="n">ArrowIpcInputStream</span> <span class="n">input</span><span class="p">;</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowIpcInputStreamInitFile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">input</span><span class="p">,</span> <span class="n">file_ptr</span><span class="p">,</span> <span class="nb">false</span><span class="p">));</span>

  <span class="k">struct</span> <span class="n">ArrowArrayStream</span> <span class="n">stream</span><span class="p">;</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowIpcArrayStreamReaderInit</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">input</span><span class="p">,</span> <span class="nb">NULL</span><span class="p">));</span>

  <span class="k">struct</span> <span class="n">ArrowSchema</span> <span class="n">schema</span><span class="p">;</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">stream</span><span class="p">.</span><span class="n">get_schema</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">schema</span><span class="p">));</span>

  <span class="k">struct</span> <span class="n">ArrowArray</span> <span class="n">array</span><span class="p">;</span>
  <span class="k">while</span> <span class="p">(</span><span class="nb">true</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">stream</span><span class="p">.</span><span class="n">get_next</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">array</span><span class="p">));</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">array</span><span class="p">.</span><span class="n">release</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">break</span><span class="p">;</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Facilities for advanced usage are also provided via the low-level <code class="language-plaintext highlighter-rouge">ArrowIpcDecoder</code>,
which takes care of the details of deserializing the flatbuffer headers and
assembling buffers into an <code class="language-plaintext highlighter-rouge">ArrowArray</code>. The current implementation can read
schema and record batch messages that contain any Arrow type that supported by
the C data interface. The initial version can read both big and little endian
streams originating from platforms of either endian. Dictionary encoding and
compression are not currently supported.</p>

<h2 id="getting-started-with-nanoarrow">Getting started with nanoarrow</h2>

<p>Early users of the nanoarrow C library respectfully noted that it was difficult
to know where to begin. This release includes improvements in reference documentation
but also includes a long-form tutorial for those just getting started with or
considering adopting the library. You can find the tutorial at the
<a href="https://apache.github.io/arrow-nanoarrow/dev/getting-started.html">nanoarrow documentation site</a>.</p>

<h2 id="c-library">C library</h2>

<p>The nanoarrow 0.2.0 release also includes a number of bugfixes and improvements
to the core C library, many of which were identified as a result of usage
connected with development of the IPC extension.</p>

<ul>
  <li>Helpers for extracting/appending <code class="language-plaintext highlighter-rouge">Decimal128</code> and <code class="language-plaintext highlighter-rouge">Decimal256</code> elements
from/to arrays were added.</li>
  <li>The C library can now perform “full” validation to validate untrusted input
(e.g., serialized IPC from the wire).</li>
  <li>The C library can now perform “minimal” validation that performs all checks
that do not access any buffer data. This feature was added to facilitate
future support for the <code class="language-plaintext highlighter-rouge">ArrowDeviceArray</code> that was recently added as the
Arrow C device interface.</li>
  <li>Release verification on Ubuntu (x86_64, arm64), Fedora (x86_64),
Archlinux (x86_64), Centos 7 (x86_64, arm64), Alpine (x86_64, arm64, s390x),
Windows (x86_64), and MacOS (x86_64) were added to the continuous
integration system. Linux verification is implemented using <code class="language-plaintext highlighter-rouge">docker compose</code>
to facilitate local checks when developing features that may affect
a specific platform.</li>
</ul>

<h2 id="r-bindings">R bindings</h2>

<p>The nanoarrow R bindings are distributed as the <code class="language-plaintext highlighter-rouge">nanoarrow</code> package on
<a href="https://cran.r-project.org/">CRAN</a>. The 0.2.0 release of the R bindings includes
improvements in type support, improvements in stability, and features required
for the forthcoming release of Arrow Database Connectivity (ADBC) R bindings.
Notably:</p>

<ul>
  <li>Support for conversion of union arrays to R objects was added to facilitate
support for an ADBC function that returns such an array.</li>
  <li>Support for adding an R-level finalizer to an <code class="language-plaintext highlighter-rouge">ArrowArrayStream</code> was added
to facilitate safely wrapping a stream resulting from an ADBC call at the
R level.</li>
</ul>

<h2 id="python-bindings">Python bindings?</h2>

<p>The nanoarrow 0.2.0 release does not include Python bindings, but improvements to the
<a href="https://github.com/apache/arrow-nanoarrow/tree/main/python">unreleased draft bindings</a>
were added to facilitate discussion among Python developers regarding the useful
scope of a potential future nanoarrow Python package. If one of those developers is
you, feel free to
<a href="https://github.com/apache/arrow-nanoarrow/issues/new/choose">open an issue</a>
or send a post to the
<a href="https://lists.apache.org/list.html?dev@arrow.apache.org">developer mailing list</a>
to engage in the discussion.</p>

<h2 id="contributors">Contributors</h2>

<p>This initial release consists of contributions from 6 contributors in addition
to the invaluable advice and support of the Apache Arrow developer mailing list.
Special thanks to David Li for reviewing nearly every PR in this release!</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog -sn d91c35b33c7b6ff94f5f929384879352e241ed71..apache-arrow-nanoarrow-0.2.0 | grep -v "GitHub Actions"
    61  Dewey Dunnington
     2  Dirk Eddelbuettel
     2  Joris Van den Bossche
     2  Kirill Müller
     2  William Ayd
     1  David Li
</code></pre></div></div>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.2.0 release of Apache Arrow nanoarrow. This initial release covers 19 resolved issues from 6 contributors. Release Highlights Addition of the Arrow IPC stream reader extension Addition of the Getting Started with nanoarrow tutorial Improvements in reliability and platform test coverage of the C library Improvements in reliability and type support in the R bindings See the Changelog for a detailed list of contributions to this release. IPC stream support This release includes support for reading schemas and record batches serialized using the Arrow IPC format. Based on the flatcc flatbuffers implementation, the nanoarrow IPC read support is implemented as an optional extension to the core nanoarrow library. The easiest way to get started is with the ArrowArrayStream provider using one of the built-in ArrowIpcInputStream implementations: #include &lt;stdio.h&gt; #include &lt;stdbool.h&gt; #include "nanoarrow_ipc.h" int main(int argc, char* argv[]) { FILE* file_ptr = freopen(NULL, "rb", stdin); struct ArrowIpcInputStream input; NANOARROW_RETURN_NOT_OK(ArrowIpcInputStreamInitFile(&amp;input, file_ptr, false)); struct ArrowArrayStream stream; NANOARROW_RETURN_NOT_OK(ArrowIpcArrayStreamReaderInit(&amp;stream, &amp;input, NULL)); struct ArrowSchema schema; NANOARROW_RETURN_NOT_OK(stream.get_schema(&amp;stream, &amp;schema)); struct ArrowArray array; while (true) { NANOARROW_RETURN_NOT_OK(stream.get_next(&amp;stream, &amp;array)); if (array.release == NULL) { break; } } return 0; } Facilities for advanced usage are also provided via the low-level ArrowIpcDecoder, which takes care of the details of deserializing the flatbuffer headers and assembling buffers into an ArrowArray. The current implementation can read schema and record batch messages that contain any Arrow type that supported by the C data interface. The initial version can read both big and little endian streams originating from platforms of either endian. Dictionary encoding and compression are not currently supported. Getting started with nanoarrow Early users of the nanoarrow C library respectfully noted that it was difficult to know where to begin. This release includes improvements in reference documentation but also includes a long-form tutorial for those just getting started with or considering adopting the library. You can find the tutorial at the nanoarrow documentation site. C library The nanoarrow 0.2.0 release also includes a number of bugfixes and improvements to the core C library, many of which were identified as a result of usage connected with development of the IPC extension. Helpers for extracting/appending Decimal128 and Decimal256 elements from/to arrays were added. The C library can now perform “full” validation to validate untrusted input (e.g., serialized IPC from the wire). The C library can now perform “minimal” validation that performs all checks that do not access any buffer data. This feature was added to facilitate future support for the ArrowDeviceArray that was recently added as the Arrow C device interface. Release verification on Ubuntu (x86_64, arm64), Fedora (x86_64), Archlinux (x86_64), Centos 7 (x86_64, arm64), Alpine (x86_64, arm64, s390x), Windows (x86_64), and MacOS (x86_64) were added to the continuous integration system. Linux verification is implemented using docker compose to facilitate local checks when developing features that may affect a specific platform. R bindings The nanoarrow R bindings are distributed as the nanoarrow package on CRAN. The 0.2.0 release of the R bindings includes improvements in type support, improvements in stability, and features required for the forthcoming release of Arrow Database Connectivity (ADBC) R bindings. Notably: Support for conversion of union arrays to R objects was added to facilitate support for an ADBC function that returns such an array. Support for adding an R-level finalizer to an ArrowArrayStream was added to facilitate safely wrapping a stream resulting from an ADBC call at the R level. Python bindings? The nanoarrow 0.2.0 release does not include Python bindings, but improvements to the unreleased draft bindings were added to facilitate discussion among Python developers regarding the useful scope of a potential future nanoarrow Python package. If one of those developers is you, feel free to open an issue or send a post to the developer mailing list to engage in the discussion. Contributors This initial release consists of contributions from 6 contributors in addition to the invaluable advice and support of the Apache Arrow developer mailing list. Special thanks to David Li for reviewing nearly every PR in this release! $ git shortlog -sn d91c35b33c7b6ff94f5f929384879352e241ed71..apache-arrow-nanoarrow-0.2.0 | grep -v "GitHub Actions" 61 Dewey Dunnington 2 Dirk Eddelbuettel 2 Joris Van den Bossche 2 Kirill Müller 2 William Ayd 1 David Li]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 0.5.0 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/06/21/adbc-0.5.0-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.5.0 (Libraries) Release" /><published>2023-06-21T00:00:00-04:00</published><updated>2023-06-21T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/06/21/adbc-0.5.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/06/21/adbc-0.5.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.5.0 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/6?closed=1"><strong>37
resolved issues</strong></a> from <a href="#contributors"><strong>12 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version
0.5.0.  The <strong>API specification</strong> is versioned separately and is
at version 1.0.0.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.5.0/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>Experimental C# and Rust codebases were added to the source tree.  No packages are released for them yet.</p>

<p>Go-based drivers, when using FFI, will now catch panics at the interface boundary and return them as errors.  Afterwards, all calls to the driver will immediately fail.  This lets applications handle errors and gracefully terminate instead of immediately aborting.</p>

<p>The JDBC adapter allows customizing the type mapping between JDBC types and Arrow types, using all the information provided by JDBC.  This allows proper mappings for certain databases (e.g. the PostgreSQL JDBC driver returns both tz-aware and tz-naive timestamps under the same type code, but differentiates them via the type name).</p>

<p>The PostgreSQL driver has better support for reading and writing different types, including datetime types in particular.  More metadata is now returned in GetObjects, including tables, columns, and constraints (primary/foreign keys).  The driver also now handles non-SELECT queries in <code class="language-plaintext highlighter-rouge">ExecuteQuery</code>.</p>

<p>The Python driver manager lets you choose whether to disable or enable autocommit.</p>

<p>The R driver manager now exposes easy convenience functions for reading/writing data.</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.4.0..apache-arrow-adbc-0.5.0
    36	David Li
    21	William Ayd
     6	Dewey Dunnington
     4	Matt Topol
     3	Kirill Müller
     2	Sutou Kouhei
     2	vipere
     1	Curt Hagenlocher
     1	Diego Fernández Giraldo
     1	Matthijs Brobbel
     1	Will Jones
     1	davidhcoe
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>Work for the proposed <a href="https://github.com/apache/arrow-adbc/milestone/3">1.1.0 API revision</a> is progressing on a <a href="https://github.com/apache/arrow-adbc/tree/spec-1.1.0">branch</a>, with several API proposals merged in and others awaiting review.  They will then receive prototype implementations before being submitted for review/voting.  This is not currently targeting any release of the ADBC libraries, although we hope to wrap up this effort in the next few months.</p>

<p>A milestone has been created to explore <a href="https://github.com/apache/arrow-adbc/milestone/9">asynchronous APIs</a>, which have been requested for a while now.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.5.0 release of the Apache Arrow ADBC libraries. This covers includes 37 resolved issues from 12 distinct contributors. This is a release of the libraries, which are at version 0.5.0. The API specification is versioned separately and is at version 1.0.0. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights Experimental C# and Rust codebases were added to the source tree. No packages are released for them yet. Go-based drivers, when using FFI, will now catch panics at the interface boundary and return them as errors. Afterwards, all calls to the driver will immediately fail. This lets applications handle errors and gracefully terminate instead of immediately aborting. The JDBC adapter allows customizing the type mapping between JDBC types and Arrow types, using all the information provided by JDBC. This allows proper mappings for certain databases (e.g. the PostgreSQL JDBC driver returns both tz-aware and tz-naive timestamps under the same type code, but differentiates them via the type name). The PostgreSQL driver has better support for reading and writing different types, including datetime types in particular. More metadata is now returned in GetObjects, including tables, columns, and constraints (primary/foreign keys). The driver also now handles non-SELECT queries in ExecuteQuery. The Python driver manager lets you choose whether to disable or enable autocommit. The R driver manager now exposes easy convenience functions for reading/writing data. Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.4.0..apache-arrow-adbc-0.5.0 36 David Li 21 William Ayd 6 Dewey Dunnington 4 Matt Topol 3 Kirill Müller 2 Sutou Kouhei 2 vipere 1 Curt Hagenlocher 1 Diego Fernández Giraldo 1 Matthijs Brobbel 1 Will Jones 1 davidhcoe Roadmap Work for the proposed 1.1.0 API revision is progressing on a branch, with several API proposals merged in and others awaiting review. They will then receive prototype implementations before being submitted for review/voting. This is not currently targeting any release of the ADBC libraries, although we hope to wrap up this effort in the next few months. A milestone has been created to explore asynchronous APIs, which have been requested for a while now. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 12.0.1 Release</title><link href="https://arrow.apache.org/blog/2023/06/13/12.0.1-release/" rel="alternate" type="text/html" title="Apache Arrow 12.0.1 Release" /><published>2023-06-13T00:00:00-04:00</published><updated>2023-06-13T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/06/13/12.0.1-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/06/13/12.0.1-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 12.0.1 release.
This is mostly a bugfix release that includes <a href="https://github.com/apache/arrow/milestone/54?closed=1"><strong>38 resolved issues</strong></a>
from <a href="/release/12.0.1.html#contributors"><strong>12 distinct contributors</strong></a>. See the Install Page to learn how to
get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/12.0.1.html#changelog">complete changelog</a>.</p>

<h2 id="c-notes">C++ notes</h2>

<ul>
  <li>Fixed a performance regression when writing data from non-arrow sources (e.g. pandas) (<a href="https://github.com/apache/arrow/pull/35565">GH-35498</a>)</li>
  <li>Fixed a “Data size too large” error that could occur when reading valid parquet files (<a href="https://github.com/apache/arrow/pull/35428">GH-35423</a>)</li>
  <li>It is now possible to specify field-level metadata in dataset writes (<a href="https://github.com/apache/arrow/pull/35860">GH-35730</a>)</li>
</ul>

<h2 id="go-notes">Go notes</h2>

<ul>
  <li>Fixed builds of the Go Arrow package on 32-bit systems (<a href="https://github.com/apache/arrow/pull/35767">GH-34784</a>)</li>
  <li>Added <code class="language-plaintext highlighter-rouge">ValueString(int) string</code> method to <code class="language-plaintext highlighter-rouge">arrow.Array</code> (<a href="https://github.com/apache/arrow/pull/34986">GH-34657</a>)</li>
  <li>Fixed ASAN failure when using go1.20+ by using <code class="language-plaintext highlighter-rouge">unsafe.StringData</code> (<a href="https://github.com/apache/arrow/pull/35338">GH-35337</a>)</li>
</ul>

<h2 id="java-notes">Java notes</h2>

<ul>
  <li>Bumped jackson-databind dependency version to avoid CVE-2022-42003. (<a href="https://github.com/apache/arrow/pull/35771">GH-35771</a>)</li>
</ul>

<h2 id="python-notes">Python notes</h2>

<ul>
  <li>Fix <code class="language-plaintext highlighter-rouge">Table.join</code> respecting the <code class="language-plaintext highlighter-rouge">coalesce_keys=False</code> option again (<a href="https://github.com/apache/arrow/issues/35389">GH-35389</a>)</li>
</ul>

<h2 id="r-notes">R notes</h2>

<ul>
  <li>Update the version of the date library vendored with Arrow C++ library for compatibility with tzdb 0.4.0 (<a href="https://github.com/apache/arrow/issues/35594">GH-35594</a>, <a href="https://github.com/apache/arrow/issues/35612">GH-35612</a>)</li>
</ul>

<h2 id="other-modules-and-languages">Other modules and languages</h2>

<p>No general changes were made to the other libraries or languages.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 12.0.1 release. This is mostly a bugfix release that includes 38 resolved issues from 12 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. C++ notes Fixed a performance regression when writing data from non-arrow sources (e.g. pandas) (GH-35498) Fixed a “Data size too large” error that could occur when reading valid parquet files (GH-35423) It is now possible to specify field-level metadata in dataset writes (GH-35730) Go notes Fixed builds of the Go Arrow package on 32-bit systems (GH-34784) Added ValueString(int) string method to arrow.Array (GH-34657) Fixed ASAN failure when using go1.20+ by using unsafe.StringData (GH-35337) Java notes Bumped jackson-databind dependency version to avoid CVE-2022-42003. (GH-35771) Python notes Fix Table.join respecting the coalesce_keys=False option again (GH-35389) R notes Update the version of the date library vendored with Arrow C++ library for compatibility with tzdb 0.4.0 (GH-35594, GH-35612) Other modules and languages No general changes were made to the other libraries or languages.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 0.4.0 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/05/15/adbc-0.4.0-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.4.0 (Libraries) Release" /><published>2023-05-15T00:00:00-04:00</published><updated>2023-05-15T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/05/15/adbc-0.4.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/05/15/adbc-0.4.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.4.0 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/5"><strong>47
resolved issues</strong></a> from <a href="#contributors"><strong>8 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version
0.4.0.  The <strong>API specification</strong> is versioned separately and is
at version 1.0.0.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.4.0/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>A Go-based driver for <a href="https://www.snowflake.com/en/">Snowflake</a> was added, along with bindings for Python and R.</p>

<p>The PostgreSQL driver now has much better support for different types, and properly handles NULL values.
It now also implements <code class="language-plaintext highlighter-rouge">AdbcConnectionGetTableSchema</code>.
All C/C++ components have been consolidated into a single build project, simplifying integration with IDEs.</p>

<p>Several <code class="language-plaintext highlighter-rouge">ArrowBuf</code> leaks in the Java libraries have been fixed, along with miscellaneous bugs in the JDBC adapter.
There was breaking change in the Java libraries: static driver instances were removed, as this was prone to the aformentioned leaks.
Instead, <code class="language-plaintext highlighter-rouge">AdbcDriver</code> instances can now be created with a <code class="language-plaintext highlighter-rouge">BufferAllocator</code>, giving the application control over allocations.</p>

<p>The Python, GLib, and Ruby bindings expose more of the API functions.
The Python bindings are now tested against the <a href="https://www.pola.rs/">polars</a> dataframe project, which has experimental integration with ADBC.
The release process was fixed to properly upload the Windows Python wheels.
The R bindings now include packages for the PostgreSQL driver.</p>

<p>There is a known issue with the Snowflake driver: it will not build on non-x86_64 (AMD64) platforms, including Linux/macOS arm64/aarch64.
This has already been fixed for the next release.</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.3.0..apache-arrow-adbc-0.4.0
    31	David Li
    15	Sutou Kouhei
     9	Dewey Dunnington
     7	William Ayd
     5	Matt Topol
     1	Jacob Marble
     1	Tornike Gurgenidze
     1	eitsupi
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>Work for the proposed <a href="https://github.com/apache/arrow-adbc/milestone/3">1.1.0 API revision</a> has begun on a new <a href="https://github.com/apache/arrow-adbc/tree/spec-1.1.0">branch</a>.
This is not currently targeting any release of the ADBC libraries.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.4.0 release of the Apache Arrow ADBC libraries. This covers includes 47 resolved issues from 8 distinct contributors. This is a release of the libraries, which are at version 0.4.0. The API specification is versioned separately and is at version 1.0.0. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights A Go-based driver for Snowflake was added, along with bindings for Python and R. The PostgreSQL driver now has much better support for different types, and properly handles NULL values. It now also implements AdbcConnectionGetTableSchema. All C/C++ components have been consolidated into a single build project, simplifying integration with IDEs. Several ArrowBuf leaks in the Java libraries have been fixed, along with miscellaneous bugs in the JDBC adapter. There was breaking change in the Java libraries: static driver instances were removed, as this was prone to the aformentioned leaks. Instead, AdbcDriver instances can now be created with a BufferAllocator, giving the application control over allocations. The Python, GLib, and Ruby bindings expose more of the API functions. The Python bindings are now tested against the polars dataframe project, which has experimental integration with ADBC. The release process was fixed to properly upload the Windows Python wheels. The R bindings now include packages for the PostgreSQL driver. There is a known issue with the Snowflake driver: it will not build on non-x86_64 (AMD64) platforms, including Linux/macOS arm64/aarch64. This has already been fixed for the next release. Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.3.0..apache-arrow-adbc-0.4.0 31 David Li 15 Sutou Kouhei 9 Dewey Dunnington 7 William Ayd 5 Matt Topol 1 Jacob Marble 1 Tornike Gurgenidze 1 eitsupi Roadmap Work for the proposed 1.1.0 API revision has begun on a new branch. This is not currently targeting any release of the ADBC libraries. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>