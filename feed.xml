<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://arrow.apache.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arrow.apache.org/" rel="alternate" type="text/html" /><updated>2023-06-17T17:25:42-04:00</updated><id>https://arrow.apache.org/feed.xml</id><title type="html">Apache Arrow</title><subtitle>Apache Arrow is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, C++, C#, Go, Java, JavaScript, MATLAB, Python, R, Ruby, and Rust.</subtitle><entry><title type="html">Apache Arrow 12.0.1 Release</title><link href="https://arrow.apache.org/blog/2023/06/13/12.0.1-release/" rel="alternate" type="text/html" title="Apache Arrow 12.0.1 Release" /><published>2023-06-13T00:00:00-04:00</published><updated>2023-06-13T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/06/13/12.0.1-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/06/13/12.0.1-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 12.0.1 release.
This is mostly a bugfix release that includes <a href="https://github.com/apache/arrow/milestone/54?closed=1"><strong>38 resolved issues</strong></a>
from <a href="/release/12.0.1.html#contributors"><strong>12 distinct contributors</strong></a>. See the Install Page to learn how to
get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/12.0.1.html#changelog">complete changelog</a>.</p>

<h2 id="c-notes">C++ notes</h2>

<ul>
  <li>Fixed a performance regression when writing data from non-arrow sources (e.g. pandas) (<a href="https://github.com/apache/arrow/pull/35565">GH-35498</a>)</li>
  <li>Fixed a “Data size too large” error that could occur when reading valid parquet files (<a href="https://github.com/apache/arrow/pull/35428">GH-35423</a>)</li>
  <li>It is now possible to specify field-level metadata in dataset writes (<a href="https://github.com/apache/arrow/pull/35860">GH-35730</a>)</li>
</ul>

<h2 id="go-notes">Go notes</h2>

<ul>
  <li>Fixed builds of the Go Arrow package on 32-bit systems (<a href="https://github.com/apache/arrow/pull/35767">GH-34784</a>)</li>
  <li>Added <code class="language-plaintext highlighter-rouge">ValueString(int) string</code> method to <code class="language-plaintext highlighter-rouge">arrow.Array</code> (<a href="https://github.com/apache/arrow/pull/34986">GH-34657</a>)</li>
  <li>Fixed ASAN failure when using go1.20+ by using <code class="language-plaintext highlighter-rouge">unsafe.StringData</code> (<a href="https://github.com/apache/arrow/pull/35338">GH-35337</a>)</li>
</ul>

<h2 id="java-notes">Java notes</h2>

<ul>
  <li>Bumped jackson-databind dependency version to avoid CVE-2022-42003. (<a href="https://github.com/apache/arrow/pull/35771">GH-35771</a>)</li>
</ul>

<h2 id="python-notes">Python notes</h2>

<ul>
  <li>Fix <code class="language-plaintext highlighter-rouge">Table.join</code> respecting the <code class="language-plaintext highlighter-rouge">coalesce_keys=False</code> option again (<a href="https://github.com/apache/arrow/issues/35389">GH-35389</a>)</li>
</ul>

<h2 id="r-notes">R notes</h2>

<ul>
  <li>Update the version of the date library vendored with Arrow C++ library for compatibility with tzdb 0.4.0 (<a href="https://github.com/apache/arrow/issues/35594">GH-35594</a>, <a href="https://github.com/apache/arrow/issues/35612">GH-35612</a>)</li>
</ul>

<h2 id="other-modules-and-languages">Other modules and languages</h2>

<p>No general changes were made to the other libraries or languages.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 12.0.1 release. This is mostly a bugfix release that includes 38 resolved issues from 12 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. C++ notes Fixed a performance regression when writing data from non-arrow sources (e.g. pandas) (GH-35498) Fixed a “Data size too large” error that could occur when reading valid parquet files (GH-35423) It is now possible to specify field-level metadata in dataset writes (GH-35730) Go notes Fixed builds of the Go Arrow package on 32-bit systems (GH-34784) Added ValueString(int) string method to arrow.Array (GH-34657) Fixed ASAN failure when using go1.20+ by using unsafe.StringData (GH-35337) Java notes Bumped jackson-databind dependency version to avoid CVE-2022-42003. (GH-35771) Python notes Fix Table.join respecting the coalesce_keys=False option again (GH-35389) R notes Update the version of the date library vendored with Arrow C++ library for compatibility with tzdb 0.4.0 (GH-35594, GH-35612) Other modules and languages No general changes were made to the other libraries or languages.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 0.4.0 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/05/15/adbc-0.4.0-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.4.0 (Libraries) Release" /><published>2023-05-15T00:00:00-04:00</published><updated>2023-05-15T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/05/15/adbc-0.4.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/05/15/adbc-0.4.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.4.0 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/5"><strong>47
resolved issues</strong></a> from <a href="#contributors"><strong>8 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version
0.4.0.  The <strong>API specification</strong> is versioned separately and is
at version 1.0.0.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.4.0/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>A Go-based driver for <a href="https://www.snowflake.com/en/">Snowflake</a> was added, along with bindings for Python and R.</p>

<p>The PostgreSQL driver now has much better support for different types, and properly handles NULL values.
It now also implements <code class="language-plaintext highlighter-rouge">AdbcConnectionGetTableSchema</code>.
All C/C++ components have been consolidated into a single build project, simplifying integration with IDEs.</p>

<p>Several <code class="language-plaintext highlighter-rouge">ArrowBuf</code> leaks in the Java libraries have been fixed, along with miscellaneous bugs in the JDBC adapter.
There was breaking change in the Java libraries: static driver instances were removed, as this was prone to the aformentioned leaks.
Instead, <code class="language-plaintext highlighter-rouge">AdbcDriver</code> instances can now be created with a <code class="language-plaintext highlighter-rouge">BufferAllocator</code>, giving the application control over allocations.</p>

<p>The Python, GLib, and Ruby bindings expose more of the API functions.
The Python bindings are now tested against the <a href="https://www.pola.rs/">polars</a> dataframe project, which has experimental integration with ADBC.
The release process was fixed to properly upload the Windows Python wheels.
The R bindings now include packages for the PostgreSQL driver.</p>

<p>There is a known issue with the Snowflake driver: it will not build on non-x86_64 (AMD64) platforms, including Linux/macOS arm64/aarch64.
This has already been fixed for the next release.</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.3.0..apache-arrow-adbc-0.4.0
    31	David Li
    15	Sutou Kouhei
     9	Dewey Dunnington
     7	William Ayd
     5	Matt Topol
     1	Jacob Marble
     1	Tornike Gurgenidze
     1	eitsupi
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>Work for the proposed <a href="https://github.com/apache/arrow-adbc/milestone/3">1.1.0 API revision</a> has begun on a new <a href="https://github.com/apache/arrow-adbc/tree/spec-1.1.0">branch</a>.
This is not currently targeting any release of the ADBC libraries.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.4.0 release of the Apache Arrow ADBC libraries. This covers includes 47 resolved issues from 8 distinct contributors. This is a release of the libraries, which are at version 0.4.0. The API specification is versioned separately and is at version 1.0.0. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights A Go-based driver for Snowflake was added, along with bindings for Python and R. The PostgreSQL driver now has much better support for different types, and properly handles NULL values. It now also implements AdbcConnectionGetTableSchema. All C/C++ components have been consolidated into a single build project, simplifying integration with IDEs. Several ArrowBuf leaks in the Java libraries have been fixed, along with miscellaneous bugs in the JDBC adapter. There was breaking change in the Java libraries: static driver instances were removed, as this was prone to the aformentioned leaks. Instead, AdbcDriver instances can now be created with a BufferAllocator, giving the application control over allocations. The Python, GLib, and Ruby bindings expose more of the API functions. The Python bindings are now tested against the polars dataframe project, which has experimental integration with ADBC. The release process was fixed to properly upload the Windows Python wheels. The R bindings now include packages for the PostgreSQL driver. There is a known issue with the Snowflake driver: it will not build on non-x86_64 (AMD64) platforms, including Linux/macOS arm64/aarch64. This has already been fixed for the next release. Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.3.0..apache-arrow-adbc-0.4.0 31 David Li 15 Sutou Kouhei 9 Dewey Dunnington 7 William Ayd 5 Matt Topol 1 Jacob Marble 1 Tornike Gurgenidze 1 eitsupi Roadmap Work for the proposed 1.1.0 API revision has begun on a new branch. This is not currently targeting any release of the ADBC libraries. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Adopting Apache Arrow at CloudQuery</title><link href="https://arrow.apache.org/blog/2023/05/04/adopting-apache-arrow-at-cloudquery/" rel="alternate" type="text/html" title="Adopting Apache Arrow at CloudQuery" /><published>2023-05-04T00:00:00-04:00</published><updated>2023-05-04T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/05/04/adopting-apache-arrow-at-cloudquery</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/05/04/adopting-apache-arrow-at-cloudquery/"><![CDATA[<!--

-->

<p>This post is a collaboration with CloudQuery and cross-posted on the CloudQuery <a href="https://cloudquery.io/blog/adopting-apache-arrow-at-cloudquery">blog</a>.</p>

<p><a href="https://github.com/cloudquery/cloudquery">CloudQuery</a> is an open source high performance ELT framework written in Go. We <a href="https://www.cloudquery.io/blog/building-cloudquery">previously</a> discussed some of the <a href="https://www.cloudquery.io/docs/developers/architecture">architecture</a> and design decisions that we took to build a performant ELT framework. A type system is a key component for creating a performant and scalable ELT framework where sources and destinations are decoupled. In this blog post we will go through why we decided to adopt Apache Arrow as our type system and replace our in-house implementation.</p>

<h1 id="what-is-a-type-system">What is a Type System?</h1>

<p>Let’s quickly <a href="https://www.cloudquery.io/blog/building-cloudquery#type-system">recap</a> what a type system is and why an ELT framework needs one. At a very high level, an ELT framework extracts data from some source and moves it to some destination with a specific schema.</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>API ---&gt; [Source Plugin]  -----&gt;    [Destination Plugin]
                          -----&gt;    [Destination Plugin]
                           gRPC
</code></pre></div></div>

<p>Sources and destinations are decoupled and communicate via gRPC. This is crucial to allowing the addition of new destinations and updating old destinations without requiring updates to source plugin code (which otherwise would introduce an unmaintainable architecture).</p>

<p>This is where a type system comes in. Source plugins extract information from APIs in the most performant way possible, defining a schema and then transforming the result from the API (JSON or any other format) to a well-defined type system. The destination plugin can then easily create the schema for its database and transform the incoming data to the destination types. So to recap, the source plugin sends mainly two things to a destination: 1) the schema 2) the records that fit the defined schema. In Arrow terminology, these are a schema and a record batch.</p>

<h1 id="why-arrow">Why Arrow?</h1>

<p>Before Arrow, we used our own type system that supported more than 14 types. This served us well, but we started to hit limitations in various use-cases. For example, in database to database replication, we needed to support many more types, including nested types. Also, performance-wise, lots of the time spent in an ELT process is around converting data from one format to another, so we wanted to take a step back and see if we can avoid this <a href="https://xkcd.com/927/">famous XKCD</a> (by building yet another format):</p>

<figure style="text-align: center;">
  <img src="https://imgs.xkcd.com/comics/standards.png" width="100%" class="img-responsive" alt="Yet another standard XKCD" />
</figure>

<p>This is where Arrow comes in. Apache Arrow defines a language-independent columnar format for flat and hierarchical data, and brings the following advantages:</p>

<ol>
  <li>Cross-language with extensive libraries for different languages - The <a href="https://arrow.apache.org/docs/format/Columnar.html">format</a> is defined via flatbuffers in such way that you can parse it in any language and already has extensive support in C/C++, C#, Go, Java, JavaScript, Julia, Matlab, Python, R, Ruby and Rust (at the time of writing). For CloudQuery this is important as it makes it much easier to develop source or destination plugins in different languages.</li>
  <li>Performance: Arrow adoption is rising especially in columnar based databases (<a href="https://duckdb.org/2021/12/03/duck-arrow.html">DuckDB</a>, <a href="https://clickhouse.com/docs/en/integrations/data-formats/arrow-avro-orc">ClickHouse</a>, <a href="https://cloud.google.com/bigquery/docs/samples/bigquerystorage-arrow-quickstart">BigQuery</a>) and file formats (<a href="https://arrow.apache.org/docs/python/parquet.html">Parquet</a>) which makes it easier to write CloudQuery destination or source plugins for databases that already support Arrow as well as much more efficient as we remove the need for additional serialization and transformation step. Moreover, just the performance of sending Arrow format from source plugin to destination is already more performant and memory efficient, given its “zero-copy” nature and not needing serialization/deserialization.</li>
  <li>Rich Data Types: Arrow supports more than <a href="https://arrow.apache.org/docs/python/api/datatypes.html">35 types</a> including composite types (i.e. lists, structs and maps of all the available types) and ability to extend the type system with custom types. Also, there is already built-in mapping from/to the arrow type system and the parquet type system (including nested types) which already supported in many of the arrow libraries as explained <a href="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/">here</a>.</li>
</ol>

<h1 id="summary">Summary</h1>

<p>Adopting Apache Arrow as the CloudQuery in-memory type system enables us to gain better performance, data interoperability and developer experience. Some plugins that are going to gain an immediate boost of rich type systems are our database-to-database replication plugins such as <a href="https://www.cloudquery.io/docs/plugins/sources/postgresql/overview">PostgreSQL CDC</a> source plugin (and all <a href="https://www.cloudquery.io/docs/plugins/destinations/overview">database destinations</a>) that are going to get support for all available types including nested ones.</p>

<p>We are excited about this step and joining the growing Arrow community. We already contributed more than <a href="https://github.com/search?q=is%3Apr+author%3Ayevgenypats+author%3Ahermanschaaf+author%3Acandiduslynx+author%3Adisq+label%3A%22Component%3A+Go%22++is%3Amerged+&amp;ref=simplesearch">30</a> upstream pull requests that were quickly reviewed by the Arrow maintainers, thank you!</p>]]></content><author><name>Yevgeny Pats</name></author><category term="application" /><summary type="html"><![CDATA[This post is a collaboration with CloudQuery and cross-posted on the CloudQuery blog. CloudQuery is an open source high performance ELT framework written in Go. We previously discussed some of the architecture and design decisions that we took to build a performant ELT framework. A type system is a key component for creating a performant and scalable ELT framework where sources and destinations are decoupled. In this blog post we will go through why we decided to adopt Apache Arrow as our type system and replace our in-house implementation. What is a Type System? Let’s quickly recap what a type system is and why an ELT framework needs one. At a very high level, an ELT framework extracts data from some source and moves it to some destination with a specific schema. API ---&gt; [Source Plugin]  -----&gt;    [Destination Plugin] -----&gt;    [Destination Plugin] gRPC Sources and destinations are decoupled and communicate via gRPC. This is crucial to allowing the addition of new destinations and updating old destinations without requiring updates to source plugin code (which otherwise would introduce an unmaintainable architecture). This is where a type system comes in. Source plugins extract information from APIs in the most performant way possible, defining a schema and then transforming the result from the API (JSON or any other format) to a well-defined type system. The destination plugin can then easily create the schema for its database and transform the incoming data to the destination types. So to recap, the source plugin sends mainly two things to a destination: 1) the schema 2) the records that fit the defined schema. In Arrow terminology, these are a schema and a record batch. Why Arrow? Before Arrow, we used our own type system that supported more than 14 types. This served us well, but we started to hit limitations in various use-cases. For example, in database to database replication, we needed to support many more types, including nested types. Also, performance-wise, lots of the time spent in an ELT process is around converting data from one format to another, so we wanted to take a step back and see if we can avoid this famous XKCD (by building yet another format): This is where Arrow comes in. Apache Arrow defines a language-independent columnar format for flat and hierarchical data, and brings the following advantages: Cross-language with extensive libraries for different languages - The format is defined via flatbuffers in such way that you can parse it in any language and already has extensive support in C/C++, C#, Go, Java, JavaScript, Julia, Matlab, Python, R, Ruby and Rust (at the time of writing). For CloudQuery this is important as it makes it much easier to develop source or destination plugins in different languages. Performance: Arrow adoption is rising especially in columnar based databases (DuckDB, ClickHouse, BigQuery) and file formats (Parquet) which makes it easier to write CloudQuery destination or source plugins for databases that already support Arrow as well as much more efficient as we remove the need for additional serialization and transformation step. Moreover, just the performance of sending Arrow format from source plugin to destination is already more performant and memory efficient, given its “zero-copy” nature and not needing serialization/deserialization. Rich Data Types: Arrow supports more than 35 types including composite types (i.e. lists, structs and maps of all the available types) and ability to extend the type system with custom types. Also, there is already built-in mapping from/to the arrow type system and the parquet type system (including nested types) which already supported in many of the arrow libraries as explained here. Summary Adopting Apache Arrow as the CloudQuery in-memory type system enables us to gain better performance, data interoperability and developer experience. Some plugins that are going to gain an immediate boost of rich type systems are our database-to-database replication plugins such as PostgreSQL CDC source plugin (and all database destinations) that are going to get support for all available types including nested ones. We are excited about this step and joining the growing Arrow community. We already contributed more than 30 upstream pull requests that were quickly reviewed by the Arrow maintainers, thank you!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 12.0.0 Release</title><link href="https://arrow.apache.org/blog/2023/05/02/12.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 12.0.0 Release" /><published>2023-05-02T00:00:00-04:00</published><updated>2023-05-02T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/05/02/12.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/05/02/12.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 12.0.0 release. This covers
over 3 months of development work and includes <a href="https://github.com/apache/arrow/milestone/51?closed=1"><strong>476 resolved issues</strong></a>
with <a href="/release/12.0.0.html#contributors"><strong>531 commits from 97 distinct contributors</strong></a>.
See the <a href="https://arrow.apache.org/install/">Install Page</a>
to learn how to get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/12.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 11.0.0 release, Wang Mingming, Mustafa Akur and Ruihang Xia
have been invited to be committers.
Will Jones have joined the Project Management Committee (PMC).</p>

<p>Thanks for your contributions and participation in the project!</p>

<h2 id="columnar-format-notes">Columnar Format Notes</h2>

<p>A first “canonical” extension type has been formalized: <code class="language-plaintext highlighter-rouge">arrow.fixed_shape_tensor</code> to
represent an Array where each slot contains a tensor, with all tensors having the same
dimension and shape, <a href="https://github.com/apache/arrow/issues/33923">GH-33923</a>.
This is based on a Fixed-Size List layout as storage array
(<a href="https://arrow.apache.org/docs/dev/format/CanonicalExtensions.html#fixed-shape-tensor-extension">https://arrow.apache.org/docs/dev/format/CanonicalExtensions.html#fixed-shape-tensor-extension</a>).</p>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>

<p>The JDBC driver for Arrow Flight SQL has had some bugfixes, and has been refactored into a core library (which is not distributed as an uberjar with shaded names) and a driver (which is distributed as an uberjar).</p>

<p>The Java server builder API now offers easier access to the underlying gRPC builder.</p>

<p>Go now implements the Flight SQL extensions for Substrait and transaction support.</p>

<h2 id="plasma-notes">Plasma notes</h2>

<p>Plasma was deprecated since 10.0.0. Plasma is removed in this
release. <a href="https://github.com/apache/arrow/issues/33243">GH-33243</a></p>

<h2 id="linux-packages-notes">Linux packages notes</h2>

<p>We dropped support for Ubuntu 18.04 because Ubuntu 18.04 reached EOL.</p>

<h2 id="c-notes">C++ notes</h2>

<ul>
  <li>Run-End Encoded Arrays have been implemented and are accessible (<a href="https://github.com/apache/arrow/issues/32104">GH-32104</a>)</li>
  <li>The FixedShapeTensor Logical value type has been implemented using ExtensionType (<a href="https://github.com/apache/arrow/issues/15483">GH-15483</a>, <a href="https://github.com/apache/arrow/issues/34796">GH-34796</a>)</li>
</ul>

<h3 id="compute">Compute</h3>

<ul>
  <li>New kernel to convert timestamp with timezone to wall time (<a href="https://github.com/apache/arrow/issues/33143">GH-33143</a>)</li>
  <li>Cast kernels are now built into libarrow by default (<a href="https://github.com/apache/arrow/issues/34388">GH-34388</a>)</li>
</ul>

<h3 id="acero">Acero</h3>

<ul>
  <li>Acero has been moved out of libarrow into it’s own shared library, allowing for smaller builds of the core libarrow (<a href="https://github.com/apache/arrow/issues/15280">GH-15280</a>)</li>
  <li>Exec nodes now can have a concept of “ordering” and will reject non-sensible plans (<a href="https://github.com/apache/arrow/issues/34136">GH-34136</a>)</li>
  <li>New exec nodes: “pivot_longer” (<a href="https://github.com/apache/arrow/issues/34266">GH-34266</a>), “order_by” (<a href="https://github.com/apache/arrow/issues/34248">GH-34248</a>) and “fetch” (<a href="https://github.com/apache/arrow/issues/34059">GH-34059</a>)</li>
  <li><em>Breaking Change:</em> Reorder output fields of “group_by” node so that keys/segment keys come before aggregates (<a href="https://github.com/apache/arrow/issues/33616">GH-33616</a>)</li>
</ul>

<h3 id="substrait">Substrait</h3>

<ul>
  <li>Add support for the <code class="language-plaintext highlighter-rouge">round</code> function <a href="https://github.com/apache/arrow/issues/33588">GH-33588</a></li>
  <li>Add support for the <code class="language-plaintext highlighter-rouge">cast</code> expression element <a href="https://github.com/apache/arrow/issues/31910">GH-31910</a></li>
  <li>Added API reference documentation <a href="https://github.com/apache/arrow/issues/34011">GH-34011</a></li>
  <li>Added an extension relation to support segmented aggregation <a href="https://github.com/apache/arrow/issues/34626">GH-34626</a></li>
  <li>The output of the aggregate relation now conforms to the spec <a href="https://github.com/apache/arrow/issues/34786">GH-34786</a></li>
</ul>

<h3 id="parquet">Parquet</h3>

<ul>
  <li>Added support for DeltaLengthByteArray encoding to the Parquet writer (<a href="https://github.com/apache/arrow/issues/33024">GH-33024</a>)</li>
  <li>NaNs are correctly handled now for Parquet predicate push-downs (<a href="https://github.com/apache/arrow/issues/18481">GH-18481</a>)</li>
  <li>Added support for reading Parquet page indexes (<a href="https://github.com/apache/arrow/issues/33596">GH-33596</a>) and writing page indexes (<a href="https://github.com/apache/arrow/issues/34053">GH-34053</a>)</li>
  <li>Parquet writer can write columns in parallel now (<a href="https://github.com/apache/arrow/issues/33655">GH-33655</a>)</li>
  <li>Fixed incorrect number of rows in Parquet V2 page headers (<a href="https://github.com/apache/arrow/issues/34086">GH-34086</a>)</li>
  <li>Fixed incorrect Parquet page null_count when stats are disabled (<a href="https://github.com/apache/arrow/issues/34326">GH-34326</a>)</li>
  <li>Added support for reading BloomFilters to the Parquet Reader (<a href="https://github.com/apache/arrow/issues/34665">GH-34665</a>)</li>
  <li>Parquet File-writer can now add additional key-value metadata after it has been opened (<a href="https://github.com/apache/arrow/issues/34888">GH-34888</a>)</li>
  <li><em>Breaking Change:</em> The default row group size for the Arrow writer changed from 64Mi rows to 1Mi rows. <a href="https://github.com/apache/arrow/issues/34280">GH-34280</a></li>
</ul>

<h3 id="orc">ORC</h3>

<ul>
  <li>Added support for the union type in ORC writer (<a href="https://github.com/apache/arrow/issues/34262">GH-34262</a>)</li>
  <li>Fixed ORC CHAR type mapping with Arrow (<a href="https://github.com/apache/arrow/issues/34823">GH-34823</a>)</li>
  <li>Fixed timestamp type mapping between ORC and arrow (<a href="https://github.com/apache/arrow/issues/34590">GH-34590</a>)</li>
</ul>

<h3 id="datasets">Datasets</h3>

<ul>
  <li>Added support for reading JSON datasets (<a href="https://github.com/apache/arrow/issues/33209">GH-33209</a>)</li>
  <li>Dataset writer now supports specifying a function callback to construct the file name in addition to the existing file name template (<a href="https://github.com/apache/arrow/issues/34565">GH-34565</a>)</li>
</ul>

<h3 id="filesystems">Filesystems</h3>

<ul>
  <li>GcsFileSystem::OpenInputFile avoids unnecessary downloads (<a href="https://github.com/apache/arrow/issues/34051">GH-34051</a>)</li>
</ul>

<h3 id="other-changes">Other changes</h3>

<ul>
  <li>Convenience Append(std::optional<T>...) methods have been added to array builders ([GH-14863](https://github.com/apache/arrow/issues/14863))</T></li>
  <li>A deprecated OpenTelemetry header was removed from the Flight library (<a href="https://github.com/apache/arrow/issues/34417">GH-34417</a>)</li>
  <li>Fixed crash in “take” kernels on ExtensionArrays with an underlying dictionary type (<a href="https://github.com/apache/arrow/issues/34619">GH-34619</a>)</li>
  <li>Fixed bug where the C-Data bridge did not preserve nullability of map values on import (<a href="https://github.com/apache/arrow/issues/34983">GH-34983</a>)</li>
  <li>Added support for EqualOptions to RecordBatch::Equals (<a href="https://github.com/apache/arrow/issues/34968">GH-34968</a>)</li>
  <li>zstd dependency upgraded to v1.5.5 (<a href="https://github.com/apache/arrow/issues/34899">GH-34899</a>)</li>
  <li>Improved handling of “logical” nulls such as with union and RunEndEncoded arrays (<a href="https://github.com/apache/arrow/issues/34361">GH-34361</a>)</li>
  <li>Fixed incorrect handling of uncompressed body buffers in IPC reader, added IpcWriteOptions::min_space_savings for optional compression optimizations (<a href="https://github.com/apache/arrow/issues/15102">GH-15102</a>)</li>
</ul>

<h2 id="c-notes-1">C# notes</h2>

<ul>
  <li>Support added for importing / exporting schemas and types via the
C data interface <a href="https://github.com/apache/arrow/issues/34737">GH-34737</a></li>
  <li>Support added for the half-float data type <a href="https://github.com/apache/arrow/issues/25163">GH-25163</a></li>
  <li>Schemas are now allowed to have multiple fields with the same name
<a href="https://github.com/apache/arrow/issues/34076">GH-34076</a></li>
  <li>Added support for reading compressed IPC files <a href="https://github.com/apache/arrow/issues/32240">GH-32240</a></li>
  <li>Add [] operator to Schema <a href="https://github.com/apache/arrow/issues/32240">GH-34119</a></li>
</ul>

<h2 id="go-notes">Go notes</h2>

<ul>
  <li>Run-End Encoded Arrays have been added to the Golang implementation (<a href="https://github.com/apache/arrow/issues/32104">GH-32104</a>, <a href="https://github.com/apache/arrow/issues/32946">GH-32946</a>, <a href="https://github.com/apache/arrow/issues/20407">GH-20407</a>, <a href="https://github.com/apache/arrow/issues/32949">GH-32949</a>)</li>
  <li>The SQLite Flight SQL Example has been improved and you can now <code class="language-plaintext highlighter-rouge">go get</code> a simple SQLite Flight SQL Server mainprog using <code class="language-plaintext highlighter-rouge">go get github.com/apache/arrow/go/v12/arrow/flight/flightsql/example/cmd/sqlite_flightsql_server</code> (<a href="https://github.com/apache/arrow/issues/33840">GH-33840</a>)</li>
  <li>Fixed bug causing builds to fail with the <code class="language-plaintext highlighter-rouge">noasm</code> build tag (<a href="https://github.com/apache/arrow/issues/34044">GH-34044</a>) and added a CI test run that uses the <code class="language-plaintext highlighter-rouge">noasm</code> tag (<a href="https://github.com/apache/arrow/issues/34055">GH-34055</a>)</li>
  <li>Fixed issue allowing building on riscv64-freebsd (<a href="https://github.com/apache/arrow/issues/34629">GH-34629</a>)</li>
  <li>Fixed issue preventing building on 32-bit platforms (<a href="https://github.com/apache/arrow/issues/35133">GH-35133</a>)</li>
</ul>

<h3 id="arrow">Arrow</h3>

<ul>
  <li>Fixed bug in C-Data handling of <code class="language-plaintext highlighter-rouge">ArrowArrayStream.get_next</code> when handling uninitialized <code class="language-plaintext highlighter-rouge">ArrowArrays</code> (<a href="https://github.com/apache/arrow/issues/33767">GH-33767</a>)</li>
  <li><em>Breaking Change:</em> Added <code class="language-plaintext highlighter-rouge">Err()</code> method to <code class="language-plaintext highlighter-rouge">RecordReader</code> interface so that it can propagate errors (<a href="https://github.com/apache/arrow/issues/33789">GH-33789</a>)</li>
  <li>Fixed potential panic in C-Data API for misusing an invalid handle (<a href="https://github.com/apache/arrow/issues/33864">GH-33864</a>)</li>
  <li>A new cgo-based Allocator that does not depend on libarrow has been added to the memory package (<a href="https://github.com/apache/arrow/issues/33901">GH-33901</a>)</li>
  <li>CSV Reader and Writer now support Extension type arrays (<a href="https://github.com/apache/arrow/issues/34334">GH-34334</a>)</li>
  <li>Fixed bug preventing the reading of IPC streams/files with compression enabled but uncompressed buffers (<a href="https://github.com/apache/arrow/issues/34385">GH-34385</a>)</li>
  <li>Added interface which can be added to an <code class="language-plaintext highlighter-rouge">ExtensionType</code> to allow Builders to be created via <code class="language-plaintext highlighter-rouge">NewBuilder</code>, enabling easy building of nested fields containing extension types (<a href="https://github.com/apache/arrow/issues/34453">GH-34453</a>)</li>
  <li>Added utilities to perform Array diffing (<a href="https://github.com/apache/arrow/issues/34790">GH-34790</a>)</li>
  <li>Added <code class="language-plaintext highlighter-rouge">SetColumn</code> method to <code class="language-plaintext highlighter-rouge">arrow.Record</code> (<a href="https://github.com/apache/arrow/issues/34832">GH-34832</a>)</li>
  <li>Added <code class="language-plaintext highlighter-rouge">GetValue</code> method to <code class="language-plaintext highlighter-rouge">arrow.Metadata</code> (<a href="https://github.com/apache/arrow/issues/34855">GH-34855</a>)</li>
  <li>Added <code class="language-plaintext highlighter-rouge">Pow</code> method to <code class="language-plaintext highlighter-rouge">decimal128.Num</code> and <code class="language-plaintext highlighter-rouge">decimal256.Num</code> (<a href="https://github.com/apache/arrow/issues/34863">GH-34863</a>)</li>
</ul>

<h4 id="flight">Flight</h4>

<ul>
  <li>Fixed bug in <code class="language-plaintext highlighter-rouge">StreamChunks</code> for Flight SQL to correctly propagate to the gRPC client (<a href="https://github.com/apache/arrow/issues/33717">GH-33717</a>)</li>
  <li>Fixed issue that prevented compatibility with gRPC &lt; v1.45 (<a href="https://github.com/apache/arrow/issues/33734">GH-33734</a>)</li>
  <li>Added support to bind a <code class="language-plaintext highlighter-rouge">RecordReader</code> for supplying parameters to a Flight SQL Prepared statement (<a href="https://github.com/apache/arrow/issues/33794">GH-33794</a>)</li>
  <li>Prepared Statement methods for FlightSQL client now allows gRPC Call Options (<a href="https://github.com/apache/arrow/issues/33867">GH-33867</a>)</li>
  <li>FlightSQL Extensions have been implemented (for transactions and Substrait support) (<a href="https://github.com/apache/arrow/issues/33935">GH-33935</a>)</li>
  <li>A driver compatible with <code class="language-plaintext highlighter-rouge">database/sql</code> for FlightSQL has been added (<a href="https://github.com/apache/arrow/issues/34332">GH-34332</a>)</li>
</ul>

<h4 id="compute-1">Compute</h4>

<ul>
  <li>“run_end_encode” and “run_end_decode” functions added to compute functions (<a href="https://github.com/apache/arrow/issues/20408">GH-20408</a>)</li>
  <li>“unique” function added (<a href="https://github.com/apache/arrow/issues/34171">GH-34171</a>)</li>
</ul>

<h3 id="parquet-1">Parquet</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">pqarrow</code> pkg now handles DICTIONARY fields natively (<a href="https://github.com/apache/arrow/issues/33466">GH-33466</a>)</li>
  <li>Fixed rare panic when writing list of 8 structs (<a href="https://github.com/apache/arrow/issues/33600">GH-33600</a>)</li>
  <li>Added support for <code class="language-plaintext highlighter-rouge">pqarrow</code> pkg to write LargeString and LargeBinary types (<a href="https://github.com/apache/arrow/issues/33875">GH-33875</a>)</li>
  <li>Fixed bug where <code class="language-plaintext highlighter-rouge">pqarrow.NewSchemaManifest</code> created the wrong field type for Array Object fields (<a href="https://github.com/apache/arrow/issues/34101">GH-34101</a>)</li>
  <li>Added support to Parquet Writer for Extension type Arrays (<a href="https://github.com/apache/arrow/issues/34330">GH-34330</a>)</li>
</ul>

<h2 id="java-notes">Java notes</h2>

<ul>
  <li>Update Java JNI modules to consider Arrow ACERO <a href="https://github.com/apache/arrow/issues/34862">GH-34862</a></li>
  <li>Ability to register additional GRPC services with FlightServer <a href="https://github.com/apache/arrow/issues/34778">GH-34778</a></li>
  <li>Allow sending custom headers/properties through Arrow Flight SQL JDBC <a href="https://github.com/apache/arrow/issues/33874">GH-33874</a></li>
</ul>

<h2 id="javascript-notes">JavaScript notes</h2>

<p>No changes.</p>

<h2 id="python-notes">Python notes</h2>

<p>Compatibility notes:</p>

<ul>
  <li>Plasma has been removed in this release (<a href="https://github.com/apache/arrow/issues/33243">GH-33243</a>). In addition, the deprecated serialization module in PyArrow was also removed (<a href="https://github.com/apache/arrow/issues/29705">GH-29705</a>). IPC (Inter-Process Communication) functionality of pyarrow or the standard library pickle should be used instead.</li>
  <li>The deprecated <code class="language-plaintext highlighter-rouge">use_async</code> keyword has been removed from the dataset module (<a href="https://github.com/apache/arrow/issues/30774">GH-30774</a>)</li>
  <li>Minimum Cython version to build PyArrow from source has been raised to 0.29.31 (<a href="https://github.com/apache/arrow/issues/34933">GH-34933</a>). In addition, PyArrow can now be compiled using Cython 3 (<a href="https://github.com/apache/arrow/issues/34564">GH-34564</a>).</li>
</ul>

<p>New features:</p>

<ul>
  <li>A new <code class="language-plaintext highlighter-rouge">pyarrow.acero</code> module with initial bindings for the Acero execution engine has been added (<a href="https://github.com/apache/arrow/issues/33976">GH-33976</a>)</li>
  <li>A new canonical extension type for fixed shaped tensor data has been defined. This is exposed in PyArrow as the FixedShapeTensorType (<a href="https://github.com/apache/arrow/issues/34882">GH-34882</a>, <a href="https://github.com/apache/arrow/issues/34956">GH-34956</a>)</li>
  <li>Run-End Encoded arrays binding has been implemented (<a href="https://github.com/apache/arrow/issues/34686">GH-34686</a>, <a href="https://github.com/apache/arrow/issues/34568">GH-34568</a>)</li>
  <li>Method <code class="language-plaintext highlighter-rouge">is_nan</code> has been added to Array, ChunkedArray and Expression (<a href="https://github.com/apache/arrow/issues/34154">GH-34154</a>)</li>
  <li>Dataframe interchange protocol has been implemented for RecordBatch (<a href="https://github.com/apache/arrow/issues/33926">GH-33926</a>)</li>
</ul>

<p>Other improvements:</p>

<ul>
  <li>Extension arrays can now be concatenated (<a href="https://github.com/apache/arrow/issues/31868">GH-31868</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">get_partition_keys</code> helper function is implemented in the <code class="language-plaintext highlighter-rouge">dataset</code> module to access the partitioning field’s key/value from the partition expression of a certain dataset fragment (<a href="https://github.com/apache/arrow/issues/33825">GH-33825</a>)</li>
  <li>PyArrow Array objects can now be accepted by the <code class="language-plaintext highlighter-rouge">pa.array()</code> constructor (<a href="https://github.com/apache/arrow/issues/34411">GH-34411</a>)</li>
  <li>The default row group size when writing parquet files has been changed (<a href="https://github.com/apache/arrow/issues/34280">GH-34280</a>)</li>
  <li>RecordBatch has the <code class="language-plaintext highlighter-rouge">select()</code> method implemented (<a href="https://github.com/apache/arrow/issues/34359">GH-34359</a>)</li>
  <li>New method <code class="language-plaintext highlighter-rouge">drop_column</code> on the <code class="language-plaintext highlighter-rouge">pyarrow.Table</code> supports passing a single column as a string (<a href="https://github.com/apache/arrow/issues/33377">GH-33377</a>)</li>
  <li>User-defined tabular functions, which are a user-functions implemented in Python that return a stateful stream of tabular data, are now also supported (<a href="https://github.com/apache/arrow/issues/32916">GH-32916</a>)</li>
  <li><a href="https://arrow.apache.org/docs/dev/developers/continuous_integration/archery.html">Arrow Archery tool</a> now includes linting of the Cython files (<a href="https://github.com/apache/arrow/issues/31905">GH-31905</a>)</li>
  <li><em>Breaking Change:</em> Reorder output fields of “group_by” node so that keys/segment keys come before aggregates (<a href="https://github.com/apache/arrow/issues/33616">GH-33616</a>)</li>
</ul>

<p>Relevant bug fixes:</p>

<ul>
  <li>Acero can now detect and raise an error in case a join operation needs too much bytes of key data (<a href="https://github.com/apache/arrow/issues/34474">GH-34474</a>)</li>
  <li>Fix for converting non-sequence object in <code class="language-plaintext highlighter-rouge">pa.array()</code> (<a href="https://github.com/apache/arrow/issues/34944">GH-34944</a>)</li>
  <li>Fix erroneous table conversion to pandas if table includes an extension array that does not implement <code class="language-plaintext highlighter-rouge">to_pandas_dtype</code> (<a href="https://github.com/apache/arrow/issues/34906">GH-34906</a>)</li>
  <li>Reading from a closed <code class="language-plaintext highlighter-rouge">ArrayStreamBatchReader</code> now returns invalid status instead of segfaulting (<a href="https://github.com/apache/arrow/issues/34165">GH-34165</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">array()</code> now returns <code class="language-plaintext highlighter-rouge">pyarrow.Array</code> and not <code class="language-plaintext highlighter-rouge">pyarrow.ChunkedArray</code> for columns with <code class="language-plaintext highlighter-rouge">__arrow_array__</code> method and only one chunk so that the conversion of pandas dataframe with categorical column of dtype <code class="language-plaintext highlighter-rouge">string[pyarrow]</code> does not fail (<a href="https://github.com/apache/arrow/issues/33727">GH-33727</a>)</li>
  <li>Custom type mapper in <code class="language-plaintext highlighter-rouge">to_pandas</code> now converts index dtypes together with column dtypes (<a href="https://github.com/apache/arrow/issues/34283">GH-34283</a>)</li>
</ul>

<h2 id="r-notes">R notes</h2>

<ul>
  <li>The <code class="language-plaintext highlighter-rouge">read_parquet()</code> and <code class="language-plaintext highlighter-rouge">read_feather()</code> functions can now accept URL
arguments.</li>
  <li>The <code class="language-plaintext highlighter-rouge">json_credentials</code> argument in <code class="language-plaintext highlighter-rouge">GcsFileSystem$create()</code> now accepts
a file path containing the appropriate authentication token.</li>
  <li>The <code class="language-plaintext highlighter-rouge">$options</code> member of <code class="language-plaintext highlighter-rouge">GcsFileSystem</code> objects can now be inspected.</li>
  <li>The <code class="language-plaintext highlighter-rouge">read_csv_arrow()</code> and <code class="language-plaintext highlighter-rouge">read_json_arrow()</code> functions now accept literal text input wrapped in
<code class="language-plaintext highlighter-rouge">I()</code> to improve compatability with <code class="language-plaintext highlighter-rouge">readr::read_csv()</code>.</li>
  <li>Nested fields can now be accessed using <code class="language-plaintext highlighter-rouge">$</code> and <code class="language-plaintext highlighter-rouge">[[</code> in dplyr expressions.</li>
</ul>

<p>For more on what’s in the 12.0.0 R package, see the <a href="/docs/r/news/">R changelog</a>.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<ul>
  <li>Flight SQL: Added support for authentication. <a href="https://github.com/apache/arrow/issues/34074">GH-34074</a></li>
  <li>Compute: Added <code class="language-plaintext highlighter-rouge">GArrowRankOptions</code>. <a href="https://github.com/apache/arrow/issues/34425">GH-34425</a></li>
  <li>Compute: Added <code class="language-plaintext highlighter-rouge">GArrowFilterOptions</code>. <a href="https://github.com/apache/arrow/issues/34650">GH-34650</a></li>
  <li>Compute: Added <code class="language-plaintext highlighter-rouge">GArrowIndexOptions</code>. <a href="https://github.com/apache/arrow/issues/15286">GH-15286</a></li>
  <li>Compute: Added <code class="language-plaintext highlighter-rouge">GArrowMatchSubstringOptions</code>. <a href="https://github.com/apache/arrow/issues/15285">GH-15285</a></li>
  <li>Added <code class="language-plaintext highlighter-rouge">GArrowDenseUnionArrayBuilder</code>. <a href="https://github.com/apache/arrow/issues/21429">GH-21429</a></li>
  <li>Added <code class="language-plaintext highlighter-rouge">GArrowSparseUnionArrayBuilder</code>. <a href="https://github.com/apache/arrow/issues/21430">GH-21430</a></li>
</ul>

<h3 id="ruby-notes">Ruby notes</h3>

<ul>
  <li>Improved <code class="language-plaintext highlighter-rouge">Arrow::Table#join</code> API. <a href="https://github.com/apache/arrow/issues/15287">GH-15287</a></li>
  <li>Flight SQL: Added <code class="language-plaintext highlighter-rouge">ArrowFlight::RecordBatchReader#each</code>. <a href="https://github.com/apache/arrow/issues/15287">GH-15287</a></li>
  <li>Added <code class="language-plaintext highlighter-rouge">Arrow::DenseUnionArray#get_value</code>. <a href="https://github.com/apache/arrow/issues/34837">GH-34837</a></li>
  <li>Added <code class="language-plaintext highlighter-rouge">Arrow::SparseUnionArray#get_value</code>. <a href="https://github.com/apache/arrow/issues/34837">GH-34837</a></li>
  <li>Expression: Added support for
<code class="language-plaintext highlighter-rouge">table.slice {|slicer| slicer.column.match_substring(string)</code> and
related shortcuts. <a href="https://github.com/apache/arrow/issues/34819">GH-34819</a> <a href="https://github.com/apache/arrow/issues/34951">GH-34951</a></li>
  <li><em>Breaking change:</em> <code class="language-plaintext highlighter-rouge">Arrow::Table#slice</code> with a filter removes null
records. <a href="https://github.com/apache/arrow/issues/34953">GH-34953</a></li>
</ul>

<h2 id="rust-notes">Rust notes</h2>

<p>The Rust projects have moved to separate repositories outside the
main Arrow monorepo. For notes on the latest release of the Rust
implementation, see the latest <a href="https://github.com/apache/arrow-rs/tags">Arrow Rust changelog</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 12.0.0 release. This covers over 3 months of development work and includes 476 resolved issues with 531 commits from 97 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 11.0.0 release, Wang Mingming, Mustafa Akur and Ruihang Xia have been invited to be committers. Will Jones have joined the Project Management Committee (PMC). Thanks for your contributions and participation in the project! Columnar Format Notes A first “canonical” extension type has been formalized: arrow.fixed_shape_tensor to represent an Array where each slot contains a tensor, with all tensors having the same dimension and shape, GH-33923. This is based on a Fixed-Size List layout as storage array (https://arrow.apache.org/docs/dev/format/CanonicalExtensions.html#fixed-shape-tensor-extension). Arrow Flight RPC notes The JDBC driver for Arrow Flight SQL has had some bugfixes, and has been refactored into a core library (which is not distributed as an uberjar with shaded names) and a driver (which is distributed as an uberjar). The Java server builder API now offers easier access to the underlying gRPC builder. Go now implements the Flight SQL extensions for Substrait and transaction support. Plasma notes Plasma was deprecated since 10.0.0. Plasma is removed in this release. GH-33243 Linux packages notes We dropped support for Ubuntu 18.04 because Ubuntu 18.04 reached EOL. C++ notes Run-End Encoded Arrays have been implemented and are accessible (GH-32104) The FixedShapeTensor Logical value type has been implemented using ExtensionType (GH-15483, GH-34796) Compute New kernel to convert timestamp with timezone to wall time (GH-33143) Cast kernels are now built into libarrow by default (GH-34388) Acero Acero has been moved out of libarrow into it’s own shared library, allowing for smaller builds of the core libarrow (GH-15280) Exec nodes now can have a concept of “ordering” and will reject non-sensible plans (GH-34136) New exec nodes: “pivot_longer” (GH-34266), “order_by” (GH-34248) and “fetch” (GH-34059) Breaking Change: Reorder output fields of “group_by” node so that keys/segment keys come before aggregates (GH-33616) Substrait Add support for the round function GH-33588 Add support for the cast expression element GH-31910 Added API reference documentation GH-34011 Added an extension relation to support segmented aggregation GH-34626 The output of the aggregate relation now conforms to the spec GH-34786 Parquet Added support for DeltaLengthByteArray encoding to the Parquet writer (GH-33024) NaNs are correctly handled now for Parquet predicate push-downs (GH-18481) Added support for reading Parquet page indexes (GH-33596) and writing page indexes (GH-34053) Parquet writer can write columns in parallel now (GH-33655) Fixed incorrect number of rows in Parquet V2 page headers (GH-34086) Fixed incorrect Parquet page null_count when stats are disabled (GH-34326) Added support for reading BloomFilters to the Parquet Reader (GH-34665) Parquet File-writer can now add additional key-value metadata after it has been opened (GH-34888) Breaking Change: The default row group size for the Arrow writer changed from 64Mi rows to 1Mi rows. GH-34280 ORC Added support for the union type in ORC writer (GH-34262) Fixed ORC CHAR type mapping with Arrow (GH-34823) Fixed timestamp type mapping between ORC and arrow (GH-34590) Datasets Added support for reading JSON datasets (GH-33209) Dataset writer now supports specifying a function callback to construct the file name in addition to the existing file name template (GH-34565) Filesystems GcsFileSystem::OpenInputFile avoids unnecessary downloads (GH-34051) Other changes Convenience Append(std::optional...) methods have been added to array builders ([GH-14863](https://github.com/apache/arrow/issues/14863)) A deprecated OpenTelemetry header was removed from the Flight library (GH-34417) Fixed crash in “take” kernels on ExtensionArrays with an underlying dictionary type (GH-34619) Fixed bug where the C-Data bridge did not preserve nullability of map values on import (GH-34983) Added support for EqualOptions to RecordBatch::Equals (GH-34968) zstd dependency upgraded to v1.5.5 (GH-34899) Improved handling of “logical” nulls such as with union and RunEndEncoded arrays (GH-34361) Fixed incorrect handling of uncompressed body buffers in IPC reader, added IpcWriteOptions::min_space_savings for optional compression optimizations (GH-15102) C# notes Support added for importing / exporting schemas and types via the C data interface GH-34737 Support added for the half-float data type GH-25163 Schemas are now allowed to have multiple fields with the same name GH-34076 Added support for reading compressed IPC files GH-32240 Add [] operator to Schema GH-34119 Go notes Run-End Encoded Arrays have been added to the Golang implementation (GH-32104, GH-32946, GH-20407, GH-32949) The SQLite Flight SQL Example has been improved and you can now go get a simple SQLite Flight SQL Server mainprog using go get github.com/apache/arrow/go/v12/arrow/flight/flightsql/example/cmd/sqlite_flightsql_server (GH-33840) Fixed bug causing builds to fail with the noasm build tag (GH-34044) and added a CI test run that uses the noasm tag (GH-34055) Fixed issue allowing building on riscv64-freebsd (GH-34629) Fixed issue preventing building on 32-bit platforms (GH-35133) Arrow Fixed bug in C-Data handling of ArrowArrayStream.get_next when handling uninitialized ArrowArrays (GH-33767) Breaking Change: Added Err() method to RecordReader interface so that it can propagate errors (GH-33789) Fixed potential panic in C-Data API for misusing an invalid handle (GH-33864) A new cgo-based Allocator that does not depend on libarrow has been added to the memory package (GH-33901) CSV Reader and Writer now support Extension type arrays (GH-34334) Fixed bug preventing the reading of IPC streams/files with compression enabled but uncompressed buffers (GH-34385) Added interface which can be added to an ExtensionType to allow Builders to be created via NewBuilder, enabling easy building of nested fields containing extension types (GH-34453) Added utilities to perform Array diffing (GH-34790) Added SetColumn method to arrow.Record (GH-34832) Added GetValue method to arrow.Metadata (GH-34855) Added Pow method to decimal128.Num and decimal256.Num (GH-34863) Flight Fixed bug in StreamChunks for Flight SQL to correctly propagate to the gRPC client (GH-33717) Fixed issue that prevented compatibility with gRPC &lt; v1.45 (GH-33734) Added support to bind a RecordReader for supplying parameters to a Flight SQL Prepared statement (GH-33794) Prepared Statement methods for FlightSQL client now allows gRPC Call Options (GH-33867) FlightSQL Extensions have been implemented (for transactions and Substrait support) (GH-33935) A driver compatible with database/sql for FlightSQL has been added (GH-34332) Compute “run_end_encode” and “run_end_decode” functions added to compute functions (GH-20408) “unique” function added (GH-34171) Parquet pqarrow pkg now handles DICTIONARY fields natively (GH-33466) Fixed rare panic when writing list of 8 structs (GH-33600) Added support for pqarrow pkg to write LargeString and LargeBinary types (GH-33875) Fixed bug where pqarrow.NewSchemaManifest created the wrong field type for Array Object fields (GH-34101) Added support to Parquet Writer for Extension type Arrays (GH-34330) Java notes Update Java JNI modules to consider Arrow ACERO GH-34862 Ability to register additional GRPC services with FlightServer GH-34778 Allow sending custom headers/properties through Arrow Flight SQL JDBC GH-33874 JavaScript notes No changes. Python notes Compatibility notes: Plasma has been removed in this release (GH-33243). In addition, the deprecated serialization module in PyArrow was also removed (GH-29705). IPC (Inter-Process Communication) functionality of pyarrow or the standard library pickle should be used instead. The deprecated use_async keyword has been removed from the dataset module (GH-30774) Minimum Cython version to build PyArrow from source has been raised to 0.29.31 (GH-34933). In addition, PyArrow can now be compiled using Cython 3 (GH-34564). New features: A new pyarrow.acero module with initial bindings for the Acero execution engine has been added (GH-33976) A new canonical extension type for fixed shaped tensor data has been defined. This is exposed in PyArrow as the FixedShapeTensorType (GH-34882, GH-34956) Run-End Encoded arrays binding has been implemented (GH-34686, GH-34568) Method is_nan has been added to Array, ChunkedArray and Expression (GH-34154) Dataframe interchange protocol has been implemented for RecordBatch (GH-33926) Other improvements: Extension arrays can now be concatenated (GH-31868) get_partition_keys helper function is implemented in the dataset module to access the partitioning field’s key/value from the partition expression of a certain dataset fragment (GH-33825) PyArrow Array objects can now be accepted by the pa.array() constructor (GH-34411) The default row group size when writing parquet files has been changed (GH-34280) RecordBatch has the select() method implemented (GH-34359) New method drop_column on the pyarrow.Table supports passing a single column as a string (GH-33377) User-defined tabular functions, which are a user-functions implemented in Python that return a stateful stream of tabular data, are now also supported (GH-32916) Arrow Archery tool now includes linting of the Cython files (GH-31905) Breaking Change: Reorder output fields of “group_by” node so that keys/segment keys come before aggregates (GH-33616) Relevant bug fixes: Acero can now detect and raise an error in case a join operation needs too much bytes of key data (GH-34474) Fix for converting non-sequence object in pa.array() (GH-34944) Fix erroneous table conversion to pandas if table includes an extension array that does not implement to_pandas_dtype (GH-34906) Reading from a closed ArrayStreamBatchReader now returns invalid status instead of segfaulting (GH-34165) array() now returns pyarrow.Array and not pyarrow.ChunkedArray for columns with __arrow_array__ method and only one chunk so that the conversion of pandas dataframe with categorical column of dtype string[pyarrow] does not fail (GH-33727) Custom type mapper in to_pandas now converts index dtypes together with column dtypes (GH-34283) R notes The read_parquet() and read_feather() functions can now accept URL arguments. The json_credentials argument in GcsFileSystem$create() now accepts a file path containing the appropriate authentication token. The $options member of GcsFileSystem objects can now be inspected. The read_csv_arrow() and read_json_arrow() functions now accept literal text input wrapped in I() to improve compatability with readr::read_csv(). Nested fields can now be accessed using $ and [[ in dplyr expressions. For more on what’s in the 12.0.0 R package, see the R changelog. Ruby and C GLib notes Flight SQL: Added support for authentication. GH-34074 Compute: Added GArrowRankOptions. GH-34425 Compute: Added GArrowFilterOptions. GH-34650 Compute: Added GArrowIndexOptions. GH-15286 Compute: Added GArrowMatchSubstringOptions. GH-15285 Added GArrowDenseUnionArrayBuilder. GH-21429 Added GArrowSparseUnionArrayBuilder. GH-21430 Ruby notes Improved Arrow::Table#join API. GH-15287 Flight SQL: Added ArrowFlight::RecordBatchReader#each. GH-15287 Added Arrow::DenseUnionArray#get_value. GH-34837 Added Arrow::SparseUnionArray#get_value. GH-34837 Expression: Added support for table.slice {|slicer| slicer.column.match_substring(string) and related shortcuts. GH-34819 GH-34951 Breaking change: Arrow::Table#slice with a filter removes null records. GH-34953 Rust notes The Rust projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Rust implementation, see the latest Arrow Rust changelog.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Our journey at F5 with Apache Arrow (part 1)</title><link href="https://arrow.apache.org/blog/2023/04/11/our-journey-at-f5-with-apache-arrow-part-1/" rel="alternate" type="text/html" title="Our journey at F5 with Apache Arrow (part 1)" /><published>2023-04-11T00:00:00-04:00</published><updated>2023-04-11T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/04/11/our-journey-at-f5-with-apache-arrow-part-1</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/04/11/our-journey-at-f5-with-apache-arrow-part-1/"><![CDATA[<!--

-->

<p>Apache Arrow is a technology widely adopted in big data, analytics, and machine learning applications. In this article, we share <a href="https://www.f5.com/">F5</a>’s experience with Arrow, specifically its application to telemetry, and the challenges we encountered while optimizing the OpenTelemetry protocol to significantly reduce bandwidth costs. The promising results we achieved inspired us to share our insights. This article specifically focuses on transforming relatively complex data structure from various formats into an efficient Arrow representation that optimizes both compression ratio, transport, and data processing. We also explore the trade-offs between different mapping and normalization strategies, as well as the nuances of streaming and batch communication using Arrow and Arrow Flight. Our benchmarks thus far have shown promising results, with compression ratio improvements ranging from 1.5x to 5x, depending on the data type (metrics, logs, traces), distribution, and compression algorithm. The approaches presented for addressing these challenges may be applicable to other Arrow domains as well. This article serves as the first installment in a two-part series.</p>

<h2 id="what-is-apache-arrow">What is Apache Arrow</h2>

<p><a href="https://arrow.apache.org/docs/index.html">Apache Arrow</a> is an open-source project offering a standardized, language-agnostic in-memory format for representing structured and semi-structured data. This enables data sharing and zero-copy data access between systems, eliminating the need for serialization and deserialization when exchanging datasets between varying CPU architectures and programming languages. Furthermore, Arrow libraries feature an extensive set of high-performance, parallel, and vectorized kernel functions designed for efficiently processing massive amounts of columnar data. These features make Arrow an appealing technology for big data processing, data transport, analytics, and machine learning applications. The growing number of <a href="https://arrow.apache.org/powered_by/">products and open-source projects</a> that have adopted Apache Arrow at their core or offer Arrow support reflects the widespread recognition and appreciation of its benefits (refer to this <a href="https://www.dremio.com/blog/apache-arrows-rapid-growth-over-the-years/">article</a> for an in-depth overview of the Arrow ecosystem and adoption). Over 11,000 GitHub users support this project, and 840+ are contributors who make this project an undeniable success.</p>

<p>Very often people ask about the differences between Arrow and <a href="https://parquet.apache.org/">Apache Parquet</a> or other columnar file formats. Arrow is designed and optimized for in-memory processing, while Parquet is tailored for disk-based storage. In reality, these technologies are complementary, with bridges existing between them to simplify interoperability. In both cases, data is represented in columns to optimize access, data locality and compressibility. However, the tradeoffs differ slightly. Arrow prioritizes data processing speed over the optimal data encoding. Complex encodings that don’t benefit from SIMD instruction sets are generally not natively supported by Arrow, unlike formats such as Parquet. Storing data in Parquet format and processing and transporting it in Arrow format has become a prevalent model within the big data community.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/row-vs-columnar.svg" width="100%" class="img-responsive" alt="Memory representations: row vs columnar data." />
  <figcaption>Fig 1: Memory representations: row vs columnar data.</figcaption>
</figure>

<p>Figure 1 illustrates the differences in memory representation between row-oriented and column-oriented approaches. The column-oriented approach groups data from the same column in a continuous memory area, which facilitates parallel processing (SIMD) and enhances compression performance.</p>

<h2 id="why-are-we-interested-in-apache-arrow">Why are we interested in Apache Arrow</h2>

<p>At <a href="https://www.f5.com/">F5</a>, we’ve adopted <a href="https://opentelemetry.io/">OpenTelemetry</a> (OTel) as the standard for all telemetry across our products, such as BIGIP and NGINX. These products may generate large volumes of metrics and logs for various reasons, from performance evaluation to forensic purposes. The data produced by these systems is typically centralized and processed in dedicated systems. Transporting and processing this data accounts for a significant portion of the cost associated with telemetry pipelines. In this context, we became interested in Apache Arrow. Instead of reinventing yet another telemetry solution, we decided to invest in the OpenTelemetry project, working on improvements to the protocol to significantly increase its efficiency with high telemetry data volumes. We collaborated with <a href="https://github.com/jmacd">Joshua MacDonald</a> from <a href="https://lightstep.com/">Lightstep</a> to integrate these optimizations into an <a href="https://github.com/open-telemetry/experimental-arrow-collector">experimental OTel collector</a> and are currently in discussions with the OTel technical committee to finalize a code <a href="https://github.com/open-telemetry/community/issues/1332">donation</a>.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/performance.svg" width="100%" class="img-responsive" alt="Performance improvement in the OpenTelemetry Arrow experimental project." />
  <figcaption>Fig 2: Performance improvement in the OpenTelemetry Arrow experimental project.</figcaption>
</figure>

<p>This project has been divided into two phases. The first phase, which is nearing completion, aims to enhance the protocol’s compression ratio. The second phase, planned for the future, focuses on improving end-to-end performance by incorporating Apache Arrow throughout all levels, eliminating the need for conversion between old and new protocols. The results so far are promising, with our benchmarks showing compression ratio improvements ranging from x1.5 to x5, depending on the data type (metrics, logs, traces), distribution, and compression algorithm. For the second phase, our estimates suggest that data processing acceleration could range from x2 to x12, again depending on the data’s nature and distribution. For more information, we encourage you to review the <a href="https://github.com/lquerel/oteps/blob/main/text/0156-columnar-encoding.md">specifications</a> and the <a href="https://github.com/f5/otel-arrow-adapter">reference implementation</a>.</p>

<p>Arrow relies on a schema to define the structure of data batches that it processes and transports. The subsequent sections will discuss various techniques that can be employed to optimize the creation of these schemas.</p>

<h2 id="how-to-leverage-arrow-to-optimize-network-transport-cost">How to leverage Arrow to optimize network transport cost</h2>

<p>Apache Arrow is a complex project with a rapidly evolving ecosystem, which can sometimes be overwhelming for newcomers. Fortunately the Arrow community has published three introductory articles <a href="https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/">1</a>, <a href="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/">2</a>, and <a href="https://arrow.apache.org/blog/2022/10/17/arrow-parquet-encoding-part-3/">3</a> that we recommend for those interested in exploring this technology.</p>

<p>This article primarily focuses on transforming data from an XYZ format into an efficient Arrow representation that optimizes both compression ratio and data processing. There are numerous approaches to this transformation, and we will examine how these methods can impact compression ratio, CPU usage, and memory consumption during the conversion process, among other factors.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/schema-optim-process.svg" width="100%" class="img-responsive" alt="Fig 3: Optimization process for the definition of an Arrow schema." />
  <figcaption>Fig 3: Optimization process for the definition of an Arrow schema.</figcaption>
</figure>

<p>The complexity of your initial model significantly impacts the Arrow mapping choices you need to make. To begin, it’s essential to identify the properties you want to optimize for your specific context. Compression rate, conversion speed, memory consumption, speed and ease of use of the final model, compatibility, and extensibility are all factors that can influence your final mapping decisions. From there, you must explore multiple alternative schemas.</p>

<p>The choice of the Arrow type and data encoding for each individual field will affect the performance of your schema. There are various ways to represent hierarchical data or highly dynamic data models, and multiple options need to be evaluated in coordination with the configuration of the transport layer. This transport layer should also be carefully considered. Arrow supports compression mechanisms and dictionary deltas that may not be active by default.</p>

<p>After several iterations of this process, you should arrive at an optimized schema that meets the goals you initially set. It’s crucial to compare the performance of your different approaches using real data, as the distribution of data in each individual field may influence whether you use dictionary encoding or not. We will now examine these choices in greater detail throughout the remainder of this article.</p>

<h2 id="arrow-data-type-selection">Arrow data type selection</h2>

<p>The principles of selecting an Arrow data type are quite similar to those used when defining a data model for databases. Arrow supports a wide range of data types. Some of these types are supported by all implementations, while others are only available for languages with the strongest Arrow community support (see this <a href="https://arrow.apache.org/docs/status.html">page</a> for a comparison matrix of the different implementations). For primitive types, it is generally preferable to choose the type that offers the most concise representation and is closest to the semantics of your initial field. For example, while it’s possible to represent a timestamp with an int64, it’s more advantageous to use the native Arrow Timestamp type. This choice isn’t due to a more efficient binary representation, but rather because it will be easier to process and manipulate in your pipeline. Query engines such as <a href="https://github.com/apache/arrow-datafusion">DataFusion</a> offer dedicated timestamp handling functions for columns of this type. The same choices can be made for primitive types such as date, time, duration, and interval. However, if your project requires maximum compatibility, it may be crucial in some cases to favor types with universal support instead of the most optimal type in terms of memory occupation.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/data-types.svg" width="100%" class="img-responsive" alt="Fig 4: Data types supported by Apache Arrow." />
  <figcaption>Fig 4: Data types supported by Apache Arrow.</figcaption>
</figure>

<p>When selecting the Arrow data type, it’s important to consider the size of the data before and after compression. It’s quite possible that the size after compression is the same for two different types, but the actual size in memory may be two, four, or even eight times larger (e.g., uint8 vs. uint64). This difference will impact your ability to process large batches of data and will also significantly influence the speed of processing these data in memory (e.g., cache optimization, SIMD instruction efficiency).</p>

<p>It’s also possible to extend these types using an <a href="https://arrow.apache.org/docs/format/Columnar.html#extension-types">extension type</a> mechanism that builds upon one of the currently supported primitive types while adding specific semantics. This extension mechanism can simplify the use of this data in your own project, while remaining transparent to intermediate systems that will interpret this data as a basic primitive type.</p>

<p>There are some variations in the encoding of primitive types, which we will explore next.</p>

<h2 id="data-encoding">Data encoding</h2>

<p>Another crucial aspect of optimizing your Arrow schema is analyzing the cardinality of your data. Fields that can have only a limited number of values will typically be more efficiently represented with a dictionary encoding.</p>

<p>The maximum cardinality of a field determines the data type characteristics of your dictionary. For instance, for a field representing the status code of an HTTP transaction, it’s preferable to use a dictionary with an index of type ‘uint8’ and a value of type ‘uint16’ (notation: ‘Dictionary&lt;uint8, uint16&gt;’). This consumes less memory because the main array will be of type ‘[]uint8’. Even if the range of possible values is greater than 255, as long as the number of distinct values does not exceed 255, the representation remains efficient. Similarly, the representation of a ‘user-agent’ will be more efficient with a dictionary of type ‘Dictionary&lt;uint16, string&gt;’ (see figure 5). In this case, the main array will be of type ‘uint16’, allowing a compact representation in memory and during transfers at the cost of an indirection during reverse conversion.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/dictionary-encoding.svg" width="90%" class="img-responsive" alt="Fig 5: Dictionary encoding." />
  <figcaption>Fig 5: Dictionary encoding.</figcaption>
</figure>

<p>Dictionary encoding is highly flexible in Apache Arrow, allowing the creation of encodings for any Arrow primitive type. The size of the indices can also be configured based on the context.</p>

<p>In general, it is advisable to use dictionaries in the following cases:</p>
<ul>
  <li>Representation of enumerations</li>
  <li>Representation of textual or binary fields with a high probability of having redundant values.</li>
  <li>Representation of fields with cardinalities known to be below 2^16 or 2^32.</li>
</ul>

<p>Sometimes, the cardinality of a field is not known a priori. For example, a proxy that transforms a data stream from a row-oriented format into a series of columnar-encoded batches (e.g., OpenTelemetry collector) may not be able to predict in advance whether a field will have a fixed number of distinct values. Two approaches are possible: 
1) a conservative approach using the largest data type (e.g., ‘int64’, ‘string’, etc., instead of dictionary), 
2) an adaptive approach that modifies the schema on the fly based on the observed cardinality of the field(s). In this second approach, without cardinality information, you can optimistically start by using a ‘Dictionary&lt;uint8, original-field-type&gt;’ dictionary, then detect a potential dictionary overflow during conversion, and change the schema to a ‘Dictionary&lt;uint16, original-field-type&gt;’ in case of an overflow. This technique of automatic management of dictionary overflows will be presented in greater detail in a future article.</p>

<p>Recent advancements in Apache Arrow include the implementation of <a href="https://arrow.apache.org/docs/format/Columnar.html#run-end-encoded-layout">run-end encoding</a>, a technique that efficiently represents data with sequences of repeated values. This encoding method is particularly beneficial for handling data sets containing long stretches of identical values, as it offers a more compact and optimized representation.</p>

<p>In conclusion, dictionary encoding not only occupies less space in memory and during transfers but also significantly improves the compression ratio and data processing speed. However, this type of representation requires indirection when extracting the initial values (although this isn’t always necessary, even during some data processing operations). Additionally, it is important to manage dictionary index overflow, especially when the encoded field doesn’t have a well-defined cardinality.</p>

<h2 id="hierarchical-data">Hierarchical data</h2>

<p>Basic hierarchical data structures translate relatively well into Arrow. However, as we will see, there are some complications to handle in more  general cases (see figure 6). While Arrow schemas do support nested structures, maps, and unions, some components of the Arrow ecosystem do not fully support them, which can make these Arrow data types unsuitable for certain scenarios. Additionally, unlike most languages and formats, such as Protobuf, Arrow doesn’t support the concept of a recursively defined schema. An Arrow schema is static in its definition, and the depth of its nested elements must be known in advance. There are multiple strategies to work around this limitation and we’ll explore these in the following sections.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/simple-vs-complex-data-model.svg" width="100%" class="img-responsive" alt="Fig 6: simple vs complex data model." />
  <figcaption>Fig 6: simple vs complex data model.</figcaption>
</figure>

<h3 id="natural-representation">Natural representation</h3>

<p>The most straightforward and intuitive approach to representing a simple hierarchical data model is to use Arrow’s list, map, and union data types. However, it’s important to note that some of these data types are not fully supported throughout the entire Arrow ecosystem. For example, the conversion of unions to Parquet is <a href="https://issues.apache.org/jira/browse/PARQUET-756">not directly supported</a> and requires a transformation step (see <a href="https://docs.google.com/document/d/11lG7Go2IgKOyW-RReBRW6r7HIdV1X7lu5WrDGlW5LbQ/edit#heading=h.nlplaj34c4ke">denormalization &amp; flattening representation</a> to decompose a sparse union into a nullable struct and type ids column). Similarly, lists and maps are <a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html">not yet supported</a> in DataFusion version 20 (nested structures are partially supported).</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/hierarchical-data-model.svg" width="80%" class="img-responsive" alt="Fig 7: initial data model." />
  <figcaption>Fig 7: initial data model.</figcaption>
</figure>

<p>The following example is a Go program snippet of an Arrow schema using these different data types to represent the model above.</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="s">"github.com/apache/arrow/go/v11/arrow"</span>


<span class="k">const</span> <span class="p">(</span>
  <span class="n">GaugeMetricCode</span> <span class="n">arrow</span><span class="o">.</span><span class="n">UnionTypeCode</span> <span class="o">=</span> <span class="m">0</span>
  <span class="n">SumMetricCode</span>   <span class="n">arrow</span><span class="o">.</span><span class="n">UnionTypeCode</span> <span class="o">=</span> <span class="m">1</span>
<span class="p">)</span>


<span class="k">var</span> <span class="p">(</span>
  <span class="c">// uint8Dictionary represent a Dictionary&lt;Uint8, String&gt;</span>
  <span class="n">uint8Dictionary</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">DictionaryType</span><span class="p">{</span>
     <span class="n">IndexType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint8</span><span class="p">,</span>
     <span class="n">ValueType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span>
  <span class="p">}</span>
  <span class="c">// uint16Dictionary represent a Dictionary&lt;Uint16, String&gt;</span>
  <span class="n">uint16Dictionary</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">DictionaryType</span><span class="p">{</span>
     <span class="n">IndexType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint16</span><span class="p">,</span>
     <span class="n">ValueType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span>
  <span class="p">}</span>


  <span class="n">Schema</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">.</span><span class="n">NewSchema</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
     <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"resource_metrics"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">ListOf</span><span class="p">(</span><span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"scope"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
           <span class="c">// Name and Version are declared as dictionaries (Dictionary&lt;Uint16, String&gt;)).</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"version"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
        <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">ListOf</span><span class="p">(</span><span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"unit"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint8Dictionary</span><span class="p">},</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"timestamp"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">TIMESTAMP</span><span class="p">},</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metric_type"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">UINT8</span><span class="p">},</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"data_point"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">ListOf</span><span class="p">(</span><span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
              <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metric"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">DenseUnionOf</span><span class="p">(</span>
                 <span class="p">[]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
                    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"gauge"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
                       <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"data_point"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FLOAT64</span><span class="p">},</span>
                    <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
                    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"sum"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
                       <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"data_point"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FLOAT64</span><span class="p">},</span>
                       <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"is_monotonic"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BOOL</span><span class="p">},</span>
                    <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
                 <span class="p">},</span>
                 <span class="p">[]</span><span class="n">arrow</span><span class="o">.</span><span class="n">UnionTypeCode</span><span class="p">{</span><span class="n">GaugeMetricCode</span><span class="p">,</span> <span class="n">SumMetricCode</span><span class="p">},</span>
              <span class="p">)},</span>
           <span class="p">}</span><span class="o">...</span><span class="p">))},</span>
        <span class="p">}</span><span class="o">...</span><span class="p">))},</span>
     <span class="p">}</span><span class="o">...</span><span class="p">))},</span>
  <span class="p">},</span> <span class="no">nil</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div>

<p>In this pattern, we use a union type to represent an inheritance relationship. There are two types of Arrow union that are optimized for different cases. The dense union type has a relatively succinct memory representation but doesn’t support vectorizable operations, making it less efficient during the processing phase. Conversely, a sparse union supports vectorization operations, but comes with a memory overhead directly proportional to the number of variants in the union. Dense and sparse unions have quite similar compression rates, with sometimes a slight advantage for sparse unions. In addition, sparse unions with a large number of variants should generally be avoided, as they can lead to excessive memory consumption. For more details on the memory representation of unions, you can consult this <a href="https://wesm.github.io/arrow-site-test/format/Layout.html#dense-union-type">page</a>.</p>

<p>In certain scenarios, it may be more idiomatic to represent the inheritance relationship using multiple schemas (i.e., one schema per subtype), thereby avoiding the use of the union type. However, applying this approach to the aforementioned model may not be optimal, as the data preceding the inheritance relationship (i.e., <code class="language-plaintext highlighter-rouge">ResourceMetrics</code>, <code class="language-plaintext highlighter-rouge">Scope</code>, and <code class="language-plaintext highlighter-rouge">Metrics</code>) could potentially be duplicated numerous times. If the relationships between <code class="language-plaintext highlighter-rouge">ResourceMetrics</code>, <code class="language-plaintext highlighter-rouge">Metrics</code>, and <code class="language-plaintext highlighter-rouge">DataPoint</code> were 0..1 (zero-to-one) relationships, then the multi-schema approach would likely be the simplest and most idiomatic solution.</p>

<h3 id="denormalization--flattening-representations">Denormalization &amp; Flattening representations</h3>

<p>If the <code class="language-plaintext highlighter-rouge">List</code> type is not supported in your telemetry pipeline, you can denormalize your data model. This process is often used in the database world to remove a join between two tables for optimization purposes. In the Arrow world, denormalization is employed to eliminate the <code class="language-plaintext highlighter-rouge">List</code> type by duplicating some data. Once transformed, the previous Arrow schema becomes.</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Schema</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">.</span><span class="n">NewSchema</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"resource_metrics"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
     <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"scope"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
        <span class="c">// Name and Version are declared as dictionaries (Dictionary&lt;Uint16, String&gt;)).</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"version"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
     <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
     <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"unit"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint8Dictionary</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"timestamp"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">TIMESTAMP</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metric_type"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">UINT8</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"data_point"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metric"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">DenseUnionOf</span><span class="p">(</span>
              <span class="p">[]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
                 <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"gauge"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
                    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"value"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FLOAT64</span><span class="p">},</span>
                 <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
                 <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"sum"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
                    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"value"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FLOAT64</span><span class="p">},</span>
                    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"is_monotonic"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BOOL</span><span class="p">},</span>
                 <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
              <span class="p">},</span>
              <span class="p">[]</span><span class="n">arrow</span><span class="o">.</span><span class="n">UnionTypeCode</span><span class="p">{</span><span class="n">GaugeMetricCode</span><span class="p">,</span> <span class="n">SumMetricCode</span><span class="p">},</span>
           <span class="p">)},</span>
        <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
     <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
  <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
<span class="p">},</span> <span class="no">nil</span><span class="p">)</span>
</code></pre></div></div>

<p>List types are eliminated at all levels. The initial semantics of the model are preserved by duplicating the data of the levels below each data point value. The memory representation will generally be much larger than the previous one, but a query engine that does not support the <code class="language-plaintext highlighter-rouge">List</code> type will still be able to process this data. Interestingly, once compressed, this way of representing data may not necessarily be larger than the previous approach. This is because the columnar representation compresses very well when there is redundancy in the data.</p>

<p>If the union type is not supported by some components of your pipeline, it is also possible to eliminate them by merging the union variants (the nested structure ‘metric’ is removed, see below).</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Schema</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">.</span><span class="n">NewSchema</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"resource_metrics"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
     <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"scope"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
        <span class="c">// Name and Version are declared as dictionaries (Dictionary&lt;Uint16, String&gt;)).</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"version"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
     <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
     <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"unit"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint8Dictionary</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"timestamp"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">TIMESTAMP</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metric_type"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">UINT8</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"data_point"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"value"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FLOAT64</span><span class="p">},</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"is_monotonic"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BOOL</span><span class="p">},</span>
        <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
     <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
  <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
<span class="p">},</span> <span class="no">nil</span><span class="p">)</span>
</code></pre></div></div>

<p>The final schema has evolved into a series of nested structures, where the fields of the union variants are merged into one structure. The trade-off of this approach is similar to that of sparse union - the more variants, the higher the memory occupation. Arrow supports the concept of bitmap validity to identify null values (1 bit per entry) for various data types, including those that do not have a unique null representation (e.g., primitive types). The use of bitmap validity makes the query part easier, and query engines such as DataFusion know how to use if efficiently, Columns with numerous nulls typically compress quite efficiently since the underlying arrays are generally initialized with 0’s. Upon compression, these extensive sequences of 0’s result in high compression efficiency, despite the memory overhead before compression in the case of sparse unions. Consequently, it is essential to select the appropriate trade-off based on your specific context.</p>

<p>In some extreme situations where nested structures are not supported, a flattening approach can be used to address this problem.</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Schema</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">.</span><span class="n">NewSchema</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"scope_name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"scope_version"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics_name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics_unit"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint8Dictionary</span><span class="p">},</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics_timestamp"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">TIMESTAMP</span><span class="p">},</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics_metric_type"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">UINT8</span><span class="p">},</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics_data_point_value"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FLOAT64</span><span class="p">},</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics_data_point_is_monotonic"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BOOL</span><span class="p">},</span>
<span class="p">},</span> <span class="no">nil</span><span class="p">)</span>
</code></pre></div></div>

<p>The terminal fields (leaves) are renamed by concatenating the names of the parent structures to provide proper scoping. This type of structure is supported by all components of the Arrow ecosystem. This approach can be useful if compatibility is a crucial criterion for your system. However, it shares the same drawbacks as other alternative denormalization models.</p>

<p>The Arrow ecosystem is evolving rapidly, so it is likely that support for List, Map, and Union data types in query engines will improve quickly. If kernel functions are sufficient or preferable for your application, it is usually possible to utilize these nested types.</p>

<h3 id="adaptivedynamic-representation">Adaptive/Dynamic representation</h3>

<p>Some data models can be more challenging to translate into an Arrow schema, such as the following Protobuf example. In this example, a collection of attributes is added to each data point. These attributes are defined using a recursive definition that most languages and formats, like Protobuf, support (see the ‘AnyValue’ definition below). Unfortunately, Arrow (like most classical database schemas) does not support such recursive definition within schemas.</p>

<div class="language-protobuf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">syntax</span> <span class="o">=</span> <span class="s">"proto3"</span><span class="p">;</span>


<span class="kd">message</span> <span class="nc">Metric</span> <span class="p">{</span>
 <span class="kd">message</span> <span class="nc">DataPoint</span> <span class="p">{</span>
   <span class="k">repeated</span> <span class="n">Attribute</span> <span class="na">attributes</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
   <span class="k">oneof</span> <span class="n">value</span> <span class="p">{</span>
     <span class="kt">int64</span> <span class="na">int_value</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
     <span class="kt">double</span> <span class="na">double_value</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
   <span class="p">}</span>
 <span class="p">}</span>


 <span class="kd">enum</span> <span class="n">MetricType</span> <span class="p">{</span>
   <span class="na">UNSPECIFIED</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
   <span class="na">GAUGE</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
   <span class="na">SUM</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
 <span class="p">}</span>


 <span class="kd">message</span> <span class="nc">Gauge</span> <span class="p">{</span>
   <span class="n">DataPoint</span> <span class="na">data_point</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
 <span class="p">}</span>


 <span class="kd">message</span> <span class="nc">Sum</span> <span class="p">{</span>
   <span class="n">DataPoint</span> <span class="na">data_point</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
   <span class="kt">bool</span> <span class="na">is_monotonic</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
 <span class="p">}</span>


 <span class="kt">string</span> <span class="na">name</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
 <span class="kt">int64</span> <span class="na">timestamp</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
 <span class="kt">string</span> <span class="na">unit</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
 <span class="n">MetricType</span> <span class="na">type</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span>
 <span class="k">oneof</span> <span class="n">metric</span> <span class="p">{</span>
   <span class="n">Gauge</span> <span class="na">gauge</span> <span class="o">=</span> <span class="mi">5</span><span class="p">;</span>
   <span class="n">Sum</span> <span class="na">sum</span> <span class="o">=</span> <span class="mi">6</span><span class="p">;</span>
 <span class="p">}</span>
<span class="p">}</span>


<span class="kd">message</span> <span class="nc">Attribute</span> <span class="p">{</span>
 <span class="kt">string</span> <span class="na">name</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
 <span class="n">AnyValue</span> <span class="na">value</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
<span class="p">}</span>


<span class="c1">// Recursive definition of AnyValue. AnyValue can be a primitive value, a list</span>
<span class="c1">// of AnyValues, or a list of key-value pairs where the key is a string and</span>
<span class="c1">// the value is an AnyValue.</span>
<span class="kd">message</span> <span class="nc">AnyValue</span> <span class="p">{</span>
 <span class="kd">message</span> <span class="nc">ArrayValue</span> <span class="p">{</span>
   <span class="k">repeated</span> <span class="n">AnyValue</span> <span class="na">values</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
 <span class="p">}</span>
 <span class="kd">message</span> <span class="nc">KeyValueList</span> <span class="p">{</span>
   <span class="kd">message</span> <span class="nc">KeyValue</span> <span class="p">{</span>
     <span class="kt">string</span> <span class="na">key</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
     <span class="n">AnyValue</span> <span class="na">value</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
   <span class="p">}</span>
   <span class="k">repeated</span> <span class="n">KeyValue</span> <span class="na">values</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
 <span class="p">}</span>


 <span class="k">oneof</span> <span class="n">value</span> <span class="p">{</span>
   <span class="kt">int64</span> <span class="na">int_value</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
   <span class="kt">double</span> <span class="na">double_value</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
   <span class="kt">string</span> <span class="na">string_value</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
   <span class="n">ArrayValue</span> <span class="na">list_value</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span>
   <span class="n">KeyValueList</span> <span class="na">kvlist_value</span> <span class="o">=</span> <span class="mi">5</span><span class="p">;</span>
 <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>If the definition of the attributes were non-recursive, it would have been possible to directly translate them into an Arrow Map type.</p>

<p>To address this kind of issue and further optimize Arrow schema definitions, you can employ an adaptive and iterative method that automatically constructs the Arrow schema based on the data being translated. With this approach, fields are automatically dictionary-encoded according to their cardinalities, unused fields are eliminated, and recursive structures are represented in a specific manner. Another solution involves using a multi-schema approach, in which attributes are depicted in a separate Arrow Record, and the inheritance relation is represented by a self-referential relationship. These strategies will be covered in more depth in a future article. For those eager to learn more, the first method is utilized in the reference implementation of the <a href="https://github.com/f5/otel-arrow-adapter">OTel Arrow Adapter</a>.</p>

<h2 id="data-transport">Data transport</h2>

<p>Unlike to Protobuf, an Arrow schema is generally not known a priori by the two parties participating in an exchange. Before being able to exchange data in Arrow format, the sender must first communicate the schema to the receiver, as well as the contents of the dictionaries used in the data. Only after this initialization phase has been completed can the sender transmit batches of data in Arrow format. This process, known as <a href="https://wesmckinney.com/blog/arrow-streaming-columnar/">Arrow IPC Stream</a>, plays an essential role transporting Arrow data between systems. Several approaches can be employed to communicate these Arrow IPC Streams. The simplest method is to use <a href="https://arrow.apache.org/docs/format/Flight.html">Arrow Flight</a>, which encapsulates Arrow IPC streams in a gRPC-based protocol. However, it is also possible to use your own implementation for specific contexts. Regardless of the solution you choose, it is crucial to understand that the underlying protocol must be stateful to take full advantage of the Arrow IPC stream approach. To achieve the best compression rates, it is vital to send schemas and dictionaries only once in order to amortize the cost and minimize data redundancy between batches. This necessitates a transport that supports stream-oriented communications, such as gRPC.</p>

<p>Using a stateless protocol is possible for large batches because the overhead of the schema will be negligible compared to the compression gains achieved using dictionary encoding and columnar representation. However, dictionaries will have to be communicated for each batch, making this approach generally less efficient than a stream-oriented approach.</p>

<p>Arrow IPC Stream also supports the concept of “delta dictionaries,” which allows for further optimization of batch transport. When a batch adds data to an existing dictionary (at the sender’s end), Arrow IPC enables sending the delta dictionary followed by the batch that references it. On the receiver side, this delta is used to update the existing dictionary, eliminating the need to retransmit the entire dictionary when changes occur. This optimization is only possible with a stateful protocol.</p>

<p>To fully leverage the column-oriented format of Apache Arrow, it is essential to consider sorting and compression. If your data model is simple (i.e., flat) and has one or more columns representing a natural order for your data (e.g., timestamp), it might be beneficial to sort your data to optimize the final compression ratio. Before implementing this optimization, it is recommended to perform tests on real data since the benefits may vary. In any case, using a compression algorithm when sending your batches is advantageous. Arrow IPC generally supports the ZSTD compression algorithm, which strikes an excellent balance between speed and compression efficiency, especially for column-oriented data.</p>

<p>Lastly, some implementations (e.g., Arrow Go) are not configured by default to support delta dictionaries and compression algorithms. Therefore, it is crucial to ensure that your code employs these options to maximize data transport efficiency.</p>

<h2 id="experiments">Experiments</h2>

<p>If your initial data is complex, it is advisable to conduct your own experiments to optimize the Arrow representation according to your data and goals (e.g., optimizing the compression ratio or enhancing the query-ability of your data in Arrow format). In our case, we developed an overlay for Apache Arrow that enables us to carry out these experiments with ease, without having to deal with the intrinsic complexity of Arrow APIs. However, this comes at the expense of a slower conversion phase compared to using Arrow APIs directly. While this library is not currently public, it may become available if there is sufficient interest.</p>

<p>We also employed a “black box optimization” approach, which automatically finds the best combination to meet the objectives we aimed to optimize (refer to “<a href="https://cloud.google.com/blog/products/ai-machine-learning/optimize-your-applications-using-google-vertex-ai-vizier">Optimize your applications using Google Vertex AI Vizier</a>” for a description of this approach).</p>

<h2 id="conclusion-and-next-steps">Conclusion and next steps</h2>

<p>Essentially, the key concept behind Apache Arrow is that it eliminates the need for serialization and deserialization, enabling zero-copy data sharing. Arrow achieves this by defining a language-agnostic, in-memory format that remains consistent across various implementations. Consequently, raw memory bytes can be transmitted directly over a network without requiring any serialization or deserialization, significantly enhancing data processing efficiency.</p>

<p>Converting a data model to Apache Arrow necessitates adaptation and optimization work, as we have begun to describe in this article. Many parameters must be considered, and it is recommended to perform a series of experiments to validate the various choices made during this process.</p>

<p>Handling highly dynamic data with Arrow can be challenging. Arrow requires the definition of a static schema, which can sometimes make representing this type of data complex or suboptimal, especially when the initial schema contains recursive definitions. This article has discussed several approaches to address this issue. The next article will be dedicated to a hybrid strategy that involves adapting the Arrow schema on-the-fly to optimize memory usage, compression ratio, and processing speed based on the data being represented. This approach is quite unique and deserves a separate article.</p>]]></content><author><name>Laurent Quérel</name></author><category term="application" /><summary type="html"><![CDATA[Apache Arrow is a technology widely adopted in big data, analytics, and machine learning applications. In this article, we share F5’s experience with Arrow, specifically its application to telemetry, and the challenges we encountered while optimizing the OpenTelemetry protocol to significantly reduce bandwidth costs. The promising results we achieved inspired us to share our insights. This article specifically focuses on transforming relatively complex data structure from various formats into an efficient Arrow representation that optimizes both compression ratio, transport, and data processing. We also explore the trade-offs between different mapping and normalization strategies, as well as the nuances of streaming and batch communication using Arrow and Arrow Flight. Our benchmarks thus far have shown promising results, with compression ratio improvements ranging from 1.5x to 5x, depending on the data type (metrics, logs, traces), distribution, and compression algorithm. The approaches presented for addressing these challenges may be applicable to other Arrow domains as well. This article serves as the first installment in a two-part series. What is Apache Arrow Apache Arrow is an open-source project offering a standardized, language-agnostic in-memory format for representing structured and semi-structured data. This enables data sharing and zero-copy data access between systems, eliminating the need for serialization and deserialization when exchanging datasets between varying CPU architectures and programming languages. Furthermore, Arrow libraries feature an extensive set of high-performance, parallel, and vectorized kernel functions designed for efficiently processing massive amounts of columnar data. These features make Arrow an appealing technology for big data processing, data transport, analytics, and machine learning applications. The growing number of products and open-source projects that have adopted Apache Arrow at their core or offer Arrow support reflects the widespread recognition and appreciation of its benefits (refer to this article for an in-depth overview of the Arrow ecosystem and adoption). Over 11,000 GitHub users support this project, and 840+ are contributors who make this project an undeniable success. Very often people ask about the differences between Arrow and Apache Parquet or other columnar file formats. Arrow is designed and optimized for in-memory processing, while Parquet is tailored for disk-based storage. In reality, these technologies are complementary, with bridges existing between them to simplify interoperability. In both cases, data is represented in columns to optimize access, data locality and compressibility. However, the tradeoffs differ slightly. Arrow prioritizes data processing speed over the optimal data encoding. Complex encodings that don’t benefit from SIMD instruction sets are generally not natively supported by Arrow, unlike formats such as Parquet. Storing data in Parquet format and processing and transporting it in Arrow format has become a prevalent model within the big data community. Fig 1: Memory representations: row vs columnar data. Figure 1 illustrates the differences in memory representation between row-oriented and column-oriented approaches. The column-oriented approach groups data from the same column in a continuous memory area, which facilitates parallel processing (SIMD) and enhances compression performance. Why are we interested in Apache Arrow At F5, we’ve adopted OpenTelemetry (OTel) as the standard for all telemetry across our products, such as BIGIP and NGINX. These products may generate large volumes of metrics and logs for various reasons, from performance evaluation to forensic purposes. The data produced by these systems is typically centralized and processed in dedicated systems. Transporting and processing this data accounts for a significant portion of the cost associated with telemetry pipelines. In this context, we became interested in Apache Arrow. Instead of reinventing yet another telemetry solution, we decided to invest in the OpenTelemetry project, working on improvements to the protocol to significantly increase its efficiency with high telemetry data volumes. We collaborated with Joshua MacDonald from Lightstep to integrate these optimizations into an experimental OTel collector and are currently in discussions with the OTel technical committee to finalize a code donation. Fig 2: Performance improvement in the OpenTelemetry Arrow experimental project. This project has been divided into two phases. The first phase, which is nearing completion, aims to enhance the protocol’s compression ratio. The second phase, planned for the future, focuses on improving end-to-end performance by incorporating Apache Arrow throughout all levels, eliminating the need for conversion between old and new protocols. The results so far are promising, with our benchmarks showing compression ratio improvements ranging from x1.5 to x5, depending on the data type (metrics, logs, traces), distribution, and compression algorithm. For the second phase, our estimates suggest that data processing acceleration could range from x2 to x12, again depending on the data’s nature and distribution. For more information, we encourage you to review the specifications and the reference implementation. Arrow relies on a schema to define the structure of data batches that it processes and transports. The subsequent sections will discuss various techniques that can be employed to optimize the creation of these schemas. How to leverage Arrow to optimize network transport cost Apache Arrow is a complex project with a rapidly evolving ecosystem, which can sometimes be overwhelming for newcomers. Fortunately the Arrow community has published three introductory articles 1, 2, and 3 that we recommend for those interested in exploring this technology. This article primarily focuses on transforming data from an XYZ format into an efficient Arrow representation that optimizes both compression ratio and data processing. There are numerous approaches to this transformation, and we will examine how these methods can impact compression ratio, CPU usage, and memory consumption during the conversion process, among other factors. Fig 3: Optimization process for the definition of an Arrow schema. The complexity of your initial model significantly impacts the Arrow mapping choices you need to make. To begin, it’s essential to identify the properties you want to optimize for your specific context. Compression rate, conversion speed, memory consumption, speed and ease of use of the final model, compatibility, and extensibility are all factors that can influence your final mapping decisions. From there, you must explore multiple alternative schemas. The choice of the Arrow type and data encoding for each individual field will affect the performance of your schema. There are various ways to represent hierarchical data or highly dynamic data models, and multiple options need to be evaluated in coordination with the configuration of the transport layer. This transport layer should also be carefully considered. Arrow supports compression mechanisms and dictionary deltas that may not be active by default. After several iterations of this process, you should arrive at an optimized schema that meets the goals you initially set. It’s crucial to compare the performance of your different approaches using real data, as the distribution of data in each individual field may influence whether you use dictionary encoding or not. We will now examine these choices in greater detail throughout the remainder of this article. Arrow data type selection The principles of selecting an Arrow data type are quite similar to those used when defining a data model for databases. Arrow supports a wide range of data types. Some of these types are supported by all implementations, while others are only available for languages with the strongest Arrow community support (see this page for a comparison matrix of the different implementations). For primitive types, it is generally preferable to choose the type that offers the most concise representation and is closest to the semantics of your initial field. For example, while it’s possible to represent a timestamp with an int64, it’s more advantageous to use the native Arrow Timestamp type. This choice isn’t due to a more efficient binary representation, but rather because it will be easier to process and manipulate in your pipeline. Query engines such as DataFusion offer dedicated timestamp handling functions for columns of this type. The same choices can be made for primitive types such as date, time, duration, and interval. However, if your project requires maximum compatibility, it may be crucial in some cases to favor types with universal support instead of the most optimal type in terms of memory occupation. Fig 4: Data types supported by Apache Arrow. When selecting the Arrow data type, it’s important to consider the size of the data before and after compression. It’s quite possible that the size after compression is the same for two different types, but the actual size in memory may be two, four, or even eight times larger (e.g., uint8 vs. uint64). This difference will impact your ability to process large batches of data and will also significantly influence the speed of processing these data in memory (e.g., cache optimization, SIMD instruction efficiency). It’s also possible to extend these types using an extension type mechanism that builds upon one of the currently supported primitive types while adding specific semantics. This extension mechanism can simplify the use of this data in your own project, while remaining transparent to intermediate systems that will interpret this data as a basic primitive type. There are some variations in the encoding of primitive types, which we will explore next. Data encoding Another crucial aspect of optimizing your Arrow schema is analyzing the cardinality of your data. Fields that can have only a limited number of values will typically be more efficiently represented with a dictionary encoding. The maximum cardinality of a field determines the data type characteristics of your dictionary. For instance, for a field representing the status code of an HTTP transaction, it’s preferable to use a dictionary with an index of type ‘uint8’ and a value of type ‘uint16’ (notation: ‘Dictionary&lt;uint8, uint16&gt;’). This consumes less memory because the main array will be of type ‘[]uint8’. Even if the range of possible values is greater than 255, as long as the number of distinct values does not exceed 255, the representation remains efficient. Similarly, the representation of a ‘user-agent’ will be more efficient with a dictionary of type ‘Dictionary&lt;uint16, string&gt;’ (see figure 5). In this case, the main array will be of type ‘uint16’, allowing a compact representation in memory and during transfers at the cost of an indirection during reverse conversion. Fig 5: Dictionary encoding. Dictionary encoding is highly flexible in Apache Arrow, allowing the creation of encodings for any Arrow primitive type. The size of the indices can also be configured based on the context. In general, it is advisable to use dictionaries in the following cases: Representation of enumerations Representation of textual or binary fields with a high probability of having redundant values. Representation of fields with cardinalities known to be below 2^16 or 2^32. Sometimes, the cardinality of a field is not known a priori. For example, a proxy that transforms a data stream from a row-oriented format into a series of columnar-encoded batches (e.g., OpenTelemetry collector) may not be able to predict in advance whether a field will have a fixed number of distinct values. Two approaches are possible: 1) a conservative approach using the largest data type (e.g., ‘int64’, ‘string’, etc., instead of dictionary), 2) an adaptive approach that modifies the schema on the fly based on the observed cardinality of the field(s). In this second approach, without cardinality information, you can optimistically start by using a ‘Dictionary&lt;uint8, original-field-type&gt;’ dictionary, then detect a potential dictionary overflow during conversion, and change the schema to a ‘Dictionary&lt;uint16, original-field-type&gt;’ in case of an overflow. This technique of automatic management of dictionary overflows will be presented in greater detail in a future article. Recent advancements in Apache Arrow include the implementation of run-end encoding, a technique that efficiently represents data with sequences of repeated values. This encoding method is particularly beneficial for handling data sets containing long stretches of identical values, as it offers a more compact and optimized representation. In conclusion, dictionary encoding not only occupies less space in memory and during transfers but also significantly improves the compression ratio and data processing speed. However, this type of representation requires indirection when extracting the initial values (although this isn’t always necessary, even during some data processing operations). Additionally, it is important to manage dictionary index overflow, especially when the encoded field doesn’t have a well-defined cardinality. Hierarchical data Basic hierarchical data structures translate relatively well into Arrow. However, as we will see, there are some complications to handle in more general cases (see figure 6). While Arrow schemas do support nested structures, maps, and unions, some components of the Arrow ecosystem do not fully support them, which can make these Arrow data types unsuitable for certain scenarios. Additionally, unlike most languages and formats, such as Protobuf, Arrow doesn’t support the concept of a recursively defined schema. An Arrow schema is static in its definition, and the depth of its nested elements must be known in advance. There are multiple strategies to work around this limitation and we’ll explore these in the following sections. Fig 6: simple vs complex data model. Natural representation The most straightforward and intuitive approach to representing a simple hierarchical data model is to use Arrow’s list, map, and union data types. However, it’s important to note that some of these data types are not fully supported throughout the entire Arrow ecosystem. For example, the conversion of unions to Parquet is not directly supported and requires a transformation step (see denormalization &amp; flattening representation to decompose a sparse union into a nullable struct and type ids column). Similarly, lists and maps are not yet supported in DataFusion version 20 (nested structures are partially supported). Fig 7: initial data model. The following example is a Go program snippet of an Arrow schema using these different data types to represent the model above. import "github.com/apache/arrow/go/v11/arrow" const ( GaugeMetricCode arrow.UnionTypeCode = 0 SumMetricCode arrow.UnionTypeCode = 1 ) var ( // uint8Dictionary represent a Dictionary&lt;Uint8, String&gt; uint8Dictionary = &amp;arrow.DictionaryType{ IndexType: arrow.PrimitiveTypes.Uint8, ValueType: arrow.BinaryTypes.String, } // uint16Dictionary represent a Dictionary&lt;Uint16, String&gt; uint16Dictionary = &amp;arrow.DictionaryType{ IndexType: arrow.PrimitiveTypes.Uint16, ValueType: arrow.BinaryTypes.String, } Schema = arrow.NewSchema([]arrow.Field{ {Name: "resource_metrics", Type: arrow.ListOf(arrow.StructOf([]arrow.Field{ {Name: "scope", Type: arrow.StructOf([]arrow.Field{ // Name and Version are declared as dictionaries (Dictionary&lt;Uint16, String&gt;)). {Name: "name", Type: uint16Dictionary}, {Name: "version", Type: uint16Dictionary}, }...)}, {Name: "metrics", Type: arrow.ListOf(arrow.StructOf([]arrow.Field{ {Name: "name", Type: uint16Dictionary}, {Name: "unit", Type: uint8Dictionary}, {Name: "timestamp", Type: arrow.TIMESTAMP}, {Name: "metric_type", Type: arrow.UINT8}, {Name: "data_point", Type: arrow.ListOf(arrow.StructOf([]arrow.Field{ {Name: "metric", Type: arrow.DenseUnionOf( []arrow.Field{ {Name: "gauge", Type: arrow.StructOf([]arrow.Field{ {Name: "data_point", Type: arrow.FLOAT64}, }...)}, {Name: "sum", Type: arrow.StructOf([]arrow.Field{ {Name: "data_point", Type: arrow.FLOAT64}, {Name: "is_monotonic", Type: arrow.BOOL}, }...)}, }, []arrow.UnionTypeCode{GaugeMetricCode, SumMetricCode}, )}, }...))}, }...))}, }...))}, }, nil) ) In this pattern, we use a union type to represent an inheritance relationship. There are two types of Arrow union that are optimized for different cases. The dense union type has a relatively succinct memory representation but doesn’t support vectorizable operations, making it less efficient during the processing phase. Conversely, a sparse union supports vectorization operations, but comes with a memory overhead directly proportional to the number of variants in the union. Dense and sparse unions have quite similar compression rates, with sometimes a slight advantage for sparse unions. In addition, sparse unions with a large number of variants should generally be avoided, as they can lead to excessive memory consumption. For more details on the memory representation of unions, you can consult this page. In certain scenarios, it may be more idiomatic to represent the inheritance relationship using multiple schemas (i.e., one schema per subtype), thereby avoiding the use of the union type. However, applying this approach to the aforementioned model may not be optimal, as the data preceding the inheritance relationship (i.e., ResourceMetrics, Scope, and Metrics) could potentially be duplicated numerous times. If the relationships between ResourceMetrics, Metrics, and DataPoint were 0..1 (zero-to-one) relationships, then the multi-schema approach would likely be the simplest and most idiomatic solution. Denormalization &amp; Flattening representations If the List type is not supported in your telemetry pipeline, you can denormalize your data model. This process is often used in the database world to remove a join between two tables for optimization purposes. In the Arrow world, denormalization is employed to eliminate the List type by duplicating some data. Once transformed, the previous Arrow schema becomes. Schema = arrow.NewSchema([]arrow.Field{ {Name: "resource_metrics", Type: arrow.StructOf([]arrow.Field{ {Name: "scope", Type: arrow.StructOf([]arrow.Field{ // Name and Version are declared as dictionaries (Dictionary&lt;Uint16, String&gt;)). {Name: "name", Type: uint16Dictionary}, {Name: "version", Type: uint16Dictionary}, }...)}, {Name: "metrics", Type: arrow.StructOf([]arrow.Field{ {Name: "name", Type: uint16Dictionary}, {Name: "unit", Type: uint8Dictionary}, {Name: "timestamp", Type: arrow.TIMESTAMP}, {Name: "metric_type", Type: arrow.UINT8}, {Name: "data_point", Type: arrow.StructOf([]arrow.Field{ {Name: "metric", Type: arrow.DenseUnionOf( []arrow.Field{ {Name: "gauge", Type: arrow.StructOf([]arrow.Field{ {Name: "value", Type: arrow.FLOAT64}, }...)}, {Name: "sum", Type: arrow.StructOf([]arrow.Field{ {Name: "value", Type: arrow.FLOAT64}, {Name: "is_monotonic", Type: arrow.BOOL}, }...)}, }, []arrow.UnionTypeCode{GaugeMetricCode, SumMetricCode}, )}, }...)}, }...)}, }...)}, }, nil) List types are eliminated at all levels. The initial semantics of the model are preserved by duplicating the data of the levels below each data point value. The memory representation will generally be much larger than the previous one, but a query engine that does not support the List type will still be able to process this data. Interestingly, once compressed, this way of representing data may not necessarily be larger than the previous approach. This is because the columnar representation compresses very well when there is redundancy in the data. If the union type is not supported by some components of your pipeline, it is also possible to eliminate them by merging the union variants (the nested structure ‘metric’ is removed, see below). Schema = arrow.NewSchema([]arrow.Field{ {Name: "resource_metrics", Type: arrow.StructOf([]arrow.Field{ {Name: "scope", Type: arrow.StructOf([]arrow.Field{ // Name and Version are declared as dictionaries (Dictionary&lt;Uint16, String&gt;)). {Name: "name", Type: uint16Dictionary}, {Name: "version", Type: uint16Dictionary}, }...)}, {Name: "metrics", Type: arrow.StructOf([]arrow.Field{ {Name: "name", Type: uint16Dictionary}, {Name: "unit", Type: uint8Dictionary}, {Name: "timestamp", Type: arrow.TIMESTAMP}, {Name: "metric_type", Type: arrow.UINT8}, {Name: "data_point", Type: arrow.StructOf([]arrow.Field{ {Name: "value", Type: arrow.FLOAT64}, {Name: "is_monotonic", Type: arrow.BOOL}, }...)}, }...)}, }...)}, }, nil) The final schema has evolved into a series of nested structures, where the fields of the union variants are merged into one structure. The trade-off of this approach is similar to that of sparse union - the more variants, the higher the memory occupation. Arrow supports the concept of bitmap validity to identify null values (1 bit per entry) for various data types, including those that do not have a unique null representation (e.g., primitive types). The use of bitmap validity makes the query part easier, and query engines such as DataFusion know how to use if efficiently, Columns with numerous nulls typically compress quite efficiently since the underlying arrays are generally initialized with 0’s. Upon compression, these extensive sequences of 0’s result in high compression efficiency, despite the memory overhead before compression in the case of sparse unions. Consequently, it is essential to select the appropriate trade-off based on your specific context. In some extreme situations where nested structures are not supported, a flattening approach can be used to address this problem. Schema = arrow.NewSchema([]arrow.Field{ {Name: "scope_name", Type: uint16Dictionary}, {Name: "scope_version", Type: uint16Dictionary}, {Name: "metrics_name", Type: uint16Dictionary}, {Name: "metrics_unit", Type: uint8Dictionary}, {Name: "metrics_timestamp", Type: arrow.TIMESTAMP}, {Name: "metrics_metric_type", Type: arrow.UINT8}, {Name: "metrics_data_point_value", Type: arrow.FLOAT64}, {Name: "metrics_data_point_is_monotonic", Type: arrow.BOOL}, }, nil) The terminal fields (leaves) are renamed by concatenating the names of the parent structures to provide proper scoping. This type of structure is supported by all components of the Arrow ecosystem. This approach can be useful if compatibility is a crucial criterion for your system. However, it shares the same drawbacks as other alternative denormalization models. The Arrow ecosystem is evolving rapidly, so it is likely that support for List, Map, and Union data types in query engines will improve quickly. If kernel functions are sufficient or preferable for your application, it is usually possible to utilize these nested types. Adaptive/Dynamic representation Some data models can be more challenging to translate into an Arrow schema, such as the following Protobuf example. In this example, a collection of attributes is added to each data point. These attributes are defined using a recursive definition that most languages and formats, like Protobuf, support (see the ‘AnyValue’ definition below). Unfortunately, Arrow (like most classical database schemas) does not support such recursive definition within schemas. syntax = "proto3"; message Metric { message DataPoint { repeated Attribute attributes = 1; oneof value { int64 int_value = 2; double double_value = 3; } } enum MetricType { UNSPECIFIED = 0; GAUGE = 1; SUM = 2; } message Gauge { DataPoint data_point = 1; } message Sum { DataPoint data_point = 1; bool is_monotonic = 2; } string name = 1; int64 timestamp = 2; string unit = 3; MetricType type = 4; oneof metric { Gauge gauge = 5; Sum sum = 6; } } message Attribute { string name = 1; AnyValue value = 2; } // Recursive definition of AnyValue. AnyValue can be a primitive value, a list // of AnyValues, or a list of key-value pairs where the key is a string and // the value is an AnyValue. message AnyValue { message ArrayValue { repeated AnyValue values = 1; } message KeyValueList { message KeyValue { string key = 1; AnyValue value = 2; } repeated KeyValue values = 1; } oneof value { int64 int_value = 1; double double_value = 2; string string_value = 3; ArrayValue list_value = 4; KeyValueList kvlist_value = 5; } } If the definition of the attributes were non-recursive, it would have been possible to directly translate them into an Arrow Map type. To address this kind of issue and further optimize Arrow schema definitions, you can employ an adaptive and iterative method that automatically constructs the Arrow schema based on the data being translated. With this approach, fields are automatically dictionary-encoded according to their cardinalities, unused fields are eliminated, and recursive structures are represented in a specific manner. Another solution involves using a multi-schema approach, in which attributes are depicted in a separate Arrow Record, and the inheritance relation is represented by a self-referential relationship. These strategies will be covered in more depth in a future article. For those eager to learn more, the first method is utilized in the reference implementation of the OTel Arrow Adapter. Data transport Unlike to Protobuf, an Arrow schema is generally not known a priori by the two parties participating in an exchange. Before being able to exchange data in Arrow format, the sender must first communicate the schema to the receiver, as well as the contents of the dictionaries used in the data. Only after this initialization phase has been completed can the sender transmit batches of data in Arrow format. This process, known as Arrow IPC Stream, plays an essential role transporting Arrow data between systems. Several approaches can be employed to communicate these Arrow IPC Streams. The simplest method is to use Arrow Flight, which encapsulates Arrow IPC streams in a gRPC-based protocol. However, it is also possible to use your own implementation for specific contexts. Regardless of the solution you choose, it is crucial to understand that the underlying protocol must be stateful to take full advantage of the Arrow IPC stream approach. To achieve the best compression rates, it is vital to send schemas and dictionaries only once in order to amortize the cost and minimize data redundancy between batches. This necessitates a transport that supports stream-oriented communications, such as gRPC. Using a stateless protocol is possible for large batches because the overhead of the schema will be negligible compared to the compression gains achieved using dictionary encoding and columnar representation. However, dictionaries will have to be communicated for each batch, making this approach generally less efficient than a stream-oriented approach. Arrow IPC Stream also supports the concept of “delta dictionaries,” which allows for further optimization of batch transport. When a batch adds data to an existing dictionary (at the sender’s end), Arrow IPC enables sending the delta dictionary followed by the batch that references it. On the receiver side, this delta is used to update the existing dictionary, eliminating the need to retransmit the entire dictionary when changes occur. This optimization is only possible with a stateful protocol. To fully leverage the column-oriented format of Apache Arrow, it is essential to consider sorting and compression. If your data model is simple (i.e., flat) and has one or more columns representing a natural order for your data (e.g., timestamp), it might be beneficial to sort your data to optimize the final compression ratio. Before implementing this optimization, it is recommended to perform tests on real data since the benefits may vary. In any case, using a compression algorithm when sending your batches is advantageous. Arrow IPC generally supports the ZSTD compression algorithm, which strikes an excellent balance between speed and compression efficiency, especially for column-oriented data. Lastly, some implementations (e.g., Arrow Go) are not configured by default to support delta dictionaries and compression algorithms. Therefore, it is crucial to ensure that your code employs these options to maximize data transport efficiency. Experiments If your initial data is complex, it is advisable to conduct your own experiments to optimize the Arrow representation according to your data and goals (e.g., optimizing the compression ratio or enhancing the query-ability of your data in Arrow format). In our case, we developed an overlay for Apache Arrow that enables us to carry out these experiments with ease, without having to deal with the intrinsic complexity of Arrow APIs. However, this comes at the expense of a slower conversion phase compared to using Arrow APIs directly. While this library is not currently public, it may become available if there is sufficient interest. We also employed a “black box optimization” approach, which automatically finds the best combination to meet the objectives we aimed to optimize (refer to “Optimize your applications using Google Vertex AI Vizier” for a description of this approach). Conclusion and next steps Essentially, the key concept behind Apache Arrow is that it eliminates the need for serialization and deserialization, enabling zero-copy data sharing. Arrow achieves this by defining a language-agnostic, in-memory format that remains consistent across various implementations. Consequently, raw memory bytes can be transmitted directly over a network without requiring any serialization or deserialization, significantly enhancing data processing efficiency. Converting a data model to Apache Arrow necessitates adaptation and optimization work, as we have begun to describe in this article. Many parameters must be considered, and it is recommended to perform a series of experiments to validate the various choices made during this process. Handling highly dynamic data with Arrow can be challenging. Arrow requires the definition of a static schema, which can sometimes make representing this type of data complex or suboptimal, especially when the initial schema contains recursive definitions. This article has discussed several approaches to address this issue. The next article will be dedicated to a hybrid strategy that involves adapting the Arrow schema on-the-fly to optimize memory usage, compression ratio, and processing speed based on the data being represented. This approach is quite unique and deserves a separate article.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 0.3.0 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/03/21/adbc-0.3.0-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.3.0 (Libraries) Release" /><published>2023-03-21T00:00:00-04:00</published><updated>2023-03-21T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/03/21/adbc-0.3.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/03/21/adbc-0.3.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.3.0 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/4"><strong>24
resolved issues</strong></a> from <a href="#contributors"><strong>7 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version
0.3.0.  The <strong>API specification</strong> is versioned separately and is
at version 1.0.0.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.3.0/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>R bindings for the driver manager have been added, as well as an R package that repackages the SQLite driver.</p>

<p>The ADBC Flight SQL driver now supports transactions and executing Substrait plans.  Also, it has had several bugs fixed, including setting timeouts and sending headers properly.</p>

<p>The Python ADBC packages now expose enums for driver-specific options.  Also, the DBAPI layer now implements <code class="language-plaintext highlighter-rouge">__del__</code> on objects to assist in cleaning up resources (though context managers are still recommended).</p>

<p>The ADBC JDBC driver now exposes more metadata about constraints via <code class="language-plaintext highlighter-rouge">getObjects</code>.</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.2.0..apache-arrow-adbc-0.3.0
    32	David Li
     5	Dewey Dunnington
     3	Matt Topol
     1	Dave Hirschfeld
     1	Jacob Marble
     1	Tornike Gurgenidze
     1	Will Jones
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>New extensions to the API specification have been proposed.  These
will be backwards-compatible and will become API specification 1.1.0.
For details, see the <a href="https://lists.apache.org/thread/247z3t06mf132nocngc1jkp3oqglz7jp">mailing list discussion</a> and the
<a href="https://github.com/apache/arrow-adbc/milestone/3">milestone</a> tracking the proposed features.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.3.0 release of the Apache Arrow ADBC libraries. This covers includes 24 resolved issues from 7 distinct contributors. This is a release of the libraries, which are at version 0.3.0. The API specification is versioned separately and is at version 1.0.0. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights R bindings for the driver manager have been added, as well as an R package that repackages the SQLite driver. The ADBC Flight SQL driver now supports transactions and executing Substrait plans. Also, it has had several bugs fixed, including setting timeouts and sending headers properly. The Python ADBC packages now expose enums for driver-specific options. Also, the DBAPI layer now implements __del__ on objects to assist in cleaning up resources (though context managers are still recommended). The ADBC JDBC driver now exposes more metadata about constraints via getObjects. Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.2.0..apache-arrow-adbc-0.3.0 32 David Li 5 Dewey Dunnington 3 Matt Topol 1 Dave Hirschfeld 1 Jacob Marble 1 Tornike Gurgenidze 1 Will Jones Roadmap New extensions to the API specification have been proposed. These will be backwards-compatible and will become API specification 1.1.0. For details, see the mailing list discussion and the milestone tracking the proposed features. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow nanoarrow 0.1 Release</title><link href="https://arrow.apache.org/blog/2023/03/07/nanoarrow-0.1.0-release/" rel="alternate" type="text/html" title="Apache Arrow nanoarrow 0.1 Release" /><published>2023-03-07T00:00:00-05:00</published><updated>2023-03-07T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/03/07/nanoarrow-0.1.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/03/07/nanoarrow-0.1.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.1.0 release of
Apache Arrow nanoarrow. This initial release covers 31 resolved issues from
6 contributors.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>This initial release includes the following:</p>

<ul>
  <li>A <a href="#using-nanoarrow-in-c">C library</a> bundled as two files (nanoarrow.c
and nanoarrow.h).</li>
  <li>An <a href="#using-nanoarrow-in-c-r-and-python">R package</a> providing bindings for users
of the R programming language.</li>
</ul>

<p>See the
<a href="https://github.com/apache/arrow-nanoarrow/blob/apache-arrow-nanoarrow-0.1.0/CHANGELOG.md">Changelog</a>
for a detailed list of contributions leading up to the initial release.</p>

<h2 id="why-nanoarrow">Why nanoarrow?</h2>

<p>The adoption of the
<a href="https://arrow.apache.org/docs/format/CDataInterface.html">Arrow C Data Interface</a>
and the <a href="https://arrow.apache.org/docs/format/CStreamInterface.html">Arrow C Stream Interface</a>
since their
<a href="https://arrow.apache.org/blog/2020/05/03/introducing-arrow-c-data-interface/">introduction</a>
have been impressive and enthusiastic: not only have Arrow language bindings
adopted the standard to pass data among themselves, a growing number of
high-profile libraries like
<a href="https://gdal.org/development/rfc/rfc86_column_oriented_api.html">GDAL</a> and
<a href="https://duckdb.org/2021/12/03/duck-arrow.html">DuckDB</a> use the standard to
improve performance and provide an ABI-stable interface to tabular input and output.</p>

<p>GDAL and DuckDB are fortunate to have hard-working and forward-thinking maintainers
that were motivated to provide support for the Arrow C Data and Stream interfaces
even though the code to do so required an intimate knowledge of both the interface
and the columnar specification on which it is based.</p>

<p>The vision of <a href="https://github.com/apache/arrow-nanoarrow">nanoarrow</a>
is that it should be trivial for a library or application to implement an Arrow-based
interface: if a library consumes or produces tabular data, Arrow should be the
first place developers look. Developers shouldn’t have to be familiar with the
details of the columnar specification—nor should they have to take on any
build-time dependencies—to get started.</p>

<p>The <a href="https://arrow.apache.org/docs/format/ADBC.html">Arrow Database Connectivity (ADBC)</a>
specification is a good example of such a project, and provided a strong
motivator for the development of nanoarrow: at the heart of ADBC is the
idea of a core “driver manager” and database-specific drivers that are distributed
as independent C/C++/Python/R/Java/Go projects. At least in R and Python,
embedding an existing Arrow implementation (e.g., Arrow C++) is challenging
in the context of multiple packages intended to be loaded into the same process.
As of this writing, ADBC includes nanoarrow-based SQLite and PostgreSQL drivers
and a nanoarrow-based validation suite for drivers.</p>

<h2 id="using-nanoarrow-in-c">Using nanoarrow in C</h2>

<p>The nanoarrow C library is distributed as
<a href="https://github.com/apache/arrow-nanoarrow/tree/main/dist">two files (nanoarrow.h and nanoarrow.c)</a>
that can be copied and vendored into an existing code base. This results in
a static library of about 50  KB and builds in less than a second. Some features
that nanoarrow provides are:</p>

<ul>
  <li><a href="https://apache.github.io/arrow-nanoarrow/dev/c.html#creating-schemas">Helpers to create types, schemas, and metadata</a></li>
  <li><a href="https://apache.github.io/arrow-nanoarrow/dev/c.html#owning-growable-buffers">Growable buffers</a>,
including the option for custom allocators/deallocators.</li>
  <li><a href="https://apache.github.io/arrow-nanoarrow/dev/c.html#bitmap-utilities">Bitmap (i.e., bitpacked boolean) utilities</a></li>
  <li>An <a href="https://apache.github.io/arrow-nanoarrow/dev/c.html#creating-arrays">API for building arrays from buffers</a></li>
  <li>An <a href="https://apache.github.io/arrow-nanoarrow/dev/c.html#creating-arrays">API for building arrays element-wise</a></li>
  <li>An <a href="https://apache.github.io/arrow-nanoarrow/dev/c.html#reading-arrays">API to extract elements element-wise</a>
from an existing array.</li>
</ul>

<p>For example, one can build an integer array element-wise:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">"nanoarrow.h"</span><span class="cp">
</span>
<span class="kt">int</span> <span class="nf">make_simple_array</span><span class="p">(</span><span class="k">struct</span> <span class="n">ArrowArray</span><span class="o">*</span> <span class="n">array_out</span><span class="p">,</span> <span class="k">struct</span> <span class="n">ArrowSchema</span><span class="o">*</span> <span class="n">schema_out</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">struct</span> <span class="n">ArrowError</span> <span class="n">error</span><span class="p">;</span>
  <span class="n">array_out</span><span class="o">-&gt;</span><span class="n">release</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
  <span class="n">schema_out</span><span class="o">-&gt;</span><span class="n">release</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>

  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowArrayInitFromType</span><span class="p">(</span><span class="n">array_out</span><span class="p">,</span> <span class="n">NANOARROW_TYPE_INT32</span><span class="p">));</span>

  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowArrayStartAppending</span><span class="p">(</span><span class="n">array_out</span><span class="p">));</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowArrayAppendInt</span><span class="p">(</span><span class="n">array_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">));</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowArrayAppendInt</span><span class="p">(</span><span class="n">array_out</span><span class="p">,</span> <span class="mi">2</span><span class="p">));</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowArrayAppendInt</span><span class="p">(</span><span class="n">array_out</span><span class="p">,</span> <span class="mi">3</span><span class="p">));</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowArrayFinishBuilding</span><span class="p">(</span><span class="n">array_out</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">error</span><span class="p">));</span>

  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowSchemaInitFromType</span><span class="p">(</span><span class="n">schema_out</span><span class="p">,</span> <span class="n">NANOARROW_TYPE_INT32</span><span class="p">));</span>

  <span class="k">return</span> <span class="n">NANOARROW_OK</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Similarly, one can extract elements from an array:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp">
#include</span> <span class="cpf">"nanoarrow.h"</span><span class="cp">
</span>
<span class="kt">int</span> <span class="nf">print_simple_array</span><span class="p">(</span><span class="k">struct</span> <span class="n">ArrowArray</span><span class="o">*</span> <span class="n">array</span><span class="p">,</span> <span class="k">struct</span> <span class="n">ArrowSchema</span><span class="o">*</span> <span class="n">schema</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">struct</span> <span class="n">ArrowError</span> <span class="n">error</span><span class="p">;</span>
  <span class="k">struct</span> <span class="n">ArrowArrayView</span> <span class="n">array_view</span><span class="p">;</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowArrayViewInitFromSchema</span><span class="p">(</span><span class="o">&amp;</span><span class="n">array_view</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">error</span><span class="p">));</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">array_view</span><span class="p">.</span><span class="n">storage_type</span> <span class="o">!=</span> <span class="n">NANOARROW_TYPE_INT32</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"Array has storage that is not int32</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="kt">int</span> <span class="n">result</span> <span class="o">=</span> <span class="n">ArrowArrayViewSetArray</span><span class="p">(</span><span class="o">&amp;</span><span class="n">array_view</span><span class="p">,</span> <span class="n">array</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">error</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">result</span> <span class="o">!=</span> <span class="n">NANOARROW_OK</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">ArrowArrayViewReset</span><span class="p">(</span><span class="o">&amp;</span><span class="n">array_view</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">result</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">int64_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">array</span><span class="o">-&gt;</span><span class="n">length</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"%d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">ArrowArrayViewGetIntUnsafe</span><span class="p">(</span><span class="o">&amp;</span><span class="n">array_view</span><span class="p">,</span> <span class="n">i</span><span class="p">));</span>
  <span class="p">}</span>

  <span class="n">ArrowArrayViewReset</span><span class="p">(</span><span class="o">&amp;</span><span class="n">array_view</span><span class="p">);</span>
  <span class="k">return</span> <span class="n">NANOARROW_OK</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<h2 id="using-nanoarrow-in-c-r-and-python">Using nanoarrow in C++, R, and Python</h2>

<p>Recognizing that many projects for which nanoarrow may be useful will have
access a higher-level runtime than C, there are experiments to provide
these users with a minimal set of useful tools.</p>

<p>For C++ projects, an experimental
<a href="https://apache.github.io/arrow-nanoarrow/dev/cpp.html">“nanoarrow.hpp”</a>
interface provides <code class="language-plaintext highlighter-rouge">unique_ptr</code>-like wrappers for nanoarrow C objects to
reduce the verbosity of using the nanoarrow API. For example, the previous
<code class="language-plaintext highlighter-rouge">print_simple_array()</code> implementation would collapse to:</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp">
#include</span> <span class="cpf">"nanoarrow.hpp"</span><span class="cp">
</span>
<span class="kt">int</span> <span class="nf">print_simple_array2</span><span class="p">(</span><span class="k">struct</span> <span class="nc">ArrowArray</span><span class="o">*</span> <span class="n">array</span><span class="p">,</span> <span class="k">struct</span> <span class="nc">ArrowSchema</span><span class="o">*</span> <span class="n">schema</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">struct</span> <span class="nc">ArrowError</span> <span class="n">error</span><span class="p">;</span>
  <span class="n">nanoarrow</span><span class="o">::</span><span class="n">UniqueArrayView</span> <span class="n">array_view</span><span class="p">;</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowArrayViewInitFromSchema</span><span class="p">(</span><span class="n">array_view</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span> <span class="n">schema</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">error</span><span class="p">));</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowArrayViewSetArray</span><span class="p">(</span><span class="n">array_view</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span> <span class="n">array</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">error</span><span class="p">));</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int64_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">array</span><span class="o">-&gt;</span><span class="n">length</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"%d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">ArrowArrayViewGetIntUnsafe</span><span class="p">(</span><span class="n">array_view</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span> <span class="n">i</span><span class="p">));</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">NANOARROW_OK</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>For R packages, experimental
<a href="https://apache.github.io/arrow-nanoarrow/dev/r/index.html">R bindings</a> provide
a limited set of conversions between R vectors and Arrow arrays such that
R bindings for a library with an Arrow-based interface do not need to provide
this behaviour themselves. Additional features include printing and validating
the content of the C structures at the heart of the C Data and C Stream
interfaces to facilitate the development of bindings to Arrow-based libraries.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># install.packages("remotes")</span><span class="w">
</span><span class="n">remotes</span><span class="o">::</span><span class="n">install_github</span><span class="p">(</span><span class="s2">"apache/arrow-nanoarrow/r"</span><span class="p">,</span><span class="w"> </span><span class="n">build</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">nanoarrow</span><span class="p">)</span><span class="w">

</span><span class="n">as_nanoarrow_array</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">5</span><span class="p">)</span><span class="w">
</span><span class="c1">#&gt; &lt;nanoarrow_array int32[5]&gt;</span><span class="w">
</span><span class="c1">#&gt;  $ length    : int 5</span><span class="w">
</span><span class="c1">#&gt;  $ null_count: int 0</span><span class="w">
</span><span class="c1">#&gt;  $ offset    : int 0</span><span class="w">
</span><span class="c1">#&gt;  $ buffers   :List of 2</span><span class="w">
</span><span class="c1">#&gt;   ..$ :&lt;nanoarrow_buffer_validity[0 b] at 0x0&gt;</span><span class="w">
</span><span class="c1">#&gt;   ..$ :&lt;nanoarrow_buffer_data_int32[20 b] at 0x135d13c28&gt;</span><span class="w">
</span><span class="c1">#&gt;  $ dictionary: NULL</span><span class="w">
</span><span class="c1">#&gt;  $ children  : list()</span><span class="w">
</span></code></pre></div></div>

<p>A <a href="https://github.com/apache/arrow-nanoarrow/tree/main/python">Python package skeleton</a>
exists in the nanoarrow repository and further functionality may be added once
the C library interface has stabilized.</p>

<h2 id="try-nanoarrow">Try nanoarrow</h2>

<p>For any interested in giving nanoarrow a try, the easiest way to get started is to clone the
<a href="https://github.com/apache/arrow-nanoarrow/tree/apache-arrow-nanoarrow-0.1.0">nanoarrow repository from GitHub</a>
and build/modify the
<a href="https://github.com/apache/arrow-nanoarrow/tree/apache-arrow-nanoarrow-0.1.0/examples/cmake-minimal">minimal CMake build example</a>.
For applied usage, one can refer to the
<a href="https://github.com/apache/arrow-adbc/tree/main/c/driver/sqlite">ADBC SQLite driver</a>
and the <a href="https://github.com/apache/arrow-adbc/tree/main/c/driver/postgresql">ADBC PostgreSQL driver</a>.</p>

<h2 id="contributors">Contributors</h2>

<p>This initial release consists of contributions from 6 contributors in addition
to the invaluable advice and support of the Apache Arrow developer mailing list.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog -sn 8339114637919b661c1c8fae6764ceed532c935e..apache-arrow-nanoarrow-0.1.0 | grep -v "GitHub Actions"
   100  Dewey Dunnington
     7  David Li
     2  Dirk Eddelbuettel
     1  Dane Pitkin
     1  Jonathan Keane
     1  Joris Van den Bossche
</code></pre></div></div>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.1.0 release of Apache Arrow nanoarrow. This initial release covers 31 resolved issues from 6 contributors. Release Highlights This initial release includes the following: A C library bundled as two files (nanoarrow.c and nanoarrow.h). An R package providing bindings for users of the R programming language. See the Changelog for a detailed list of contributions leading up to the initial release. Why nanoarrow? The adoption of the Arrow C Data Interface and the Arrow C Stream Interface since their introduction have been impressive and enthusiastic: not only have Arrow language bindings adopted the standard to pass data among themselves, a growing number of high-profile libraries like GDAL and DuckDB use the standard to improve performance and provide an ABI-stable interface to tabular input and output. GDAL and DuckDB are fortunate to have hard-working and forward-thinking maintainers that were motivated to provide support for the Arrow C Data and Stream interfaces even though the code to do so required an intimate knowledge of both the interface and the columnar specification on which it is based. The vision of nanoarrow is that it should be trivial for a library or application to implement an Arrow-based interface: if a library consumes or produces tabular data, Arrow should be the first place developers look. Developers shouldn’t have to be familiar with the details of the columnar specification—nor should they have to take on any build-time dependencies—to get started. The Arrow Database Connectivity (ADBC) specification is a good example of such a project, and provided a strong motivator for the development of nanoarrow: at the heart of ADBC is the idea of a core “driver manager” and database-specific drivers that are distributed as independent C/C++/Python/R/Java/Go projects. At least in R and Python, embedding an existing Arrow implementation (e.g., Arrow C++) is challenging in the context of multiple packages intended to be loaded into the same process. As of this writing, ADBC includes nanoarrow-based SQLite and PostgreSQL drivers and a nanoarrow-based validation suite for drivers. Using nanoarrow in C The nanoarrow C library is distributed as two files (nanoarrow.h and nanoarrow.c) that can be copied and vendored into an existing code base. This results in a static library of about 50 KB and builds in less than a second. Some features that nanoarrow provides are: Helpers to create types, schemas, and metadata Growable buffers, including the option for custom allocators/deallocators. Bitmap (i.e., bitpacked boolean) utilities An API for building arrays from buffers An API for building arrays element-wise An API to extract elements element-wise from an existing array. For example, one can build an integer array element-wise: #include "nanoarrow.h" int make_simple_array(struct ArrowArray* array_out, struct ArrowSchema* schema_out) { struct ArrowError error; array_out-&gt;release = NULL; schema_out-&gt;release = NULL; NANOARROW_RETURN_NOT_OK(ArrowArrayInitFromType(array_out, NANOARROW_TYPE_INT32)); NANOARROW_RETURN_NOT_OK(ArrowArrayStartAppending(array_out)); NANOARROW_RETURN_NOT_OK(ArrowArrayAppendInt(array_out, 1)); NANOARROW_RETURN_NOT_OK(ArrowArrayAppendInt(array_out, 2)); NANOARROW_RETURN_NOT_OK(ArrowArrayAppendInt(array_out, 3)); NANOARROW_RETURN_NOT_OK(ArrowArrayFinishBuilding(array_out, &amp;error)); NANOARROW_RETURN_NOT_OK(ArrowSchemaInitFromType(schema_out, NANOARROW_TYPE_INT32)); return NANOARROW_OK; } Similarly, one can extract elements from an array: #include &lt;stdio.h&gt; #include "nanoarrow.h" int print_simple_array(struct ArrowArray* array, struct ArrowSchema* schema) { struct ArrowError error; struct ArrowArrayView array_view; NANOARROW_RETURN_NOT_OK(ArrowArrayViewInitFromSchema(&amp;array_view, schema, &amp;error)); if (array_view.storage_type != NANOARROW_TYPE_INT32) { printf("Array has storage that is not int32\n"); } int result = ArrowArrayViewSetArray(&amp;array_view, array, &amp;error); if (result != NANOARROW_OK) { ArrowArrayViewReset(&amp;array_view); return result; } for (int64_t i = 0; i &lt; array-&gt;length; i++) { printf("%d\n", (int)ArrowArrayViewGetIntUnsafe(&amp;array_view, i)); } ArrowArrayViewReset(&amp;array_view); return NANOARROW_OK; } Using nanoarrow in C++, R, and Python Recognizing that many projects for which nanoarrow may be useful will have access a higher-level runtime than C, there are experiments to provide these users with a minimal set of useful tools. For C++ projects, an experimental “nanoarrow.hpp” interface provides unique_ptr-like wrappers for nanoarrow C objects to reduce the verbosity of using the nanoarrow API. For example, the previous print_simple_array() implementation would collapse to: #include &lt;stdio.h&gt; #include "nanoarrow.hpp" int print_simple_array2(struct ArrowArray* array, struct ArrowSchema* schema) { struct ArrowError error; nanoarrow::UniqueArrayView array_view; NANOARROW_RETURN_NOT_OK(ArrowArrayViewInitFromSchema(array_view.get(), schema, &amp;error)); NANOARROW_RETURN_NOT_OK(ArrowArrayViewSetArray(array_view.get(), array, &amp;error)); for (int64_t i = 0; i &lt; array-&gt;length; i++) { printf("%d\n", (int)ArrowArrayViewGetIntUnsafe(array_view.get(), i)); } return NANOARROW_OK; } For R packages, experimental R bindings provide a limited set of conversions between R vectors and Arrow arrays such that R bindings for a library with an Arrow-based interface do not need to provide this behaviour themselves. Additional features include printing and validating the content of the C structures at the heart of the C Data and C Stream interfaces to facilitate the development of bindings to Arrow-based libraries. # install.packages("remotes") remotes::install_github("apache/arrow-nanoarrow/r", build = FALSE) library(nanoarrow) as_nanoarrow_array(1:5) #&gt; &lt;nanoarrow_array int32[5]&gt; #&gt; $ length : int 5 #&gt; $ null_count: int 0 #&gt; $ offset : int 0 #&gt; $ buffers :List of 2 #&gt; ..$ :&lt;nanoarrow_buffer_validity[0 b] at 0x0&gt; #&gt; ..$ :&lt;nanoarrow_buffer_data_int32[20 b] at 0x135d13c28&gt; #&gt; $ dictionary: NULL #&gt; $ children : list() A Python package skeleton exists in the nanoarrow repository and further functionality may be added once the C library interface has stabilized. Try nanoarrow For any interested in giving nanoarrow a try, the easiest way to get started is to clone the nanoarrow repository from GitHub and build/modify the minimal CMake build example. For applied usage, one can refer to the ADBC SQLite driver and the ADBC PostgreSQL driver. Contributors This initial release consists of contributions from 6 contributors in addition to the invaluable advice and support of the Apache Arrow developer mailing list. $ git shortlog -sn 8339114637919b661c1c8fae6764ceed532c935e..apache-arrow-nanoarrow-0.1.0 | grep -v "GitHub Actions" 100 Dewey Dunnington 7 David Li 2 Dirk Eddelbuettel 1 Dane Pitkin 1 Jonathan Keane 1 Joris Van den Bossche]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 0.2.0 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/02/16/adbc-0.2.0-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.2.0 (Libraries) Release" /><published>2023-02-16T00:00:00-05:00</published><updated>2023-02-16T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/02/16/adbc-0.2.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/02/16/adbc-0.2.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.2.0 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/2"><strong>34
resolved issues</strong></a> from <a href="#contributors"><strong>7 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version
0.2.0.  The <strong>API specification</strong> is versioned separately and is
at version 1.0.0.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.2.0/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>An ADBC Flight SQL driver in Go has been added.  This driver has
bindings for C/C++ and Python as well.  This lets us distribute and
update the driver independently of PyArrow, without causing conflicts
in dependencies like Protobuf and gRPC.</p>

<p>The Go database/sql interface now returns standard library <code class="language-plaintext highlighter-rouge">time.Time</code>
values for Arrow time and date columns.</p>

<p>The PostgreSQL driver has support for more types.</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.1.0..apache-arrow-adbc-0.2.0
    56	David Li
     8	Sutou Kouhei
     7	Matt Topol
     4	Jacob Marble
     2	Benson Muite
     1	Dave Hirschfeld
     1	Jianfeng Mao
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>New extensions to the API specification have been proposed.  These
will be backwards-compatible and will become API specification 1.1.0.
For details, see the <a href="https://lists.apache.org/thread/247z3t06mf132nocngc1jkp3oqglz7jp">mailing list discussion</a> and the
<a href="https://github.com/apache/arrow-adbc/milestone/3">milestone</a> tracking the proposed features.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.2.0 release of the Apache Arrow ADBC libraries. This covers includes 34 resolved issues from 7 distinct contributors. This is a release of the libraries, which are at version 0.2.0. The API specification is versioned separately and is at version 1.0.0. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights An ADBC Flight SQL driver in Go has been added. This driver has bindings for C/C++ and Python as well. This lets us distribute and update the driver independently of PyArrow, without causing conflicts in dependencies like Protobuf and gRPC. The Go database/sql interface now returns standard library time.Time values for Arrow time and date columns. The PostgreSQL driver has support for more types. Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.1.0..apache-arrow-adbc-0.2.0 56 David Li 8 Sutou Kouhei 7 Matt Topol 4 Jacob Marble 2 Benson Muite 1 Dave Hirschfeld 1 Jianfeng Mao Roadmap New extensions to the API specification have been proposed. These will be backwards-compatible and will become API specification 1.1.0. For details, see the mailing list discussion and the milestone tracking the proposed features. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">February 2023 Rust Apache Arrow Highlights</title><link href="https://arrow.apache.org/blog/2023/02/13/rust-32.0.0/" rel="alternate" type="text/html" title="February 2023 Rust Apache Arrow Highlights" /><published>2023-02-13T19:00:00-05:00</published><updated>2023-02-13T19:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/02/13/rust-32.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/02/13/rust-32.0.0/"><![CDATA[<!--

-->

<h1 id="introduction">Introduction</h1>

<p>With the recent release of <a href="https://crates.io/crates/arrow/32.0.0">32.0.0</a> of the Rust implementation of <a href="https://arrow.apache.org/">Apache Arrow</a>, it seemed timely to highlight some of the community works since the <a href="https://arrow.apache.org/blog/2022/06/16/rust-16.0.0/">last update</a>.</p>

<p>The most recent list of detailed changes can always be found in the <a href="https://github.com/apache/arrow-rs/blob/master/CHANGELOG.md">CHANGELOG</a>, with the full historical list available <a href="https://github.com/apache/arrow-rs/blob/master/CHANGELOG-old.md">here</a>.</p>

<h1 id="arrow">Arrow</h1>

<p><a href="https://crates.io/crates/arrow">arrow</a> and <a href="https://crates.io/crates/arrow-flight">arrow-flight</a> are native Rust implementations of Apache Arrow. Apache Arrow defines a language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware like CPUs and GPUs. The Arrow memory format also supports zero-copy reads for lightning-fast data access without serialization overhead.</p>

<p>The <a href="https://www.rust-lang.org/">Rust language</a> offers best in class performance,  memory safety, and the developer productivity of a modern programming language. These features make Rust an excellent choice for building modern high performance analytical systems. When combined, Rust and the Apache Arrow Ecosystem are a compelling toolkit for building the next generation of systems.</p>

<p>The <a href="https://github.com/apache/arrow-rs">repository</a> recently passed 1400 stars on github, and the community has been focused on performance and feature completeness.</p>

<p><strong>Major Highlights</strong></p>

<ul>
  <li><strong>New CSV and JSON Readers</strong>: The CSV and JSON readers have been revamped. Their performance has more than doubled, and they now support push-driven parsing facilitating async streaming decode from object storage.</li>
  <li><strong>Faster Build Times and Reduced Codegen</strong>: The <code class="language-plaintext highlighter-rouge">arrow</code> crate has been split into multiple smaller crates, and large kernels have been moved behind optional feature flags. These changes allow downstream projects to choose a smaller dependency footprint and build times, if desired.</li>
  <li><strong>Support for Copy-On-Write</strong>: Arrow arrays now support copy-on-write, via the <a href="https://docs.rs/arrow/32.0.0/arrow/array/struct.ArrayData.html#method.into_builder"><code class="language-plaintext highlighter-rouge">into_builder</code></a> methods</li>
  <li><strong>Comparable Row Format</strong>: <a href="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1/">Much faster multi-column Sorting and Grouping</a> is now possible with the the new spillable, comparable <a href="https://docs.rs/arrow-row/32.0.0/arrow_row/index.html">row-format</a></li>
  <li><strong>FlightSQL Support</strong>: <a href="https://arrow.apache.org/docs/format/FlightSql.html">FlightSQL</a> <a href="https://docs.rs/arrow-flight/32.0.0/arrow_flight/sql/index.html">support</a> has been expanded</li>
  <li><strong>Mid-Level Flight Client</strong>: A new <a href="https://docs.rs/arrow-flight/32.0.0/arrow_flight/client/struct.FlightClient.html">FlightClient</a> is available that handles lower level protocol details, and easier to use <a href="https://docs.rs/arrow-flight/32.0.0/arrow_flight/encode/struct.FlightDataEncoderBuilder.html">encoding</a> and <a href="https://docs.rs/arrow-flight/32.0.0/arrow_flight/decode/struct.FlightDataDecoder.html">decoding</a> APIs.</li>
  <li><strong>IPC File Compression</strong>: Arrow IPC file <a href="https://docs.rs/arrow-ipc/32.0.0/arrow_ipc/gen/Message/struct.CompressionType.html">compression</a> with ZSTD and LZ4 is now fully supported.</li>
  <li><strong>Full Decimal Support</strong>: 256-bit decimals and negative scales can be created and manipulated using many kernels, such as arithmetic.</li>
  <li><strong>Improved Dictionary Support</strong>: Dictionaries are now transparently supported in most kernels.</li>
  <li><strong>Improved Temporal Support</strong>: Timestamps with Timezones and other temporal types are supported in many more kernels.</li>
  <li><strong>Improved Generics</strong>: Improved generics allow writing code generic over all arrays, or all arrays with the same layout</li>
  <li><strong>Downcast Macros</strong>: Various <a href="https://docs.rs/arrow/32.0.0/arrow/macro.downcast_primitive_array.html">helper</a> <a href="https://docs.rs/arrow/32.0.0/arrow/macro.downcast_dictionary_array.html">macros</a> are now available to simplify dynamic dispatch to statically typed implementations.</li>
</ul>

<h1 id="parquet">Parquet</h1>

<p><a href="https://parquet.apache.org/">Apache Parquet</a> is an open source, column-oriented data file format designed for efficient data storage and retrieval. It provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk. The Apache Parquet implementation in Rust is one of the <a href="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/">fastest and most sophisticated</a> open source implementations available.</p>

<p><strong>Major Highlights</strong></p>

<ul>
  <li><strong>Arbitrarily Nested Schema</strong>: Arbitrarily nested schemas can be read to and written from arrow, as described in the series of <a href="https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/">blog posts</a> on the topic.</li>
  <li><strong>Predicate Pushdown</strong>: The <a href="https://docs.rs/parquet/32.0.0/parquet/arrow/index.html">arrow reader</a> now supports advanced predicate pushdown, including late materialization, as described <a href="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/">here</a>. See <a href="https://docs.rs/parquet/32.0.0/parquet/arrow/arrow_reader/struct.RowSelection.html">RowSelection</a> and <a href="https://docs.rs/parquet/32.0.0/parquet/arrow/arrow_reader/trait.ArrowPredicate.html">ArrowPredicate</a>.</li>
  <li><strong>Bloom Filter Support</strong>: Support for both <a href="https://docs.rs/parquet/32.0.0/parquet/bloom_filter/index.html">reading and writing bloom filters</a> has been added.</li>
  <li><strong>CLI Tools</strong>: additional <a href="https://github.com/apache/arrow-rs/tree/master/parquet/src/bin">CLI tools</a> have been added to introspect and manipulate parquet data.</li>
</ul>

<h1 id="object-store">Object Store</h1>

<p>Modern analytic workloads increasingly make use of blob storage facilities, such as S3, to store large volumes of queryable data. A native Rust object storage implementation that works well with the Rust Ecosystem in general, and the Arrow IO abstractions, is an important building block for many applications.
The <a href="https://docs.rs/object_store/latest/object_store/">object_store</a> crate was <a href="https://www.influxdata.com/blog/rust-object-store-donation/">donated to the Apache Arrow project in July 2022</a> to fill this need, and while it follows a separate release schedule than the <code class="language-plaintext highlighter-rouge">arrow</code> and <code class="language-plaintext highlighter-rouge">parquet</code> crates, it forms an integral part of the overarching Arrow IO story.</p>

<p>Recent improvements include the following:</p>

<p><strong>Major Highlights</strong></p>

<ul>
  <li><strong>Streaming Upload</strong>: Multipart uploads are now supported.</li>
  <li><strong>Minimised dependency footprint</strong>: Upstream SDKs are no longer used, improving consistency and reducing dependencies.</li>
  <li><strong>HTTP / WebDAV Support</strong>: Applications can read from arbitrary HTTP servers, with mutation and listing supported on WebDAV-compatible endpoints.</li>
  <li><strong>Configurable Networking</strong>: Socks proxies, and advanced HTTP client configuration are now supported.</li>
  <li><strong>Serializable Configuration</strong>: Configuration information can now be easily serialized and deserialized.</li>
  <li><strong>Additional Authentication</strong>: Additional authentication options are now available for the various cloud providers.</li>
</ul>

<h1 id="contributors">Contributors:</h1>

<p>While some open source software can be created mostly by a single contributor, we believe the greatest software with the largest impact and reach is built around its community. Thus, Arrow is proud to part of the <a href="https://www.apache.org/">Apache Software Foundation</a> and our releases both past and present are a result of our amazing community’s effort.</p>

<p>We would like to thank everyone who has contributed to the arrow-rs repository since the <code class="language-plaintext highlighter-rouge">16.0.0</code> release. Keep up the great work, and we look forward to continued improvements:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">% git shortlog -sn 16.0.0..32.0.0
   347  Raphael Taylor-Davies
   166  Liang-Chi Hsieh
    94  Andrew Lamb
    36  Remzi Yang
    30  Kun Liu
    21  Yang Jiang
    20  askoa
    17  dependabot[bot]
    15  Vrishabh
    12  Dan Harris
    12  Wei-Ting Kuo
    11  Daniël Heres
    11  Jörn Horstmann
     9  Brent Gardner
     9  Ian Alexander Joiner
     9  Jiayu Liu
     9  Martin Grigorov
     8  Palladium
     7  Jeffrey
     7  Marco Neumann
     6  Robert Pack
     6  Will Jones
     4  Andy Grove
     4  comphead
     3  Adrián Gallego Castellanos
     3  Markus Westerlind
     3  Quentin
     2  Alex Qyoun-ae
     2  Dmitry Patsura
     2  Frank
     2  Jiacai Liu
     2  Marc Garcia
     2  Marko Grujic
     2  Max Burke
     2  Your friendly neighborhood geek
     2  sachin agarwal
     1  Aarash Heydari
     1  Adam Gutglick
     1  Andrey Frolov
     1  Anthony Poncet
     1  Artjoms Iskovs
     1  Ben Kimock
     1  Brian Phillips
     1  Carol (Nichols || Goulding)
     1  Christian Salvati
     1  Dalton Modlin
     1  Daniel Martinez Maqueda
     1  Daniel Poelzleithner
     1  Davis Silverman
     1  Dhruv Vats
     1  Fabio Silva
     1  GeauxEric
     1  George Andronchik
     1  Ismail-Maj
     1  Ismaël Mejía
     1  JanKaul
     1  JasonLi
     1  Javier Goday
     1  Jayjeet Chakraborty
     1  Jean-Charles Campagne
     1  Jie Han
     1  John Hughes
     1  Jon Mease
     1  Kevin Lim
     1  Kohei Suzuki
     1  Konstantin Fastov
     1  Marius S
     1  Masato Kato
     1  Matthijs Brobbel
     1  Michael Edwards
     1  Pier-Olivier Thibault
     1  Remco Verhoef
     1  Rutvik Patel
     1  Sean Smith
     1  Sid
     1  Stanislav Lukeš
     1  Steve Vaughan
     1  Stuart Carnie
     1  Sumit
     1  Trent Feda
     1  Valeriy V. Vorotyntsev
     1  Wenjun L
     1  X
     1  aksharau
     1  bmmeijers
     1  chunshao.rcs
     1  jakevin
     1  kastolars
     1  nvartolomei
     1  xudong.w
     1  哇呜哇呜呀咦耶
     1  尹吉峰
</span></code></pre></div></div>

<h1 id="join-the-community">Join the community</h1>

<p>If you are interested in contributing to the Rust subproject in Apache Arrow, we encourage you to try out Arrow on some of your data, help
improve the documentation, or submit a PR. You can find a list of open issues
suitable for beginners <a href="https://github.com/apache/arrow-rs/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a>
and the full list <a href="https://github.com/apache/arrow-rs/issues">here</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[Introduction With the recent release of 32.0.0 of the Rust implementation of Apache Arrow, it seemed timely to highlight some of the community works since the last update. The most recent list of detailed changes can always be found in the CHANGELOG, with the full historical list available here. Arrow arrow and arrow-flight are native Rust implementations of Apache Arrow. Apache Arrow defines a language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware like CPUs and GPUs. The Arrow memory format also supports zero-copy reads for lightning-fast data access without serialization overhead. The Rust language offers best in class performance, memory safety, and the developer productivity of a modern programming language. These features make Rust an excellent choice for building modern high performance analytical systems. When combined, Rust and the Apache Arrow Ecosystem are a compelling toolkit for building the next generation of systems. The repository recently passed 1400 stars on github, and the community has been focused on performance and feature completeness. Major Highlights New CSV and JSON Readers: The CSV and JSON readers have been revamped. Their performance has more than doubled, and they now support push-driven parsing facilitating async streaming decode from object storage. Faster Build Times and Reduced Codegen: The arrow crate has been split into multiple smaller crates, and large kernels have been moved behind optional feature flags. These changes allow downstream projects to choose a smaller dependency footprint and build times, if desired. Support for Copy-On-Write: Arrow arrays now support copy-on-write, via the into_builder methods Comparable Row Format: Much faster multi-column Sorting and Grouping is now possible with the the new spillable, comparable row-format FlightSQL Support: FlightSQL support has been expanded Mid-Level Flight Client: A new FlightClient is available that handles lower level protocol details, and easier to use encoding and decoding APIs. IPC File Compression: Arrow IPC file compression with ZSTD and LZ4 is now fully supported. Full Decimal Support: 256-bit decimals and negative scales can be created and manipulated using many kernels, such as arithmetic. Improved Dictionary Support: Dictionaries are now transparently supported in most kernels. Improved Temporal Support: Timestamps with Timezones and other temporal types are supported in many more kernels. Improved Generics: Improved generics allow writing code generic over all arrays, or all arrays with the same layout Downcast Macros: Various helper macros are now available to simplify dynamic dispatch to statically typed implementations. Parquet Apache Parquet is an open source, column-oriented data file format designed for efficient data storage and retrieval. It provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk. The Apache Parquet implementation in Rust is one of the fastest and most sophisticated open source implementations available. Major Highlights Arbitrarily Nested Schema: Arbitrarily nested schemas can be read to and written from arrow, as described in the series of blog posts on the topic. Predicate Pushdown: The arrow reader now supports advanced predicate pushdown, including late materialization, as described here. See RowSelection and ArrowPredicate. Bloom Filter Support: Support for both reading and writing bloom filters has been added. CLI Tools: additional CLI tools have been added to introspect and manipulate parquet data. Object Store Modern analytic workloads increasingly make use of blob storage facilities, such as S3, to store large volumes of queryable data. A native Rust object storage implementation that works well with the Rust Ecosystem in general, and the Arrow IO abstractions, is an important building block for many applications. The object_store crate was donated to the Apache Arrow project in July 2022 to fill this need, and while it follows a separate release schedule than the arrow and parquet crates, it forms an integral part of the overarching Arrow IO story. Recent improvements include the following: Major Highlights Streaming Upload: Multipart uploads are now supported. Minimised dependency footprint: Upstream SDKs are no longer used, improving consistency and reducing dependencies. HTTP / WebDAV Support: Applications can read from arbitrary HTTP servers, with mutation and listing supported on WebDAV-compatible endpoints. Configurable Networking: Socks proxies, and advanced HTTP client configuration are now supported. Serializable Configuration: Configuration information can now be easily serialized and deserialized. Additional Authentication: Additional authentication options are now available for the various cloud providers. Contributors: While some open source software can be created mostly by a single contributor, we believe the greatest software with the largest impact and reach is built around its community. Thus, Arrow is proud to part of the Apache Software Foundation and our releases both past and present are a result of our amazing community’s effort. We would like to thank everyone who has contributed to the arrow-rs repository since the 16.0.0 release. Keep up the great work, and we look forward to continued improvements: % git shortlog -sn 16.0.0..32.0.0 347 Raphael Taylor-Davies 166 Liang-Chi Hsieh 94 Andrew Lamb 36 Remzi Yang 30 Kun Liu 21 Yang Jiang 20 askoa 17 dependabot[bot] 15 Vrishabh 12 Dan Harris 12 Wei-Ting Kuo 11 Daniël Heres 11 Jörn Horstmann 9 Brent Gardner 9 Ian Alexander Joiner 9 Jiayu Liu 9 Martin Grigorov 8 Palladium 7 Jeffrey 7 Marco Neumann 6 Robert Pack 6 Will Jones 4 Andy Grove 4 comphead 3 Adrián Gallego Castellanos 3 Markus Westerlind 3 Quentin 2 Alex Qyoun-ae 2 Dmitry Patsura 2 Frank 2 Jiacai Liu 2 Marc Garcia 2 Marko Grujic 2 Max Burke 2 Your friendly neighborhood geek 2 sachin agarwal 1 Aarash Heydari 1 Adam Gutglick 1 Andrey Frolov 1 Anthony Poncet 1 Artjoms Iskovs 1 Ben Kimock 1 Brian Phillips 1 Carol (Nichols || Goulding) 1 Christian Salvati 1 Dalton Modlin 1 Daniel Martinez Maqueda 1 Daniel Poelzleithner 1 Davis Silverman 1 Dhruv Vats 1 Fabio Silva 1 GeauxEric 1 George Andronchik 1 Ismail-Maj 1 Ismaël Mejía 1 JanKaul 1 JasonLi 1 Javier Goday 1 Jayjeet Chakraborty 1 Jean-Charles Campagne 1 Jie Han 1 John Hughes 1 Jon Mease 1 Kevin Lim 1 Kohei Suzuki 1 Konstantin Fastov 1 Marius S 1 Masato Kato 1 Matthijs Brobbel 1 Michael Edwards 1 Pier-Olivier Thibault 1 Remco Verhoef 1 Rutvik Patel 1 Sean Smith 1 Sid 1 Stanislav Lukeš 1 Steve Vaughan 1 Stuart Carnie 1 Sumit 1 Trent Feda 1 Valeriy V. Vorotyntsev 1 Wenjun L 1 X 1 aksharau 1 bmmeijers 1 chunshao.rcs 1 jakevin 1 kastolars 1 nvartolomei 1 xudong.w 1 哇呜哇呜呀咦耶 1 尹吉峰 Join the community If you are interested in contributing to the Rust subproject in Apache Arrow, we encourage you to try out Arrow on some of your data, help improve the documentation, or submit a PR. You can find a list of open issues suitable for beginners here and the full list here.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 11.0.0 Release</title><link href="https://arrow.apache.org/blog/2023/01/25/11.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 11.0.0 Release" /><published>2023-01-25T00:00:00-05:00</published><updated>2023-01-25T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/01/25/11.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/01/25/11.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 11.0.0 release. This covers
over 3 months of development work and includes <a href="https://github.com/apache/arrow/milestone/1?closed=1"><strong>423 resolved issues</strong></a>
from <a href="/release/11.0.0.html#contributors"><strong>95 distinct contributors</strong></a>. See the <a href="https://arrow.apache.org/install/">Install Page</a>
to learn how to get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/11.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 10.0.0 release, Ben Baumgold, Will Jones, Eric Patrick Hanson,
Curtis Vogt, Yang Jiang, Jarrett Revels, Raúl Cumplido, Jacob Wujciak,
Jie Wen and Brent Gardner have been invited to be committers.
Kun Liu have joined the Project Management Committee (PMC).</p>

<p>As per our newly started tradition of rotating the PMC chair once a year
Andrew Lamb was elected as the new PMC chair and VP.</p>

<p>Thanks for your contributions and participation in the project!</p>

<h2 id="columnar-format-notes">Columnar Format Notes</h2>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>

<p>In the C++/Python Flight clients, DoAction now properly streams the results, instead of blocking until the call finishes. Applications that did not consume the iterator before should fully consume the result. (<a href="https://github.com/apache/arrow/issues/15069">#15069</a>)</p>

<h2 id="c-notes">C++ notes</h2>
<ul>
  <li>It is now possible to specify alignment when making allocations with a MemoryPool <a href="https://github.com/apache/arrow/issues/33056">GH-33056</a></li>
  <li>It is now possible to run an ExecPlan without using any CPU threads</li>
  <li>Added kernel for slicing list values <a href="https://github.com/apache/arrow/issues/33168">GH-33168</a></li>
  <li>Added kernel for slicing binary arrays <a href="https://github.com/apache/arrow/issues/20357">GH-20357</a></li>
  <li>When comparing list arrays for equality the list field name is now ignored <a href="https://github.com/apache/arrow/issues/30519">GH-30519</a></li>
  <li>Add support for partitioning on columns that contain special characters <a href="https://github.com/apache/arrow/issues/33448">GH-33448</a></li>
  <li>Added a streaming reader for JSON <a href="https://github.com/apache/arrow/issues/33140">GH-33140</a></li>
  <li>Added support for incremental writes to the ORC writer <a href="https://github.com/apache/arrow/issues/33047">GH-33047</a></li>
  <li>Added support for casting decimal to string and writing decimal to CSV <a href="https://github.com/apache/arrow/issues/33002">GH-33002</a></li>
  <li>Fixed an assert in the scanner that would occur when batch_readahead was set to 0 <a href="https://github.com/apache/arrow/issues/15264">GH-15264</a></li>
  <li>Fixed bug where arrays with a null data buffer would not be accepted when imported via the C data API <a href="https://github.com/apache/arrow/issues/14875">GH-14875</a></li>
  <li>Fixed bug where arrays with a zero-case union data type would not be accepted when imported via the C data API <a href="https://github.com/apache/arrow/issues/14855">GH-14855</a></li>
  <li>Fixed bug where case_when could return incorrect values <a href="https://github.com/apache/arrow/issues/33382">GH-33382</a></li>
  <li>Fixed bug where RecordBatch::Equals was ignoring field names <a href="https://github.com/apache/arrow/issues/33285">GH-33285</a>
    <h2 id="c-notes-1">C# notes</h2>
  </li>
</ul>

<p>No major changes to C#.</p>

<h2 id="go-notes">Go notes</h2>
<ul>
  <li>Go’s benchmarks will now get added to <a href="https://conbench.ursa.dev">Conbench</a> alongside the benchmarks for other implementations <a href="https://github.com/apache/arrow/issues/32983">GH-32983</a></li>
  <li>Exposed FlightService_ServiceDesc and RegisterFlightServiceServer to allow easily incorporating a flight service into an existing gRPC server <a href="https://github.com/apache/arrow/issues/15174">GH-15174</a></li>
</ul>

<h3 id="arrow">Arrow</h3>
<ul>
  <li>Function <code class="language-plaintext highlighter-rouge">ApproxEquals</code> was implemented for scalar values <a href="https://github.com/apache/arrow/issues/29581">GH-29581</a></li>
  <li><code class="language-plaintext highlighter-rouge">UnmarshalJSON</code> for the <code class="language-plaintext highlighter-rouge">RecordBuilder</code> now properly handles extra unknown fields with complex/nested values <a href="https://github.com/apache/arrow/issues/31840">GH-31840</a></li>
  <li>Decimal128 and Decimal256 type support has been added to the CSV reader <a href="https://github.com/apache/arrow/issues/33111">GH-33111</a></li>
  <li>Fixed bug in <code class="language-plaintext highlighter-rouge">array.UnionBuilder</code> where <code class="language-plaintext highlighter-rouge">Len</code> method always returned 0 <a href="https://github.com/apache/arrow/issues/14775">GH-14775</a></li>
  <li>Fixed bug for handling slices of Map arrays when marshalling to JSON and for IPC <a href="https://github.com/apache/arrow/issues/14780">GH-14780</a></li>
  <li>Fixed memory leak when compressing IPC message body buffers <a href="https://github.com/apache/arrow/issues/14883">GH-14883</a></li>
  <li>Added the ability to easily append scalar values to array builders <a href="https://github.com/apache/arrow/issues/15005">GH-15005</a></li>
</ul>

<h4 id="compute">Compute</h4>
<ul>
  <li>Scalar binary (add/subtract/multiply/divide/etc.) and unary arithmetic (abs/neg/sqrt/sign/etc.) has been implemented for the compute package <a href="https://github.com/apache/arrow/issues/33086">GH-33086</a> this includes easy functions like <code class="language-plaintext highlighter-rouge">compute.Add</code> and <code class="language-plaintext highlighter-rouge">compute.Divide</code> etc.</li>
  <li>Scalar boolean functions like AND/OR/XOR/etc. have been implemented for compute <a href="https://github.com/apache/arrow/issues/33279">GH-33279</a></li>
  <li>Scalar comparison function kernels have been implemented for compute (equal/greater/greater_equal/less/less_equal) <a href="https://github.com/apache/arrow/issues/33308">GH-33308</a></li>
  <li>Scalar compute functions are compatible with dictionary encoded arrays by casting them to their value types <a href="https://github.com/apache/arrow/issues/33502">GH-33502</a></li>
</ul>

<h3 id="parquet">Parquet</h3>
<ul>
  <li>Panic when decoding a delta_bit_packed encoded column has been fixed <a href="https://github.com/apache/arrow/issues/33483">GH-33483</a></li>
  <li>Fixed memory leak from Allocator in <code class="language-plaintext highlighter-rouge">pqarrow.WriteArrowToColumn</code> <a href="https://github.com/apache/arrow/issues/14865">GH-14865</a></li>
  <li>Fixed <code class="language-plaintext highlighter-rouge">writer.WriteBatch</code> to properly handle writing encrypted parquet columns and no longer silently fail, but instead propagate an error <a href="https://github.com/apache/arrow/issues/14940">GH-14940</a></li>
</ul>

<h2 id="java-notes">Java notes</h2>
<ul>
  <li>Implement support for writing compressed files (<a href="https://github.com/apache/arrow/pull/15223">#15223</a>)</li>
  <li>Improve performance by short-circuiting null checks when comparing non null field types (<a href="https://github.com/apache/arrow/pull/15106">#15106</a>)</li>
  <li>Several enhancements to dictionary encoding (<a href="https://github.com/apache/arrow/pull/14891">#14891</a>, (<a href="https://github.com/apache/arrow/pull/14902">#14902</a>, (<a href="https://github.com/apache/arrow/pull/14874">#14874</a>)</li>
  <li>Extend Table to support additional vector types (<a href="https://github.com/apache/arrow/pull/14573">#14573</a>)</li>
  <li>Enhance and simplify handling of allocation management by integrating C Data into allocator hierarchy (<a href="https://github.com/apache/arrow/pull/14506">#14506</a>)</li>
  <li>Make ComplexCopier agnostic of specific implementation of MapWriter (<a href="https://github.com/apache/arrow/pull/14557">#14557</a>)</li>
  <li>Distribute Apple M1 compatible JNI libraries via mavencentral (<a href="https://github.com/apache/arrow/pull/14472">#14472</a>)</li>
  <li>Extend Table copy functionality, and support returning copies of individual vectors (<a href="https://github.com/apache/arrow/pull/14389">#14389</a>)</li>
</ul>

<h2 id="javascript-notes">JavaScript notes</h2>

<ul>
  <li>Bugfixes and dependency updates.</li>
  <li>Arrow now requires BigInt support. <a href="https://github.com/apache/arrow/pull/33682">GH-33681</a></li>
</ul>

<h2 id="python-notes">Python notes</h2>

<p>Compatibility notes:</p>

<ul>
  <li>PyArrow now requires pandas &gt;= 1.0 (<a href="https://issues.apache.org/jira/browse/ARROW-18173">ARROW-18173</a>)</li>
  <li>The <code class="language-plaintext highlighter-rouge">pyarrow.parquet.ParquetDataset()</code> class now by default uses the new Dataset API
under the hood (<code class="language-plaintext highlighter-rouge">use_legacy_dataset=False</code>). You can still pass
<code class="language-plaintext highlighter-rouge">use_legacy_dataset=True</code> to get the legacy implementation, but this option will be
removed in a next release
(<a href="https://issues.apache.org/jira/browse/ARROW-16728">ARROW-16728</a>).</li>
</ul>

<p>New features:</p>

<ul>
  <li>Added support for the <a href="https://data-apis.org/dataframe-protocol/latest/purpose_and_scope.html">DataFrame Interchange Protocol</a>
for <code class="language-plaintext highlighter-rouge">pyarrow.Table</code> (<a href="https://github.com/apache/arrow/issues/33346">GH-33346</a>).</li>
  <li>New kernels: <code class="language-plaintext highlighter-rouge">list_slice()</code> to slice each list element of a ListArray
returning a new ListArray (<a href="https://issues.apache.org/jira/browse/ARROW-17960">ARROW-17960</a>).</li>
  <li>A new <code class="language-plaintext highlighter-rouge">filter()</code> method on the Dataset class as additional API to filter a Dataset
before consuming it (<a href="https://issues.apache.org/jira/browse/ARROW-16616">ARROW-16616</a>).</li>
  <li>New <code class="language-plaintext highlighter-rouge">sort()</code> method for (Chunked)Array and <code class="language-plaintext highlighter-rouge">sort_by()</code> method for RecordBatch,
providing a convenience on top of the <code class="language-plaintext highlighter-rouge">sort_indices</code> kernel
(<a href="https://github.com/apache/arrow/issues/14778">GH-14778</a>), and a new
<code class="language-plaintext highlighter-rouge">Dataset.sort_by()</code> method (<a href="https://github.com/apache/arrow/issues/14975">GH-14975</a>).</li>
</ul>

<p>Other improvements:</p>

<ul>
  <li>Support for custom metadata of record batches in the IPC read and write APIs
(<a href="https://issues.apache.org/jira/browse/ARROW-16430">ARROW-16430</a>).</li>
  <li>Support URIs and the <code class="language-plaintext highlighter-rouge">filesystem</code> parameter in <code class="language-plaintext highlighter-rouge">pyarrow.parquet.ParquetFile</code>
(<a href="https://issues.apache.org/jira/browse/ARROW-18272">ARROW-18272</a>) and
<code class="language-plaintext highlighter-rouge">pyarrow.parquet.write_metadata</code>
(<a href="https://issues.apache.org/jira/browse/ARROW-18225">ARROW-18225</a>).</li>
  <li>When writing a dataset to IPC using <code class="language-plaintext highlighter-rouge">pyarrow.dataset.write_dataset()</code>, you can now
specify IPC specific options, such as compression
(<a href="https://issues.apache.org/jira/browse/ARROW-17991">ARROW-17991</a>)</li>
  <li>The <code class="language-plaintext highlighter-rouge">pyarrow.array()</code> function now allows to construct a MapArray from a sequence of
dicts (in addition to a sequence of tuples)
(<a href="https://issues.apache.org/jira/browse/ARROW-17832">ARROW-17832</a>).</li>
  <li>The <code class="language-plaintext highlighter-rouge">struct_field()</code> kernel now also accepts field names in addition to integer
indices (<a href="https://issues.apache.org/jira/browse/ARROW-17989">ARROW-17989</a>).</li>
  <li>Casting to string is now supported for duration (<a href="https://issues.apache.org/jira/browse/ARROW-15822">ARROW-15822</a>)
and decimal (<a href="https://issues.apache.org/jira/browse/ARROW-17458">ARROW-17458</a>) types,
which also means those can now be written to CSV.</li>
  <li>When writing to CSV, you can now specify the quoting style
(<a href="https://github.com/apache/arrow/issues/14755">GH-14755</a>).</li>
  <li>The <code class="language-plaintext highlighter-rouge">pyarrow.ipc.read_schema()</code> function now accepts a Message object
(<a href="https://issues.apache.org/jira/browse/ARROW-18423">ARROW-18423</a>).</li>
  <li>The Time32Scalar, Time64Scalar, Date32Scalar and Date64Scalar classes got a <code class="language-plaintext highlighter-rouge">.value</code>
attribute to access the underlying integer value, similar to the other date-time
related scalars (<a href="https://issues.apache.org/jira/browse/ARROW-18264">ARROW-18264</a>)</li>
  <li>Duration type is now supported in the hash kernels like <code class="language-plaintext highlighter-rouge">dictionary_encode</code>
(<a href="https://github.com/apache/arrow/issues/15226">GH-15226</a>).</li>
  <li>Fix silent overflow when converting <code class="language-plaintext highlighter-rouge">datetime.timedelta</code> to duration type
(<a href="https://issues.apache.org/jira/browse/ARROW-15026">ARROW-15026</a>).</li>
</ul>

<p>Relevant bug fixes:</p>

<ul>
  <li>Numpy conversion for ListArray is improved taking into account sliced offset, avoiding
increased memory usage (<a href="https://github.com/apache/arrow/issues/20512">GH-20512</a></li>
  <li>Fix writing files with multi-byte characters in file name
(<a href="https://issues.apache.org/jira/browse/ARROW-18123">ARROW-18123</a>).</li>
</ul>

<h2 id="r-notes">R notes</h2>
<ul>
  <li>map_batches() is lazy by default; it now returns a RecordBatchReader instead of a list of RecordBatch objects unless lazy = FALSE. <a href="https://github.com/apache/arrow/issues/14521">GH-14521</a></li>
  <li>A substantial reorganisation, rewrite of and addition to, many of the vignettes and README. <a href="https://github.com/apache/arrow/issues/14514">GH-14514</a></li>
</ul>

<p>For more on what’s in the 11.0.0 R package, see the <a href="/docs/r/news/">R changelog</a>.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<h3 id="ruby">Ruby</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Arrow::Table#save</code> now always returns self instead of the result of its <code class="language-plaintext highlighter-rouge">raw_records</code><a href="https://github.com/apache/arrow/issues/15289">GH-15289</a></li>
  <li>Improve the GC-related crash prevention system by guarding the shared objects from GC <a href="https://issues.apache.org/jira/browse/ARROW-18161">ARROW-18161</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">Arrow::HalfFloat</code> and <code class="language-plaintext highlighter-rouge">raw_records</code> support in <code class="language-plaintext highlighter-rouge">Arrow::HalfFloatArray</code> <a href="https://issues.apache.org/jira/browse/ARROW-18086">ARROW-18086</a></li>
  <li>Support omitting join keys in <code class="language-plaintext highlighter-rouge">Table#join</code> <a href="https://github.com/apache/arrow/issues/15084">GH-15084</a></li>
  <li>Add support for <code class="language-plaintext highlighter-rouge">Arrow::Table.load(uri, schema:)</code> <a href="https://issues.apache.org/jira/browse/ARROW-15206">ARROW-15206</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">Arrow::ColumnContainable#column_names</code> (e.g. <code class="language-plaintext highlighter-rouge">Arrow::Table#column_names</code>) <a href="https://github.com/apache/arrow/issues/15085">GH-15085</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">to_arrow_chunked_array</code> methods to support converting to <code class="language-plaintext highlighter-rouge">Arrow::ChunkedArray</code> <a href="https://issues.apache.org/jira/browse/ARROW-18405">ARROW-18405</a></li>
</ul>

<h3 id="c-glib">C GLib</h3>

<ul>
  <li>Add <code class="language-plaintext highlighter-rouge">garrow_chunked_array_new_empty()</code> <a href="https://github.com/apache/arrow/issues/33671">GH-33671</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">GArrowProjectNodeOptions</code> <a href="https://github.com/apache/arrow/issues/33670">GH-33670</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">GADatasetHivePartitioning</code> <a href="https://github.com/apache/arrow/issues/15257">GH-15257</a></li>
  <li>The signature of <code class="language-plaintext highlighter-rouge">garrow_execute_plain_wait()</code> was changed to take the <code class="language-plaintext highlighter-rouge">error</code> argument and to return the finished status <a href="https://github.com/apache/arrow/issues/15254">GH-15254</a></li>
  <li>Add support for half float <a href="https://github.com/apache/arrow/issues/15168">GH-15168</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">GADatasetFinishOptions</code> <a href="https://github.com/apache/arrow/issues/15146">GH-15146</a></li>
</ul>

<h2 id="rust-notes">Rust notes</h2>

<p>The Rust projects have moved to separate repositories outside the
main Arrow monorepo. For notes on the latest release of the Rust
implementation, see the latest <a href="https://github.com/apache/arrow-rs/tags">Arrow Rust changelog</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 11.0.0 release. This covers over 3 months of development work and includes 423 resolved issues from 95 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 10.0.0 release, Ben Baumgold, Will Jones, Eric Patrick Hanson, Curtis Vogt, Yang Jiang, Jarrett Revels, Raúl Cumplido, Jacob Wujciak, Jie Wen and Brent Gardner have been invited to be committers. Kun Liu have joined the Project Management Committee (PMC). As per our newly started tradition of rotating the PMC chair once a year Andrew Lamb was elected as the new PMC chair and VP. Thanks for your contributions and participation in the project! Columnar Format Notes Arrow Flight RPC notes In the C++/Python Flight clients, DoAction now properly streams the results, instead of blocking until the call finishes. Applications that did not consume the iterator before should fully consume the result. (#15069) C++ notes It is now possible to specify alignment when making allocations with a MemoryPool GH-33056 It is now possible to run an ExecPlan without using any CPU threads Added kernel for slicing list values GH-33168 Added kernel for slicing binary arrays GH-20357 When comparing list arrays for equality the list field name is now ignored GH-30519 Add support for partitioning on columns that contain special characters GH-33448 Added a streaming reader for JSON GH-33140 Added support for incremental writes to the ORC writer GH-33047 Added support for casting decimal to string and writing decimal to CSV GH-33002 Fixed an assert in the scanner that would occur when batch_readahead was set to 0 GH-15264 Fixed bug where arrays with a null data buffer would not be accepted when imported via the C data API GH-14875 Fixed bug where arrays with a zero-case union data type would not be accepted when imported via the C data API GH-14855 Fixed bug where case_when could return incorrect values GH-33382 Fixed bug where RecordBatch::Equals was ignoring field names GH-33285 C# notes No major changes to C#. Go notes Go’s benchmarks will now get added to Conbench alongside the benchmarks for other implementations GH-32983 Exposed FlightService_ServiceDesc and RegisterFlightServiceServer to allow easily incorporating a flight service into an existing gRPC server GH-15174 Arrow Function ApproxEquals was implemented for scalar values GH-29581 UnmarshalJSON for the RecordBuilder now properly handles extra unknown fields with complex/nested values GH-31840 Decimal128 and Decimal256 type support has been added to the CSV reader GH-33111 Fixed bug in array.UnionBuilder where Len method always returned 0 GH-14775 Fixed bug for handling slices of Map arrays when marshalling to JSON and for IPC GH-14780 Fixed memory leak when compressing IPC message body buffers GH-14883 Added the ability to easily append scalar values to array builders GH-15005 Compute Scalar binary (add/subtract/multiply/divide/etc.) and unary arithmetic (abs/neg/sqrt/sign/etc.) has been implemented for the compute package GH-33086 this includes easy functions like compute.Add and compute.Divide etc. Scalar boolean functions like AND/OR/XOR/etc. have been implemented for compute GH-33279 Scalar comparison function kernels have been implemented for compute (equal/greater/greater_equal/less/less_equal) GH-33308 Scalar compute functions are compatible with dictionary encoded arrays by casting them to their value types GH-33502 Parquet Panic when decoding a delta_bit_packed encoded column has been fixed GH-33483 Fixed memory leak from Allocator in pqarrow.WriteArrowToColumn GH-14865 Fixed writer.WriteBatch to properly handle writing encrypted parquet columns and no longer silently fail, but instead propagate an error GH-14940 Java notes Implement support for writing compressed files (#15223) Improve performance by short-circuiting null checks when comparing non null field types (#15106) Several enhancements to dictionary encoding (#14891, (#14902, (#14874) Extend Table to support additional vector types (#14573) Enhance and simplify handling of allocation management by integrating C Data into allocator hierarchy (#14506) Make ComplexCopier agnostic of specific implementation of MapWriter (#14557) Distribute Apple M1 compatible JNI libraries via mavencentral (#14472) Extend Table copy functionality, and support returning copies of individual vectors (#14389) JavaScript notes Bugfixes and dependency updates. Arrow now requires BigInt support. GH-33681 Python notes Compatibility notes: PyArrow now requires pandas &gt;= 1.0 (ARROW-18173) The pyarrow.parquet.ParquetDataset() class now by default uses the new Dataset API under the hood (use_legacy_dataset=False). You can still pass use_legacy_dataset=True to get the legacy implementation, but this option will be removed in a next release (ARROW-16728). New features: Added support for the DataFrame Interchange Protocol for pyarrow.Table (GH-33346). New kernels: list_slice() to slice each list element of a ListArray returning a new ListArray (ARROW-17960). A new filter() method on the Dataset class as additional API to filter a Dataset before consuming it (ARROW-16616). New sort() method for (Chunked)Array and sort_by() method for RecordBatch, providing a convenience on top of the sort_indices kernel (GH-14778), and a new Dataset.sort_by() method (GH-14975). Other improvements: Support for custom metadata of record batches in the IPC read and write APIs (ARROW-16430). Support URIs and the filesystem parameter in pyarrow.parquet.ParquetFile (ARROW-18272) and pyarrow.parquet.write_metadata (ARROW-18225). When writing a dataset to IPC using pyarrow.dataset.write_dataset(), you can now specify IPC specific options, such as compression (ARROW-17991) The pyarrow.array() function now allows to construct a MapArray from a sequence of dicts (in addition to a sequence of tuples) (ARROW-17832). The struct_field() kernel now also accepts field names in addition to integer indices (ARROW-17989). Casting to string is now supported for duration (ARROW-15822) and decimal (ARROW-17458) types, which also means those can now be written to CSV. When writing to CSV, you can now specify the quoting style (GH-14755). The pyarrow.ipc.read_schema() function now accepts a Message object (ARROW-18423). The Time32Scalar, Time64Scalar, Date32Scalar and Date64Scalar classes got a .value attribute to access the underlying integer value, similar to the other date-time related scalars (ARROW-18264) Duration type is now supported in the hash kernels like dictionary_encode (GH-15226). Fix silent overflow when converting datetime.timedelta to duration type (ARROW-15026). Relevant bug fixes: Numpy conversion for ListArray is improved taking into account sliced offset, avoiding increased memory usage (GH-20512 Fix writing files with multi-byte characters in file name (ARROW-18123). R notes map_batches() is lazy by default; it now returns a RecordBatchReader instead of a list of RecordBatch objects unless lazy = FALSE. GH-14521 A substantial reorganisation, rewrite of and addition to, many of the vignettes and README. GH-14514 For more on what’s in the 11.0.0 R package, see the R changelog. Ruby and C GLib notes Ruby Arrow::Table#save now always returns self instead of the result of its raw_recordsGH-15289 Improve the GC-related crash prevention system by guarding the shared objects from GC ARROW-18161 Add Arrow::HalfFloat and raw_records support in Arrow::HalfFloatArray ARROW-18086 Support omitting join keys in Table#join GH-15084 Add support for Arrow::Table.load(uri, schema:) ARROW-15206 Add Arrow::ColumnContainable#column_names (e.g. Arrow::Table#column_names) GH-15085 Add to_arrow_chunked_array methods to support converting to Arrow::ChunkedArray ARROW-18405 C GLib Add garrow_chunked_array_new_empty() GH-33671 Add GArrowProjectNodeOptions GH-33670 Add GADatasetHivePartitioning GH-15257 The signature of garrow_execute_plain_wait() was changed to take the error argument and to return the finished status GH-15254 Add support for half float GH-15168 Add GADatasetFinishOptions GH-15146 Rust notes The Rust projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Rust implementation, see the latest Arrow Rust changelog.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>