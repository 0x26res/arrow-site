<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.4">Jekyll</generator><link href="https://arrow.apache.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arrow.apache.org/" rel="alternate" type="text/html" /><updated>2019-11-01T17:22:27-04:00</updated><id>https://arrow.apache.org/feed.xml</id><title type="html">Apache Arrow</title><subtitle>Apache Arrow is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, C++, C#, Go, Java, JavaScript, MATLAB, Python, R, Ruby, and Rust.</subtitle><entry><title type="html">Apache Arrow Flightの紹介：高速データトランスポートフレームワーク</title><link href="https://arrow.apache.org/blog/2019/10/13/introducing-arrow-flight-japanese/" rel="alternate" type="text/html" title="Apache Arrow Flightの紹介：高速データトランスポートフレームワーク" /><published>2019-10-13T02:00:00-04:00</published><updated>2019-10-13T02:00:00-04:00</updated><id>https://arrow.apache.org/blog/2019/10/13/introducing-arrow-flight-japanese</id><content type="html" xml:base="https://arrow.apache.org/blog/2019/10/13/introducing-arrow-flight-japanese/">&lt;!--

--&gt;

&lt;p&gt;&lt;a href=&quot;/blog/2019/10/13/introducing-arrow-flight/&quot;&gt;原文（English）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;この1.5年、Apache Arrowコミュニティーは&lt;strong&gt;Flight&lt;/strong&gt;の設計と実装を進めてきました。Flightは高速なデータトランスポートを実現するための新しいクライアント・サーバー型のフレームワークです。Flightを使うとネットワーク越しに大きなデータセットを送る処理を簡単に実現できます。Flightは特定用途向けに設計されたものではないため、幅広い用途で利用できます。&lt;/p&gt;

&lt;p&gt;Flightの実装は、まず、&lt;a href=&quot;https://grpc.io/&quot;&gt;gRPC&lt;/a&gt;を使ったArrow列指向フォーマット（つまり「Arrowレコードバッチ」）のトランスポートの最適化に注力しました。gRPCはGoogleが開発しているHTTP/2ベースのRPCライブラリー・フレームワークで、広く利用されています。gRPCも特定用途向けではなく幅広い用途で使えるように設計されています。これまでFlightをgRPCベースで実装することに注力してきましたが、gRPCでだけ使えるようにしたいわけではありません。&lt;/p&gt;

&lt;p&gt;Flightと他のデータトランスポートフレームワークとの大きな違いは並列転送機能です。クライアントとサーバークラスター間で同時にデータをストリームで転送できます。この機能により、簡単にスケーラブルなデータサービスを開発できます。スケーラブルなデータサービスとはクライアント数が増えても大丈夫なサービスです。&lt;/p&gt;

&lt;p&gt;Apache Arrow 0.15.0でC++（Pytonバインディングあり）とJavaでFlightを使えるようになっています。 現時点ではベータユーザー向けです。ベータユーザーとはFlight内部の低レベルの改良によりAPIやプロトコルが変わっても適応できるユーザーのことです。&lt;/p&gt;

&lt;h2 id=&quot;モチベーション&quot;&gt;モチベーション&lt;/h2&gt;

&lt;p&gt;多くの人がネットワーク越しに大きなデータセットにアクセスすることに関して困っています。リモートのデータサービスからデータセットを読むためのさまざまな転送プロトコルやツールがたくさんあります。たとえばODBCやJDBCです。この10年、ファイルベースでデータを保管することが多くなりました。このときにはCSVやAvroやParquetといったフォーマットがよく使われます。しかし、この方法ではデシリアライズする前に生データをローカルのホストに転送しなければいけないという問題があります。&lt;/p&gt;

&lt;p&gt;Apache Arrowの初期からやってきた作業によりさまざまな方法でデータトランスポートを加速できます。&lt;a href=&quot;https://github.com/apache/arrow/blob/master/docs/source/format/Columnar.rst&quot;&gt;Arrow列指向フォーマット&lt;/a&gt;には次の重要な機能があります。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;表形式データの「転送用の」表現です。この表現はデータ受信側でデシリアライズが必要ありません。&lt;/li&gt;
  &lt;li&gt;標準で「バッチをストリーム送信」するためのモードがあります。このモードでは、大きなデータセットを複数の行ごとにまとめて転送します。（Arrowの用語では「レコードバッチ」と呼んでいます。）この記事では「データストリーム」について話します。データストリームとはApache Arrowプロジェクトのバイナリープロトコルを使った一連のArrowレコードバッチです。&lt;/li&gt;
  &lt;li&gt;このフォーマットはプログラミング言語に依存していません。このフォーマットは現在11のプログラミング言語がサポートしています。サポートしているプログラミング言語は増え続けています。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ODBCのような標準的なプロトコルの各実装は、通常、それぞれ独自の転送用バイナリープロトコルを実装します。これらのプロトコルは各ライブラリーの公開インターフェイスの表現と相互に変換しなければいけません。ODBC・JDBCライブラリーのパフォーマンスは場合によって大きく異なります。&lt;/p&gt;

&lt;p&gt;私たちのFlightの設計で目指していることは、データサービス用の新しいプロトコルを作ることです。このプロトコルは転送用のデータ表現にも開発者向けの公開APIにもArrow列指向フォーマットを使います。こうすることで、データトランスポート関連のシリアライズコストを減らし、分散データシステム全体を効率化できます。さらに、すでに別の用途にApache Arrowを使っているシステム間では非常に効率的にデータをやりとりできます。&lt;/p&gt;

&lt;h2 id=&quot;flightの基礎&quot;&gt;Flightの基礎&lt;/h2&gt;

&lt;p&gt;Arrow Flightライブラリーはデータストリームを送受信できるサービスを実装するための開発者向けフレームワークを提供します。Flightサーバーは次の基本的なリクエストをサポートしています。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Handshake&lt;/strong&gt;：クライアントが認証済みかを確認するシンプルなリクエスト。いくつかのケースでは、以降のリクエストのために実装定義のセッショントークンを確立します。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ListFlights&lt;/strong&gt;：利用可能なデータストリームのリストを返します。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;GetSchema&lt;/strong&gt;：データストリームのスキーマを返します。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;GetFlightInfo&lt;/strong&gt;：対象のデータセット用の「アクセスプラン」を返します。複数のデータストリームを消費しなければいけないかもしれません。このリクエストにはシリアライズしたカスタムコマンドを含めることができます。たとえば、アプリケーション固有のパラメーターを含めることができます。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;DoGet&lt;/strong&gt;：クライアントにデータストリームを送信します。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;DoPut&lt;/strong&gt;: クライアントからデータストリームを受信します。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;DoAction&lt;/strong&gt;：実装依存のアクションを実行し、結果を返します。つまり、一般的な関数呼び出しです。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ListActions&lt;/strong&gt;：利用可能なアクションの種類を返します。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;gRPCの「双方向の」ストリーミングサポート（&lt;a href=&quot;https://grpc.io/docs/guides/concepts/&quot;&gt;HTTP/2ストリーミング&lt;/a&gt;上に実装されています）を活用して、リクエスト処理中でもデータとメタデータをクライアント・サーバー間でやりとりできます。&lt;/p&gt;

&lt;p&gt;単純なFlightの構成は1台のサーバーとそのサーバーに接続し&lt;code class=&quot;highlighter-rouge&quot;&gt;DoGet&lt;/code&gt;リクエストをするクライアントという構成です。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/img/20191014_flight_simple.png&quot; alt=&quot;Flight Simple Architecture&quot; width=&quot;50%&quot; class=&quot;img-responsive&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;grpcごしのデータスループットの最適化&quot;&gt;gRPCごしのデータスループットの最適化&lt;/h2&gt;

&lt;p&gt;gRPCのような汎用メッセージングライブラリーを使うことには多くの利点があります。汎用ライブラリーはすでに多数の問題を解決しているからです。gRPCの場合はGoogleが多数の問題を解決していました。しかし、大きなデータセットのトランスポート性能を改善するためにいくつかの処理を改善する必要がありました。多くのgRPCユーザーは比較的小さなメッセージしか扱っていないからです。&lt;/p&gt;

&lt;p&gt;一番よくサポートされているgRPCを使う方法はサービスを&lt;a href=&quot;https://github.com/protocolbuffers/protobuf&quot;&gt;Protocol Buffers&lt;/a&gt;（「Protobuf」と呼ばれることもあります）の&lt;code class=&quot;highlighter-rouge&quot;&gt;.proto&lt;/code&gt;ファイルで定義する方法です。gRPCのProtobufプラグインはgRPCサービスのスタブを生成します。このスタブを使ってアプリケーションを実装します。RPCコマンドとデータメッセージは&lt;a href=&quot;https://developers.google.com/protocol-buffers/docs/encoding&quot;&gt;Protobufワイヤーフォーマット&lt;/a&gt;を使ってシリアライズします。Flightでは「普通のgRPCとProtocol Buffers」を使っているので、Arrow列指向フォーマットのことを知らないgRPCクライアントでもFlightサービスとやりとりできますし、Arrowデータの中身を気にせずに処理できます。&lt;/p&gt;

&lt;p&gt;Flightの中の主要なデータ関連のProtobufの型は&lt;code class=&quot;highlighter-rouge&quot;&gt;FlightData&lt;/code&gt;と呼ばれています。一般的にProtobufメッセージの読み書きにはコストがかかります。そのため、C++でもJavaでもgRPCにいくつか次のような低レベルの最適化を実装しています。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;FlightData&lt;/code&gt;用のProtobufワイヤーフォーマットを生成します。&lt;code class=&quot;highlighter-rouge&quot;&gt;FlightData&lt;/code&gt;には送信対象のArrowレコードバッチが含まれていますが、メモリーコピー・シリアライズ処理は一切ありません。&lt;/li&gt;
  &lt;li&gt;Protobufで表現された&lt;code class=&quot;highlighter-rouge&quot;&gt;FlightData&lt;/code&gt;からメモリーコピー・デシリアライズ処理なしでArrowレコードバッチを再構築できます。実際には、Protocol Bufersライブラリーにエンコードされたデータペイロードを触らせないようにしています。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Protobufを使うがProtobufのメッセージパースのオーバーヘッドはなくしたいという両立できない2つのことを両立させようとしているということです。Flight実装は上述の最適化をして高速化しています。素のgRPCクライアントでもFlightサービスとやりとりできますが、素のgRPCクライアントにはこのような最適化はないので、Protobufライブラリーを使って&lt;code class=&quot;highlighter-rouge&quot;&gt;FlightData&lt;/code&gt;をデシリアライズすることになります。そのため、素のgRPCクライアントを使うといくらか性能が落ちます。&lt;/p&gt;

&lt;p&gt;FlightのC++実装でのデータスループットベンチマークの結果での絶対的な性能ですが、どちらもローカルホストで動いているサーバー・クライアント間のTCPスループットは2-3GB/sを上回っていました。ただし、TLSは無効にした状態です。このベンチマークは約4秒で12GBのデータを転送できることを示しています。&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;./arrow-flight-benchmark &lt;span class=&quot;nt&quot;&gt;--records_per_stream&lt;/span&gt; 100000000
Bytes &lt;span class=&quot;nb&quot;&gt;read&lt;/span&gt;: 12800000000
Nanos: 3900466413
Speed: 3129.63 MB/s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;この結果から次の2つのことを言えます。1つはFlightとgRPCを使うと相対的に小さなオーバーヘッドはあるということです。もう1つは多くの実際のFlightアプリケーションではネットワークの帯域がボトルネックになりそうということです。&lt;/p&gt;

&lt;h2 id=&quot;水平方向のスケーラビリティ並列データアクセスとパーティション化したデータアクセス&quot;&gt;水平方向のスケーラビリティ：並列データアクセスとパーティション化したデータアクセス&lt;/h2&gt;

&lt;p&gt;分散型のデータベースシステムの多くは「コーディネーター」を通してクライアントのリクエストを処理するアーキテクチャーパターンを使っています。クライアントへのデータセットを複数回転送するという明らかに効率に課題がある点はさておき、巨大なデータセットへのアクセスに対するスケーラビリティの問題もあります。&lt;/p&gt;

&lt;p&gt;Flightで次のようなシステムを作れるようにしました。それはこのようなボトルネックに取り組まずに水平方向にスケーラブルなデータサービスを作れるシステムです。&lt;code class=&quot;highlighter-rouge&quot;&gt;GetFlightInfo&lt;/code&gt; RPCを使ったクライアントのリクエストは &lt;strong&gt;エンドポイント&lt;/strong&gt; のリストを返します。返ってくる各エンドポイントにはサーバーの位置と &lt;strong&gt;チケット&lt;/strong&gt; の情報が入っています。チケットはデータセットの一部を取得する&lt;code class=&quot;highlighter-rouge&quot;&gt;DoGet&lt;/code&gt;リクエストに入れてサーバーに送ります。データセット全体にアクセスするためにはすべてのエンドポイントを処理する必要があります。どのエンドポイントのFlightのストリームから処理しなければいけないということはありません。どのエンドポイントのFlightのストリームから処理しても構いません。しかし、特定の順序で処理するための仕組みは用意しています。その仕組みとはアプリケーション固有のメタデータを使えるという仕組みです。順序の情報はメタデータで表現できます。&lt;/p&gt;

&lt;p&gt;この複数エンドポイントパターンにはたくさんの利点があります。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;複数のクライアントが複数のエンドポイントから並列にデータを読み込めます。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;GetFlightInfo&lt;/code&gt;「プランニング」リクエストを提供するサービスは兄弟サービスに処理を移譲できます。これにより、データの局所性の利点を得られたり、単純にロードバランスしやすくなったりします。&lt;/li&gt;
  &lt;li&gt;分散クラスター中のノードは異なる役割を引き受けることができます。たとえば、クラスター内の一部のノードはクエリープランニングに責任を持つかもしれません。一方、他のノードはデータストリームリクエスト（&lt;code class=&quot;highlighter-rouge&quot;&gt;DoGet&lt;/code&gt;または&lt;code class=&quot;highlighter-rouge&quot;&gt;DoPut&lt;/code&gt;）だけを処理するかもしれません。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;次の図はサービスの役割を分けた複数ノードアーキテクチャーの例です。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/img/20191014_flight_complex.png&quot; alt=&quot;Flight Complex Architecture&quot; width=&quot;60%&quot; class=&quot;img-responsive&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;アクションアプリケーション固有のロジックでflightを拡張&quot;&gt;アクション：アプリケーション固有のロジックでFlightを拡張&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;GetFlightInfo&lt;/code&gt;リクエストはデータセットをリクエストするときにシリアライズしたコマンドを中身を気にせずに送ることができますが、クライアントはデータストリームの送受信以外の操作をサーバーに依頼できなければいけないかもしれません。たとえば、クライアントは特定のデータセットをメモリー上に「ピン止め」することを要求するかもしれません。ピン止めすることで他のクライアントからの後続のリクエストを高速に処理できます。&lt;/p&gt;

&lt;p&gt;Flightサービスは追加で「アクション」を定義できます。&lt;code class=&quot;highlighter-rouge&quot;&gt;DoAction&lt;/code&gt; RPCでアクションを実行できます。アクションリクエストには実行したいアクションの名前と追加情報が入っています。追加情報は省略可能です。アクションの結果はgRPCストリームです。このgRPCストリーム中には任意の結果を入れられます。&lt;/p&gt;

&lt;p&gt;いくつかアクションの例を紹介します。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;メタデータを見つけるアクション。組み込みの&lt;code class=&quot;highlighter-rouge&quot;&gt;ListFlights&lt;/code&gt; RPCでも提供されている機能ですが、&lt;code class=&quot;highlighter-rouge&quot;&gt;ListFlights&lt;/code&gt;の機能で不十分な場合はアクションで実現できます。&lt;/li&gt;
  &lt;li&gt;セッション固有のパラメーターを設定するアクション。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;サーバーはアクションを1つも実装しなくてもよいことに注意してください。また、アクションは結果を返さなくてもよいです。&lt;/p&gt;

&lt;h2 id=&quot;暗号化と認証&quot;&gt;暗号化と認証&lt;/h2&gt;

&lt;p&gt;Flightは組み込みで暗号化をサポートしています。gRPCの組み込みのTLS/OpenSSLの機能を使っています。&lt;/p&gt;

&lt;p&gt;クライアント側・サーバー側ともに拡張可能な認証ハンドラーがあります。この認証ハンドラーを使えば、ユーザー名とパスワードのようなシンプルな認証スキーマも使えますし、ケルベロスのような複雑な認証も使えます。Flightプロトコルには組み込みの&lt;code class=&quot;highlighter-rouge&quot;&gt;BasicAuth&lt;/code&gt;機能がついています。そのため、追加の開発なしでそのままユーザー名とパスワードの認証を実現できます。&lt;/p&gt;

&lt;h2 id=&quot;ミドルウェアとトレース&quot;&gt;ミドルウェアとトレース&lt;/h2&gt;

&lt;p&gt;gRPCには「インターセプター」というコンセプトがあります。インターセプターを使うと開発者が定義した「ミドルウェア」を開発できます。ミドルウェアを使うと届いたリクエストと送るリクエストに介在することができます。このような処理をするフレームワークに&lt;a href=&quot;https://opentracing.io/&quot;&gt;OpenTracing&lt;/a&gt;があります。&lt;/p&gt;

&lt;p&gt;ミドルウェアの機能は最近Flightに追加された機能です。そのため、今のところはmasterブランチでしか使えません。&lt;/p&gt;

&lt;h2 id=&quot;grpcを使っているがgrpcだけではない&quot;&gt;gRPCを使っているがgRPCだけではない&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;DoGet&lt;/code&gt;リクエストでサーバーの位置を指定する方法にはRFC 3986準拠のURIを使っています。たとえば、TLSを使ったgRPCは&lt;code class=&quot;highlighter-rouge&quot;&gt;grpc+tls://$HOST:$PORT&lt;/code&gt;というように指定します。&lt;/p&gt;

&lt;p&gt;Flightサーバーの「コマンド」レイヤーにgRPCを使っているのは妥当だと思っていますが、&lt;a href=&quot;https://en.wikipedia.org/wiki/Remote_direct_memory_access&quot;&gt;RDMA&lt;/a&gt;のようなTCP以外のデータトランスポート層もサポートしたくなるかもしれません。設計・開発時間が必要になりますが、おそらく、TCP以外のプロトコル上でデータを転送するときでもgRPCを使えるでしょう。&lt;/p&gt;

&lt;h2 id=&quot;はじめかたと今後の話&quot;&gt;はじめかたと今後の話&lt;/h2&gt;

&lt;p&gt;Flightユーザー向けのドキュメントは作成中です。しかし、このライブラリーはベータユーザー向けには十分に使い物になります。ベータユーザーとは今後1年で発生するだろう軽微なAPI・プロトコルの変更に耐えられるユーザーです。&lt;/p&gt;

&lt;p&gt;Flightをためす簡単な方法はPython APIを使う方法です。なぜならカスタムサーバーもカスタムクライアントもすべてPythonだけで定義できるからです。なにもコンパイルする必要はありません。Arrowのコードにある&lt;a href=&quot;https://github.com/apache/arrow/tree/apache-arrow-0.15.0/python/examples/flight&quot;&gt;PythonでのFlightクライアントとサーバーの例&lt;/a&gt;を参考にできます。&lt;/p&gt;

&lt;p&gt;実際に使っている例もあります。Dremioは&lt;a href=&quot;https://github.com/dremio-hub/dremio-flight-connector&quot;&gt;Arrow Flightベースの&lt;/a&gt;コネクターを開発しました。このコネクターは&lt;a href=&quot;https://www.dremio.com/is-time-to-replace-odbc-jdbc/&quot;&gt;ODBCよりも20-50倍よい性能を発揮する&lt;/a&gt;ことを示しました。ArrowのコントリビューターであるRyan MurrayはApache Sparkユーザー向けにFlight対応エンドポイントに接続する&lt;a href=&quot;https://github.com/rymurr/flight-spark-source&quot;&gt;データソース実装&lt;/a&gt;を作りました。&lt;/p&gt;

&lt;p&gt;最後に今後の話をします。gRPCではない（あるいはTCPではない）データトランスポートをサポートできないか研究開発を進めるかもしれません。Flightの開発が進むとユーザーが使えるFlight対応サービスが増えていくでしょう。Flightは開発フレームワークなので、ユーザーが使うAPIは高レベルなAPIだけになるようにするつもりです。高レベルなAPIではFlightの詳細と特定のFlightアプリケーションに関連する詳細を隠します。&lt;/p&gt;</content><author><name>wesm</name></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /></entry><entry><title type="html">Introducing Apache Arrow Flight: A Framework for Fast Data Transport</title><link href="https://arrow.apache.org/blog/2019/10/13/introducing-arrow-flight/" rel="alternate" type="text/html" title="Introducing Apache Arrow Flight: A Framework for Fast Data Transport" /><published>2019-10-13T02:00:00-04:00</published><updated>2019-10-13T02:00:00-04:00</updated><id>https://arrow.apache.org/blog/2019/10/13/introducing-arrow-flight</id><content type="html" xml:base="https://arrow.apache.org/blog/2019/10/13/introducing-arrow-flight/">&lt;!--

--&gt;

&lt;p&gt;Translations: &lt;a href=&quot;/blog/2019/10/13/introducing-arrow-flight-japanese/&quot;&gt;日本語&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Over the last 18 months, the Apache Arrow community has been busy designing and
implementing &lt;strong&gt;Flight&lt;/strong&gt;, a new general-purpose client-server framework to
simplify high performance transport of large datasets over network interfaces.&lt;/p&gt;

&lt;p&gt;Flight initially is focused on optimized transport of the Arrow columnar format
(i.e. “Arrow record batches”) over &lt;a href=&quot;https://grpc.io/&quot;&gt;gRPC&lt;/a&gt;, Google’s popular HTTP/2-based
general-purpose RPC library and framework. While we have focused on integration
with gRPC, as a development framework Flight is not intended to be exclusive to
gRPC.&lt;/p&gt;

&lt;p&gt;One of the biggest features that sets apart Flight from other data transport
frameworks is parallel transfers, allowing data to be streamed to or from a
cluster of servers simultaneously. This enables developers to more easily
create scalable data services that can serve a growing client base.&lt;/p&gt;

&lt;p&gt;In the 0.15.0 Apache Arrow release, we have ready-to-use Flight implementations
in C++ (with Python bindings) and Java. These libraries are suitable for beta
users who are comfortable with API or protocol changes while we continue to
refine some low-level details in the Flight internals.&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;Many people have experienced the pain associated with accessing large datasets
over a network. There are many different transfer protocols and tools for
reading datasets from remote data services, such as ODBC and JDBC. Over the
last 10 years, file-based data warehousing in formats like CSV, Avro, and
Parquet has become popular, but this also presents challenges as raw data must
be transferred to local hosts before being deserialized.&lt;/p&gt;

&lt;p&gt;The work we have done since the beginning of Apache Arrow holds exciting
promise for accelerating data transport in a number of ways. The &lt;a href=&quot;https://github.com/apache/arrow/blob/master/docs/source/format/Columnar.rst&quot;&gt;Arrow
columnar format&lt;/a&gt; has key features that can help us:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It is an “on-the-wire” representation of tabular data that does not require
deserialization on receipt&lt;/li&gt;
  &lt;li&gt;Its natural mode is that of “streaming batches”, larger datasets are
transported a batch of rows at a time (called “record batches” in Arrow
parlance). In this post we will talk about “data streams”, these are
sequences of Arrow record batches using the project’s binary protocol&lt;/li&gt;
  &lt;li&gt;The format is language-independent and now has library support in 11
languages and counting.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Implementations of standard protocols like ODBC generally implement their own
custom on-wire binary protocols that must be marshalled to and from each
library’s public interface. The performance of ODBC or JDBC libraries varies
greatly from case to case.&lt;/p&gt;

&lt;p&gt;Our design goal for Flight is to create a new protocol for data services that
uses the Arrow columnar format as both the over-the-wire data representation as
well as the public API presented to developers. In doing so, we reduce or
remove the serialization costs associated with data transport and increase the
overall efficiency of distributed data systems. Additionally, two systems that
are already using Apache Arrow for other purposes can communicate data to each
other with extreme efficiency.&lt;/p&gt;

&lt;h2 id=&quot;flight-basics&quot;&gt;Flight Basics&lt;/h2&gt;

&lt;p&gt;The Arrow Flight libraries provide a development framework for implementing a
service that can send and receive data streams. A Flight server supports
several basic kinds of requests:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Handshake&lt;/strong&gt;: a simple request to determine whether the client is authorized
and, in some cases, to establish an implementation-defined session token to
use for future requests&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ListFlights&lt;/strong&gt;: return a list of available data streams&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;GetSchema&lt;/strong&gt;: return the schema for a data stream&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;GetFlightInfo&lt;/strong&gt;: return an “access plan” for a dataset of interest,
possibly requiring consuming multiple data streams. This request can accept
custom serialized commands containing, for example, your specific application
parameters.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;DoGet&lt;/strong&gt;: send a data stream to a client&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;DoPut&lt;/strong&gt;: receive a data stream from a client&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;DoAction&lt;/strong&gt;: perform an implementation-specific action and return any
results, i.e. a generalized function call&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ListActions&lt;/strong&gt;: return a list of available action types&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We take advantage of gRPC’s elegant “bidirectional” streaming support (built on
top of &lt;a href=&quot;https://grpc.io/docs/guides/concepts/&quot;&gt;HTTP/2 streaming&lt;/a&gt;) to allow clients and servers to send data and metadata
to each other simultaneously while requests are being served.&lt;/p&gt;

&lt;p&gt;A simple Flight setup might consist of a single server to which clients connect
and make &lt;code class=&quot;highlighter-rouge&quot;&gt;DoGet&lt;/code&gt; requests.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/img/20191014_flight_simple.png&quot; alt=&quot;Flight Simple Architecture&quot; width=&quot;50%&quot; class=&quot;img-responsive&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;optimizing-data-throughput-over-grpc&quot;&gt;Optimizing Data Throughput over gRPC&lt;/h2&gt;

&lt;p&gt;While using a general-purpose messaging library like gRPC has numerous specific
benefits beyond the obvious ones (taking advantage of all the engineering that
Google has done on the problem), some work was needed to improve the
performance of transporting large datasets. Many kinds of gRPC users only deal
with relatively small messages, for example.&lt;/p&gt;

&lt;p&gt;The best-supported way to use gRPC is to define services in a &lt;a href=&quot;https://github.com/protocolbuffers/protobuf&quot;&gt;Protocol
Buffers&lt;/a&gt; (aka “Protobuf”) &lt;code class=&quot;highlighter-rouge&quot;&gt;.proto&lt;/code&gt; file. A Protobuf plugin for gRPC
generates gRPC service stubs that you can use to implement your
applications. RPC commands and data messages are serialized using the &lt;a href=&quot;https://developers.google.com/protocol-buffers/docs/encoding&quot;&gt;Protobuf
wire format&lt;/a&gt;. Because we use “vanilla gRPC and Protocol Buffers”, gRPC
clients that are ignorant of the Arrow columnar format can still interact with
Flight services and handle the Arrow data opaquely.&lt;/p&gt;

&lt;p&gt;The main data-related Protobuf type in Flight is called &lt;code class=&quot;highlighter-rouge&quot;&gt;FlightData&lt;/code&gt;. Reading
and writing Protobuf messages in general is not free, so we implemented some
low-level optimizations in gRPC in both C++ and Java to do the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Generate the Protobuf wire format for &lt;code class=&quot;highlighter-rouge&quot;&gt;FlightData&lt;/code&gt; including the Arrow record
batch being sent without going through any intermediate memory copying or
serialization steps.&lt;/li&gt;
  &lt;li&gt;Reconstruct a Arrow record batch from the Protobuf representation of
&lt;code class=&quot;highlighter-rouge&quot;&gt;FlightData&lt;/code&gt; without any memory copying or deserialization. In fact, we
intercept the encoded data payloads without allowing the Protocol Buffers
library to touch them.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In a sense we are “having our cake and eating it, too”. Flight implementations
having these optimizations will have better performance, while naive gRPC
clients can still talk to the Flight service and use a Protobuf library to
deserialize &lt;code class=&quot;highlighter-rouge&quot;&gt;FlightData&lt;/code&gt; (albeit with some performance penalty).&lt;/p&gt;

&lt;p&gt;As far as absolute speed, in our C++ data throughput benchmarks, we are seeing
end-to-end TCP throughput in excess of 2-3GB/s on localhost without TLS
enabled. This benchmark shows a transfer of ~12 gigabytes of data in about 4
seconds:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;./arrow-flight-benchmark &lt;span class=&quot;nt&quot;&gt;--records_per_stream&lt;/span&gt; 100000000
Bytes &lt;span class=&quot;nb&quot;&gt;read&lt;/span&gt;: 12800000000
Nanos: 3900466413
Speed: 3129.63 MB/s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From this we can conclude that the machinery of Flight and gRPC adds relatively
little overhead, and it suggests that many real-world applications of Flight
will be bottlenecked on network bandwidth.&lt;/p&gt;

&lt;h2 id=&quot;horizontal-scalability-parallel-and-partitioned-data-access&quot;&gt;Horizontal Scalability: Parallel and Partitioned Data Access&lt;/h2&gt;

&lt;p&gt;Many distributed database-type systems make use of an architectural pattern
where the results of client requests are routed through a “coordinator” and
sent to the client. Aside from the obvious efficiency issues of transporting a
dataset multiple times on its way to a client, it also presents a scalability
problem for getting access to very large datasets.&lt;/p&gt;

&lt;p&gt;We wanted Flight to enable systems to create horizontally scalable data
services without having to deal with such bottlenecks. A client request to a
dataset using the &lt;code class=&quot;highlighter-rouge&quot;&gt;GetFlightInfo&lt;/code&gt; RPC returns a list of &lt;strong&gt;endpoints&lt;/strong&gt;, each of
which contains a server location and a &lt;strong&gt;ticket&lt;/strong&gt; to send that server in a
&lt;code class=&quot;highlighter-rouge&quot;&gt;DoGet&lt;/code&gt; request to obtain a part of the full dataset. To get access to the
entire dataset, all of the endpoints must be consumed. While Flight streams are
not necessarily ordered, we provide for application-defined metadata which can
be used to serialize ordering information.&lt;/p&gt;

&lt;p&gt;This multiple-endpoint pattern has a number of benefits:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Endpoints can be read by clients in parallel.&lt;/li&gt;
  &lt;li&gt;The service that serves the &lt;code class=&quot;highlighter-rouge&quot;&gt;GetFlightInfo&lt;/code&gt; “planning” request can delegate
work to sibling services to take advantage of data locality or simply to help
with load balancing.&lt;/li&gt;
  &lt;li&gt;Nodes in a distributed cluster can take on different roles. For example, a
subset of nodes might be responsible for planning queries while other nodes
exclusively fulfill data stream (&lt;code class=&quot;highlighter-rouge&quot;&gt;DoGet&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;DoPut&lt;/code&gt;) requests.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is an example diagram of a multi-node architecture with split service
roles:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/img/20191014_flight_complex.png&quot; alt=&quot;Flight Complex Architecture&quot; width=&quot;60%&quot; class=&quot;img-responsive&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;actions-extending-flight-with-application-business-logic&quot;&gt;Actions: Extending Flight with application business logic&lt;/h2&gt;

&lt;p&gt;While the &lt;code class=&quot;highlighter-rouge&quot;&gt;GetFlightInfo&lt;/code&gt; request supports sending opaque serialized commands
when requesting a dataset, a client may need to be able to ask a server to
perform other kinds of operations. For example, a client may request for a
particular dataset to be “pinned” in memory so that subsequent requests from
other clients are served faster.&lt;/p&gt;

&lt;p&gt;A Flight service can thus optionally define “actions” which are carried out by
the &lt;code class=&quot;highlighter-rouge&quot;&gt;DoAction&lt;/code&gt; RPC. An action request contains the name of the action being
performed and optional serialized data containing further needed
information. The result of an action is a gRPC stream of opaque binary results.&lt;/p&gt;

&lt;p&gt;Some example actions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Metadata discovery, beyond the capabilities provided by the built-in
&lt;code class=&quot;highlighter-rouge&quot;&gt;ListFlights&lt;/code&gt; RPC&lt;/li&gt;
  &lt;li&gt;Setting session-specific parameters and settings&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that it is not required for a server to implement any actions, and actions
need not return results.&lt;/p&gt;

&lt;h2 id=&quot;encryption-and-authentication&quot;&gt;Encryption and Authentication&lt;/h2&gt;

&lt;p&gt;Flight supports encryption out of the box using gRPC’s built in TLS / OpenSSL
capabilities.&lt;/p&gt;

&lt;p&gt;For authentication, there are extensible authentication handlers for the client
and server that permit simple authentication schemes (like user and password)
as well as more involved authentication such as Kerberos. The Flight protocol
comes with a built-in &lt;code class=&quot;highlighter-rouge&quot;&gt;BasicAuth&lt;/code&gt; so that user/password authentication can be
implemented out of the box without custom development.&lt;/p&gt;

&lt;h2 id=&quot;middleware-and-tracing&quot;&gt;Middleware and Tracing&lt;/h2&gt;

&lt;p&gt;gRPC has the concept of “interceptors” which have allowed us to develop
developer-defined “middleware” that can provide instrumentation of or telemetry
for incoming and outgoing requests. One such framework for such instrumentation
is &lt;a href=&quot;https://opentracing.io/&quot;&gt;OpenTracing&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Note that middleware functionality is one of the newest areas of the project
and is only currently available in the project’s master branch.&lt;/p&gt;

&lt;h2 id=&quot;grpc-but-not-only-grpc&quot;&gt;gRPC, but not only gRPC&lt;/h2&gt;

&lt;p&gt;We specify server locations for &lt;code class=&quot;highlighter-rouge&quot;&gt;DoGet&lt;/code&gt; requests using RFC 3986 compliant
URIs. For example, TLS-secured gRPC may be specified like
&lt;code class=&quot;highlighter-rouge&quot;&gt;grpc+tls://$HOST:$PORT&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;While we think that using gRPC for the “command” layer of Flight servers makes
sense, we may wish to support data transport layers other than TCP such as
&lt;a href=&quot;https://en.wikipedia.org/wiki/Remote_direct_memory_access&quot;&gt;RDMA&lt;/a&gt;. While some design and development work is required to make this
possible, the idea is that gRPC could be used to coordinate get and put
transfers which may be carried out on protocols other than TCP.&lt;/p&gt;

&lt;h2 id=&quot;getting-started-and-whats-next&quot;&gt;Getting Started and What’s Next&lt;/h2&gt;

&lt;p&gt;Documentation for Flight users is a work in progress, but the libraries
themselves are mature enough for beta users that are tolerant of some minor API
or protocol changes over the coming year.&lt;/p&gt;

&lt;p&gt;One of the easiest ways to experiment with Flight is using the Python API,
since custom servers and clients can be defined entirely in Python without any
compilation required. You can see an &lt;a href=&quot;https://github.com/apache/arrow/tree/apache-arrow-0.15.0/python/examples/flight&quot;&gt;example Flight client and server in
Python&lt;/a&gt; in the Arrow codebase.&lt;/p&gt;

&lt;p&gt;In real-world use, Dremio has developed an &lt;a href=&quot;https://github.com/dremio-hub/dremio-flight-connector&quot;&gt;Arrow Flight-based&lt;/a&gt; connector
which has been shown to &lt;a href=&quot;https://www.dremio.com/is-time-to-replace-odbc-jdbc/&quot;&gt;deliver 20-50x better performance over ODBC&lt;/a&gt;. For
Apache Spark users, Arrow contributor Ryan Murray has created a &lt;a href=&quot;https://github.com/rymurr/flight-spark-source&quot;&gt;data source
implementation&lt;/a&gt; to connect to Flight-enabled endpoints.&lt;/p&gt;

&lt;p&gt;As far as “what’s next” in Flight, support for non-gRPC (or non-TCP) data
transport may be an interesting direction of research and development work. A
lot of the Flight work from here will be creating user-facing Flight-enabled
services. Since Flight is a development framework, we expect that user-facing
APIs will utilize a layer of API veneer that hides many general Flight details
and details related to a particular application of Flight in a custom data
service.&lt;/p&gt;</content><author><name>wesm</name></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /></entry><entry><title type="html">Apache Arrow 0.15.0 Release</title><link href="https://arrow.apache.org/blog/2019/10/06/0.15.0-release/" rel="alternate" type="text/html" title="Apache Arrow 0.15.0 Release" /><published>2019-10-06T02:00:00-04:00</published><updated>2019-10-06T02:00:00-04:00</updated><id>https://arrow.apache.org/blog/2019/10/06/0.15.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2019/10/06/0.15.0-release/">&lt;!--

--&gt;

&lt;p&gt;The Apache Arrow team is pleased to announce the 0.15.0 release. This covers
about 3 months of development work and includes &lt;a href=&quot;https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%200.15.0&quot;&gt;&lt;strong&gt;687 resolved issues&lt;/strong&gt;&lt;/a&gt;
from &lt;a href=&quot;https://arrow.apache.org/release/0.15.0.html#contributors&quot;&gt;&lt;strong&gt;80 distinct contributors&lt;/strong&gt;&lt;/a&gt;.  See the Install Page to learn how to
get the libraries for your platform. The &lt;a href=&quot;https://arrow.apache.org/release/0.15.0.html&quot;&gt;complete changelog&lt;/a&gt; is also
available.&lt;/p&gt;

&lt;p&gt;About a third of issues closed (240) were classified as bug fixes, so this
release brings many stability, memory use, and performance improvements over
0.14.x. We will discuss some of the language-specific improvements and new
features below.&lt;/p&gt;

&lt;h2 id=&quot;new-committers&quot;&gt;New committers&lt;/h2&gt;

&lt;p&gt;Since the 0.14.0 release, we’ve added four new committers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/bkietz&quot;&gt;Ben Kietzman&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/lidavidm&quot;&gt;David Li&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/mrkn&quot;&gt;Kenta Murata&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/nealrichardson&quot;&gt;Neal Richardson&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, &lt;a href=&quot;https://github.com/sbinet&quot;&gt;Sebastien Binet&lt;/a&gt; and &lt;a href=&quot;https://github.com/emkornfield&quot;&gt;Micah Kornfield&lt;/a&gt; have joined the PMC.&lt;/p&gt;

&lt;p&gt;Thank you for all your contributions!&lt;/p&gt;

&lt;h2 id=&quot;columnar-format-notes&quot;&gt;Columnar Format Notes&lt;/h2&gt;

&lt;p&gt;The format gets new datatypes : LargeList(ARROW-4810), LargeBinary and
LargeString (ARROW-750).  LargeList is similar to List but with 64-bit
offsets instead of 32-bit.  The same relationship holds for LargeBinary
and LargeString with respect to Binary and String.&lt;/p&gt;

&lt;p&gt;Since the last major release, we have also made a significant overhaul of the
&lt;a href=&quot;https://github.com/apache/arrow/blob/master/docs/source/format/Columnar.rst&quot;&gt;columnar format documentation&lt;/a&gt; to be clearer and easier to follow for
implementation creators.&lt;/p&gt;

&lt;h2 id=&quot;upcoming-columnar-format-stability-and-library--format-version-split&quot;&gt;Upcoming Columnar Format Stability and Library / Format Version Split&lt;/h2&gt;

&lt;p&gt;The Arrow community has decided to make a 1.0.0 release of the project marking
formal stability of the columnar format and binary protocol, including explicit
forward and backward compatibility guarantees. You can read about these
guarantees in the &lt;a href=&quot;https://github.com/apache/arrow/blob/master/docs/source/format/Versioning.rst&quot;&gt;new documentation page&lt;/a&gt; about versioning.&lt;/p&gt;

&lt;p&gt;Starting with 1.0.0, we will give the columnar format and libraries separate
version numbers. This will allow the library versions to evolve without
creating confusion or uncertainty about whether the Arrow columnar format
remains stable or not.&lt;/p&gt;

&lt;h2 id=&quot;columnar-streaming-protocol-change-since-0140&quot;&gt;Columnar “Streaming Protocol” Change since 0.14.0&lt;/h2&gt;

&lt;p&gt;Since 0.14.0 we have modified the IPC “encapsulated message” format to insert 4
bytes of additional data in the message preamble to ensure that the Flatbuffers
metadata starts on an aligned offset. By default, IPC streams generated by
0.15.0 and later will not be readable by library versions 0.14.1 and
prior. Implementations have offered options to write messages using the now
“legacy” message format.&lt;/p&gt;

&lt;p&gt;For users who cannot upgrade to version 0.15.0 in all parts of their system,
such as Apache Spark users, we recommend one of the two routes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If using pyarrow, set the environment variable &lt;code class=&quot;highlighter-rouge&quot;&gt;ARROW_PRE_0_15_IPC_FORMAT=1&lt;/code&gt;
when using 0.15.0 and sending data to an old library&lt;/li&gt;
  &lt;li&gt;Wait to upgrade all components simultaneously&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We do not anticipate making this kind of change again in the near future and
would not have made such a non-forward-compatible change unless we deemed it
very important.&lt;/p&gt;

&lt;h2 id=&quot;arrow-flight-notes&quot;&gt;Arrow Flight notes&lt;/h2&gt;

&lt;p&gt;A GetFlightSchema method is added to the Flight RPC protocol (ARROW-6094).
As the name suggests, it returns the schema for a given Flight descriptor
on the server.  This is useful for cases where the Flight locations are
not immediately available, depending on the server implementation.&lt;/p&gt;

&lt;p&gt;Flight implementations for C++ and Java now implement half-closed
semantics for DoPut (ARROW-6063).  The client can close the writing
end of the stream to signal that it has finished sending the Flight
data, but still receive the batch-specific response and its associated
metadata.&lt;/p&gt;

&lt;h2 id=&quot;c-notes&quot;&gt;C++ notes&lt;/h2&gt;

&lt;p&gt;C++ now supports the LargeList, LargeBinary and LargeString datatypes.&lt;/p&gt;

&lt;p&gt;The Status class gains the ability to carry additional subsystem-specific
data with it, under the form of an opaque StatusDetail interface (ARROW-4036).
This allows, for example, to store not only an exception message coming from
Python but the actual Python exception object, such as to raise it again if
the Status is propagated back to Python.  It can also enable the consumer
of Status to inspect the subsystem-specific error, such as a finer-grained
Flight error code.&lt;/p&gt;

&lt;p&gt;DataType and Schema equality are significantly faster (ARROW-6038).&lt;/p&gt;

&lt;p&gt;The Column class is completely removed, as it did not have a strong enough
motivation for existing between ChunkedArray and RecordBatch / Table
(ARROW-5893).&lt;/p&gt;

&lt;h3 id=&quot;c-parquet&quot;&gt;C++: Parquet&lt;/h3&gt;

&lt;p&gt;The 0.15 release includes many improvements to the Apache Parquet C++ internals,
resulting in greatly improved read and write performance. We described the work
and published some benchmarks in a &lt;a href=&quot;https://arrow.apache.org/blog/2019/09/05/faster-strings-cpp-parquet/&quot;&gt;recent blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;c-csv-reader&quot;&gt;C++: CSV reader&lt;/h3&gt;

&lt;p&gt;The CSV reader is now more flexible in terms of how column names are chosen
(ARROW-6231) and column selection (ARROW-5977).&lt;/p&gt;

&lt;h3 id=&quot;c-memory-allocation-layer&quot;&gt;C++: Memory Allocation Layer&lt;/h3&gt;

&lt;p&gt;Arrow now has the option to allocate memory using the mimalloc memory
allocator.  jemalloc is still preferred for best performance, but mimalloc
is a reasonable alternative to the system allocator on Windows where jemalloc
is not currently supported.&lt;/p&gt;

&lt;p&gt;Also, we now expose explicit global functions to get a MemoryPool for each
of the jemalloc allocator, mimalloc allocator and system allocator (ARROW-6292).&lt;/p&gt;

&lt;p&gt;The vendored jemalloc version is bumped from 4.5.x to 5.2.x (ARROW-6549).
Performance characteristics may differ on memory allocation-heavy workloads,
though we did not notice any significant regression on our suite of
micro-benchmarks (and a multi-threaded benchmark of reading a CSV file
showed a 25% speedup).&lt;/p&gt;

&lt;h3 id=&quot;c-filesystem-layer&quot;&gt;C++: Filesystem layer&lt;/h3&gt;

&lt;p&gt;A FileSystem implementation to access Amazon S3-compatible filesystems is now
available.  It depends on the AWS SDK for C++.&lt;/p&gt;

&lt;h3 id=&quot;c-io-layer&quot;&gt;C++: I/O layer&lt;/h3&gt;

&lt;p&gt;Significant improvements were made to the Arrow I/O stack.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ARROW-6180: Add RandomAccessFile::GetStream that returns an InputStream over
a fixed subset of the file.&lt;/li&gt;
  &lt;li&gt;ARROW-6381: Improve performance of small writes with BufferOutputStream&lt;/li&gt;
  &lt;li&gt;ARROW-2490: Streamline concurrency semantics of InputStream implementations,
and add debug checks for race conditions between non-thread-safe InputStream
operations.&lt;/li&gt;
  &lt;li&gt;ARROW-6527: Add an OutputStream::Write overload that takes an owned Buffer
rather than a raw memory area.  This allows OutputStream implementations
to safely implement delayed writing without having to copy the data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;c-tensors&quot;&gt;C++: Tensors&lt;/h3&gt;

&lt;p&gt;There are three improvements of Tensor and SparseTensor in this release.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Add Tensor::Value template function for element access&lt;/li&gt;
  &lt;li&gt;Add EqualOptions support in Tensor::Equals function, that allows us to control the way to compare two float tensors&lt;/li&gt;
  &lt;li&gt;Add smaller bit-width index supports in SparseTensor&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;c-notes-1&quot;&gt;C# Notes&lt;/h2&gt;

&lt;p&gt;We have fixed some bugs causing incompatibilities between C# and other Arrow
implementations.&lt;/p&gt;

&lt;h2 id=&quot;java-notes&quot;&gt;Java notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Added an initial version of an Avro adapter&lt;/li&gt;
  &lt;li&gt;To improve the JDBC adapter performance, refactored consume data logic and
implemented an iterator API to prevent loading all data into one vector&lt;/li&gt;
  &lt;li&gt;Implemented subField encoding for complex type, now List and Struct vectors
subField encoding is available&lt;/li&gt;
  &lt;li&gt;Implemented visitor API for vector/range/type/approx equals compare&lt;/li&gt;
  &lt;li&gt;Performed a lot of optimization and refactoring for DictionaryEncoder,
supporting all data types and avoiding memory copy via hash table and visitor
API&lt;/li&gt;
  &lt;li&gt;Introduced &lt;a href=&quot;https://github.com/google/error-prone&quot;&gt;Error Prone&lt;/a&gt; into code base to catch more potential errors
earlier&lt;/li&gt;
  &lt;li&gt;Fixed the bug where dictionary entries were required in IPC streams even when
empty; readers can now also read interleaved messages&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;python-notes&quot;&gt;Python notes&lt;/h2&gt;

&lt;p&gt;The FileSystem API, implemented in C++, is now available in Python (ARROW-5494).&lt;/p&gt;

&lt;p&gt;The API for extension types has been straightened and definition of
custom extension types in Python is now more powerful (ARROW-5610).&lt;/p&gt;

&lt;p&gt;Sparse tensors are now available in Python (ARROW-4453).&lt;/p&gt;

&lt;p&gt;A potential crash when handling Python dates and datetimes was fixed
(ARROW-6597).&lt;/p&gt;

&lt;p&gt;Based on a mailing list discussion, we are looking for help with maintaining
our Python wheels. Community members have found that the wheels take up a great
deal of maintenance time, so if you or your organization depend on &lt;code class=&quot;highlighter-rouge&quot;&gt;pip install
pyarrow&lt;/code&gt; working, we would appreciate your assistance.&lt;/p&gt;

&lt;h2 id=&quot;ruby-and-c-glib-notes&quot;&gt;Ruby and C GLib notes&lt;/h2&gt;

&lt;p&gt;Ruby and C GLib continues to follow the features in the C++ project.
Ruby includes the following backward incompatible changes.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Remove Arrow::Struct and use Hash instead.&lt;/li&gt;
  &lt;li&gt;Add Arrow::Time for Arrow::Time{32,64}DataType value.&lt;/li&gt;
  &lt;li&gt;Arrow::Decimal128Array#get_value returns BigDecimal.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ruby improves the performance of Arrow#values.&lt;/p&gt;

&lt;h2 id=&quot;rust-notes&quot;&gt;Rust notes&lt;/h2&gt;

&lt;p&gt;A number of core Arrow improvements were made to the Rust library.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Add explicit SIMD vectorization for the divide kernel&lt;/li&gt;
  &lt;li&gt;Add a feature to disable SIMD&lt;/li&gt;
  &lt;li&gt;Use “if cfg!” pattern&lt;/li&gt;
  &lt;li&gt;Optimizations to BooleanBufferBuilder::append_slice&lt;/li&gt;
  &lt;li&gt;Implemented Debug trait for List/Struct/BinaryArray&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Improvements related to Rust Parquet and DataFusion are detailed next.&lt;/p&gt;

&lt;h3 id=&quot;rust-parquet&quot;&gt;Rust Parquet&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Implement Arrow record reader&lt;/li&gt;
  &lt;li&gt;Add converter that is used to convert record reader’s content to arrow primitive array.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rust-datafusion&quot;&gt;Rust DataFusion&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Preview of new query execution engine using an extensible trait-based
physical execution plan that supports parallel execution using threads&lt;/li&gt;
  &lt;li&gt;ExecutionContext now has a register_parquet convenience method for
registering Parquet data sources&lt;/li&gt;
  &lt;li&gt;Fixed bug in type coercion optimizer rule&lt;/li&gt;
  &lt;li&gt;TableProvider.scan() now returns a thread-safe BatchIterator&lt;/li&gt;
  &lt;li&gt;Remove use of bare trait objects (switched to using dyn syntax)&lt;/li&gt;
  &lt;li&gt;Adds casting from unsigned to signed integer data types&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;r-notes&quot;&gt;R notes&lt;/h2&gt;

&lt;p&gt;A major development since the 0.14 release was the arrival of the &lt;code class=&quot;highlighter-rouge&quot;&gt;arrow&lt;/code&gt; R
package on &lt;a href=&quot;https://cran.r-project.org/package=arrow&quot;&gt;CRAN&lt;/a&gt;. We wrote about this in August on the &lt;a href=&quot;https://arrow.apache.org/blog/2019/08/08/r-package-on-cran/&quot;&gt;Arrow blog&lt;/a&gt;.
In addition to the package availability on CRAN, we also published
&lt;a href=&quot;https://arrow.apache.org/docs/r&quot;&gt;package documentation&lt;/a&gt; on the Arrow website.&lt;/p&gt;

&lt;p&gt;The 0.15 R package includes many of the enhancements in the C++ library
release, such as the Parquet performance improvements and the FileSystem API.
In addition, there are a number of upgrades that make it easier to read and
write data, specify types and schema, and interact with Arrow tables and record
batches in R.&lt;/p&gt;

&lt;p&gt;For more on what’s in the 0.15 R package, see the &lt;a href=&quot;http://arrow.apache.org/docs/r/news/&quot;&gt;changelog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;community-discussions-ongoing&quot;&gt;Community Discussions Ongoing&lt;/h2&gt;

&lt;p&gt;There are a number of active discussions ongoing on the developer
dev@arrow.apache.org mailing list. We look forward to hearing from the
community there.&lt;/p&gt;</content><author><name>pmc</name></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /></entry><entry><title type="html">Faster C++ Apache Parquet performance on dictionary-encoded string data coming in Apache Arrow 0.15</title><link href="https://arrow.apache.org/blog/2019/09/05/faster-strings-cpp-parquet/" rel="alternate" type="text/html" title="Faster C++ Apache Parquet performance on dictionary-encoded string data coming in Apache Arrow 0.15" /><published>2019-09-05T02:00:00-04:00</published><updated>2019-09-05T02:00:00-04:00</updated><id>https://arrow.apache.org/blog/2019/09/05/faster-strings-cpp-parquet</id><content type="html" xml:base="https://arrow.apache.org/blog/2019/09/05/faster-strings-cpp-parquet/">&lt;!--

--&gt;

&lt;p&gt;We have been implementing a series of optimizations in the Apache Parquet C++
internals to improve read and write efficiency (both performance and memory
use) for Arrow columnar binary and string data, with new “native” support for
Arrow’s dictionary types. This should have a big impact on users of the C++,
MATLAB, Python, R, and Ruby interfaces to Parquet files.&lt;/p&gt;

&lt;p&gt;This post reviews work that was done and shows benchmarks comparing Arrow
0.12.1 with the current development version (to be released soon as Arrow
0.15.0).&lt;/p&gt;

&lt;h1 id=&quot;summary-of-work&quot;&gt;Summary of work&lt;/h1&gt;

&lt;p&gt;One of the largest and most complex optimizations involves encoding and
decoding Parquet files’ internal dictionary-encoded data streams to and from
Arrow’s in-memory dictionary-encoded &lt;code class=&quot;highlighter-rouge&quot;&gt;DictionaryArray&lt;/code&gt;
representation. Dictionary encoding is a compression strategy in Parquet, and
there is no formal “dictionary” or “categorical” type. I will go into more
detail about this below.&lt;/p&gt;

&lt;p&gt;Some of the particular JIRA issues related to this work include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Vectorize comparators for computing statistics (&lt;a href=&quot;https://issues.apache.org/jira/browse/PARQUET-1523&quot;&gt;PARQUET-1523&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Read binary directly data directly into dictionary builder
(&lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-3769&quot;&gt;ARROW-3769&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Writing Parquet’s dictionary indices directly into dictionary builder
(&lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-3772&quot;&gt;ARROW-3772&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Write dense (non-dictionary) Arrow arrays directly into Parquet data encoders
(&lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-6152&quot;&gt;ARROW-6152&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Direct writing of &lt;code class=&quot;highlighter-rouge&quot;&gt;arrow::DictionaryArray&lt;/code&gt; to Parquet column writers (&lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-3246&quot;&gt;ARROW-3246&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Supporting changing dictionaries (&lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-3144&quot;&gt;ARROW-3144&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Internal IO optimizations and improved raw &lt;code class=&quot;highlighter-rouge&quot;&gt;BYTE_ARRAY&lt;/code&gt; encoding performance
(&lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-4398&quot;&gt;ARROW-4398&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the challenges of developing the Parquet C++ library is that we maintain
low-level read and write APIs that do not involve the Arrow columnar data
structures. So we have had to take care to implement Arrow-related
optimizations without impacting non-Arrow Parquet users, which includes
database systems like Clickhouse and Vertica.&lt;/p&gt;

&lt;h1 id=&quot;background-how-parquet-files-do-dictionary-encoding&quot;&gt;Background: how Parquet files do dictionary encoding&lt;/h1&gt;

&lt;p&gt;Many direct and indirect users of Apache Arrow use dictionary encoding to
improve performance and memory use on binary or string data types that include
many repeated values. MATLAB or pandas users will know this as the Categorical
type (see &lt;a href=&quot;https://www.mathworks.com/help/matlab/categorical-arrays.html&quot;&gt;MATLAB docs&lt;/a&gt; or &lt;a href=&quot;https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html&quot;&gt;pandas docs&lt;/a&gt;) while in R such encoding is
known as &lt;a href=&quot;https://stat.ethz.ch/R-manual/R-devel/library/base/html/factor.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;factor&lt;/code&gt;&lt;/a&gt;. In the Arrow C++ library and various bindings we have
the &lt;code class=&quot;highlighter-rouge&quot;&gt;DictionaryArray&lt;/code&gt; object for representing such data in memory.&lt;/p&gt;

&lt;p&gt;For example, an array such as&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;['apple', 'orange', 'apple', NULL, 'orange', 'orange']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;has dictionary-encoded form&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dictionary: ['apple', 'orange']
indices: [0, 1, 0, NULL, 1, 1]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/apache/parquet-format/blob/master/Encodings.md&quot;&gt;Parquet format uses dictionary encoding&lt;/a&gt; to compress data, and it is
used for all Parquet data types, not just binary or string data. Parquet
further uses bit-packing and run-length encoding (RLE) to compress the
dictionary indices, so if you had data like&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;['apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'orange']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;the indices would be encoded like&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[rle-run=(6, 0),
 bit-packed-run=[1]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The full details of the rle-bitpacking encoding are found in the &lt;a href=&quot;https://github.com/apache/parquet-format/blob/master/Encodings.md&quot;&gt;Parquet
specification&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When writing a Parquet file, most implementations will use dictionary encoding
to compress a column until the dictionary itself reaches a certain size
threshold, usually around 1 megabyte. At this point, the column writer will
“fall back” to &lt;code class=&quot;highlighter-rouge&quot;&gt;PLAIN&lt;/code&gt; encoding where values are written end-to-end in “data
pages” and then usually compressed with Snappy or Gzip. See the following rough
diagram:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/img/20190903-parquet-dictionary-column-chunk.png&quot; alt=&quot;Internal ColumnChunk structure&quot; width=&quot;80%&quot; class=&quot;img-responsive&quot; /&gt;
&lt;/div&gt;

&lt;h1 id=&quot;faster-reading-and-writing-of-dictionary-encoded-data&quot;&gt;Faster reading and writing of dictionary-encoded data&lt;/h1&gt;

&lt;p&gt;When reading a Parquet file, the dictionary-encoded portions are usually
materialized to their non-dictionary-encoded form, causing binary or string
values to be duplicated in memory. So an obvious (but not trivial) optimization
is to skip this “dense” materialization. There are several issues to deal with:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A Parquet file often contains multiple ColumnChunks for each semantic column,
and the dictionary values may be different in each ColumnChunk&lt;/li&gt;
  &lt;li&gt;We must gracefully handle the “fall back” portion which is not
dictionary-encoded&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We pursued several avenues to help with this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Allowing each &lt;code class=&quot;highlighter-rouge&quot;&gt;DictionaryArray&lt;/code&gt; to have a different dictionary (before, the
dictionary was part of the &lt;code class=&quot;highlighter-rouge&quot;&gt;DictionaryType&lt;/code&gt;, which caused problems)&lt;/li&gt;
  &lt;li&gt;We enabled the Parquet dictionary indices to be directly written into an
Arrow &lt;code class=&quot;highlighter-rouge&quot;&gt;DictionaryBuilder&lt;/code&gt; without rehashing the data&lt;/li&gt;
  &lt;li&gt;When decoding a ColumnChunk, we first append the dictionary values and
indices into an Arrow &lt;code class=&quot;highlighter-rouge&quot;&gt;DictionaryBuilder&lt;/code&gt;, and when we encounter the “fall
back” portion we use a hash table to convert those values to
dictionary-encoded form&lt;/li&gt;
  &lt;li&gt;We override the “fall back” logic when writing a ColumnChunk from an
&lt;code class=&quot;highlighter-rouge&quot;&gt;DictionaryArray&lt;/code&gt; so that reading such data back is more efficient&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All of these things together have produced some excellent performance results
that we will detail below.&lt;/p&gt;

&lt;p&gt;The other class of optimizations we implemented was removing an abstraction
layer between the low-level Parquet column data encoder and decoder classes and
the Arrow columnar data structures. This involves:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Adding &lt;code class=&quot;highlighter-rouge&quot;&gt;ColumnWriter::WriteArrow&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Encoder::Put&lt;/code&gt; methods that accept
&lt;code class=&quot;highlighter-rouge&quot;&gt;arrow::Array&lt;/code&gt; objects directly&lt;/li&gt;
  &lt;li&gt;Adding &lt;code class=&quot;highlighter-rouge&quot;&gt;ByteArrayDecoder::DecodeArrow&lt;/code&gt; method to decode binary data directly
into an &lt;code class=&quot;highlighter-rouge&quot;&gt;arrow::BinaryBuilder&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While the performance improvements from this work are less dramatic than for
dictionary-encoded data, they are still meaningful in real-world applications.&lt;/p&gt;

&lt;h1 id=&quot;performance-benchmarks&quot;&gt;Performance Benchmarks&lt;/h1&gt;

&lt;p&gt;We ran some benchmarks comparing Arrow 0.12.1 with the current master
branch. We construct two kinds of Arrow tables with 10 columns each:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“Low cardinality” and “high cardinality” variants. The “low cardinality” case
has 1,000 unique string values of 32-bytes each. The “high cardinality” has
100,000 unique values&lt;/li&gt;
  &lt;li&gt;“Dense” (non-dictionary) and “Dictionary” variants&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://gist.github.com/wesm/b4554e2d6028243a30eeed2c644a9066&quot;&gt;See the full benchmark script.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We show both single-threaded and multithreaded read performance. The test
machine is an Intel i9-9960X using gcc 8.3.0 (on Ubuntu 18.04) with 16 physical
cores and 32 virtual cores. All time measurements are reported in seconds, but
we are most interested in showing the relative performance.&lt;/p&gt;

&lt;p&gt;First, the writing benchmarks:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/img/20190903_parquet_write_perf.png&quot; alt=&quot;Parquet write benchmarks&quot; width=&quot;80%&quot; class=&quot;img-responsive&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Writing &lt;code class=&quot;highlighter-rouge&quot;&gt;DictionaryArray&lt;/code&gt; is dramatically faster due to the optimizations
described above. We have achieved a small improvement in writing dense
(non-dictionary) binary arrays.&lt;/p&gt;

&lt;p&gt;Then, the reading benchmarks:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/img/20190903_parquet_read_perf.png&quot; alt=&quot;Parquet read benchmarks&quot; width=&quot;80%&quot; class=&quot;img-responsive&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Here, similarly reading &lt;code class=&quot;highlighter-rouge&quot;&gt;DictionaryArray&lt;/code&gt; directly is many times faster.&lt;/p&gt;

&lt;p&gt;These benchmarks show that parallel reads of dense binary data may be slightly
slower though single-threaded reads are now faster. We may want to do some
profiling and see what we can do to bring read performance back in
line. Optimizing the dense read path has not been too much of a priority
relative to the dictionary read path in this work.&lt;/p&gt;

&lt;h1 id=&quot;memory-use-improvements&quot;&gt;Memory Use Improvements&lt;/h1&gt;

&lt;p&gt;In addition to faster performance, reading columns as dictionary-encoded can
yield significantly less memory use.&lt;/p&gt;

&lt;p&gt;In the &lt;code class=&quot;highlighter-rouge&quot;&gt;dict-random&lt;/code&gt; case above, we found that the master branch uses 405 MB of
RAM at peak while loading a 152 MB dataset. In v0.12.1, loading the same
Parquet file without the accelerated dictionary support uses 1.94 GB of peak
memory while the resulting non-dictionary table occupies 1.01 GB.&lt;/p&gt;

&lt;p&gt;Note that we had a memory overuse bug in versions 0.14.0 and 0.14.1 fixed in
ARROW-6060, so if you are hitting this bug you will want to upgrade to 0.15.0
as soon as it comes out.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;There are still many Parquet-related optimizations that we may pursue in the
future, but the ones here can be very helpful to people working with
string-heavy datasets, both in performance and memory use. If you’d like to
discuss this development work, we’d be glad to hear from you on our developer
mailing list dev@arrow.apache.org.&lt;/p&gt;</content><author><name>wesm</name></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /></entry><entry><title type="html">Apache Arrow R Package On CRAN</title><link href="https://arrow.apache.org/blog/2019/08/08/r-package-on-cran/" rel="alternate" type="text/html" title="Apache Arrow R Package On CRAN" /><published>2019-08-08T08:00:00-04:00</published><updated>2019-08-08T08:00:00-04:00</updated><id>https://arrow.apache.org/blog/2019/08/08/r-package-on-cran</id><content type="html" xml:base="https://arrow.apache.org/blog/2019/08/08/r-package-on-cran/">&lt;!--

--&gt;

&lt;p&gt;We are very excited to announce that the &lt;code class=&quot;highlighter-rouge&quot;&gt;arrow&lt;/code&gt; R package is now available on
&lt;a href=&quot;https://cran.r-project.org/&quot;&gt;CRAN&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arrow.apache.org/&quot;&gt;Apache Arrow&lt;/a&gt; is a cross-language development
platform for in-memory data that specifies a standardized columnar memory
format for flat and hierarchical data, organized for efficient analytic
operations on modern hardware. The &lt;code class=&quot;highlighter-rouge&quot;&gt;arrow&lt;/code&gt; package provides an R interface to
the Arrow C++ library, including support for working with Parquet and Feather
files, as well as lower-level access to Arrow memory and messages.&lt;/p&gt;

&lt;p&gt;You can install the package from CRAN with&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;install.packages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;arrow&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On macOS and Windows, installing a binary package from CRAN will generally
handle Arrow’s C++ dependencies for you. However, the macOS CRAN binaries are
unfortunately incomplete for this version, so to install 0.14.1, you’ll first
need to use Homebrew to get the Arrow C++ library (&lt;code class=&quot;highlighter-rouge&quot;&gt;brew install
apache-arrow&lt;/code&gt;), and then from R you can &lt;code class=&quot;highlighter-rouge&quot;&gt;install.packages(&quot;arrow&quot;, type =
&quot;source&quot;)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Windows binaries are not yet available on CRAN but should be published soon.&lt;/p&gt;

&lt;p&gt;On Linux, you’ll need to first install the C++ library. See the &lt;a href=&quot;https://arrow.apache.org/install/&quot;&gt;Arrow project
installation page&lt;/a&gt; to find pre-compiled
binary packages for some common Linux distributions, including Debian, Ubuntu,
and CentOS. You’ll need to install &lt;code class=&quot;highlighter-rouge&quot;&gt;libparquet-dev&lt;/code&gt; on Debian and Ubuntu, or
&lt;code class=&quot;highlighter-rouge&quot;&gt;parquet-devel&lt;/code&gt; on CentOS. This will also automatically install the Arrow C++
library as a dependency. Other Linux distributions must install the C++ library
from source.&lt;/p&gt;

&lt;p&gt;If you install the &lt;code class=&quot;highlighter-rouge&quot;&gt;arrow&lt;/code&gt; R package from source and the C++ library is not
found, the R package functions will notify you that Arrow is not
available. Call&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;arrow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;install_arrow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;for version- and platform-specific guidance on installing the Arrow C++
library.&lt;/p&gt;

&lt;h2 id=&quot;parquet-files&quot;&gt;Parquet files&lt;/h2&gt;

&lt;p&gt;This package introduces basic read and write support for the &lt;a href=&quot;https://parquet.apache.org/&quot;&gt;Apache
Parquet&lt;/a&gt; columnar data file format. Prior to its
availability, options for accessing Parquet data in R were limited; the most
common recommendation was to use Apache Spark. The &lt;code class=&quot;highlighter-rouge&quot;&gt;arrow&lt;/code&gt; package greatly
simplifies this access and lets you go from a Parquet file to a &lt;code class=&quot;highlighter-rouge&quot;&gt;data.frame&lt;/code&gt;
and back easily, without having to set up a database.&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arrow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_parquet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;path/to/file.parquet&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This function, along with the other readers in the package, takes an optional
&lt;code class=&quot;highlighter-rouge&quot;&gt;col_select&lt;/code&gt; argument, inspired by the
&lt;a href=&quot;https://vroom.r-lib.org/reference/vroom.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;vroom&lt;/code&gt;&lt;/a&gt; package. This argument
lets you use the &lt;a href=&quot;https://tidyselect.r-lib.org/reference/select_helpers.html&quot;&gt;“tidyselect” helper
functions&lt;/a&gt;, as you
can do in &lt;code class=&quot;highlighter-rouge&quot;&gt;dplyr::select()&lt;/code&gt;, to specify that you only want to keep certain
columns. By narrowing your selection at read time, you can load a &lt;code class=&quot;highlighter-rouge&quot;&gt;data.frame&lt;/code&gt;
with less memory overhead.&lt;/p&gt;

&lt;p&gt;For example, suppose you had written the &lt;code class=&quot;highlighter-rouge&quot;&gt;iris&lt;/code&gt; dataset to Parquet. You could
read a &lt;code class=&quot;highlighter-rouge&quot;&gt;data.frame&lt;/code&gt; with only the columns &lt;code class=&quot;highlighter-rouge&quot;&gt;c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;)&lt;/code&gt; by
doing&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_parquet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;iris.parquet&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col_select&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;starts_with&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Sepal&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Just as you can read, you can write Parquet files:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;write_parquet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;path/to/different_file.parquet&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that this read and write support for Parquet files in R is in its early
stages of development. The Python Arrow library
(&lt;a href=&quot;https://arrow.apache.org/docs/python/&quot;&gt;pyarrow&lt;/a&gt;) still has much richer
support for Parquet files, including working with multi-file datasets. We
intend to reach feature equivalency between the R and Python packages in the
future.&lt;/p&gt;

&lt;h2 id=&quot;feather-files&quot;&gt;Feather files&lt;/h2&gt;

&lt;p&gt;This package also includes a faster and more robust implementation of the
Feather file format, providing &lt;code class=&quot;highlighter-rouge&quot;&gt;read_feather()&lt;/code&gt; and
&lt;code class=&quot;highlighter-rouge&quot;&gt;write_feather()&lt;/code&gt;. &lt;a href=&quot;https://github.com/wesm/feather&quot;&gt;Feather&lt;/a&gt; was one of the
initial applications of Apache Arrow for Python and R, providing an efficient,
common file format language-agnostic data frame storage, along with
implementations in R and Python.&lt;/p&gt;

&lt;p&gt;As Arrow progressed, development of Feather moved to the
&lt;a href=&quot;https://github.com/apache/arrow&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;apache/arrow&lt;/code&gt;&lt;/a&gt; project, and for the last two
years, the Python implementation of Feather has just been a wrapper around
&lt;code class=&quot;highlighter-rouge&quot;&gt;pyarrow&lt;/code&gt;. This meant that as Arrow progressed and bugs were fixed, the Python
version of Feather got the improvements but sadly R did not.&lt;/p&gt;

&lt;p&gt;With the &lt;code class=&quot;highlighter-rouge&quot;&gt;arrow&lt;/code&gt; package, the R implementation of Feather catches up and now
depends on the same underlying C++ library as the Python version does. This
should result in more reliable and consistent behavior across the two
languages, as well as &lt;a href=&quot;https://wesmckinney.com/blog/feather-arrow-future/&quot;&gt;improved
performance&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We encourage all R users of &lt;code class=&quot;highlighter-rouge&quot;&gt;feather&lt;/code&gt; to switch to using
&lt;code class=&quot;highlighter-rouge&quot;&gt;arrow::read_feather()&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;arrow::write_feather()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Note that both Feather and Parquet are columnar data formats that allow sharing
data frames across R, Pandas, and other tools. When should you use Feather and
when should you use Parquet? Parquet balances space-efficiency with
deserialization costs, making it an ideal choice for remote storage systems
like HDFS or Amazon S3. Feather is designed for fast local reads, particularly
with solid-state drives, and is not intended for use with remote storage
systems. Feather files can be memory-mapped and accessed as Arrow columnar data
in-memory without any deserialization while Parquet files always must be
decompressed and decoded. See the &lt;a href=&quot;https://arrow.apache.org/faq/&quot;&gt;Arrow project
FAQ&lt;/a&gt; for more.&lt;/p&gt;

&lt;h2 id=&quot;other-capabilities&quot;&gt;Other capabilities&lt;/h2&gt;

&lt;p&gt;In addition to these readers and writers, the &lt;code class=&quot;highlighter-rouge&quot;&gt;arrow&lt;/code&gt; package has wrappers for
other readers in the C++ library; see &lt;code class=&quot;highlighter-rouge&quot;&gt;?read_csv_arrow&lt;/code&gt; and
&lt;code class=&quot;highlighter-rouge&quot;&gt;?read_json_arrow&lt;/code&gt;. These readers are being developed to optimize for the
memory layout of the Arrow columnar format and are not intended as a direct
replacement for existing R CSV readers (&lt;code class=&quot;highlighter-rouge&quot;&gt;base::read.csv&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;readr::read_csv&lt;/code&gt;,
&lt;code class=&quot;highlighter-rouge&quot;&gt;data.table::fread&lt;/code&gt;) that return an R &lt;code class=&quot;highlighter-rouge&quot;&gt;data.frame&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;It also provides many lower-level bindings to the C++ library, which enable you
to access and manipulate Arrow objects. You can use these to build connectors
to other applications and services that use Arrow. One example is Spark: the
&lt;a href=&quot;https://spark.rstudio.com/&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sparklyr&lt;/code&gt;&lt;/a&gt; package has support for using Arrow to
move data to and from Spark, yielding &lt;a href=&quot;http://arrow.apache.org/blog/2019/01/25/r-spark-improvements/&quot;&gt;significant performance
gains&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;In addition to the work on wiring the R package up to the Arrow Parquet C++
library, a lot of effort went into building and packaging Arrow for R users,
ensuring its ease of installation across platforms. We’d like to thank the
support of Jeroen Ooms, Javier Luraschi, JJ Allaire, Davis Vaughan, the CRAN
team, and many others in the Apache Arrow community for helping us get to this
point.&lt;/p&gt;</content><author><name>npr</name></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /></entry><entry><title type="html">Apache Arrow 0.14.0 Release</title><link href="https://arrow.apache.org/blog/2019/07/02/0.14.0-release/" rel="alternate" type="text/html" title="Apache Arrow 0.14.0 Release" /><published>2019-07-02T02:00:00-04:00</published><updated>2019-07-02T02:00:00-04:00</updated><id>https://arrow.apache.org/blog/2019/07/02/0.14.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2019/07/02/0.14.0-release/">&lt;!--

--&gt;

&lt;p&gt;The Apache Arrow team is pleased to announce the 0.14.0 release. This
covers 3 months of development work and includes &lt;a href=&quot;https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%200.14.0&quot;&gt;&lt;strong&gt;602 resolved
issues&lt;/strong&gt;&lt;/a&gt; from &lt;a href=&quot;https://arrow.apache.org/release/0.14.0.html#contributors&quot;&gt;&lt;strong&gt;75 distinct contributors&lt;/strong&gt;&lt;/a&gt;.  See the Install
Page to learn how to get the libraries for your platform. The
&lt;a href=&quot;https://arrow.apache.org/release/0.14.0.html&quot;&gt;complete changelog&lt;/a&gt; is also available.&lt;/p&gt;

&lt;p&gt;This post will give some brief highlights in the project since the
0.13.0 release from April.&lt;/p&gt;

&lt;h2 id=&quot;new-committers&quot;&gt;New committers&lt;/h2&gt;

&lt;p&gt;Since the 0.13.0 release, the following have been added:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/nevi-me&quot;&gt;Neville Dipale&lt;/a&gt; was added as a committer&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/fsaintjacques&quot;&gt;François Saint-Jacques&lt;/a&gt; was added as a committer&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/praveenbingo&quot;&gt;Praveen Kumar&lt;/a&gt; was added as a committer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thank you for all your contributions!&lt;/p&gt;

&lt;h2 id=&quot;upcoming-100-format-stability-release&quot;&gt;Upcoming 1.0.0 Format Stability Release&lt;/h2&gt;

&lt;p&gt;We are planning for our next major release to move from 0.14.0 to
1.0.0. The major version number will indicate stability of the Arrow
columnar format and binary protocol. While the format has already been
stable since December 2017, we believe it is a good idea to make this
stability official and to indicate that it is safe to persist
serialized Arrow data in applications. This means that applications
will be able to safely upgrade to new Arrow versions without having to
worry about backwards incompatibilities. We will write in a future
blog post about the stability guarantees we intend to provide to help
application developers plan accordingly.&lt;/p&gt;

&lt;h2 id=&quot;packaging&quot;&gt;Packaging&lt;/h2&gt;

&lt;p&gt;We added support for the following platforms:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Debian GNU/Linux buster&lt;/li&gt;
  &lt;li&gt;Ubuntu 19.04&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We dropped support for Ubuntu 14.04.&lt;/p&gt;

&lt;h2 id=&quot;development-infrastructure-and-tooling&quot;&gt;Development Infrastructure and Tooling&lt;/h2&gt;

&lt;p&gt;As the project has grown larger and more diverse, we are increasingly
outgrowing what we can test in public continuous integration services
like Travis CI and Appveyor. In addition, we share these resources
with the entire Apache Software Foundation, and given the high volume
of pull requests into Apache Arrow, maintainers are frequently waiting
many hours for the green light to merge patches.&lt;/p&gt;

&lt;p&gt;The complexity of our testing is driven by the number of different
components and programming languages as well as increasingly long
compilation and test execution times as individual libraries grow
larger. The 50 minute time limit of public CI services is simply too
limited to comprehensively test the project. Additionally, the CI host
machines are constrained in their features and memory limits,
preventing us from testing features that are only relevant on large
amounts of data (10GB or more) or functionality that requires a
CUDA-enabled GPU.&lt;/p&gt;

&lt;p&gt;Organizations that contribute to Apache Arrow are working on physical
build infrastructure and tools to improve build times and build
scalability. One such new tool is &lt;code class=&quot;highlighter-rouge&quot;&gt;ursabot&lt;/code&gt;, a GitHub-enabled bot
that can be used to trigger builds either on physical build or in the
cloud. It can also be used to trigger benchmark timing comparisons. If
you are contributing to the project, you may see Ursabot being
employed to trigger tests in pull requests.&lt;/p&gt;

&lt;p&gt;To help assist with migrating away from Travis CI, we are also working
to make as many of our builds reproducible with Docker and not reliant
on Travis CI-specific configuration details. This will also help
contributors reproduce build failures locally without having to wait
for Travis CI.&lt;/p&gt;

&lt;h2 id=&quot;columnar-format-notes&quot;&gt;Columnar Format Notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;User-defined “extension” types have been formalized in the Arrow
format, enabling library users to embed custom data types in the
Arrow columnar format. Initial support is available in C++, Java,
and Python.&lt;/li&gt;
  &lt;li&gt;A new Duration logical type was added to represent absolute lengths
of time.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;arrow-flight-notes&quot;&gt;Arrow Flight notes&lt;/h2&gt;

&lt;p&gt;Flight now supports many of the features of a complete RPC
framework.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Authentication APIs are now supported across all languages (ARROW-5137)&lt;/li&gt;
  &lt;li&gt;Encrypted communication using OpenSSL is supported (ARROW-5643,
ARROW-5529)&lt;/li&gt;
  &lt;li&gt;Clients can specify timeouts on remote calls (ARROW-5136)&lt;/li&gt;
  &lt;li&gt;On the protocol level, endpoints are now identified with URIs, to
support an open-ended number of potential transports (including TLS
and Unix sockets, and perhaps even non-gRPC-based transports in the
future) (ARROW-4651)&lt;/li&gt;
  &lt;li&gt;Application-defined metadata can be sent alongside data (ARROW-4626,
ARROW-4627).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Windows is now a supported platform for Flight in C++ and Python
(ARROW-3294), and Python wheels are shipped for all languages
(ARROW-3150, ARROW-5656). C++, Python, and Java have been brought to
parity, now that actions can return streaming results in Java
(ARROW-5254).&lt;/p&gt;

&lt;h2 id=&quot;c-notes&quot;&gt;C++ notes&lt;/h2&gt;

&lt;p&gt;188 resolved issues related to the C++ implementation, so we summarize
some of the work here.&lt;/p&gt;

&lt;h3 id=&quot;general-platform-improvements&quot;&gt;General platform improvements&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;A FileSystem abstraction (ARROW-767) has been added, which paves the
way for a future Arrow Datasets library allowing to access sharded
data on arbitrary storage systems, including remote or cloud
storage. A first draft of the Datasets API was committed in
ARROW-5512. Right now, this comes with no implementation, but we
expect to slowly build it up in the coming weeks or months. Early
feedback is welcome on this API.&lt;/li&gt;
  &lt;li&gt;The dictionary API has been reworked in ARROW-3144. The dictionary
values used to be tied to the DictionaryType instance, which ended
up too inflexible. Since dictionary-encoding is more often an
optimization than a semantic property of the data, we decided to
move the dictionary values to the ArrayData structure, making it
natural for dictionary-encoded arrays to share the same DataType
instance, regardless of the encoding details.&lt;/li&gt;
  &lt;li&gt;The FixedSizeList and Map types have been implemented, including in
integration tests. The Map type is akin to a List of Struct(key,
value) entries, but making it explicit that the underlying data has
key-value mapping semantics. Also, map entries are always non-null.&lt;/li&gt;
  &lt;li&gt;A &lt;code class=&quot;highlighter-rouge&quot;&gt;Result&amp;lt;T&amp;gt;&lt;/code&gt; class has been introduced in ARROW-4800. The aim is to
allow to return an error as w ell as a function’s logical result
without resorting to pointer-out arguments.&lt;/li&gt;
  &lt;li&gt;The Parquet C++ library has been refactored to use common Arrow IO
classes for improved C++ platform interoperability.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;line-delimited-json-reader&quot;&gt;Line-delimited JSON reader&lt;/h3&gt;

&lt;p&gt;A multithreaded line-delimited JSON reader (powered internally by
RapidJSON) is now available for use (also in Python and R via
bindings) . This will likely be expanded to support more kinds of JSON
storage in the future.&lt;/p&gt;

&lt;h3 id=&quot;new-computational-kernels&quot;&gt;New computational kernels&lt;/h3&gt;

&lt;p&gt;A number of new computational kernels have been developed&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Compare filter for logical comparisons yielding boolean arrays&lt;/li&gt;
  &lt;li&gt;Filter kernel for selecting elements of an input array according to
a boolean selection array.&lt;/li&gt;
  &lt;li&gt;Take kernel, which selects elements by integer index, has been
expanded to support nested types&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;c-notes-1&quot;&gt;C# Notes&lt;/h2&gt;

&lt;p&gt;The native C# implementation has continued to mature since 0.13. This
release includes a number of performance, memory use, and usability
improvements.&lt;/p&gt;

&lt;h2 id=&quot;go-notes&quot;&gt;Go notes&lt;/h2&gt;

&lt;p&gt;Go’s support for the Arrow columnar format continues to expand. Go now
supports reading and writing the Arrow columnar binary protocol, and
it has also been &lt;strong&gt;added to the cross language integration
tests&lt;/strong&gt;. There are now four languages (C++, Go, Java, and JavaScript)
included in our integration tests to verify cross-language
interoperability.&lt;/p&gt;

&lt;h2 id=&quot;java-notes&quot;&gt;Java notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Support for referencing arbitrary memory using &lt;code class=&quot;highlighter-rouge&quot;&gt;ArrowBuf&lt;/code&gt; has been
implemented, paving the way for memory map support in Java&lt;/li&gt;
  &lt;li&gt;A number of performance improvements around vector value access were
added (see ARROW-5264, ARROW-5290).&lt;/li&gt;
  &lt;li&gt;The Map type has been implemented in Java and integration tested
with C++&lt;/li&gt;
  &lt;li&gt;Several microbenchmarks have been added and improved.  Including a
significant speed-up of zeroing out buffers.&lt;/li&gt;
  &lt;li&gt;A new algorithms package has been started to contain reference
implementations of common algorithms.  The initial contribution is
for Array/Vector sorting.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;javascript-notes&quot;&gt;JavaScript Notes&lt;/h2&gt;

&lt;p&gt;A new incremental &lt;a href=&quot;https://github.com/apache/arrow/tree/master/js/src/builder&quot;&gt;array builder API&lt;/a&gt; is available.&lt;/p&gt;

&lt;h2 id=&quot;matlab-notes&quot;&gt;MATLAB Notes&lt;/h2&gt;

&lt;p&gt;Version 0.14.0 features improved Feather file support in the MEX bindings.&lt;/p&gt;

&lt;h2 id=&quot;python-notes&quot;&gt;Python notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;We fixed a problem with the Python wheels causing the Python wheels
to be much larger in 0.13.0 than they were in 0.12.0. Since the
introduction of LLVM into our build toolchain, the wheels are going
to still be significantly bigger. We are interested in approaches to
enable pyarrow to be installed in pieces with pip or conda rather
than monolithically.&lt;/li&gt;
  &lt;li&gt;It is now possible to define ExtensionTypes with a Python
implementation (ARROW-840). Those ExtensionTypes can survive a
roundtrip through C++ and serialization.&lt;/li&gt;
  &lt;li&gt;The Flight improvements highlighted above (see C++ notes) are all
available from Python. Furthermore, Flight is now bundled in our
binary wheels and conda packages for Linux, Windows and macOS
(ARROW-3150, ARROW-5656).&lt;/li&gt;
  &lt;li&gt;We will build “manylinux2010” binary wheels for Linux systems, in
addition to “manylinux1” wheels (ARROW-2461). Manylinux2010 is a
newer standard for more recent systems, with less limiting toolchain
constraints. Installing manylinux2010 wheels requires an up-to-date
version of pip.&lt;/li&gt;
  &lt;li&gt;Various bug fixes for CSV reading in Python and C++ including the
ability to parse Decimal(x, y) columns.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;parquet-improvements&quot;&gt;Parquet improvements&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Column statistics for logical types like unicode strings, unsigned
integers, and timestamps are casted to compatible Python types (see
ARROW-4139)&lt;/li&gt;
  &lt;li&gt;It’s now possible to configure “data page” sizes when writing a file
from Python&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ruby-and-c-glib-notes&quot;&gt;Ruby and C GLib notes&lt;/h2&gt;

&lt;p&gt;The GLib and Ruby bindings have been tracking features in the C++
project. This release includes bindings for Gandiva, JSON reader, and
other C++ features.&lt;/p&gt;

&lt;h2 id=&quot;rust-notes&quot;&gt;Rust notes&lt;/h2&gt;

&lt;p&gt;There is ongoing work in Rust happening on Parquet file support,
computational kernels, and the DataFusion query engine. See the full
changelog for details.&lt;/p&gt;

&lt;h2 id=&quot;r-notes&quot;&gt;R notes&lt;/h2&gt;

&lt;p&gt;We have been working on build and packaging for R so that community
members can hopefully release the project to CRAN in the near
future. Feature development for R has continued to follow the upstream
C++ project.&lt;/p&gt;

&lt;h2 id=&quot;community-discussions-ongoing&quot;&gt;Community Discussions Ongoing&lt;/h2&gt;

&lt;p&gt;There are a number of active discussions ongoing on the developer
dev@arrow.apache.org mailing list. We look forward to hearing from the
community there:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://lists.apache.org/thread.html/44a7a3d256ab5dbd62da6fe45b56951b435697426bf4adedb6520907@%3Cdev.arrow.apache.org%3E&quot;&gt;Timing and scope of 1.0.0 release&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lists.apache.org/thread.html/96b2e22606e8a7b0ad7dc4aae16f232724d1059b34636676ed971d40@%3Cdev.arrow.apache.org%3E&quot;&gt;Solutions to increase continuous integration capacity&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lists.apache.org/thread.html/5715a4d402c835d22d929a8069c5c0cf232077a660ee98639d544af8@%3Cdev.arrow.apache.org%3E&quot;&gt;A proposal for versioning and forward/backward compatibility
guarantees for the 1.0.0 release&lt;/a&gt; was shared, not much discussion has
occurred yet.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lists.apache.org/thread.html/8440be572c49b7b2ffb76b63e6d935ada9efd9c1c2021369b6d27786@%3Cdev.arrow.apache.org%3E&quot;&gt;Addressing possible unaligned access and undefined behavior concerns&lt;/a&gt;
in the Arrow binary protocol&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lists.apache.org/thread.html/31b00086c2991104bd71fb1a2173f32b4a2f569d8e7b5b41e836f3a3@%3Cdev.arrow.apache.org%3E&quot;&gt;Supporting smaller than 128-bit encoding of fixed width decimals&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lists.apache.org/thread.html/97d78112ab583eecb155a7d78342c1063df65d64ec3ccfa0b18737c3@%3Cdev.arrow.apache.org%3E&quot;&gt;Forking the Avro C++ implementation&lt;/a&gt; so as to adapt it to Arrow’s
needs&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lists.apache.org/thread.html/a99124e57c14c3c9ef9d98f3c80cfe1dd25496bf3ff7046778add937@%3Cdev.arrow.apache.org%3E&quot;&gt;Sparse representation and compression in Arrow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lists.apache.org/thread.html/82a7c026ad18dbe9fdbcffa3560979aff6fd86dd56a49f40d9cfb46e@%3Cdev.arrow.apache.org%3E&quot;&gt;Flight extensions: middleware API and generalized Put operations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>pmc</name></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /></entry><entry><title type="html">Apache Arrow 0.13.0 Release</title><link href="https://arrow.apache.org/blog/2019/04/02/0.13.0-release/" rel="alternate" type="text/html" title="Apache Arrow 0.13.0 Release" /><published>2019-04-02T09:00:00-04:00</published><updated>2019-04-02T09:00:00-04:00</updated><id>https://arrow.apache.org/blog/2019/04/02/0.13.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2019/04/02/0.13.0-release/">&lt;!--

--&gt;

&lt;p&gt;The Apache Arrow team is pleased to announce the 0.13.0 release. This covers
more than 2 months of development work and includes &lt;a href=&quot;https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%200.13.0&quot;&gt;&lt;strong&gt;550 resolved
issues&lt;/strong&gt;&lt;/a&gt; from &lt;a href=&quot;https://arrow.apache.org/release/0.13.0.html#contributors&quot;&gt;&lt;strong&gt;81 distinct contributors&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;See the &lt;a href=&quot;https://arrow.apache.org/install&quot;&gt;Install Page&lt;/a&gt; to learn how to get the libraries for your
platform. The &lt;a href=&quot;https://arrow.apache.org/release/0.13.0.html&quot;&gt;complete changelog&lt;/a&gt; is also available.&lt;/p&gt;

&lt;p&gt;While it’s a large release, this post will give some brief highlights in the
project since the 0.12.0 release from January.&lt;/p&gt;

&lt;h2 id=&quot;new-committers-and-pmc-member&quot;&gt;New committers and PMC member&lt;/h2&gt;

&lt;p&gt;The Arrow team is growing! Since the 0.12.0 release we have increased the size
of our committer and PMC rosters.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/andygrove&quot;&gt;Andy Grove&lt;/a&gt; was promoted to PMC member&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/paddyhoran&quot;&gt;Paddy Horan&lt;/a&gt; was added as a committer&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/emkornfield&quot;&gt;Micah Kornfield&lt;/a&gt; was added as a committer&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pravindra&quot;&gt;Ravindra Pindikura&lt;/a&gt; was added as a committer&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/sunchao&quot;&gt;Chao Sun&lt;/a&gt; was added as a committer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thank you for all your contributions!&lt;/p&gt;

&lt;h2 id=&quot;rust-datafusion-query-engine-donation&quot;&gt;Rust DataFusion Query Engine donation&lt;/h2&gt;

&lt;p&gt;Since the last release, we received a donation of &lt;a href=&quot;http://incubator.apache.org/ip-clearance/arrow-rust-datafusion.html&quot;&gt;DataFusion&lt;/a&gt;, a
Rust-native query engine for the Arrow columnar format, whose development had
been led prior by Andy Grove. &lt;a href=&quot;http://arrow.apache.org/blog/2019/02/04/datafusion-donation/&quot;&gt;Read more about DataFusion&lt;/a&gt; in our February
blog post.&lt;/p&gt;

&lt;p&gt;This is an exciting development for the Rust community, and we look forward to
developing more analytical query processing within the Apache Arrow project.&lt;/p&gt;

&lt;h2 id=&quot;arrow-flight-grpc-progress&quot;&gt;Arrow Flight gRPC progress&lt;/h2&gt;

&lt;p&gt;Over the last couple months, we have made significant progress on Arrow Flight,
an Arrow-native data messaging framework. We have integration tests to check
C++ and Java compatibility, and we have added Python bindings for the C++
library. We will write a future blog post to go into more detail about how
Flight works.&lt;/p&gt;

&lt;h2 id=&quot;c-notes&quot;&gt;C++ notes&lt;/h2&gt;

&lt;p&gt;There were 231 issues relating to C++ in this release, far too much to
summarize in a blog post. Some notable items include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An experimental &lt;code class=&quot;highlighter-rouge&quot;&gt;ExtensionType&lt;/code&gt; was developed for creating user-defined data
types that can be embedded in the Arrow binary protocol. This is not yet
finalized, but &lt;a href=&quot;https://github.com/apache/arrow/blob/master/cpp/src/arrow/extension_type.h&quot;&gt;feedback would be welcome&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;We have undertaken a significant reworking of our CMake build system for C++
to make the third party dependencies more configurable. Among other things,
this eases work on packaging for Linux distributions. Read more about this in
the &lt;a href=&quot;https://github.com/apache/arrow/blob/master/docs/source/developers/cpp.rst#build-dependency-management&quot;&gt;C++ developer documentation&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Laying more groundwork for an Arrow-native in-memory query engine&lt;/li&gt;
  &lt;li&gt;We began building a reader for line-delimited JSON files&lt;/li&gt;
  &lt;li&gt;Gandiva can now be compiled on Windows with Visual Studio&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;c-notes-1&quot;&gt;C# Notes&lt;/h2&gt;

&lt;p&gt;C# .NET development has picked up since the initial code donation last
fall. 11 issues were resolved this release cycle.&lt;/p&gt;

&lt;p&gt;The Arrow C# package is &lt;a href=&quot;https://www.nuget.org/packages/Apache.Arrow/0.13.0&quot;&gt;now available via NuGet&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;go-notes&quot;&gt;Go notes&lt;/h2&gt;

&lt;p&gt;8 Go-related issues were resolved. A notable feature is the addition of a CSV
file writer.&lt;/p&gt;

&lt;h2 id=&quot;java-notes&quot;&gt;Java notes&lt;/h2&gt;

&lt;p&gt;26 Java issues were resolved. Outside of Flight-related work, some notable
items include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Migration to Java 8 date and time APIs from Joda&lt;/li&gt;
  &lt;li&gt;Array type support in JDBC adapter&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;javascript-notes&quot;&gt;Javascript Notes&lt;/h2&gt;

&lt;p&gt;The recent &lt;a href=&quot;https://www.npmjs.com/package/apache-arrow/v/0.4.1&quot;&gt;JavaScript 0.4.1 release&lt;/a&gt; is the last JavaScript-only release
of Apache Arrow. Starting with 0.13 the Javascript implementation is now
included in mainline Arrow releases! The version number of the released
JavaScript packages will now be in sync with the mainline version number.&lt;/p&gt;

&lt;h2 id=&quot;python-notes&quot;&gt;Python notes&lt;/h2&gt;

&lt;p&gt;86 Python-related issues were resolved. Some highlights include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The Gandiva LLVM expression compiler is now available in the Python wheels
through the &lt;code class=&quot;highlighter-rouge&quot;&gt;pyarrow.gandiva&lt;/code&gt; module.&lt;/li&gt;
  &lt;li&gt;Flight RPC bindings&lt;/li&gt;
  &lt;li&gt;Improved pandas serialization performance with RangeIndex&lt;/li&gt;
  &lt;li&gt;pyarrow can be used without pandas installed&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that Apache Arrow will continue to support Python 2.7 until January 2020.&lt;/p&gt;

&lt;h2 id=&quot;ruby-and-c-glib-notes&quot;&gt;Ruby and C GLib notes&lt;/h2&gt;

&lt;p&gt;36 C/GLib- and Ruby-related issues were resolved. The work continues to follow
the upstream work in the C++ project.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Arrow::RecordBatch#raw_records&lt;/code&gt; was added. It can convert a record batch to
a Ruby’s array in 10x-200x faster than the same conversion by a pure-Ruby
implementation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;rust-notes&quot;&gt;Rust notes&lt;/h2&gt;

&lt;p&gt;69 Rust-related issues were resolved. Many of these relate to ongoing work in
the DataFusion query engine. Some notable items include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Date/time support&lt;/li&gt;
  &lt;li&gt;SIMD for arithmetic operations&lt;/li&gt;
  &lt;li&gt;Writing CSV and reading line-delimited JSON&lt;/li&gt;
  &lt;li&gt;Parquet data source support for DataFusion&lt;/li&gt;
  &lt;li&gt;Prototype DataFrame-style API for DataFusion&lt;/li&gt;
  &lt;li&gt;Continued evolution of Parquet file reader&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;r-development-progress&quot;&gt;R development progress&lt;/h2&gt;

&lt;p&gt;The Arrow R developers have expanded the scope of the R language bindings and
additionally worked on packaging support to be able to submit the package to
CRAN in the near future. 23 issues were resolved for this release.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://arrow.apache.org/blog/2019/01/25/r-spark-improvements/&quot;&gt;We wrote in January about ongoing work&lt;/a&gt; to accelerate R work on Apache Spark
using Arrow.&lt;/p&gt;

&lt;h2 id=&quot;community-discussions-ongoing&quot;&gt;Community Discussions Ongoing&lt;/h2&gt;

&lt;p&gt;There are a number of active discussions ongoing on the developer
&lt;code class=&quot;highlighter-rouge&quot;&gt;dev@arrow.apache.org&lt;/code&gt; mailing list. We look forward to hearing from the
community there:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Benchmarking&lt;/strong&gt;: we are working to create tools for tracking all of our
benchmark results on a commit-by-commit basis in a centralized database
schema so that we can monitor for performance regressions over time. We hope
to develop a publicly viewable benchmark result dashboard.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;C++ Datasets&lt;/strong&gt;: development of a unified API for reading and writing
datasets stored in various common formats like Parquet, JSON, and CSV.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;C++ Query Engine&lt;/strong&gt;: architecture of a parallel Arrow-native query engine
for C++&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Arrow Flight Evolution&lt;/strong&gt;: adding features to support different real-world
data messaging use cases&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Arrow Columnar Format evolution&lt;/strong&gt;: we are discussing a new “duration” or
“time interval” type and some other additions to the Arrow columnar format.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>wesm</name></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /></entry><entry><title type="html">Reducing Python String Memory Use in Apache Arrow 0.12</title><link href="https://arrow.apache.org/blog/2019/02/05/python-string-memory-0.12/" rel="alternate" type="text/html" title="Reducing Python String Memory Use in Apache Arrow 0.12" /><published>2019-02-05T08:00:00-05:00</published><updated>2019-02-05T08:00:00-05:00</updated><id>https://arrow.apache.org/blog/2019/02/05/python-string-memory-0.12</id><content type="html" xml:base="https://arrow.apache.org/blog/2019/02/05/python-string-memory-0.12/">&lt;!--

--&gt;

&lt;p&gt;Python users who upgrade to recently released &lt;code class=&quot;highlighter-rouge&quot;&gt;pyarrow&lt;/code&gt; 0.12 may find that
their applications use significantly less memory when converting Arrow string
data to pandas format. This includes using &lt;code class=&quot;highlighter-rouge&quot;&gt;pyarrow.parquet.read_table&lt;/code&gt; and
&lt;code class=&quot;highlighter-rouge&quot;&gt;pandas.read_parquet&lt;/code&gt;. This article details some of what is going on under the
hood, and why Python applications dealing with large amounts of strings are
prone to memory use problems.&lt;/p&gt;

&lt;h2 id=&quot;why-python-strings-can-use-a-lot-of-memory&quot;&gt;Why Python strings can use a lot of memory&lt;/h2&gt;

&lt;p&gt;Let’s start with some possibly surprising facts. I’m going to create an empty
&lt;code class=&quot;highlighter-rouge&quot;&gt;bytes&lt;/code&gt; object and an empty &lt;code class=&quot;highlighter-rouge&quot;&gt;str&lt;/code&gt; (unicode) object in Python 3.7:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;In [1]: val = b''

In [2]: unicode_val = u''
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;sys.getsizeof&lt;/code&gt; function accurately reports the number of bytes used by
built-in Python objects. You might be surprised to find that:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;In [4]: import sys
In [5]: sys.getsizeof(val)
Out[5]: 33

In [6]: sys.getsizeof(unicode_val)
Out[6]: 49
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since strings in Python are nul-terminated, we can infer that a bytes object
has 32 bytes of overhead while unicode has 48 bytes. One must also account for
&lt;code class=&quot;highlighter-rouge&quot;&gt;PyObject*&lt;/code&gt; pointer references to the objects, so the actual overhead is 40 and
56 bytes, respectively. With large strings and text, this overhead may not
matter much, but when you have a lot of small strings, such as those arising
from reading a CSV or Apache Parquet file, they can take up an unexpected
amount of memory. pandas represents strings in NumPy arrays of &lt;code class=&quot;highlighter-rouge&quot;&gt;PyObject*&lt;/code&gt;
pointers, so the total memory used by a unique unicode string is&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;8 (PyObject*) + 48 (Python C struct) + string_length + 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Suppose that we read a CSV file with&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1 column&lt;/li&gt;
  &lt;li&gt;1 million rows&lt;/li&gt;
  &lt;li&gt;Each value in the column is a string with 10 characters&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On disk this file would take approximately 10MB. Read into memory, however, it
could take up over 60MB, as a 10 character string object takes up 67 bytes in a
&lt;code class=&quot;highlighter-rouge&quot;&gt;pandas.Series&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;how-apache-arrow-represents-strings&quot;&gt;How Apache Arrow represents strings&lt;/h2&gt;

&lt;p&gt;While a Python unicode string can have 57 bytes of overhead, a string in the
Arrow columnar format has only 4 (32 bits) or 4.125 (33 bits) bytes of
overhead. 32-bit integer offsets encodes the position and size of a string
value in a contiguous chunk of memory:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/img/20190205-arrow-string.png&quot; alt=&quot;Apache Arrow string memory layout&quot; width=&quot;80%&quot; class=&quot;img-responsive&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;When you call &lt;code class=&quot;highlighter-rouge&quot;&gt;table.to_pandas()&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;array.to_pandas()&lt;/code&gt; with &lt;code class=&quot;highlighter-rouge&quot;&gt;pyarrow&lt;/code&gt;, we
have to convert this compact string representation back to pandas’s
Python-based strings. This can use a huge amount of memory when we have a large
number of small strings. It is a quite common occurrence when working with web
analytics data, which compresses to a compact size when stored in the Parquet
columnar file format.&lt;/p&gt;

&lt;p&gt;Note that the Arrow string memory format has other benefits beyond memory
use. It is also much more efficient for analytics due to the guarantee of data
locality; all strings are next to each other in memory. In the case of pandas
and Python strings, the string data can be located anywhere in the process
heap. Arrow PMC member Uwe Korn did some work to &lt;a href=&quot;https://www.slideshare.net/xhochy/extending-pandas-using-apache-arrow-and-numba&quot;&gt;extend pandas with Arrow
string arrays&lt;/a&gt; for improved performance and memory use.&lt;/p&gt;

&lt;h2 id=&quot;reducing-pandas-memory-use-when-converting-from-arrow&quot;&gt;Reducing pandas memory use when converting from Arrow&lt;/h2&gt;

&lt;p&gt;For many years, the &lt;code class=&quot;highlighter-rouge&quot;&gt;pandas.read_csv&lt;/code&gt; function has relied on a trick to limit
the amount of string memory allocated. Because pandas uses arrays of
&lt;code class=&quot;highlighter-rouge&quot;&gt;PyObject*&lt;/code&gt; pointers to refer to objects in the Python heap, we can avoid
creating multiple strings with the same value, instead reusing existing objects
and incrementing their reference counts.&lt;/p&gt;

&lt;p&gt;Schematically, we have the following:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/img/20190205-numpy-string.png&quot; alt=&quot;pandas string memory optimization&quot; width=&quot;80%&quot; class=&quot;img-responsive&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;In &lt;code class=&quot;highlighter-rouge&quot;&gt;pyarrow&lt;/code&gt; 0.12, we have implemented this when calling &lt;code class=&quot;highlighter-rouge&quot;&gt;to_pandas&lt;/code&gt;. It
requires using a hash table to deduplicate the Arrow string data as it’s being
converted to pandas. Hashing data is not free, but counterintuitively it can be
faster in addition to being vastly more memory efficient in the common case in
analytics where we have table columns with many instances of the same string
values.&lt;/p&gt;

&lt;h2 id=&quot;memory-and-performance-benchmarks&quot;&gt;Memory and Performance Benchmarks&lt;/h2&gt;

&lt;p&gt;We can use the &lt;a href=&quot;https://pypi.org/project/memory-profiler/&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;memory_profiler&lt;/code&gt;&lt;/a&gt; Python package to easily get process
memory usage within a running Python application.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;memory_profiler&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mem&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;memory_profiler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;memory_usage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In a new application I have:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;In [7]: mem()
Out[7]: 86.21875
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I will generate approximate 1 gigabyte of string data represented as Python
strings with length 10. The &lt;code class=&quot;highlighter-rouge&quot;&gt;pandas.util.testing&lt;/code&gt; module has a handy &lt;code class=&quot;highlighter-rouge&quot;&gt;rands&lt;/code&gt;
function for generating random strings. Here is the data generation function:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas.util.testing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rands&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generate_strings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nunique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_length&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;unique_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rands&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nunique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unique_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nunique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This generates a certain number of unique strings, then duplicates then to
yield the desired number of total strings. So I’m going to create 100 million
strings with only 10000 unique values:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;In [8]: values = generate_strings(100000000, 10000)

In [9]: mem()
Out[9]: 852.140625
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;100 million &lt;code class=&quot;highlighter-rouge&quot;&gt;PyObject*&lt;/code&gt; values is only 745 MB, so this increase of a little
over 770 MB is consistent with what we know so far. Now I’m going to convert
this to Arrow format:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;In [11]: arr = pa.array(values)

In [12]: mem()
Out[12]: 2276.9609375
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since &lt;code class=&quot;highlighter-rouge&quot;&gt;pyarrow&lt;/code&gt; exactly accounts for all of its memory allocations, we also
check that&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;In [13]: pa.total_allocated_bytes()
Out[13]: 1416777280
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since each string takes about 14 bytes (10 bytes plus 4 bytes of overhead),
this is what we expect.&lt;/p&gt;

&lt;p&gt;Now, converting &lt;code class=&quot;highlighter-rouge&quot;&gt;arr&lt;/code&gt; back to pandas is where things get tricky. The &lt;em&gt;minimum&lt;/em&gt;
amount of memory that pandas can use is a little under 800 MB as above as we
need 100 million &lt;code class=&quot;highlighter-rouge&quot;&gt;PyObject*&lt;/code&gt; values, which are 8 bytes each.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;In [14]: arr_as_pandas = arr.to_pandas()

In [15]: mem()
Out[15]: 3041.78125
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Doing the math, we used 765 MB which seems right. We can disable the string
deduplication logic by passing &lt;code class=&quot;highlighter-rouge&quot;&gt;deduplicate_objects=False&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;to_pandas&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;In [16]: arr_as_pandas_no_dedup = arr.to_pandas(deduplicate_objects=False)

In [17]: mem()
Out[17]: 10006.95703125
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Without object deduplication, we use 6965 megabytes, or an average of 73 bytes
per value. This is a little bit higher than the theoretical size of 67 bytes
computed above.&lt;/p&gt;

&lt;p&gt;One of the more surprising results is that the new behavior is about twice as fast:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;In [18]: %time arr_as_pandas_time = arr.to_pandas()
CPU times: user 2.94 s, sys: 213 ms, total: 3.15 s
Wall time: 3.14 s

In [19]: %time arr_as_pandas_no_dedup_time = arr.to_pandas(deduplicate_objects=False)
CPU times: user 4.19 s, sys: 2.04 s, total: 6.23 s
Wall time: 6.21 s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The reason for this is that creating so many Python objects is more expensive
than hashing the 10 byte values and looking them up in a hash table.&lt;/p&gt;

&lt;p&gt;Note that when you convert Arrow data with mostly unique values back to pandas,
the memory use benefits here won’t have as much of an impact.&lt;/p&gt;

&lt;h2 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;In Apache Arrow, our goal is to develop computational tools to operate natively
on the cache- and SIMD-friendly efficient Arrow columnar format. In the
meantime, though, we recognize that users have legacy applications using the
native memory layout of pandas or other analytics tools. We will do our best to
provide fast and memory-efficient interoperability with pandas and other
popular libraries.&lt;/p&gt;</content><author><name>wesm</name></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /></entry><entry><title type="html">DataFusion: A Rust-native Query Engine for Apache Arrow</title><link href="https://arrow.apache.org/blog/2019/02/04/datafusion-donation/" rel="alternate" type="text/html" title="DataFusion: A Rust-native Query Engine for Apache Arrow" /><published>2019-02-04T01:00:00-05:00</published><updated>2019-02-04T01:00:00-05:00</updated><id>https://arrow.apache.org/blog/2019/02/04/datafusion-donation</id><content type="html" xml:base="https://arrow.apache.org/blog/2019/02/04/datafusion-donation/">&lt;!--

--&gt;

&lt;p&gt;We are excited to announce that &lt;a href=&quot;https://github.com/apache/arrow/tree/master/rust/datafusion&quot;&gt;DataFusion&lt;/a&gt; has been donated to the Apache Arrow project. DataFusion is an in-memory query engine for the Rust implementation of Apache Arrow.&lt;/p&gt;

&lt;p&gt;Although DataFusion was started two years ago, it was recently re-implemented to be Arrow-native and currently has limited capabilities but does support SQL queries against iterators of RecordBatch and has support for CSV files. There are plans to &lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-4466&quot;&gt;add support for Parquet files&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;SQL support is limited to projection (&lt;code class=&quot;highlighter-rouge&quot;&gt;SELECT&lt;/code&gt;), selection (&lt;code class=&quot;highlighter-rouge&quot;&gt;WHERE&lt;/code&gt;), and simple aggregates (&lt;code class=&quot;highlighter-rouge&quot;&gt;MIN&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;MAX&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;SUM&lt;/code&gt;) with an optional &lt;code class=&quot;highlighter-rouge&quot;&gt;GROUP BY&lt;/code&gt; clause.&lt;/p&gt;

&lt;p&gt;Supported expressions are identifiers, literals, simple math operations (&lt;code class=&quot;highlighter-rouge&quot;&gt;+&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;-&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;*&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;/&lt;/code&gt;), binary expressions (&lt;code class=&quot;highlighter-rouge&quot;&gt;AND&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;OR&lt;/code&gt;), equality and comparison operators (&lt;code class=&quot;highlighter-rouge&quot;&gt;=&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;!=&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;=&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;gt;=&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;gt;&lt;/code&gt;), and &lt;code class=&quot;highlighter-rouge&quot;&gt;CAST(expr AS type)&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;example&quot;&gt;Example&lt;/h2&gt;

&lt;p&gt;The following example demonstrates running a simple aggregate SQL query against a CSV file.&lt;/p&gt;

&lt;div class=&quot;language-rust highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;// create execution context&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;mut&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;ExecutionContext&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;// define schema for data source (csv file)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;Arc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nn&quot;&gt;Schema&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;vec!&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;nn&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;c1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;DataType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Utf8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;nn&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;c2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;DataType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;UInt32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;nn&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;c3&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;DataType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Int8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;nn&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;c4&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;DataType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Int16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;nn&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;c5&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;DataType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Int32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;nn&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;c6&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;DataType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Int64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;nn&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;c7&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;DataType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;UInt8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;nn&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;c8&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;DataType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;UInt16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;nn&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;c9&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;DataType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;UInt32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;nn&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;c10&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;DataType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;UInt64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;nn&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;c11&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;DataType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;nn&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;c12&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;DataType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;nn&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;c13&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;DataType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Utf8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]));&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;// register csv file with the execution context&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csv_datasource&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;
    &lt;span class=&quot;nn&quot;&gt;CsvDataSource&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;test/data/aggregate_test_100.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;.clone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;.register_datasource&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;aggregate_test_100&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;Rc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nn&quot;&gt;RefCell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;csv_datasource&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)));&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;SELECT c1, MIN(c12), MAX(c12) FROM aggregate_test_100 WHERE c11 &amp;gt; 0.1 AND c11 &amp;lt; 0.9 GROUP BY c1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;// execute the query&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;relation&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;.sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;.unwrap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;mut&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;relation&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;.borrow_mut&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;// iterate over the results&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Some&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;.next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;.unwrap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nd&quot;&gt;println!&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;RecordBatch has {} rows and {} columns&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;.num_rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;.num_columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;
        &lt;span class=&quot;nf&quot;&gt;.column&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;nf&quot;&gt;.as_any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;py&quot;&gt;.downcast_ref&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BinaryArray&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;nf&quot;&gt;.unwrap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;
        &lt;span class=&quot;nf&quot;&gt;.column&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;nf&quot;&gt;.as_any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;py&quot;&gt;.downcast_ref&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Float64Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;nf&quot;&gt;.unwrap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;
        &lt;span class=&quot;nf&quot;&gt;.column&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;nf&quot;&gt;.as_any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;py&quot;&gt;.downcast_ref&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Float64Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;nf&quot;&gt;.unwrap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;..&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;.num_rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c1_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;from_utf8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c1&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;.value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;.to_vec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;.unwrap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
        &lt;span class=&quot;nd&quot;&gt;println!&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{}, Min: {}, Max: {}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c1_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;.value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;.value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;roadmap&quot;&gt;Roadmap&lt;/h2&gt;

&lt;p&gt;The roadmap for DataFusion will depend on interest from the Rust community, but here are some of the short term items that are planned:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Extending test coverage of the existing functionality&lt;/li&gt;
  &lt;li&gt;Adding support for Parquet data sources&lt;/li&gt;
  &lt;li&gt;Implementing more SQL features such as &lt;code class=&quot;highlighter-rouge&quot;&gt;JOIN&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;ORDER BY&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;LIMIT&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Implement a DataFrame API as an alternative to SQL&lt;/li&gt;
  &lt;li&gt;Adding support for partitioning and parallel query execution using Rust’s async and await functionality&lt;/li&gt;
  &lt;li&gt;Creating a Docker image to make it easy to use DataFusion as a standalone query tool for interactive and batch queries&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;contributors-welcome&quot;&gt;Contributors Welcome!&lt;/h2&gt;

&lt;p&gt;If you are excited about being able to use Rust for data science and would like to contribute to this work then there are many ways to get involved. The simplest way to get started is to try out DataFusion against your own data sources and file bug reports for any issues that you find. You could also check out the current &lt;a href=&quot;https://cwiki.apache.org/confluence/display/ARROW/Rust+JIRA+Dashboard&quot;&gt;list of issues&lt;/a&gt; and have a go at fixing one. You can also join the &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/arrow-user/&quot;&gt;user mailing list&lt;/a&gt; to ask questions.&lt;/p&gt;</content><author><name>agrove</name></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /></entry><entry><title type="html">Speeding up R and Apache Spark using Apache Arrow</title><link href="https://arrow.apache.org/blog/2019/01/25/r-spark-improvements/" rel="alternate" type="text/html" title="Speeding up R and Apache Spark using Apache Arrow" /><published>2019-01-25T01:00:00-05:00</published><updated>2019-01-25T01:00:00-05:00</updated><id>https://arrow.apache.org/blog/2019/01/25/r-spark-improvements</id><content type="html" xml:base="https://arrow.apache.org/blog/2019/01/25/r-spark-improvements/">&lt;!--

--&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://github.com/javierluraschi&quot;&gt;Javier Luraschi&lt;/a&gt; is a software engineer at &lt;a href=&quot;https://rstudio.com&quot;&gt;RStudio&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Support for Apache Arrow in Apache Spark with R is currently under active
development in the &lt;a href=&quot;https://github.com/rstudio/sparklyr&quot;&gt;sparklyr&lt;/a&gt; and &lt;a href=&quot;https://spark.apache.org/docs/latest/sparkr.html&quot;&gt;SparkR&lt;/a&gt; projects. This post explores early, yet
promising, performance improvements achieved when using R with &lt;a href=&quot;https://spark.apache.org&quot;&gt;Apache Spark&lt;/a&gt;,
Arrow and &lt;code class=&quot;highlighter-rouge&quot;&gt;sparklyr&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;setup&quot;&gt;Setup&lt;/h1&gt;

&lt;p&gt;Since this work is under active development, install &lt;code class=&quot;highlighter-rouge&quot;&gt;sparklyr&lt;/code&gt; and
&lt;code class=&quot;highlighter-rouge&quot;&gt;arrow&lt;/code&gt; from GitHub as follows:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;devtools&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;install_github&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;apache/arrow&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subdir&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;r&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ref&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;apache-arrow-0.12.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;devtools&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;install_github&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;rstudio/sparklyr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ref&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;apache-arrow-0.12.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this benchmark, we will use &lt;a href=&quot;https://dplyr.tidyverse.org&quot;&gt;dplyr&lt;/a&gt;, but similar improvements can
be  expected from using &lt;a href=&quot;https://cran.r-project.org/package=DBI&quot;&gt;DBI&lt;/a&gt;, or &lt;a href=&quot;https://spark.rstudio.com/reference/#section-spark-dataframes&quot;&gt;Spark DataFrames&lt;/a&gt; in &lt;code class=&quot;highlighter-rouge&quot;&gt;sparklyr&lt;/code&gt;.
The local Spark connection and dataframe with 10M numeric rows was
initialized as follows:&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sparklyr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dplyr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spark_connect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;master&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;local&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;sparklyr.shell.driver-memory&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;6g&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;runif&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;copying&quot;&gt;Copying&lt;/h1&gt;

&lt;p&gt;Currently, copying data to Spark using &lt;code class=&quot;highlighter-rouge&quot;&gt;sparklyr&lt;/code&gt; is performed by persisting
data on-disk from R and reading it back from Spark. This was meant to be used
for small datasets since there are better tools to transfer data into
distributed storage systems. Nevertheless, many users have requested support to
transfer more data at fast speeds into Spark.&lt;/p&gt;

&lt;p&gt;Using &lt;code class=&quot;highlighter-rouge&quot;&gt;arrow&lt;/code&gt; with &lt;code class=&quot;highlighter-rouge&quot;&gt;sparklyr&lt;/code&gt;, we can transfer data directly from R to
Spark without having to serialize this data in R or persist in disk.&lt;/p&gt;

&lt;p&gt;The following example copies 10M rows from R into Spark using &lt;code class=&quot;highlighter-rouge&quot;&gt;sparklyr&lt;/code&gt;
with and without &lt;code class=&quot;highlighter-rouge&quot;&gt;arrow&lt;/code&gt;, there is close to a 16x improvement using &lt;code class=&quot;highlighter-rouge&quot;&gt;arrow&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This benchmark uses the &lt;a href=&quot;https://CRAN.R-project.org/package=microbenchmark&quot;&gt;microbenchmark&lt;/a&gt; R package, which runs code
multiple times, provides stats on total execution time and plots each
excecution time to understand the distribution over each iteration.&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;microbenchmark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;microbenchmark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setup&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arrow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arrow_on&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sparklyr_df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy_to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;overwrite&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sparklyr_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arrow_off&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;arrow&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%in%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;.packages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;package:arrow&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sparklyr_df&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy_to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;overwrite&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sparklyr_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;times&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%T&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autoplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; Unit: seconds
      expr       min        lq       mean    median         uq       max neval
  arrow_on  3.011515  4.250025   7.257739  7.273011   8.974331  14.23325    10
 arrow_off 50.051947 68.523081 119.946947 71.898908 138.743419 390.44028    10
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/img/arrow-r-spark-copying.png&quot; alt=&quot;Copying data with R into Spark with and without Arrow&quot; width=&quot;60%&quot; class=&quot;img-responsive&quot; /&gt;
&lt;/div&gt;

&lt;h1 id=&quot;collecting&quot;&gt;Collecting&lt;/h1&gt;

&lt;p&gt;Similarly, &lt;code class=&quot;highlighter-rouge&quot;&gt;arrow&lt;/code&gt; with &lt;code class=&quot;highlighter-rouge&quot;&gt;sparklyr&lt;/code&gt; can now avoid deserializing data in R
while collecting data from Spark into R. These improvements are not as
significant as copying data since, &lt;code class=&quot;highlighter-rouge&quot;&gt;sparklyr&lt;/code&gt; already collects data in
columnar format.&lt;/p&gt;

&lt;p&gt;The following benchmark collects 10M rows from Spark into R and shows that
&lt;code class=&quot;highlighter-rouge&quot;&gt;arrow&lt;/code&gt; can bring 3x improvements.&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;microbenchmark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;microbenchmark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setup&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arrow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arrow_on&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sparklyr_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arrow_off&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;arrow&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%in%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;.packages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;package:arrow&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sparklyr_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;times&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%T&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autoplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Unit: seconds
      expr      min        lq      mean    median        uq       max neval
  arrow_on 4.520593  5.609812  6.154509  5.928099  6.217447  9.432221    10
 arrow_off 7.882841 13.358113 16.670708 16.127704 21.051382 24.373331    10
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/img/arrow-r-spark-collecting.png&quot; alt=&quot;Collecting data with R from Spark with and without Arrow&quot; width=&quot;60%&quot; class=&quot;img-responsive&quot; /&gt;
&lt;/div&gt;

&lt;h1 id=&quot;transforming&quot;&gt;Transforming&lt;/h1&gt;

&lt;p&gt;Today, custom transformations of data using R functions are performed in
&lt;code class=&quot;highlighter-rouge&quot;&gt;sparklyr&lt;/code&gt; by moving data in row-format from Spark into an R process through a
socket connection, transferring data in row-format is inefficient since
multiple data types need to be deserialized over each row, then the data gets
converted to columnar format (R was originally designed to use columnar data),
once R finishes this computation, data is again converted to row-format,
serialized row-by-row and then sent back to Spark over the socket connection.&lt;/p&gt;

&lt;p&gt;By adding support for &lt;code class=&quot;highlighter-rouge&quot;&gt;arrow&lt;/code&gt; in &lt;code class=&quot;highlighter-rouge&quot;&gt;sparklyr&lt;/code&gt;, it makes Spark perform the
row-format to column-format conversion in parallel in Spark. Data
is then transferred through the socket but no custom serialization takes place.
All the R process needs to do is copy this data from the socket into its heap,
transform it and copy it back to the socket connection.&lt;/p&gt;

&lt;p&gt;The following example transforms 100K rows with and without &lt;code class=&quot;highlighter-rouge&quot;&gt;arrow&lt;/code&gt; enabled,
&lt;code class=&quot;highlighter-rouge&quot;&gt;arrow&lt;/code&gt; makes transformation with R functions close to 41x faster.&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;microbenchmark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;microbenchmark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setup&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arrow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arrow_on&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sparklyr_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spark_apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;.x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arrow_off&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;arrow&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%in%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;.packages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;package:arrow&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sparklyr_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spark_apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;.x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;times&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%T&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autoplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Unit: seconds
      expr        min         lq       mean     median         uq        max neval
  arrow_on   3.881293   4.038376   5.136604   4.772739   5.759082   7.873711    10
 arrow_off 178.605733 183.654887 213.296238 227.182018 233.601885 238.877341    10
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;/img/arrow-r-spark-transforming.png&quot; alt=&quot;Transforming data with R in Spark with and without Arrow&quot; width=&quot;60%&quot; class=&quot;img-responsive&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Additional benchmarks and fine-tuning parameters can be found under &lt;code class=&quot;highlighter-rouge&quot;&gt;sparklyr&lt;/code&gt;
&lt;a href=&quot;https://github.com/rstudio/sparklyr/pull/1611&quot;&gt;/rstudio/sparklyr/pull/1611&lt;/a&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;SparkR&lt;/code&gt; &lt;a href=&quot;https://github.com/apache/spark/pull/22954&quot;&gt;/apache/spark/pull/22954&lt;/a&gt;. Looking forward to bringing this feature
to the Spark, Arrow and R communities.&lt;/p&gt;</content><author><name>Javier Luraschi</name></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /></entry></feed>