<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://arrow.apache.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arrow.apache.org/" rel="alternate" type="text/html" /><updated>2022-11-29T12:20:07-05:00</updated><id>https://arrow.apache.org/feed.xml</id><title type="html">Apache Arrow</title><subtitle>Apache Arrow is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, C++, C#, Go, Java, JavaScript, MATLAB, Python, R, Ruby, and Rust.</subtitle><entry><title type="html">Fast and Memory Efficient Multi-Column Sorts in Apache Arrow Rust, Part 1</title><link href="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1/" rel="alternate" type="text/html" title="Fast and Memory Efficient Multi-Column Sorts in Apache Arrow Rust, Part 1" /><published>2022-11-07T00:00:00-05:00</published><updated>2022-11-07T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1/"><![CDATA[<!--

-->

<h2 id="introduction">Introduction</h2>

<p>Sorting is one of the most fundamental operations in modern databases and other analytic systems, underpinning important operators such as aggregates, joins, window functions, merge, and more. By some estimates, more than half of the execution time in data processing systems is spent sorting. Optimizing sorts is therefore vital to improving query performance and overall system efficiency.</p>

<p>Sorting is also one of the most well studied topics in computer science. The classic survey paper for databases is <a href="https://dl.acm.org/doi/10.1145/1132960.1132964">Implementing Sorting in Database Systems</a> by Goetz Graefe which provides a thorough academic treatment and is still very applicable today. However, it may not be obvious how to apply the wisdom and advanced techniques described in that paper to modern systems. In addition, the excellent <a href="https://duckdb.org/2021/08/27/external-sorting.html">DuckDB blog on sorting</a> highlights many sorting techniques, and mentions a comparable row format, but it does not explain how to efficiently sort variable length strings or dictionary encoded data.</p>

<p>In this series we explain in detail the new <a href="https://docs.rs/arrow/27.0.0/arrow/row/index.html">row format</a> in the <a href="https://github.com/apache/arrow-rs">Rust implementation</a> of <a href="https://arrow.apache.org/">Apache Arrow</a>, and how we used to make sorting more than <a href="https://github.com/apache/arrow-rs/pull/2929">3x</a> faster than an alternate comparator based approach. The benefits are especially pronounced for strings, dictionary encoded data, and sorts with large numbers of columns.</p>

<h2 id="multicolumn--lexicographical-sort-problem">Multicolumn / Lexicographical Sort Problem</h2>

<p>Most languages have native, optimized operations to sort a single column (array) of data, which are specialized based on the type of data being sorted. The reason that sorting is typically more challenging in analytic systems is that:</p>

<ol>
  <li>They must support multiple columns of data</li>
  <li>The column types are not knowable at compile time, and thus the compiler can not typically generate optimized code</li>
</ol>

<p>Multicolumn sorting is also referred to as lexicographical sorting in some libraries.</p>

<p>For example, given sales data for various customers and their state of residence, a user might want to find the lowest 10 orders for each state.</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Customer | State | Orders
—--------+-------+-------
12345    |  MA   |  10.12
532432   |  MA   |  8.44
12345    |  CA   |  3.25
56232    |  WA   |  6.00
23442    |  WA   |  132.50
7844     |  CA   |  9.33
852353   |  MA   |  1.30
</code></pre></div></div>

<p>One way to do so is to order the data first by <code class="language-plaintext highlighter-rouge">State</code> and then by <code class="language-plaintext highlighter-rouge">Orders</code>:</p>
<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Customer | State | Orders
—--------+-------+-------
12345    |  CA   |  3.25
7844     |  CA   |  9.33
852353   |  MA   |  1.30
532432   |  MA   |  8.44
12345    |  MA   |  10.12
56232    |  WA   |  6.00
23442    |  WA   |  132.50
</code></pre></div></div>

<p>(Note: While there are specialized ways for computing this particular query other than fully sorting the entire input (e.g. “TopK”), they typically need the same multi-column comparison operation described below. Thus while we will use the simplified example in this series, it applies much more broadly)</p>

<h2 id="basic-implementation">Basic Implementation</h2>

<p>Let us take the example of a basic sort kernel which takes a set of columns as input, and returns a list of indices identifying a sorted order.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="n">lexsort_to_indices</span><span class="p">([</span>
    <span class="p">[</span><span class="s">"MA"</span><span class="p">,</span> <span class="s">"MA"</span><span class="p">,</span> <span class="s">"CA"</span><span class="p">,</span> <span class="s">"WA"</span><span class="p">,</span> <span class="s">"WA"</span><span class="p">,</span> <span class="s">"CA"</span><span class="p">,</span> <span class="s">"MA"</span><span class="p">]</span>
  <span class="p">])</span>

<span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>

<span class="o">&gt;</span> <span class="n">lexsort_to_indices</span><span class="p">([</span>
    <span class="p">[</span><span class="s">"MA"</span><span class="p">,</span> <span class="s">"MA"</span><span class="p">,</span> <span class="s">"CA"</span><span class="p">,</span> <span class="s">"WA"</span><span class="p">,</span> <span class="s">"WA"</span><span class="p">,</span>   <span class="s">"CA"</span><span class="p">,</span> <span class="s">"MA"</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">10.10</span><span class="p">,</span> <span class="mf">8.44</span><span class="p">,</span> <span class="mf">3.25</span><span class="p">,</span> <span class="mf">6.00</span><span class="p">,</span> <span class="mf">132.50</span><span class="p">,</span> <span class="mf">9.33</span><span class="p">,</span> <span class="mf">1.30</span><span class="p">]</span>
  <span class="p">])</span>

<span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
</code></pre></div></div>

<p>This function returns a list of indices instead of sorting the columns directly because it:</p>
<ol>
  <li>Avoids expensive copying data during the sorting process</li>
  <li>Allows deferring copying of values until the latest possible moment</li>
  <li>Can be used to reorder additional columns that weren’t part of the sort key</li>
</ol>

<p>A straightforward implementation of lexsort_to_indices uses a comparator function,</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   row
  index
        ┌─────┐   ┌─────┐   ┌─────┐     compare(left_index, right_index)
      0 │     │   │     │   │     │
       ┌├─────┤─ ─├─────┤─ ─├─────┤┐                   │             │
        │     │   │     │   │     │ ◀──────────────────┘             │
       └├─────┤─ ─├─────┤─ ─├─────┤┘                                 │
        │     │   │     │   │     │Comparator function compares one  │
        ├─────┤   ├─────┤   ├─────┤ multi-column row with another.   │
        │     │   │     │   │     │                                  │
        ├─────┤   ├─────┤   ├─────┤ The data types of the columns    │
        │     │   │     │   │     │  and the sort options are not    │
        └─────┘   └─────┘   └─────┘  known at compile time, only     │
                    ...                        runtime               │
                                                                     │
       ┌┌─────┐─ ─┌─────┐─ ─┌─────┐┐                                 │
        │     │   │     │   │     │ ◀────────────────────────────────┘
       └├─────┤─ ─├─────┤─ ─├─────┤┘
        │     │   │     │   │     │
        ├─────┤   ├─────┤   ├─────┤
    N-1 │     │   │     │   │     │
        └─────┘   └─────┘   └─────┘
        Customer    State    Orders
         UInt64      Utf8     F64
</code></pre></div></div>

<p>The comparator function compares each row a column at a time, based on the column types</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                         ┌────────────────────────────────┐
                         │                                │
                         ▼                                │
                     ┌ ─ ─ ─ ┐ ┌ ─ ─ ─ ┐                  │
                                                          │
            ┌─────┐  │┌─────┐│ │┌─────┐│                  │
left_index  │     │   │     │   │     │                   │
            └─────┘  │└─────┘│ │└─────┘│   Step 1: Compare State
                                                    (UInt64)
                     │       │ │       │

                     │       │ │       │
            ┌─────┐   ┌─────┐   ┌─────┐
 right_index│     │  ││     ││ ││     ││
            └─────┘   └─────┘   └─────┘    Step 2: If State values equal
                     │       │ │       │   compare Orders (F64)
            Customer   State     Orders                     │
             UInt64  │  Utf8 │ │  F64  │                    │
                      ─ ─ ─ ─   ─ ─ ─ ─                     │
                                    ▲                       │
                                    │                       │
                                    └───────────────────────┘
</code></pre></div></div>

<p>Pseudocode for this operation might look something like</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Takes a list of columns and returns the lexicographically
# sorted order as a list of indices
</span><span class="k">def</span> <span class="nf">lexsort_to_indices</span><span class="p">(</span><span class="n">columns</span><span class="p">):</span>
  <span class="n">comparator</span> <span class="o">=</span> <span class="n">build_comparator</span><span class="p">(</span><span class="n">columns</span><span class="p">)</span>

  <span class="c1"># Construct a list of integers from 0 to the number of rows
</span>  <span class="c1"># and sort it according to the comparator
</span>  <span class="p">[</span><span class="mf">0.</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">num_rows</span><span class="p">()].</span><span class="n">sort_by</span><span class="p">(</span><span class="n">comparator</span><span class="p">)</span>

<span class="c1"># Build a function that given indexes (left_idx, right_idx)
# returns the comparison of the sort keys at the left
# and right indices respectively
</span><span class="k">def</span> <span class="nf">build_comparator</span><span class="p">(</span><span class="n">columns</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">comparator</span><span class="p">(</span><span class="n">left_idx</span><span class="p">,</span> <span class="n">right_idx</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">:</span>
      <span class="c1"># call a compare function which performs
</span>      <span class="c1"># dynamic dispatch on type of left and right columns
</span>      <span class="n">ordering</span> <span class="o">=</span> <span class="n">compare</span><span class="p">(</span><span class="n">column</span><span class="p">,</span> <span class="n">left_idx</span><span class="p">,</span><span class="n">right_idx</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">ordering</span> <span class="o">!=</span> <span class="n">Equal</span> <span class="p">{</span>
        <span class="k">return</span> <span class="n">ordering</span>
      <span class="p">}</span>
    <span class="c1"># All values equal
</span>    <span class="n">Equal</span>
  <span class="c1"># Return comparator function
</span>  <span class="n">comparator</span>

  <span class="c1"># compares the values in a single column at left_idx and right_idx
</span>  <span class="k">def</span> <span class="nf">compare</span><span class="p">(</span><span class="n">column</span><span class="p">,</span> <span class="n">left_idx</span><span class="p">,</span> <span class="n">right_idx</span><span class="p">):</span>
    <span class="c1"># Choose comparison based on type of column ("dynamic dispatch")
</span>    <span class="k">if</span> <span class="n">column</span><span class="p">.</span><span class="nb">type</span> <span class="o">==</span> <span class="n">Int</span><span class="p">:</span>
     <span class="nb">cmp</span><span class="p">(</span><span class="n">column</span><span class="p">[</span><span class="n">left_idx</span><span class="p">].</span><span class="n">as_int</span><span class="p">(),</span> <span class="n">column</span><span class="p">[</span><span class="n">right_idx</span><span class="p">].</span><span class="n">as_int</span><span class="p">())</span>
    <span class="k">elif</span> <span class="n">column</span><span class="p">.</span><span class="nb">type</span> <span class="o">==</span> <span class="n">Float</span><span class="p">:</span>
     <span class="nb">cmp</span><span class="p">(</span><span class="n">column</span><span class="p">[</span><span class="n">left_idx</span><span class="p">].</span><span class="n">as_float</span><span class="p">(),</span> <span class="n">column</span><span class="p">[</span><span class="n">right_idx</span><span class="p">].</span><span class="n">as_float</span><span class="p">())</span>
    <span class="p">...</span>
</code></pre></div></div>

<p>Greater detail is beyond the scope of this post, but in general the more predictable the behavior of a block of code, the better its performance will be. In the case of this pseudocode,  there is clear room for improvement:</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">comparator</code> performs a large number of unpredictable conditional branches, where the path execution takes depends on the data values</li>
  <li><code class="language-plaintext highlighter-rouge">comparator</code> and <code class="language-plaintext highlighter-rouge">compare</code> use dynamic dispatch, which not only adds further conditional branches, but also function call overhead</li>
  <li><code class="language-plaintext highlighter-rouge">comparator</code> performs a large number of reads of memory at unpredictable locations</li>
</ol>

<p>You can find the complete implementation of multi-column comparator construction in arrow-rs in <a href="https://github.com/apache/arrow-rs/blob/f629a2ebe08033e7b78585d82e98c50a4439e7a2/arrow/src/compute/kernels/sort.rs#L905-L1036">sort.rs</a> and <a href="https://github.com/apache/arrow-rs/blob/f629a2e/arrow/src/array/ord.rs#L178-L313">ord.rs</a>.</p>

<h1 id="normalized-keys--byte-array-comparisons">Normalized Keys / Byte Array Comparisons</h1>

<p>Now imagine we had a way to represent each logical row of data as a sequence of bytes, and that byte-wise comparison of that sequence yielded the same result as comparing the actual column values using the code above. Such a representation would require no switching on column types, and the kernel would become</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">lexsort_to_indices</span><span class="p">(</span><span class="n">columns</span><span class="p">):</span>
  <span class="n">rows</span> <span class="o">=</span> <span class="n">convert_to_rows</span><span class="p">(</span><span class="n">columns</span><span class="p">)</span>
  <span class="p">[</span><span class="mf">0.</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">num_rows</span><span class="p">()].</span><span class="n">sort_by</span><span class="p">(</span><span class="k">lambda</span> <span class="n">l</span><span class="p">,</span> <span class="n">r</span><span class="p">:</span> <span class="nb">cmp</span><span class="p">(</span><span class="n">rows</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">rows</span><span class="p">[</span><span class="n">r</span><span class="p">]))</span>
</code></pre></div></div>

<p>While this approach does require converting to/from the byte array representation, it has some major advantages:</p>

<ul>
  <li>Rows can be compared by comparing bytes in memory, which modern computer hardware excels at with the extremely well optimized <a href="https://www.man7.org/linux/man-pages/man3/memcmp.3.html">memcmp</a></li>
  <li>Memory accesses are largely predictable</li>
  <li>There is no dynamic dispatch overhead</li>
  <li>Extends straightforwardly to more sophisticated sorting strategies such as
    <ul>
      <li>Distribution-based sorting techniques such as radix sort</li>
      <li>Parallel merge sort</li>
      <li>External sort</li>
      <li>…</li>
    </ul>
  </li>
</ul>

<p>You can find more information on how to leverage such representation in the “Binary String Comparison” section of the <a href="https://duckdb.org/2021/08/27/external-sorting.html">DuckDB blog post</a> on the topic as well as <a href="https://dl.acm.org/doi/10.1145/1132960.1132964">Graefe’s paper</a>. However, we found it wasn’t immediately obvious how to apply this technique to variable length string or dictionary encoded data, which we will explain in the next post in this series.</p>

<h2 id="next-up-row-format">Next up: Row Format</h2>

<p>This post has introduced the concept and challenges of multi column sorting, and shown why a comparable byte array representation, such as the <a href="https://docs.rs/arrow/27.0.0/arrow/row/index.html">row format</a> introduced to the <a href="https://github.com/apache/arrow-rs">Rust implementation</a> of <a href="https://arrow.apache.org/">Apache Arrow</a>, is such a compelling primitive.</p>

<p>In <a href="/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-2/">the next post</a> we explain how this encoding works, but if you just want to use it, check out the <a href="https://docs.rs/arrow/27.0.0/arrow/row/index.html">docs</a> for getting started, and report any issues on our <a href="https://github.com/apache/arrow-rs/issues">bugtracker</a>. As always, the <a href="https://github.com/apache/arrow-rs#arrow-rust-community">Arrow community</a> very much looks forward to seeing what you build with it!</p>]]></content><author><name>tustvold and alamb</name></author><category term="arrow" /><summary type="html"><![CDATA[Introduction Sorting is one of the most fundamental operations in modern databases and other analytic systems, underpinning important operators such as aggregates, joins, window functions, merge, and more. By some estimates, more than half of the execution time in data processing systems is spent sorting. Optimizing sorts is therefore vital to improving query performance and overall system efficiency. Sorting is also one of the most well studied topics in computer science. The classic survey paper for databases is Implementing Sorting in Database Systems by Goetz Graefe which provides a thorough academic treatment and is still very applicable today. However, it may not be obvious how to apply the wisdom and advanced techniques described in that paper to modern systems. In addition, the excellent DuckDB blog on sorting highlights many sorting techniques, and mentions a comparable row format, but it does not explain how to efficiently sort variable length strings or dictionary encoded data. In this series we explain in detail the new row format in the Rust implementation of Apache Arrow, and how we used to make sorting more than 3x faster than an alternate comparator based approach. The benefits are especially pronounced for strings, dictionary encoded data, and sorts with large numbers of columns. Multicolumn / Lexicographical Sort Problem Most languages have native, optimized operations to sort a single column (array) of data, which are specialized based on the type of data being sorted. The reason that sorting is typically more challenging in analytic systems is that: They must support multiple columns of data The column types are not knowable at compile time, and thus the compiler can not typically generate optimized code Multicolumn sorting is also referred to as lexicographical sorting in some libraries. For example, given sales data for various customers and their state of residence, a user might want to find the lowest 10 orders for each state. Customer | State | Orders —--------+-------+------- 12345 | MA | 10.12 532432 | MA | 8.44 12345 | CA | 3.25 56232 | WA | 6.00 23442 | WA | 132.50 7844 | CA | 9.33 852353 | MA | 1.30 One way to do so is to order the data first by State and then by Orders: Customer | State | Orders —--------+-------+------- 12345 | CA | 3.25 7844 | CA | 9.33 852353 | MA | 1.30 532432 | MA | 8.44 12345 | MA | 10.12 56232 | WA | 6.00 23442 | WA | 132.50 (Note: While there are specialized ways for computing this particular query other than fully sorting the entire input (e.g. “TopK”), they typically need the same multi-column comparison operation described below. Thus while we will use the simplified example in this series, it applies much more broadly) Basic Implementation Let us take the example of a basic sort kernel which takes a set of columns as input, and returns a list of indices identifying a sorted order. &gt; lexsort_to_indices([ ["MA", "MA", "CA", "WA", "WA", "CA", "MA"] ]) [2, 5, 0, 1, 6, 3, 4] &gt; lexsort_to_indices([ ["MA", "MA", "CA", "WA", "WA", "CA", "MA"], [10.10, 8.44, 3.25, 6.00, 132.50, 9.33, 1.30] ]) [2, 5, 6, 1, 0, 3, 4] This function returns a list of indices instead of sorting the columns directly because it: Avoids expensive copying data during the sorting process Allows deferring copying of values until the latest possible moment Can be used to reorder additional columns that weren’t part of the sort key A straightforward implementation of lexsort_to_indices uses a comparator function, row index ┌─────┐ ┌─────┐ ┌─────┐ compare(left_index, right_index) 0 │ │ │ │ │ │ ┌├─────┤─ ─├─────┤─ ─├─────┤┐ │ │ │ │ │ │ │ │ ◀──────────────────┘ │ └├─────┤─ ─├─────┤─ ─├─────┤┘ │ │ │ │ │ │ │Comparator function compares one │ ├─────┤ ├─────┤ ├─────┤ multi-column row with another. │ │ │ │ │ │ │ │ ├─────┤ ├─────┤ ├─────┤ The data types of the columns │ │ │ │ │ │ │ and the sort options are not │ └─────┘ └─────┘ └─────┘ known at compile time, only │ ... runtime │ │ ┌┌─────┐─ ─┌─────┐─ ─┌─────┐┐ │ │ │ │ │ │ │ ◀────────────────────────────────┘ └├─────┤─ ─├─────┤─ ─├─────┤┘ │ │ │ │ │ │ ├─────┤ ├─────┤ ├─────┤ N-1 │ │ │ │ │ │ └─────┘ └─────┘ └─────┘ Customer State Orders UInt64 Utf8 F64 The comparator function compares each row a column at a time, based on the column types ┌────────────────────────────────┐ │ │ ▼ │ ┌ ─ ─ ─ ┐ ┌ ─ ─ ─ ┐ │ │ ┌─────┐ │┌─────┐│ │┌─────┐│ │ left_index │ │ │ │ │ │ │ └─────┘ │└─────┘│ │└─────┘│ Step 1: Compare State (UInt64) │ │ │ │ │ │ │ │ ┌─────┐ ┌─────┐ ┌─────┐ right_index│ │ ││ ││ ││ ││ └─────┘ └─────┘ └─────┘ Step 2: If State values equal │ │ │ │ compare Orders (F64) Customer State Orders │ UInt64 │ Utf8 │ │ F64 │ │ ─ ─ ─ ─ ─ ─ ─ ─ │ ▲ │ │ │ └───────────────────────┘ Pseudocode for this operation might look something like # Takes a list of columns and returns the lexicographically # sorted order as a list of indices def lexsort_to_indices(columns): comparator = build_comparator(columns) # Construct a list of integers from 0 to the number of rows # and sort it according to the comparator [0..columns.num_rows()].sort_by(comparator) # Build a function that given indexes (left_idx, right_idx) # returns the comparison of the sort keys at the left # and right indices respectively def build_comparator(columns): def comparator(left_idx, right_idx): for column in columns: # call a compare function which performs # dynamic dispatch on type of left and right columns ordering = compare(column, left_idx,right_idx) if ordering != Equal { return ordering } # All values equal Equal # Return comparator function comparator # compares the values in a single column at left_idx and right_idx def compare(column, left_idx, right_idx): # Choose comparison based on type of column ("dynamic dispatch") if column.type == Int: cmp(column[left_idx].as_int(), column[right_idx].as_int()) elif column.type == Float: cmp(column[left_idx].as_float(), column[right_idx].as_float()) ... Greater detail is beyond the scope of this post, but in general the more predictable the behavior of a block of code, the better its performance will be. In the case of this pseudocode, there is clear room for improvement: comparator performs a large number of unpredictable conditional branches, where the path execution takes depends on the data values comparator and compare use dynamic dispatch, which not only adds further conditional branches, but also function call overhead comparator performs a large number of reads of memory at unpredictable locations You can find the complete implementation of multi-column comparator construction in arrow-rs in sort.rs and ord.rs. Normalized Keys / Byte Array Comparisons Now imagine we had a way to represent each logical row of data as a sequence of bytes, and that byte-wise comparison of that sequence yielded the same result as comparing the actual column values using the code above. Such a representation would require no switching on column types, and the kernel would become def lexsort_to_indices(columns): rows = convert_to_rows(columns) [0..columns.num_rows()].sort_by(lambda l, r: cmp(rows[l], rows[r])) While this approach does require converting to/from the byte array representation, it has some major advantages: Rows can be compared by comparing bytes in memory, which modern computer hardware excels at with the extremely well optimized memcmp Memory accesses are largely predictable There is no dynamic dispatch overhead Extends straightforwardly to more sophisticated sorting strategies such as Distribution-based sorting techniques such as radix sort Parallel merge sort External sort … You can find more information on how to leverage such representation in the “Binary String Comparison” section of the DuckDB blog post on the topic as well as Graefe’s paper. However, we found it wasn’t immediately obvious how to apply this technique to variable length string or dictionary encoded data, which we will explain in the next post in this series. Next up: Row Format This post has introduced the concept and challenges of multi column sorting, and shown why a comparable byte array representation, such as the row format introduced to the Rust implementation of Apache Arrow, is such a compelling primitive. In the next post we explain how this encoding works, but if you just want to use it, check out the docs for getting started, and report any issues on our bugtracker. As always, the Arrow community very much looks forward to seeing what you build with it!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Fast and Memory Efficient Multi-Column Sorts in Apache Arrow Rust, Part 2</title><link href="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-2/" rel="alternate" type="text/html" title="Fast and Memory Efficient Multi-Column Sorts in Apache Arrow Rust, Part 2" /><published>2022-11-07T00:00:00-05:00</published><updated>2022-11-07T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-2</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-2/"><![CDATA[<!--

-->

<h2 id="introduction">Introduction</h2>

<p>In <a href="/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1/">Part 1</a> of this post, we described the problem of Multi-Column Sorting and the challenges of implementing it efficiently. This second post explains how the new <a href="https://docs.rs/arrow/27.0.0/arrow/row/index.html">row format</a> in the <a href="https://github.com/apache/arrow-rs">Rust implementation</a> of <a href="https://arrow.apache.org/">Apache Arrow</a> works and is constructed.</p>

<h2 id="row-format">Row Format</h2>

<p>The row format is a variable length byte sequence created by concatenating the encoded form of each column. The encoding for each column depends on its datatype (and sort options).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   ┌─────┐   ┌─────┐   ┌─────┐
   │     │   │     │   │     │
   ├─────┤ ┌ ┼─────┼ ─ ┼─────┼ ┐              ┏━━━━━━━━━━━━━┓
   │     │   │     │   │     │  ─────────────▶┃             ┃
   ├─────┤ └ ┼─────┼ ─ ┼─────┼ ┘              ┗━━━━━━━━━━━━━┛
   │     │   │     │   │     │
   └─────┘   └─────┘   └─────┘
               ...
   ┌─────┐ ┌ ┬─────┬ ─ ┬─────┬ ┐              ┏━━━━━━━━┓
   │     │   │     │   │     │  ─────────────▶┃        ┃
   └─────┘ └ ┴─────┴ ─ ┴─────┴ ┘              ┗━━━━━━━━┛
   Customer    State    Orders
    UInt64      Utf8     F64

          Input Arrays                          Row Format
           (Columns)
</code></pre></div></div>

<p>The encoding is carefully designed in such a way that escaping is unnecessary: it is never ambiguous as to whether a byte is part of a sentinel (e.g. null) or a value.</p>

<h3 id="unsigned-integers">Unsigned Integers</h3>

<p>To encode a non-null unsigned integer, the byte <code class="language-plaintext highlighter-rouge">0x01</code> is written, followed by the integer’s bytes starting with the most significant, i.e. big endian. A null is encoded as a <code class="language-plaintext highlighter-rouge">0x00</code> byte, followed by the encoded bytes of the integer’s zero value</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>              ┌──┬──┬──┬──┐      ┌──┬──┬──┬──┬──┐
   3          │03│00│00│00│      │01│00│00│00│03│
              └──┴──┴──┴──┘      └──┴──┴──┴──┴──┘
              ┌──┬──┬──┬──┐      ┌──┬──┬──┬──┬──┐
  258         │02│01│00│00│      │01│00│00│01│02│
              └──┴──┴──┴──┘      └──┴──┴──┴──┴──┘
              ┌──┬──┬──┬──┐      ┌──┬──┬──┬──┬──┐
 23423        │7F│5B│00│00│      │01│00│00│5B│7F│
              └──┴──┴──┴──┘      └──┴──┴──┴──┴──┘
              ┌──┬──┬──┬──┐      ┌──┬──┬──┬──┬──┐
 NULL         │??│??│??│??│      │00│00│00│00│00│
              └──┴──┴──┴──┘      └──┴──┴──┴──┴──┘

             32-bit (4 bytes)        Row Format
 Value        Little Endian
</code></pre></div></div>

<h3 id="signed-integers">Signed Integers</h3>

<p>In Rust and most modern computer architectures, signed integers are encoded using <a href="https://en.wikipedia.org/wiki/Two%27s_complement">two’s complement</a>, where a number is negated by flipping all the bits, and adding 1. Therefore, flipping the top-most bit and treating the result as an unsigned integer preserves the order. This unsigned integer can then be encoded using the same encoding for unsigned integers described in the previous section. For example</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>       ┌──┬──┬──┬──┐       ┌──┬──┬──┬──┐       ┌──┬──┬──┬──┬──┐
    5  │05│00│00│00│       │05│00│00│80│       │01│80│00│00│05│
       └──┴──┴──┴──┘       └──┴──┴──┴──┘       └──┴──┴──┴──┴──┘
       ┌──┬──┬──┬──┐       ┌──┬──┬──┬──┐       ┌──┬──┬──┬──┬──┐
   -5  │FB│FF│FF│FF│       │FB│FF│FF│7F│       │01│7F│FF│FF│FB│
       └──┴──┴──┴──┘       └──┴──┴──┴──┘       └──┴──┴──┴──┴──┘

 Value  32-bit (4 bytes)    High bit flipped      Row Format
         Little Endian
</code></pre></div></div>

<h3 id="floating-point">Floating Point</h3>

<p>Floating point values can be ordered according to the <a href="https://en.wikipedia.org/wiki/IEEE_754#Total-ordering_predicate">IEEE 754 totalOrder predicate</a> (implemented in Rust by <a href="https://doc.rust-lang.org/std/primitive.f32.html#method.total_cmp">f32::total_cmp</a>). This ordering interprets the bytes of the floating point value as the correspondingly sized, signed, little-endian integer, flipping all the bits except the sign bit in the case of negatives.</p>

<p>Floating point values are therefore encoded to row format by converting them to the appropriate sized signed integer representation, and then using the same encoding for signed integers described in the previous section.</p>

<h3 id="byte-arrays-including-strings">Byte Arrays (Including Strings)</h3>

<p>Unlike primitive types above, byte arrays are variable length. For short strings, such as <code class="language-plaintext highlighter-rouge">state</code> in our example above, it is possible to pad all values to the length of the longest one with some fixed value such as <code class="language-plaintext highlighter-rouge">0x00</code> and produce a fixed length row. This is the approach described in the DuckDB blog for encoding <code class="language-plaintext highlighter-rouge">c_birth_country</code>.</p>

<p>However, often values in string columns differ substantially in length or the maximum length is not known at the start of execution, making it inadvisable and/or impractical to pad the strings to a fixed length. The Rust Arrow row format therefore uses a variable length encoding.</p>

<p>We need an encoding that unambiguously terminates the end of the byte array. This not only permits recovering the original value from the row format, but ensures that bytes of a longer byte array are not compared against bytes from a different column when compared against a row containing a shorter byte array.</p>

<p>A null byte array is encoded as a single <code class="language-plaintext highlighter-rouge">0x00</code> byte. Similarly, an empty byte array is encoded as a single <code class="language-plaintext highlighter-rouge">0x01</code> byte.</p>

<p>To encode a non-null, non-empty array, first a single <code class="language-plaintext highlighter-rouge">0x02</code> byte  is written. Then the array is written in 32-byte blocks, with each complete block followed by a <code class="language-plaintext highlighter-rouge">0xFF</code> byte as a continuation token. The final block is padded to 32-bytes with <code class="language-plaintext highlighter-rouge">0x00</code>, and is then followed by the unpadded length of this final block as a single byte in place of a continuation token</p>

<p>Note the following example encodings use a block size of 4 bytes, as opposed to 32 bytes for brevity</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                      ┌───┬───┬───┬───┬───┬───┐
 "MEEP"               │02 │'M'│'E'│'E'│'P'│04 │
                      └───┴───┴───┴───┴───┴───┘

                      ┌───┐
 ""                   │01 |
                      └───┘

 NULL                 ┌───┐
                      │00 │
                      └───┘

"Defenestration"      ┌───┬───┬───┬───┬───┬───┐
                      │02 │'D'│'e'│'f'│'e'│FF │
                      └───┼───┼───┼───┼───┼───┤
                          │'n'│'e'│'s'│'t'│FF │
                          ├───┼───┼───┼───┼───┤
                          │'r'│'a'│'t'│'r'│FF │
                          ├───┼───┼───┼───┼───┤
                          │'a'│'t'│'i'│'o'│FF │
                          ├───┼───┼───┼───┼───┤
                          │'n'│00 │00 │00 │01 │
                          └───┴───┴───┴───┴───┘
</code></pre></div></div>

<p>This approach is loosely inspired by <a href="https://en.wikipedia.org/wiki/Consistent_Overhead_Byte_Stuffing">COBS encoding</a>, and chosen over more traditional <a href="https://en.wikipedia.org/wiki/High-Level_Data_Link_Control#Asynchronous_framing">byte stuffing</a> as it is more amenable to vectorization, in particular hardware with AVX-256 can copy a 32-byte block in a single instruction.</p>

<h3 id="dictionary-arrays">Dictionary Arrays</h3>
<p>Dictionary Encoded Data (called <a href="https://pandas.pydata.org/docs/user_guide/categorical.html">categorical</a> in pandas) is increasingly important because they can store and process low cardinality data very efficiently.</p>

<p>A simple approach to encoding dictionary arrays would be to encode the logical values directly using the encodings for primitive values described previously. However, this would lose the benefits of dictionary encoding to reduce memory and CPU consumption.</p>

<p>To further complicate matters, the <a href="https://arrow.apache.org/docs/format/Columnar.html#dictionary-encoded-layout">Arrow implementation of Dictionary encoding</a> is quite general, and we can make no assumptions about the contents of the dictionaries. In particular, we cannot assume that the dictionary values are sorted, nor that the same dictionary is used for all arrays within a column</p>

<p>The following example shows how a string column might be encoded in two arrays using two different dictionaries. The dictionary keys <code class="language-plaintext highlighter-rouge">0</code>, <code class="language-plaintext highlighter-rouge">1</code>, and <code class="language-plaintext highlighter-rouge">2</code> in the first batch correspond to different values than the same keys in the second dictionary.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
  ┌───────────┐ ┌─────┐    │
│ │"Fabulous" │ │  0  │
  ├───────────┤ ├─────┤    │
│ │   "Bar"   │ │  2  │
  ├───────────┤ ├─────┤    │       ┌───────────┐
│ │  "Soup"   │ │  2  │            │"Fabulous" │
  └───────────┘ ├─────┤    │       ├───────────┤
│               │  0  │            │  "Soup"   │
                ├─────┤    │       ├───────────┤
│               │  1  │            │  "Soup"   │
                └─────┘    │       ├───────────┤
│                                  │"Fabulous" │
                 Values    │       ├───────────┤
│ Dictionary   (indexes in         │   "Bar"   │
               dictionary) │       ├───────────┤
│                                  │   "ZZ"    │
 ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘       ├───────────┤
┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─        │   "Bar"   │
                           │       ├───────────┤
│ ┌───────────┐ ┌─────┐            │   "ZZ"    │
  │"Fabulous" │ │  1  │    │       ├───────────┤
│ ├───────────┤ ├─────┤            │"Fabulous" │
  │   "ZZ"    │ │  2  │    │       └───────────┘
│ ├───────────┤ ├─────┤
  │   "Bar"   │ │  1  │    │
│ └───────────┘ ├─────┤
                │  0  │    │      Logical column
│               └─────┘               values
                Values     │
│  Dictionary (indexes in
              dictionary)  │
 ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘
</code></pre></div></div>

<p>The key observation which allows us to efficiently create a row format for this kind of data is that given a byte array, a new byte array can always be created which comes before or after it in the sort order by adding an additional byte.</p>

<p>Therefore we can incrementally build an order-preserving mapping from dictionary values to variable length byte arrays, without needing to know all possible dictionary values beforehand, instead introducing mappings for new dictionary values as we encounter them.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌──────────┐                 ┌─────┐
│  "Bar"   │ ───────────────▶│ 01  │
└──────────┘                 └─────┘
┌──────────┐                 ┌─────┬─────┐
│"Fabulous"│ ───────────────▶│ 01  │ 02  │
└──────────┘                 └─────┴─────┘
┌──────────┐                 ┌─────┐
│  "Soup"  │ ───────────────▶│ 05  │
└──────────┘                 └─────┘
┌──────────┐                 ┌─────┐
│   "ZZ"   │ ───────────────▶│ 07  │
└──────────┘                 └─────┘

    Example Order Preserving Mapping
</code></pre></div></div>

<p>The details of the data structure used to generate this mapping are beyond the scope of this blog post, but may be the topic of a future post. You can find <a href="https://github.com/apache/arrow-rs/blob/07024f6a16b870fda81cba5779b8817b20386ebf/arrow/src/row/interner.rs">the code here</a>.</p>

<p>The data structure also ensures that no values contain <code class="language-plaintext highlighter-rouge">0x00</code> and therefore we can encode the arrays directly using <code class="language-plaintext highlighter-rouge">0x00</code> as an end-delimiter.</p>

<p>A null value is encoded as a single <code class="language-plaintext highlighter-rouge">0x00</code> byte, and a non-null value encoded as a single <code class="language-plaintext highlighter-rouge">0x01</code> byte, followed by the <code class="language-plaintext highlighter-rouge">0x00</code> terminated byte array determined by the order preserving mapping</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                          ┌─────┬─────┬─────┬─────┐
   "Fabulous"             │ 01  │ 03  │ 05  │ 00  │
                          └─────┴─────┴─────┴─────┘

                          ┌─────┬─────┬─────┐
   "ZZ"                   │ 01  │ 07  │ 00  │
                          └─────┴─────┴─────┘

                          ┌─────┐
    NULL                  │ 00  │
                          └─────┘

     Input                  Row Format
</code></pre></div></div>

<h3 id="sort-options">Sort Options</h3>

<p>One detail we have so far ignored over is how to support ascending and descending sorts (e.g. <code class="language-plaintext highlighter-rouge">ASC</code> or <code class="language-plaintext highlighter-rouge">DESC</code> in SQL). The Arrow Rust row format supports these options by simply inverting the bytes of the encoded representation, except the initial byte used for nullability encoding, on a per column basis.</p>

<p>Similarly, supporting SQL compatible sorting also requires a format that can specify the order of <code class="language-plaintext highlighter-rouge">NULL</code>s (before or after all non <code class="language-plaintext highlighter-rouge">NULL</code> values). The row format supports this option by optionally encoding nulls as <code class="language-plaintext highlighter-rouge">0xFF</code> instead of <code class="language-plaintext highlighter-rouge">0x00</code> on a per column basis.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Hopefully these two articles have given you a flavor of what is possible with a comparable row format and how it works. Feel free to check out the <a href="https://docs.rs/arrow/27.0.0/arrow/row/index.html">docs</a> for instructions on getting started, and report any issues on our <a href="https://github.com/apache/arrow-rs/issues">bugtracker</a>.</p>

<p>Using this format for lexicographic sorting is more than <a href="https://github.com/apache/arrow-rs/pull/2929">3x</a> faster than the comparator based approach, with the benefits especially pronounced for strings, dictionaries and sorts with large numbers of columns.</p>

<p>We have also already used it to more than <a href="https://github.com/apache/arrow-datafusion/pull/3386">double</a> the performance of sort preserving merge in the <a href="https://arrow.apache.org/datafusion/">DataFusion project</a>, and expect similar or greater performance uplift as we apply it to sort, grouping, join, and window function operators as well.</p>

<p>As always, the <a href="https://github.com/apache/arrow-rs#arrow-rust-community">Arrow community</a> very much looks forward to seeing what you build with it!</p>]]></content><author><name>tustvold and alamb</name></author><category term="arrow" /><summary type="html"><![CDATA[Introduction In Part 1 of this post, we described the problem of Multi-Column Sorting and the challenges of implementing it efficiently. This second post explains how the new row format in the Rust implementation of Apache Arrow works and is constructed. Row Format The row format is a variable length byte sequence created by concatenating the encoded form of each column. The encoding for each column depends on its datatype (and sort options). ┌─────┐ ┌─────┐ ┌─────┐ │ │ │ │ │ │ ├─────┤ ┌ ┼─────┼ ─ ┼─────┼ ┐ ┏━━━━━━━━━━━━━┓ │ │ │ │ │ │ ─────────────▶┃ ┃ ├─────┤ └ ┼─────┼ ─ ┼─────┼ ┘ ┗━━━━━━━━━━━━━┛ │ │ │ │ │ │ └─────┘ └─────┘ └─────┘ ... ┌─────┐ ┌ ┬─────┬ ─ ┬─────┬ ┐ ┏━━━━━━━━┓ │ │ │ │ │ │ ─────────────▶┃ ┃ └─────┘ └ ┴─────┴ ─ ┴─────┴ ┘ ┗━━━━━━━━┛ Customer State Orders UInt64 Utf8 F64 Input Arrays Row Format (Columns) The encoding is carefully designed in such a way that escaping is unnecessary: it is never ambiguous as to whether a byte is part of a sentinel (e.g. null) or a value. Unsigned Integers To encode a non-null unsigned integer, the byte 0x01 is written, followed by the integer’s bytes starting with the most significant, i.e. big endian. A null is encoded as a 0x00 byte, followed by the encoded bytes of the integer’s zero value ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┬──┐ 3 │03│00│00│00│ │01│00│00│00│03│ └──┴──┴──┴──┘ └──┴──┴──┴──┴──┘ ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┬──┐ 258 │02│01│00│00│ │01│00│00│01│02│ └──┴──┴──┴──┘ └──┴──┴──┴──┴──┘ ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┬──┐ 23423 │7F│5B│00│00│ │01│00│00│5B│7F│ └──┴──┴──┴──┘ └──┴──┴──┴──┴──┘ ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┬──┐ NULL │??│??│??│??│ │00│00│00│00│00│ └──┴──┴──┴──┘ └──┴──┴──┴──┴──┘ 32-bit (4 bytes) Row Format Value Little Endian Signed Integers In Rust and most modern computer architectures, signed integers are encoded using two’s complement, where a number is negated by flipping all the bits, and adding 1. Therefore, flipping the top-most bit and treating the result as an unsigned integer preserves the order. This unsigned integer can then be encoded using the same encoding for unsigned integers described in the previous section. For example ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┬──┐ 5 │05│00│00│00│ │05│00│00│80│ │01│80│00│00│05│ └──┴──┴──┴──┘ └──┴──┴──┴──┘ └──┴──┴──┴──┴──┘ ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┬──┐ -5 │FB│FF│FF│FF│ │FB│FF│FF│7F│ │01│7F│FF│FF│FB│ └──┴──┴──┴──┘ └──┴──┴──┴──┘ └──┴──┴──┴──┴──┘ Value 32-bit (4 bytes) High bit flipped Row Format Little Endian Floating Point Floating point values can be ordered according to the IEEE 754 totalOrder predicate (implemented in Rust by f32::total_cmp). This ordering interprets the bytes of the floating point value as the correspondingly sized, signed, little-endian integer, flipping all the bits except the sign bit in the case of negatives. Floating point values are therefore encoded to row format by converting them to the appropriate sized signed integer representation, and then using the same encoding for signed integers described in the previous section. Byte Arrays (Including Strings) Unlike primitive types above, byte arrays are variable length. For short strings, such as state in our example above, it is possible to pad all values to the length of the longest one with some fixed value such as 0x00 and produce a fixed length row. This is the approach described in the DuckDB blog for encoding c_birth_country. However, often values in string columns differ substantially in length or the maximum length is not known at the start of execution, making it inadvisable and/or impractical to pad the strings to a fixed length. The Rust Arrow row format therefore uses a variable length encoding. We need an encoding that unambiguously terminates the end of the byte array. This not only permits recovering the original value from the row format, but ensures that bytes of a longer byte array are not compared against bytes from a different column when compared against a row containing a shorter byte array. A null byte array is encoded as a single 0x00 byte. Similarly, an empty byte array is encoded as a single 0x01 byte. To encode a non-null, non-empty array, first a single 0x02 byte is written. Then the array is written in 32-byte blocks, with each complete block followed by a 0xFF byte as a continuation token. The final block is padded to 32-bytes with 0x00, and is then followed by the unpadded length of this final block as a single byte in place of a continuation token Note the following example encodings use a block size of 4 bytes, as opposed to 32 bytes for brevity ┌───┬───┬───┬───┬───┬───┐ "MEEP" │02 │'M'│'E'│'E'│'P'│04 │ └───┴───┴───┴───┴───┴───┘ ┌───┐ "" │01 | └───┘ NULL ┌───┐ │00 │ └───┘ "Defenestration" ┌───┬───┬───┬───┬───┬───┐ │02 │'D'│'e'│'f'│'e'│FF │ └───┼───┼───┼───┼───┼───┤ │'n'│'e'│'s'│'t'│FF │ ├───┼───┼───┼───┼───┤ │'r'│'a'│'t'│'r'│FF │ ├───┼───┼───┼───┼───┤ │'a'│'t'│'i'│'o'│FF │ ├───┼───┼───┼───┼───┤ │'n'│00 │00 │00 │01 │ └───┴───┴───┴───┴───┘ This approach is loosely inspired by COBS encoding, and chosen over more traditional byte stuffing as it is more amenable to vectorization, in particular hardware with AVX-256 can copy a 32-byte block in a single instruction. Dictionary Arrays Dictionary Encoded Data (called categorical in pandas) is increasingly important because they can store and process low cardinality data very efficiently. A simple approach to encoding dictionary arrays would be to encode the logical values directly using the encodings for primitive values described previously. However, this would lose the benefits of dictionary encoding to reduce memory and CPU consumption. To further complicate matters, the Arrow implementation of Dictionary encoding is quite general, and we can make no assumptions about the contents of the dictionaries. In particular, we cannot assume that the dictionary values are sorted, nor that the same dictionary is used for all arrays within a column The following example shows how a string column might be encoded in two arrays using two different dictionaries. The dictionary keys 0, 1, and 2 in the first batch correspond to different values than the same keys in the second dictionary. ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌───────────┐ ┌─────┐ │ │ │"Fabulous" │ │ 0 │ ├───────────┤ ├─────┤ │ │ │ "Bar" │ │ 2 │ ├───────────┤ ├─────┤ │ ┌───────────┐ │ │ "Soup" │ │ 2 │ │"Fabulous" │ └───────────┘ ├─────┤ │ ├───────────┤ │ │ 0 │ │ "Soup" │ ├─────┤ │ ├───────────┤ │ │ 1 │ │ "Soup" │ └─────┘ │ ├───────────┤ │ │"Fabulous" │ Values │ ├───────────┤ │ Dictionary (indexes in │ "Bar" │ dictionary) │ ├───────────┤ │ │ "ZZ" │ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ├───────────┤ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ "Bar" │ │ ├───────────┤ │ ┌───────────┐ ┌─────┐ │ "ZZ" │ │"Fabulous" │ │ 1 │ │ ├───────────┤ │ ├───────────┤ ├─────┤ │"Fabulous" │ │ "ZZ" │ │ 2 │ │ └───────────┘ │ ├───────────┤ ├─────┤ │ "Bar" │ │ 1 │ │ │ └───────────┘ ├─────┤ │ 0 │ │ Logical column │ └─────┘ values Values │ │ Dictionary (indexes in dictionary) │ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ The key observation which allows us to efficiently create a row format for this kind of data is that given a byte array, a new byte array can always be created which comes before or after it in the sort order by adding an additional byte. Therefore we can incrementally build an order-preserving mapping from dictionary values to variable length byte arrays, without needing to know all possible dictionary values beforehand, instead introducing mappings for new dictionary values as we encounter them. ┌──────────┐ ┌─────┐ │ "Bar" │ ───────────────▶│ 01 │ └──────────┘ └─────┘ ┌──────────┐ ┌─────┬─────┐ │"Fabulous"│ ───────────────▶│ 01 │ 02 │ └──────────┘ └─────┴─────┘ ┌──────────┐ ┌─────┐ │ "Soup" │ ───────────────▶│ 05 │ └──────────┘ └─────┘ ┌──────────┐ ┌─────┐ │ "ZZ" │ ───────────────▶│ 07 │ └──────────┘ └─────┘ Example Order Preserving Mapping The details of the data structure used to generate this mapping are beyond the scope of this blog post, but may be the topic of a future post. You can find the code here. The data structure also ensures that no values contain 0x00 and therefore we can encode the arrays directly using 0x00 as an end-delimiter. A null value is encoded as a single 0x00 byte, and a non-null value encoded as a single 0x01 byte, followed by the 0x00 terminated byte array determined by the order preserving mapping ┌─────┬─────┬─────┬─────┐ "Fabulous" │ 01 │ 03 │ 05 │ 00 │ └─────┴─────┴─────┴─────┘ ┌─────┬─────┬─────┐ "ZZ" │ 01 │ 07 │ 00 │ └─────┴─────┴─────┘ ┌─────┐ NULL │ 00 │ └─────┘ Input Row Format Sort Options One detail we have so far ignored over is how to support ascending and descending sorts (e.g. ASC or DESC in SQL). The Arrow Rust row format supports these options by simply inverting the bytes of the encoded representation, except the initial byte used for nullability encoding, on a per column basis. Similarly, supporting SQL compatible sorting also requires a format that can specify the order of NULLs (before or after all non NULL values). The row format supports this option by optionally encoding nulls as 0xFF instead of 0x00 on a per column basis. Conclusion Hopefully these two articles have given you a flavor of what is possible with a comparable row format and how it works. Feel free to check out the docs for instructions on getting started, and report any issues on our bugtracker. Using this format for lexicographic sorting is more than 3x faster than the comparator based approach, with the benefits especially pronounced for strings, dictionaries and sorts with large numbers of columns. We have also already used it to more than double the performance of sort preserving merge in the DataFusion project, and expect similar or greater performance uplift as we apply it to sort, grouping, join, and window function operators as well. As always, the Arrow community very much looks forward to seeing what you build with it!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Expanding Arrow’s Reach with a JDBC Driver for Arrow Flight SQL</title><link href="https://arrow.apache.org/blog/2022/11/01/arrow-flight-sql-jdbc/" rel="alternate" type="text/html" title="Expanding Arrow’s Reach with a JDBC Driver for Arrow Flight SQL" /><published>2022-11-01T00:00:00-04:00</published><updated>2022-11-01T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/11/01/arrow-flight-sql-jdbc</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/11/01/arrow-flight-sql-jdbc/"><![CDATA[<!--

-->

<p>We’re excited to announce that as of version 10.0.0, the Arrow project
now includes a <a href="https://docs.oracle.com/javase/tutorial/jdbc/overview/index.html">JDBC</a> driver <a href="https://github.com/apache/arrow/tree/master/java/flight/flight-sql-jdbc-driver">implementation</a> based on
<a href="/docs/format/FlightSql.html">Arrow Flight SQL</a>.  This is courtesy of a software grant
from <a href="https://www.dremio.com/resources/guides/apache-arrow/">Dremio</a>, a data lakehouse platform. Contributors
from Dremio developed and open-sourced this driver implementation, in
addition to designing and contributing Flight SQL itself.</p>

<p>Flight SQL is a protocol for client-server database interactions.  It
defines how a client should talk to a server and execute queries,
fetch result sets, and so on.  Note that despite the name, Flight SQL
is <em>not</em> a SQL dialect, or even specific to SQL itself.  Underneath,
it builds on <a href="/docs/format/Flight.html">Arrow Flight RPC</a>, a framework for efficient
transfer of Arrow data across the network.  While Flight RPC is
flexible and can be used in any type of application, from the
beginning, it was designed with an eye towards the kinds of use cases
that Flight SQL supports.</p>

<p>With this new JDBC driver, applications can talk to any database
server implementing the Flight SQL protocol using familiar JDBC APIs.
Underneath, the driver sends queries to the server via Flight SQL and
adapts the Arrow result set to the JDBC interface, so that the
database can support JDBC users without implementing additional APIs
or its own JDBC driver.</p>

<h2 id="why-use-jdbc-with-flight-sql">Why use JDBC with Flight SQL?</h2>

<p>JDBC offers a row-oriented API, which is opposite of Arrow’s columnar
structure.  However, it is a popular and time-tested choice for many
applications.  For example, many business intelligence (BI) tools take
advantage of JDBC to interoperate generically with multiple databases.
An Arrow-native database may still wish to be accessible to all of
this existing software, without having to implement multiple client
drivers itself.  Additionally, columnar data transfer alone can be a
<a href="https://ir.cwi.nl/pub/26415">significant speedup</a> for analytical use cases.</p>

<p>This JDBC driver implementation demonstrates the generality of Arrow
and Flight SQL, and increases the reach of Arrow-based applications.
Additionally, an <a href="https://docs.dremio.com/software/drivers/arrow-flight-sql-odbc-driver/">ODBC driver implementation</a> based on
Flight SQL is also available courtesy of Dremio, though it is not yet
part of the Arrow project due to dependency licensing issues.</p>

<p>Now, a database can support the vast body of existing code that uses
JDBC or ODBC, as well as Arrow-native applications, just by
implementing a single wire protocol: Flight SQL.  Some projects
instead do things like reimplementing the Postgres wire protocol to
benefit from its existing drivers.  But for Arrow-native databases,
this gives up the benefits of columnar data.  On the other hand,
Flight SQL is:</p>

<ol>
  <li>Columnar and Arrow-native, using Arrow for result sets to avoid
unnecessary data copies and transformations;</li>
  <li>Designed for implementation by multiple databases, with high-level
C++ and Java libraries and a Protobuf protocol definition; and</li>
  <li>Usable both through APIs like JDBC and ODBC thanks to this software
grant, as well as directly (or via <a href="htttps://github.com/apache/arrow-adbc">ADBC</a>) for applications
that want columnar data.</li>
</ol>

<h2 id="getting-involved">Getting Involved</h2>

<p>The JDBC driver was merged for the <a href="/blog/2022/10/31/10.0.0-release/">Arrow 10.0.0 release</a>, and
the <a href="https://github.com/apache/arrow/tree/master/java/flight/flight-sql-jdbc-driver">source code</a> can be found in the Arrow repository.
Official builds of the driver are <a href="https://search.maven.org/search?q=a:flight-sql-jdbc-driver">available on Maven Central</a>.
Dremio is already making use of the driver, and we’re looking forward
to seeing what else gets built on top.  Of course, there are still
improvements to be made.  If you’re interested in contributing, or
have feedback or questions, please reach out on the <a href="/community/">mailing list</a>
or <a href="htttps://github.com/apache/arrow">GitHub</a>.</p>

<p>To learn more about when to use the Flight SQL JDBC driver vs the
Flight SQL native client libraries, see this section of Dremio’s
presentation, <a href="https://www.youtube.com/watch?v=6q8AMrQV3vE&amp;t=1343s">“Apache Arrow Flight SQL: a universal standard for high
performance data transfers from databases”</a>
(starting at 22:23).  For more about how Dremio uses Apache Arrow, see
their <a href="https://www.dremio.com/resources/guides/apache-arrow/">guide</a>.</p>]]></content><author><name>pmc</name></author><category term="application" /><summary type="html"><![CDATA[We’re excited to announce that as of version 10.0.0, the Arrow project now includes a JDBC driver implementation based on Arrow Flight SQL. This is courtesy of a software grant from Dremio, a data lakehouse platform. Contributors from Dremio developed and open-sourced this driver implementation, in addition to designing and contributing Flight SQL itself. Flight SQL is a protocol for client-server database interactions. It defines how a client should talk to a server and execute queries, fetch result sets, and so on. Note that despite the name, Flight SQL is not a SQL dialect, or even specific to SQL itself. Underneath, it builds on Arrow Flight RPC, a framework for efficient transfer of Arrow data across the network. While Flight RPC is flexible and can be used in any type of application, from the beginning, it was designed with an eye towards the kinds of use cases that Flight SQL supports. With this new JDBC driver, applications can talk to any database server implementing the Flight SQL protocol using familiar JDBC APIs. Underneath, the driver sends queries to the server via Flight SQL and adapts the Arrow result set to the JDBC interface, so that the database can support JDBC users without implementing additional APIs or its own JDBC driver. Why use JDBC with Flight SQL? JDBC offers a row-oriented API, which is opposite of Arrow’s columnar structure. However, it is a popular and time-tested choice for many applications. For example, many business intelligence (BI) tools take advantage of JDBC to interoperate generically with multiple databases. An Arrow-native database may still wish to be accessible to all of this existing software, without having to implement multiple client drivers itself. Additionally, columnar data transfer alone can be a significant speedup for analytical use cases. This JDBC driver implementation demonstrates the generality of Arrow and Flight SQL, and increases the reach of Arrow-based applications. Additionally, an ODBC driver implementation based on Flight SQL is also available courtesy of Dremio, though it is not yet part of the Arrow project due to dependency licensing issues. Now, a database can support the vast body of existing code that uses JDBC or ODBC, as well as Arrow-native applications, just by implementing a single wire protocol: Flight SQL. Some projects instead do things like reimplementing the Postgres wire protocol to benefit from its existing drivers. But for Arrow-native databases, this gives up the benefits of columnar data. On the other hand, Flight SQL is: Columnar and Arrow-native, using Arrow for result sets to avoid unnecessary data copies and transformations; Designed for implementation by multiple databases, with high-level C++ and Java libraries and a Protobuf protocol definition; and Usable both through APIs like JDBC and ODBC thanks to this software grant, as well as directly (or via ADBC) for applications that want columnar data. Getting Involved The JDBC driver was merged for the Arrow 10.0.0 release, and the source code can be found in the Arrow repository. Official builds of the driver are available on Maven Central. Dremio is already making use of the driver, and we’re looking forward to seeing what else gets built on top. Of course, there are still improvements to be made. If you’re interested in contributing, or have feedback or questions, please reach out on the mailing list or GitHub. To learn more about when to use the Flight SQL JDBC driver vs the Flight SQL native client libraries, see this section of Dremio’s presentation, “Apache Arrow Flight SQL: a universal standard for high performance data transfers from databases” (starting at 22:23). For more about how Dremio uses Apache Arrow, see their guide.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 10.0.0 Release</title><link href="https://arrow.apache.org/blog/2022/10/31/10.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 10.0.0 Release" /><published>2022-10-31T00:00:00-04:00</published><updated>2022-10-31T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/10/31/10.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/10/31/10.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 10.0.0 release. This covers
over 3 months of development work and includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%2010.0.0"><strong>473 resolved issues</strong></a>
from <a href="/release/10.0.0.html#contributors"><strong>100 distinct contributors</strong></a>. See the <a href="https://arrow.apache.org/install/">Install Page</a>
to learn how to get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/10.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 9.0.0 release, Yanghong Zhong, Remzi Yang, Dan Harris and
Bogumił Kamińsk have been invited to be committers.
L.C. Hsieh, Weston Pace, Raphael Taylor-Davies, Nicola Crane and
Jacob Quinn have joined the Project Management Committee (PMC).
Thanks for your contributions and participation in the project!</p>

<h2 id="columnar-format-notes">Columnar Format Notes</h2>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>

<p>A JDBC driver based on <a href="https://arrow.apache.org/docs/format/FlightSql.html">Arrow Flight SQL</a> is now available, courtesy of a code donation from Dremio (<a href="https://issues.apache.org/jira/browse/ARROW-7744">ARROW-7744</a>).
For more details, see <a href="/blog/2022/11/01/arrow-flight-sql-jdbc/">“Expanding Arrow’s Reach with a JDBC Driver for Arrow Flight SQL”</a>.
Flight SQL is now supported in Go (<a href="https://issues.apache.org/jira/browse/ARROW-17326">ARROW-17326</a>).
Protocol definitions for transactions and <a href="https://substrait.io">Substrait</a> plans were added to Flight SQL and are implemented in C++ and Java (<a href="https://issues.apache.org/jira/browse/ARROW-17688">ARROW-17688</a>).
General “best practices” documentation was added for C++ (<a href="https://issues.apache.org/jira/browse/ARROW-17407">ARROW-17407</a>).
The C++ implementation now has basic <a href="https://opentelemetry.io/">OpenTelemetry</a> integration (<a href="https://issues.apache.org/jira/browse/ARROW-14958">ARROW-14958</a>).</p>

<h2 id="c-notes">C++ notes</h2>

<h3 id="c11-is-no-longer-supported">C++11 is no longer supported</h3>

<p>The Arrow C++ codebase has moved to C++17 as its language compatibility
standard (<a href="https://issues.apache.org/jira/browse/ARROW-17545">ARROW-17545</a>). This means Arrow C++, including its header files,
now requires a C++17-compliant compiler and standard library to be used.
Such compilers are widely available on most platforms.</p>

<p>Compatibility backports of C++14 and C++17 features, such as <code class="language-plaintext highlighter-rouge">std::string_view</code>
or <code class="language-plaintext highlighter-rouge">std::variant</code>, have been removed in favor of the standard library version
of these APIs (<a href="https://issues.apache.org/jira/browse/ARROW-17546">ARROW-17546</a>). This will also make integration of Arrow C++
with other codebases easier.</p>

<p>It is expected that the Arrow C++ codebase will be gradually modernized to use
C++17 features in subsequent releases, when the need arises.</p>

<h3 id="plasma-is-deprecated">Plasma is deprecated</h3>

<p>The Plasma module is deprecated and will be removed in a future release.</p>

<h3 id="compute--acero">Compute / Acero</h3>

<p>Extension types are now supported in hash joins (<a href="https://issues.apache.org/jira/browse/ARROW-16695">ARROW-16695</a>).</p>

<h3 id="datasets">Datasets</h3>

<p>The fragments of a dataset can now be iterated in an asynchronous fashion,
using <code class="language-plaintext highlighter-rouge">Dataset::GetFragmentsAsync</code> (<a href="https://issues.apache.org/jira/browse/ARROW-17318">ARROW-17318</a>).</p>

<h3 id="filesystems">Filesystems</h3>

<p>It is now possible to configure a timeout policy for S3 (<a href="https://issues.apache.org/jira/browse/ARROW-16521">ARROW-16521</a>).</p>

<p>Error messages for S3 have been improved to give more context about the
error (<a href="https://issues.apache.org/jira/browse/ARROW-17079">ARROW-17079</a>).</p>

<p><code class="language-plaintext highlighter-rouge">GetFileInfoGenerator</code> has been optimized for local filesystems, with
dedicated options to tune chunking and readahead (<a href="https://issues.apache.org/jira/browse/ARROW-17306">ARROW-17306</a>).</p>

<h3 id="json">JSON</h3>

<p>Previously, the JSON reader could only read Decimal fields from JSON strings
(i.e. quoted). Now it can also read Decimal fields from JSON numbers as well
(<a href="https://issues.apache.org/jira/browse/ARROW-17847">ARROW-17847</a>).</p>

<h3 id="parquet">Parquet</h3>

<p>Before Arrow 3.0.0, data pages version 2 were incorrectly written out, making
them unreadable with spec-compliant readers. A compatibility fix has been
introduced so that they can still be read with contemporary versions of
Arrow (<a href="https://issues.apache.org/jira/browse/ARROW-17100">ARROW-17100</a>).</p>

<h3 id="substrait">Substrait</h3>

<p>The Substrait consumer, which allows Substrait plans to be executed by the
Acero execution engine, has received some improvements:</p>

<ul>
  <li>
    <p>Aggregations are now supported (<a href="https://issues.apache.org/jira/browse/ARROW-15591">ARROW-15591</a>).</p>
  </li>
  <li>
    <p>Conversion options have been added so that the level of compliance and
rountrippability can be chosen when converting between Substrait and
Acero representations of a plan (<a href="https://issues.apache.org/jira/browse/ARROW-16988">ARROW-16988</a>).</p>
  </li>
  <li>
    <p>Support for many standard Substrait functions has been added
(<a href="https://issues.apache.org/jira/browse/ARROW-15582">ARROW-15582</a>, <a href="https://issues.apache.org/jira/browse/ARROW-17523">ARROW-17523</a>)</p>
  </li>
</ul>

<p>Some work has also been done in the reverse direction, to allow Acero execution
plans to be serialized as Substrait plans (<a href="https://issues.apache.org/jira/browse/ARROW-16855">ARROW-16855</a>).</p>

<h3 id="other-changes">Other changes</h3>

<p>Our CMake package files have been overhauled (<a href="https://issues.apache.org/jira/browse/ARROW-12175">ARROW-12175</a>).  As a result,
namespaced targets are now exported, such as <code class="language-plaintext highlighter-rouge">Arrow::arrow_shared</code>.
Legacy (non-namespaced) names are still available, for example <code class="language-plaintext highlighter-rouge">arrow_shared</code>.</p>

<p>Compiling in release mode now uses <code class="language-plaintext highlighter-rouge">-O2</code>, not <code class="language-plaintext highlighter-rouge">-O3</code>, by default (<a href="https://issues.apache.org/jira/browse/ARROW-17436">ARROW-17436</a>).</p>

<p>The RISC-V architecture is now recongnized at build time (<a href="https://issues.apache.org/jira/browse/ARROW-17440">ARROW-17440</a>).</p>

<p>The PyArrow-specific C++ code was moved into the PyArrow source tree
(see below, <a href="https://issues.apache.org/jira/browse/ARROW-16340">ARROW-16340</a>). The <code class="language-plaintext highlighter-rouge">ARROW_PYTHON</code> CMake variable has been deprecated
and will be removed in a later release; you should instead enable the necessary
components explicitly (<a href="https://issues.apache.org/jira/browse/ARROW-17868">ARROW-17868</a>).</p>

<p>Some classes with a <code class="language-plaintext highlighter-rouge">Equals</code> method now also support <code class="language-plaintext highlighter-rouge">operator==</code> (<a href="https://issues.apache.org/jira/browse/ARROW-6772">ARROW-6772</a>).
It was decided to only do this when equality is computationally cheap (i.e.
not on data collections such as Array, RecordBatch…).</p>

<h2 id="c-notes-1">C# notes</h2>

<h4 id="bug-fixes">Bug Fixes</h4>

<ul>
  <li>DecimalArray incorrectly appends values very large and very small values. (<a href="https://github.com/apache/arrow/pull/13732">ARROW-17223</a>)</li>
</ul>

<h2 id="gandiva-notes">Gandiva notes</h2>

<p>Gandiva has been migrated to use LLVM opaque pointer types, as typed
pointers had been deprecated (<a href="https://issues.apache.org/jira/browse/ARROW-17790">ARROW-17790</a>).</p>

<h2 id="go-notes">Go notes</h2>

<ul>
  <li>A new CI job has been added to run all of the tests with the <code class="language-plaintext highlighter-rouge">-asan</code> option using go1.18 (<a href="https://issues.apache.org/jira/browse/ARROW-17324">ARROW-17324</a>)</li>
  <li>Go now passes all integration tests on data types and IPC handling.</li>
  <li>The Go Arrow and Parquet packages now require go1.17+ (<a href="https://issues.apache.org/jira/browse/ARROW-17646">ARROW-17646</a>)</li>
</ul>

<h3 id="compute">Compute</h3>

<p>The compute package that was importable via <code class="language-plaintext highlighter-rouge">github.com/apache/arrow/go/v9/arrow/compute</code> is now a separate module which requires go1.18+ (only the compute module, the rest of the packages still work fine under go1.17). (<a href="https://issues.apache.org/jira/browse/ARROW-17456">ARROW-17456</a>).</p>

<p>Scalar and Vector kernel infrastructure has been implemented for performing compute operations providing the following functionality:</p>

<ul>
  <li>casting Arrow Arrays from one type to another (<a href="https://issues.apache.org/jira/browse/ARROW-17454">ARROW-17454</a>)</li>
  <li>Using Filter and Take functions on an Arrow Array, Record Batch, or Table (<a href="https://issues.apache.org/jira/browse/ARROW-17669">ARROW-17669</a>)</li>
</ul>

<h3 id="arrow">Arrow</h3>

<ul>
  <li>Sparse and Dense Union Arrays have been implemented along with appropriate builders and data type support including in IPC reading and writing. (<a href="https://issues.apache.org/jira/browse/ARROW-3678">ARROW-3678</a>, <a href="https://issues.apache.org/jira/browse/ARROW-17276">ARROW-17276</a>). This includes scalar types for Dense and Sparse union (<a href="https://issues.apache.org/jira/browse/ARROW-17390">ARROW-17390</a>)</li>
  <li>LargeBinary, LargeString and LargeList arrays have been implemented for handling arrays with 64-bit offsets. This also included fixing a bug so that binary builders are correctly resettable. (<a href="https://issues.apache.org/jira/browse/ARROW-8226">ARROW-8226</a>, <a href="https://issues.apache.org/jira/browse/ARROW-17275">ARROW-17275</a>)</li>
  <li>Support for Decimal256 arrays has been implemented (<a href="https://issues.apache.org/jira/browse/ARROW-10600">ARROW-10600</a>)</li>
  <li>Automatic Endianness Conversion for non-native endianness is now an option for IPC streams (<a href="https://issues.apache.org/jira/browse/ARROW-17219">ARROW-17219</a>)</li>
  <li>CSV Writer now supports Timestamp, Date32 and Date64 types (<a href="https://issues.apache.org/jira/browse/ARROW-17273">ARROW-17273</a>)</li>
  <li>CSV Writer now supports custom formatting for boolean values (<a href="https://issues.apache.org/jira/browse/ARROW-17277">ARROW-17277</a>)</li>
  <li>The Go Arrow Library now provides a FlightSQL client and server implementation (<a href="https://issues.apache.org/jira/browse/ARROW-17326">ARROW-17326</a>). An example server implementation is provided for a FlightSQL server using SQLite (<a href="https://issues.apache.org/jira/browse/ARROW-17359">ARROW-17359</a>)</li>
  <li>CSV Reader now supports schema type inference via NewInferringReader, along with options for specifying the type of some columns and skipping columns (<a href="https://issues.apache.org/jira/browse/ARROW-17778">ARROW-17778</a>)</li>
</ul>

<h3 id="parquet-1">Parquet</h3>

<ul>
  <li>RowGroupReader.Column(index int) no longer panics if provided an invalid column index. Instead the signature has been changed to now return (PageReader, error) similar to other methods in the codebase. (<a href="https://issues.apache.org/jira/browse/ARROW-17274">ARROW-17274</a>)</li>
  <li>Bitpacking and other internal required implementations for ppc64le have been added for the Parquet library (<a href="https://issues.apache.org/jira/browse/ARROW-17372">ARROW-17372</a>)</li>
  <li>A bug has been fixed that caused inconsistent row information data from a table written by Athena (<a href="https://issues.apache.org/jira/browse/ARROW-17453">ARROW-17453</a>)</li>
  <li>Fixed a bug that caused panics when writing a Nullable List of Structs (<a href="https://issues.apache.org/jira/browse/ARROW-17169">ARROW-17169</a>)</li>
  <li>Key Value metadata in an Arrow Schema will be propagated to the Parquet file when using pqarrow even when not using StoreSchema (<a href="https://issues.apache.org/jira/browse/ARROW-17627">ARROW-17627</a>)</li>
  <li>A memory leak when using statistics on ByteArray columns has been fixed (<a href="https://issues.apache.org/jira/browse/ARROW-17573">ARROW-17573</a>)</li>
</ul>

<h2 id="java-notes">Java notes</h2>
<p>Many important features, enhancements, and bug fixes are included in this release, as are documentation enhancements, and a large number of improvements to build processes and project infrastructure. Selected highlights can be found below.</p>

<h4 id="new-features-and-enhancements">New Features and Enhancements</h4>

<ul>
  <li>JDBC Driver for Arrow Flight SQL (<a href="https://github.com/apache/arrow/pull/13800">13800</a>)</li>
  <li>Initial implementation of immutable Table API (<a href="https://github.com/apache/arrow/pull/14316">14316</a>)</li>
  <li>Substrait, transaction, cancellation for Flight SQL (<a href="https://github.com/apache/arrow/pull/13492">13492</a>)</li>
  <li>Read Arrow IPC, CSV, and ORC files by NativeDatasetFactory (<a href="https://github.com/apache/arrow/pull/13811">13811</a>, <a href="https://github.com/apache/arrow/pull/13973">13973</a>, <a href="https://github.com/apache/arrow/pull/14182">14182</a>)</li>
  <li>Add utility to bind Arrow data to JDBC parameters (<a href="https://github.com/apache/arrow/pull/13589">13589</a>)</li>
</ul>

<h4 id="build-enhancements">Build enhancements</h4>

<ul>
  <li>Add Windows build script that produces DLLs (<a href="https://github.com/apache/arrow/pull/14203">14203</a>)</li>
  <li>C Data Interface and Dataset libraries can now be built with mvn commands (<a href="https://github.com/apache/arrow/pull/13881">13881</a>, <a href="https://github.com/apache/arrow/pull/13889">13889</a>)</li>
</ul>

<h4 id="java-notes-1">Java notes:</h4>

<ul>
  <li>Java Plasma JNI bindings have been deprecated (<a href="https://github.com/apache/arrow/pull/14262">14262</a>)
    <h2 id="javascript-notes">JavaScript notes</h2>
  </li>
  <li>No major changes.</li>
</ul>

<h2 id="python-notes">Python notes</h2>

<p>Compatibility notes:</p>

<ul>
  <li>Some deprecated APIs (deprecated at least since pyarrow 1.0.0) have been removed: the
<code class="language-plaintext highlighter-rouge">RecordBatchReader.get_next_batch</code> method, <code class="language-plaintext highlighter-rouge">DataType.num_children</code> attribute, etc
(<a href="https://issues.apache.org/jira/browse/ARROW-17649">ARROW-17649</a>).</li>
  <li>When writing to Arrow IPC file format with <code class="language-plaintext highlighter-rouge">pyarrow.dataset.write_dataset</code> using
<code class="language-plaintext highlighter-rouge">format="ipc"</code> or <code class="language-plaintext highlighter-rouge">format="arrow"</code>, the default extension for the resulting files is
changed to <code class="language-plaintext highlighter-rouge">.arrow</code> instead of <code class="language-plaintext highlighter-rouge">.feather</code>. You can still use <code class="language-plaintext highlighter-rouge">format="feather"</code> to
write identical files but using the <code class="language-plaintext highlighter-rouge">.feather</code> extension
(<a href="https://issues.apache.org/jira/browse/ARROW-17089">ARROW-17089</a>).</li>
</ul>

<p>New features and improvements:</p>

<ul>
  <li>Filters in <code class="language-plaintext highlighter-rouge">pq.read_table()</code> can be passed as an expression in addition to the legacy
list of tuples. For example, <code class="language-plaintext highlighter-rouge">filters=pc.field("col") &lt; 4</code> is equivalent to
<code class="language-plaintext highlighter-rouge">filters=[("col", "&lt;", 4)]</code>
(<a href="https://issues.apache.org/jira/browse/ARROW-17483">ARROW-17483</a>).</li>
  <li>The <code class="language-plaintext highlighter-rouge">batch_readahead</code> and <code class="language-plaintext highlighter-rouge">fragment_readahead</code> arguments for scanning Datasets are
exposed in Python (<a href="https://issues.apache.org/jira/browse/ARROW-17299">ARROW-17299</a>).</li>
  <li>ExtensionArrays can now be created from a storage array through the <code class="language-plaintext highlighter-rouge">pa.array(..)</code>
constructor (<a href="https://issues.apache.org/jira/browse/ARROW-17834">ARROW-17834</a>).</li>
  <li>Converting ListArrays containing ExtensionArray values to numpy or pandas works by
falling back to the storage array
(<a href="https://issues.apache.org/jira/browse/ARROW-17813">ARROW-17813</a>).</li>
  <li>The <code class="language-plaintext highlighter-rouge">pyarrow.substrait.run_query()</code> function gained a <code class="language-plaintext highlighter-rouge">table_provider</code> keyword to run
the query against in-memory tables
(<a href="https://issues.apache.org/jira/browse/ARROW-17521">ARROW-17521</a>).</li>
  <li>The <code class="language-plaintext highlighter-rouge">StructType</code> class gained a <code class="language-plaintext highlighter-rouge">field()</code> method to retrieve a child field
(<a href="https://issues.apache.org/jira/browse/ARROW-17131">ARROW-17131</a>).</li>
  <li>Casting Tables to a new schema now honors the nullability flag in the target schema
(<a href="https://issues.apache.org/jira/browse/ARROW-16651">ARROW-16651</a>).</li>
  <li>Parquet files are now explicitly closed after reading
(<a href="https://issues.apache.org/jira/browse/ARROW-13763">ARROW-13763</a>).</li>
</ul>

<p>Further, the Python bindings benefit from improvements in the C++ library
(e.g. new compute functions); see the C++ notes above for additional details.</p>

<p><strong>Build notes</strong></p>

<p>The PyArrow-specific C++ code, previously part of Arrow C++ codebase, is now integrated
into PyArrow. The tests are run automatically as part of the PyArrow test suite. See:
<a href="https://issues.apache.org/jira/browse/ARROW-16340">ARROW-16340</a>,
<a href="https://issues.apache.org/jira/browse/ARROW-17122">ARROW-17122</a> and
<a href="https://arrow.apache.org/docs/dev/python/integration/extending.html#c-api">PyArrow C++ API notes</a>).</p>

<p>The build process is generally not affected by the change, but the <code class="language-plaintext highlighter-rouge">ARROW_PYTHON</code> CMake
variable has been deprecated and will be removed in a later release; you should instead
enable the necessary components explicitly
(<a href="https://issues.apache.org/jira/browse/ARROW-17868">ARROW-17868</a>).</p>

<h2 id="r-notes">R notes</h2>
<p>Many improvements to Arrow dplyr queries are added in this version, including:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">dplyr::across()</code> can be used to apply the same computation across multiple columns;</li>
  <li>long-running queries can now be cancelled;</li>
  <li>the data source file name can be added as a column when reading multi-file datasets with <code class="language-plaintext highlighter-rouge">add_filename()</code>;</li>
  <li>joins now support extension arrays;</li>
  <li>and all supported Arrow dplyr functions are now documented on the <a href="https://arrow.apache.org/docs/r/reference/acero.html">R documentation site</a>.</li>
</ul>

<p>For more on what’s in the 10.0.0 R package, see the <a href="/docs/r/news/">R changelog</a>.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<h3 id="ruby">Ruby</h3>

<ul>
  <li>Plasma binding has been deprecated (<a href="https://issues.apache.org/jira/browse/ARROW-17864">ARROW-17864</a>)</li>
</ul>

<h3 id="c-glib">C GLib</h3>

<ul>
  <li>Plasma binding has been deprecated (<a href="https://issues.apache.org/jira/browse/ARROW-17862">ARROW-17862</a>)</li>
</ul>

<h2 id="rust-notes">Rust notes</h2>

<p>The Rust projects have moved to separate repositories outside the
main Arrow monorepo. For notes on the latest release of the Rust
implementation, see the latest <a href="https://github.com/apache/arrow-rs/tags">Arrow Rust changelog</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 10.0.0 release. This covers over 3 months of development work and includes 473 resolved issues from 100 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 9.0.0 release, Yanghong Zhong, Remzi Yang, Dan Harris and Bogumił Kamińsk have been invited to be committers. L.C. Hsieh, Weston Pace, Raphael Taylor-Davies, Nicola Crane and Jacob Quinn have joined the Project Management Committee (PMC). Thanks for your contributions and participation in the project! Columnar Format Notes Arrow Flight RPC notes A JDBC driver based on Arrow Flight SQL is now available, courtesy of a code donation from Dremio (ARROW-7744). For more details, see “Expanding Arrow’s Reach with a JDBC Driver for Arrow Flight SQL”. Flight SQL is now supported in Go (ARROW-17326). Protocol definitions for transactions and Substrait plans were added to Flight SQL and are implemented in C++ and Java (ARROW-17688). General “best practices” documentation was added for C++ (ARROW-17407). The C++ implementation now has basic OpenTelemetry integration (ARROW-14958). C++ notes C++11 is no longer supported The Arrow C++ codebase has moved to C++17 as its language compatibility standard (ARROW-17545). This means Arrow C++, including its header files, now requires a C++17-compliant compiler and standard library to be used. Such compilers are widely available on most platforms. Compatibility backports of C++14 and C++17 features, such as std::string_view or std::variant, have been removed in favor of the standard library version of these APIs (ARROW-17546). This will also make integration of Arrow C++ with other codebases easier. It is expected that the Arrow C++ codebase will be gradually modernized to use C++17 features in subsequent releases, when the need arises. Plasma is deprecated The Plasma module is deprecated and will be removed in a future release. Compute / Acero Extension types are now supported in hash joins (ARROW-16695). Datasets The fragments of a dataset can now be iterated in an asynchronous fashion, using Dataset::GetFragmentsAsync (ARROW-17318). Filesystems It is now possible to configure a timeout policy for S3 (ARROW-16521). Error messages for S3 have been improved to give more context about the error (ARROW-17079). GetFileInfoGenerator has been optimized for local filesystems, with dedicated options to tune chunking and readahead (ARROW-17306). JSON Previously, the JSON reader could only read Decimal fields from JSON strings (i.e. quoted). Now it can also read Decimal fields from JSON numbers as well (ARROW-17847). Parquet Before Arrow 3.0.0, data pages version 2 were incorrectly written out, making them unreadable with spec-compliant readers. A compatibility fix has been introduced so that they can still be read with contemporary versions of Arrow (ARROW-17100). Substrait The Substrait consumer, which allows Substrait plans to be executed by the Acero execution engine, has received some improvements: Aggregations are now supported (ARROW-15591). Conversion options have been added so that the level of compliance and rountrippability can be chosen when converting between Substrait and Acero representations of a plan (ARROW-16988). Support for many standard Substrait functions has been added (ARROW-15582, ARROW-17523) Some work has also been done in the reverse direction, to allow Acero execution plans to be serialized as Substrait plans (ARROW-16855). Other changes Our CMake package files have been overhauled (ARROW-12175). As a result, namespaced targets are now exported, such as Arrow::arrow_shared. Legacy (non-namespaced) names are still available, for example arrow_shared. Compiling in release mode now uses -O2, not -O3, by default (ARROW-17436). The RISC-V architecture is now recongnized at build time (ARROW-17440). The PyArrow-specific C++ code was moved into the PyArrow source tree (see below, ARROW-16340). The ARROW_PYTHON CMake variable has been deprecated and will be removed in a later release; you should instead enable the necessary components explicitly (ARROW-17868). Some classes with a Equals method now also support operator== (ARROW-6772). It was decided to only do this when equality is computationally cheap (i.e. not on data collections such as Array, RecordBatch…). C# notes Bug Fixes DecimalArray incorrectly appends values very large and very small values. (ARROW-17223) Gandiva notes Gandiva has been migrated to use LLVM opaque pointer types, as typed pointers had been deprecated (ARROW-17790). Go notes A new CI job has been added to run all of the tests with the -asan option using go1.18 (ARROW-17324) Go now passes all integration tests on data types and IPC handling. The Go Arrow and Parquet packages now require go1.17+ (ARROW-17646) Compute The compute package that was importable via github.com/apache/arrow/go/v9/arrow/compute is now a separate module which requires go1.18+ (only the compute module, the rest of the packages still work fine under go1.17). (ARROW-17456). Scalar and Vector kernel infrastructure has been implemented for performing compute operations providing the following functionality: casting Arrow Arrays from one type to another (ARROW-17454) Using Filter and Take functions on an Arrow Array, Record Batch, or Table (ARROW-17669) Arrow Sparse and Dense Union Arrays have been implemented along with appropriate builders and data type support including in IPC reading and writing. (ARROW-3678, ARROW-17276). This includes scalar types for Dense and Sparse union (ARROW-17390) LargeBinary, LargeString and LargeList arrays have been implemented for handling arrays with 64-bit offsets. This also included fixing a bug so that binary builders are correctly resettable. (ARROW-8226, ARROW-17275) Support for Decimal256 arrays has been implemented (ARROW-10600) Automatic Endianness Conversion for non-native endianness is now an option for IPC streams (ARROW-17219) CSV Writer now supports Timestamp, Date32 and Date64 types (ARROW-17273) CSV Writer now supports custom formatting for boolean values (ARROW-17277) The Go Arrow Library now provides a FlightSQL client and server implementation (ARROW-17326). An example server implementation is provided for a FlightSQL server using SQLite (ARROW-17359) CSV Reader now supports schema type inference via NewInferringReader, along with options for specifying the type of some columns and skipping columns (ARROW-17778) Parquet RowGroupReader.Column(index int) no longer panics if provided an invalid column index. Instead the signature has been changed to now return (PageReader, error) similar to other methods in the codebase. (ARROW-17274) Bitpacking and other internal required implementations for ppc64le have been added for the Parquet library (ARROW-17372) A bug has been fixed that caused inconsistent row information data from a table written by Athena (ARROW-17453) Fixed a bug that caused panics when writing a Nullable List of Structs (ARROW-17169) Key Value metadata in an Arrow Schema will be propagated to the Parquet file when using pqarrow even when not using StoreSchema (ARROW-17627) A memory leak when using statistics on ByteArray columns has been fixed (ARROW-17573) Java notes Many important features, enhancements, and bug fixes are included in this release, as are documentation enhancements, and a large number of improvements to build processes and project infrastructure. Selected highlights can be found below. New Features and Enhancements JDBC Driver for Arrow Flight SQL (13800) Initial implementation of immutable Table API (14316) Substrait, transaction, cancellation for Flight SQL (13492) Read Arrow IPC, CSV, and ORC files by NativeDatasetFactory (13811, 13973, 14182) Add utility to bind Arrow data to JDBC parameters (13589) Build enhancements Add Windows build script that produces DLLs (14203) C Data Interface and Dataset libraries can now be built with mvn commands (13881, 13889) Java notes: Java Plasma JNI bindings have been deprecated (14262) JavaScript notes No major changes. Python notes Compatibility notes: Some deprecated APIs (deprecated at least since pyarrow 1.0.0) have been removed: the RecordBatchReader.get_next_batch method, DataType.num_children attribute, etc (ARROW-17649). When writing to Arrow IPC file format with pyarrow.dataset.write_dataset using format="ipc" or format="arrow", the default extension for the resulting files is changed to .arrow instead of .feather. You can still use format="feather" to write identical files but using the .feather extension (ARROW-17089). New features and improvements: Filters in pq.read_table() can be passed as an expression in addition to the legacy list of tuples. For example, filters=pc.field("col") &lt; 4 is equivalent to filters=[("col", "&lt;", 4)] (ARROW-17483). The batch_readahead and fragment_readahead arguments for scanning Datasets are exposed in Python (ARROW-17299). ExtensionArrays can now be created from a storage array through the pa.array(..) constructor (ARROW-17834). Converting ListArrays containing ExtensionArray values to numpy or pandas works by falling back to the storage array (ARROW-17813). The pyarrow.substrait.run_query() function gained a table_provider keyword to run the query against in-memory tables (ARROW-17521). The StructType class gained a field() method to retrieve a child field (ARROW-17131). Casting Tables to a new schema now honors the nullability flag in the target schema (ARROW-16651). Parquet files are now explicitly closed after reading (ARROW-13763). Further, the Python bindings benefit from improvements in the C++ library (e.g. new compute functions); see the C++ notes above for additional details. Build notes The PyArrow-specific C++ code, previously part of Arrow C++ codebase, is now integrated into PyArrow. The tests are run automatically as part of the PyArrow test suite. See: ARROW-16340, ARROW-17122 and PyArrow C++ API notes). The build process is generally not affected by the change, but the ARROW_PYTHON CMake variable has been deprecated and will be removed in a later release; you should instead enable the necessary components explicitly (ARROW-17868). R notes Many improvements to Arrow dplyr queries are added in this version, including: dplyr::across() can be used to apply the same computation across multiple columns; long-running queries can now be cancelled; the data source file name can be added as a column when reading multi-file datasets with add_filename(); joins now support extension arrays; and all supported Arrow dplyr functions are now documented on the R documentation site. For more on what’s in the 10.0.0 R package, see the R changelog. Ruby and C GLib notes Ruby Plasma binding has been deprecated (ARROW-17864) C GLib Plasma binding has been deprecated (ARROW-17862) Rust notes The Rust projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Rust implementation, see the latest Arrow Rust changelog.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow Ballista 0.9.0 Release</title><link href="https://arrow.apache.org/blog/2022/10/28/ballista-0.9.0/" rel="alternate" type="text/html" title="Apache Arrow Ballista 0.9.0 Release" /><published>2022-10-28T00:00:00-04:00</published><updated>2022-10-28T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/10/28/ballista-0.9.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/10/28/ballista-0.9.0/"><![CDATA[<!--

-->

<h1 id="introduction">Introduction</h1>

<p><a href="https://github.com/apache/arrow-ballista">Ballista</a> is an Arrow-native distributed SQL query engine implemented in Rust.</p>

<p>Ballista 0.9.0 is now available and is the most significant release since the project was <a href="http://arrow.apache.org/blog/2021/04/12/ballista-donation/">donated</a> to Apache
Arrow in 2021.</p>

<p>This release represents 4 weeks of work, with 66 commits from 14 contributors:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    22  Andy Grove
    12  yahoNanJing
     6  Daniël Heres
     4  Brent Gardner
     4  dependabot[bot]
     4  r.4ntix
     3  Stefan Stanciulescu
     3  mingmwang
     2  Ken Suenobu
     2  Yang Jiang
     1  Metehan Yıldırım
     1  Trent Feda
     1  askoa
     1  yangzhong
</code></pre></div></div>

<h2 id="release-highlights">Release Highlights</h2>

<p>The release notes below are not exhaustive and only expose selected highlights of the release. Many other bug fixes
and improvements have been made: we refer you to the <a href="https://github.com/apache/arrow-ballista/blob/0.9.0-rc2/ballista/CHANGELOG.md">complete changelog</a>.</p>

<h3 id="support-for-cloud-object-stores-and-distributed-file-systems">Support for Cloud Object Stores and Distributed File Systems</h3>

<p>This is the first release of Ballista to have documented support for querying data from distributed file systems and
object stores. Currently, S3 and HDFS are supported. Support for Google Cloud Storage and Azure Blob Storage is planned
for the next release.</p>

<h3 id="flight-sql--jdbc-support">Flight SQL &amp; JDBC support</h3>

<p>The Ballista scheduler now implements the <a href="https://arrow.apache.org/blog/2022/02/16/introducing-arrow-flight-sql/">Flight SQL protocol</a>, enabling any compliant Flight SQL client
to connect to and run queries against a Ballista cluster.</p>

<p>The Apache Arrow Flight SQL JDBC driver can be used to connect Business Intelligence tools to a Ballista cluster.</p>

<h3 id="python-bindings">Python Bindings</h3>

<p>It is now possible to connect to a Ballista cluster from Python and execute queries using both the DataFrame and SQL
interfaces.</p>

<h3 id="scheduler-web-user-interface-and-rest-api">Scheduler Web User Interface and REST API</h3>

<p>The scheduler now has a web user interface for monitoring queries. It is also possible to view graphical query plans
that show how the query was executed, along with metrics.</p>

<p><img src="/img/2022-10-28-ballista-web-ui.png" width="800" /></p>

<p>The REST API that powers the user interface can also be accessed directly.</p>

<h3 id="simplified-kubernetes-deployment">Simplified Kubernetes Deployment</h3>

<p>Ballista now provides a <a href="https://github.com/apache/arrow-ballista/tree/master/helm">Helm chart</a> for simplified Kubernetes deployment.</p>

<h3 id="user-guide">User Guide</h3>

<p>The user guide is published at <a href="https://arrow.apache.org/ballista/">https://arrow.apache.org/ballista/</a> and provides
deployment instructions for Docker, Docker Compose, and Kubernetes, as well as references for configuring and
tuning Ballista.</p>

<h2 id="roadmap">Roadmap</h2>

<p>The Ballista community is currently focused on the following tasks for the next release:</p>

<ul>
  <li>Support for Azure Blob Storage and Google Cloud Storage</li>
  <li>Improve benchmark performance by implementing more query optimizations</li>
  <li>Improve scheduler web user interface</li>
  <li>Publish Docker images to GitHub Container Registry</li>
</ul>

<p>The detailed list of issues planned for the 0.10.0 release can be found in the <a href="https://github.com/apache/arrow-ballista/issues/361">tracking issue</a>.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>Ballista has a friendly community and we welcome contributions. A good place to start is to following the instructions
in the <a href="https://arrow.apache.org/ballista/">user guide</a> and try using Ballista with your own SQL queries and ETL pipelines, and file issues
for any bugs or feature suggestions.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[Introduction Ballista is an Arrow-native distributed SQL query engine implemented in Rust. Ballista 0.9.0 is now available and is the most significant release since the project was donated to Apache Arrow in 2021. This release represents 4 weeks of work, with 66 commits from 14 contributors: 22 Andy Grove 12 yahoNanJing 6 Daniël Heres 4 Brent Gardner 4 dependabot[bot] 4 r.4ntix 3 Stefan Stanciulescu 3 mingmwang 2 Ken Suenobu 2 Yang Jiang 1 Metehan Yıldırım 1 Trent Feda 1 askoa 1 yangzhong Release Highlights The release notes below are not exhaustive and only expose selected highlights of the release. Many other bug fixes and improvements have been made: we refer you to the complete changelog. Support for Cloud Object Stores and Distributed File Systems This is the first release of Ballista to have documented support for querying data from distributed file systems and object stores. Currently, S3 and HDFS are supported. Support for Google Cloud Storage and Azure Blob Storage is planned for the next release. Flight SQL &amp; JDBC support The Ballista scheduler now implements the Flight SQL protocol, enabling any compliant Flight SQL client to connect to and run queries against a Ballista cluster. The Apache Arrow Flight SQL JDBC driver can be used to connect Business Intelligence tools to a Ballista cluster. Python Bindings It is now possible to connect to a Ballista cluster from Python and execute queries using both the DataFrame and SQL interfaces. Scheduler Web User Interface and REST API The scheduler now has a web user interface for monitoring queries. It is also possible to view graphical query plans that show how the query was executed, along with metrics. The REST API that powers the user interface can also be accessed directly. Simplified Kubernetes Deployment Ballista now provides a Helm chart for simplified Kubernetes deployment. User Guide The user guide is published at https://arrow.apache.org/ballista/ and provides deployment instructions for Docker, Docker Compose, and Kubernetes, as well as references for configuring and tuning Ballista. Roadmap The Ballista community is currently focused on the following tasks for the next release: Support for Azure Blob Storage and Google Cloud Storage Improve benchmark performance by implementing more query optimizations Improve scheduler web user interface Publish Docker images to GitHub Container Registry The detailed list of issues planned for the 0.10.0 release can be found in the tracking issue. Getting Involved Ballista has a friendly community and we welcome contributions. A good place to start is to following the instructions in the user guide and try using Ballista with your own SQL queries and ETL pipelines, and file issues for any bugs or feature suggestions.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow DataFusion 13.0.0 Project Update</title><link href="https://arrow.apache.org/blog/2022/10/25/datafusion-13.0.0/" rel="alternate" type="text/html" title="Apache Arrow DataFusion 13.0.0 Project Update" /><published>2022-10-25T00:00:00-04:00</published><updated>2022-10-25T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/10/25/datafusion-13.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/10/25/datafusion-13.0.0/"><![CDATA[<!--

-->

<h1 id="introduction">Introduction</h1>

<p><a href="https://arrow.apache.org/datafusion/">Apache Arrow DataFusion</a> <a href="https://crates.io/crates/datafusion"><code class="language-plaintext highlighter-rouge">13.0.0</code></a> is released, and this blog contains an update on the project for the 5 months since our <a href="https://arrow.apache.org/blog/2022/05/16/datafusion-8.0.0/">last update in May 2022</a>.</p>

<p>DataFusion is an extensible and embeddable query engine, written in Rust used to create modern, fast and efficient data pipelines, ETL processes, and database systems. You may want to check out DataFusion to extend your Rust project to:</p>

<ul>
  <li>Support <a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html">SQL support</a></li>
  <li>Support <a href="https://docs.rs/datafusion/13.0.0/datafusion/dataframe/struct.DataFrame.html">DataFrame API</a></li>
  <li>Support a Domain Specific Query Language</li>
  <li>Easily and quickly read and process Parquet, JSON, Avro or CSV data.</li>
  <li>Read from remote object stores such as AWS S3, Azure Blob Storage, GCP.</li>
</ul>

<p>Even though DataFusion is 4 years “young,” it has seen significant community growth in the last few months and the momentum continues to accelerate.</p>

<h1 id="background">Background</h1>

<p>DataFusion is used as the engine in <a href="https://github.com/apache/arrow-datafusion#known-uses">many open source and commercial projects</a> and was one of the early open source projects to provide this capability. 2022 has validated our belief in the need for such a <a href="https://docs.google.com/presentation/d/1iNX_35sWUakee2q3zMFPyHE4IV2nC3lkCK_H6Y2qK84/edit#slide=id.p">“LLVM for database and AI systems”</a><a href="https://www.slideshare.net/AndrewLamb32/20220623-apache-arrow-and-datafusion-changing-the-game-for-implementing-database-systemspdf">(alternate link)</a> with announcements such as the <a href="https://engineering.fb.com/2022/08/31/open-source/velox/">release of FaceBook’s Velox</a> engine, the major investments in <a href="https://arrow.apache.org/docs/cpp/streaming_execution.html">Acero</a> as well as the continued popularity of <a href="https://calcite.apache.org/">Apache Calcite</a> and other similar technologies.</p>

<p>While Velox and Acero focus on execution engines, DataFusion provides the entire suite of components needed to build most analytic systems, including a SQL frontend, a dataframe API, and  extension points for just about everything. Some <a href="https://github.com/apache/arrow-datafusion#known-uses">DataFusion users</a> use a subset of the features such as the frontend (e.g. <a href="https://dask-sql.readthedocs.io/en/latest/">dask-sql</a>) or the execution engine, (e.g.  <a href="https://github.com/blaze-init/blaze">Blaze</a>), and some use many different components to build both SQL based and customized DSL based systems such as <a href="https://github.com/influxdata/influxdb_iox/pulls">InfluxDB IOx</a> and <a href="https://github.com/vegafusion/vegafusion">VegaFusion</a>.</p>

<p>One of DataFusion’s advantages is its implementation in <a href="https://www.rust-lang.org/">Rust</a> and thus its easy integration with the broader Rust ecosystem. Rust continues to be a major source of benefit, from the <a href="https://www.influxdata.com/blog/using-rustlangs-async-tokio-runtime-for-cpu-bound-tasks/">ease of parallelization with the high quality and standardized <code class="language-plaintext highlighter-rouge">async</code> ecosystem</a> , as well as its modern dependency management system and wonderful performance. <!-- I wonder if we should link to clickbench?? -->
<!--While we haven’t invested in the benchmarking ratings game datafusion continues to be quite speedy (todo quantity this, with some evidence) – maybe clickbench?--></p>

<!--
Maybe we can do this un a future post
# DataFusion in Action

While DataFusion really shines as an embeddable query engine, if you want to try it out and get a feel for its power, you can use the basic[`datafusion-cli`](https://docs.rs/datafusion-cli/13.0.0/datafusion_cli/) tool to get a sense for what is possible to add in your application

(TODO example here of using datafusion-cli to query from local parquet files on disk)

TODO: also mention you can use the same thing to query data from S3
-->

<h1 id="summary">Summary</h1>

<p>We have increased the frequency of DataFusion releases to monthly instead of quarterly. This
makes it easier for the increasing number of projects that now depend on DataFusion.</p>

<p>We have also completed the “graduation” of <a href="https://github.com/apache/arrow-ballista">Ballista to its own top-level arrow-ballista repository</a>
which decouples the two projects and allows each project to move even faster.</p>

<p>Along with numerous other bug fixes and smaller improvements, here are some of the major advances:</p>

<h1 id="improved-support-for-cloud-object-stores">Improved Support for Cloud Object Stores</h1>

<p>DataFusion now supports many major cloud object stores (Amazon S3, Azure Blob Storage, and Google Cloud Storage) “out of the box” via the <a href="https://crates.io/crates/object_store">object_store</a> crate. Using this integration, DataFusion optimizes reading parquet files by reading only the parts of the files that are needed.</p>

<h2 id="advanced-sql">Advanced SQL</h2>

<p>DataFusion now supports correlated subqueries, by rewriting them as joins. See the <a href="https://arrow.apache.org/datafusion/user-guide/sql/subqueries.html">Subquery</a> page in the User Guide for more information.</p>

<p>In addition to numerous other small improvements, the following SQL features are now supported:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ROWS</code>, <code class="language-plaintext highlighter-rouge">RANGE</code>, <code class="language-plaintext highlighter-rouge">PRECEDING</code> and <code class="language-plaintext highlighter-rouge">FOLLOWING</code> in <code class="language-plaintext highlighter-rouge">OVER</code> clauses <a href="https://github.com/apache/arrow-datafusion/issues/3570">#3570</a></li>
  <li><code class="language-plaintext highlighter-rouge">ROLLUP</code> and <code class="language-plaintext highlighter-rouge">CUBE</code> grouping set expressions  <a href="https://github.com/apache/arrow-datafusion/issues/2446">#2446</a></li>
  <li><code class="language-plaintext highlighter-rouge">SUM DISTINCT</code> aggregate support  <a href="https://github.com/apache/arrow-datafusion/issues/2405">#2405</a></li>
  <li><code class="language-plaintext highlighter-rouge">IN</code> and <code class="language-plaintext highlighter-rouge">NOT IN</code> Subqueries by rewriting them to <code class="language-plaintext highlighter-rouge">SEMI</code> / <code class="language-plaintext highlighter-rouge">ANTI</code> <a href="https://github.com/apache/arrow-datafusion/issues/2885">#2421</a></li>
  <li>Non equality predicates in  <code class="language-plaintext highlighter-rouge">ON</code> clause of  <code class="language-plaintext highlighter-rouge">LEFT</code>, <code class="language-plaintext highlighter-rouge">RIGHT, </code>and <code class="language-plaintext highlighter-rouge">FULL</code> joins <a href="https://github.com/apache/arrow-datafusion/issues/2591">#2591</a></li>
  <li>Exact <code class="language-plaintext highlighter-rouge">MEDIAN</code> <a href="https://github.com/apache/arrow-datafusion/issues/3009">#3009</a></li>
  <li><code class="language-plaintext highlighter-rouge">GROUPING SETS</code>/<code class="language-plaintext highlighter-rouge">CUBE</code>/<code class="language-plaintext highlighter-rouge">ROLLUP</code> <a href="https://github.com/apache/arrow-datafusion/issues/2716">#2716</a></li>
</ul>

<h1 id="more-ddl-support">More DDL Support</h1>

<p>Just as it is important to query, it is also important to give users the ability to define their data sources. We have added:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">CREATE VIEW</code> <a href="https://github.com/apache/arrow-datafusion/issues/2279">#2279</a></li>
  <li><code class="language-plaintext highlighter-rouge">DESCRIBE &lt;table&gt;</code> <a href="https://github.com/apache/arrow-datafusion/issues/2642">#2642</a></li>
  <li>Custom / Dynamic table provider factories <a href="https://github.com/apache/arrow-datafusion/issues/3311">#3311</a></li>
  <li><code class="language-plaintext highlighter-rouge">SHOW CREATE TABLE</code> for support for views <a href="https://github.com/apache/arrow-datafusion/issues/2830">#2830</a></li>
</ul>

<h1 id="faster-execution">Faster Execution</h1>
<p>Performance is always an important goal for DataFusion, and there are a number of significant new optimizations such as</p>

<ul>
  <li>Optimizations of TopK (queries with a <code class="language-plaintext highlighter-rouge">LIMIT</code> or <code class="language-plaintext highlighter-rouge">OFFSET</code> clause):  <a href="https://github.com/apache/arrow-datafusion/issues/3527">#3527</a>, <a href="https://github.com/apache/arrow-datafusion/issues/2521">#2521</a></li>
  <li>Reduce <code class="language-plaintext highlighter-rouge">left</code>/<code class="language-plaintext highlighter-rouge">right</code>/<code class="language-plaintext highlighter-rouge">full</code> joins to <code class="language-plaintext highlighter-rouge">inner</code> join <a href="https://github.com/apache/arrow-datafusion/issues/2750">#2750</a></li>
  <li>Convert  cross joins to inner joins when possible <a href="https://github.com/apache/arrow-datafusion/issues/3482">#3482</a></li>
  <li>Sort preserving <code class="language-plaintext highlighter-rouge">SortMergeJoin</code> <a href="https://github.com/apache/arrow-datafusion/issues/2699">#2699</a></li>
  <li>Improvements in group by and sort performance <a href="https://github.com/apache/arrow-datafusion/issues/2375">#2375</a></li>
  <li>Adaptive <code class="language-plaintext highlighter-rouge">regex_replace</code> implementation <a href="https://github.com/apache/arrow-datafusion/issues/3518">#3518</a></li>
</ul>

<h1 id="optimizer-enhancements">Optimizer Enhancements</h1>
<p>Internally the optimizer has been significantly enhanced as well.</p>

<ul>
  <li>Casting / coercion now happens during logical planning <a href="https://github.com/apache/arrow-datafusion/issues/3396">#3185</a> <a href="https://github.com/apache/arrow-datafusion/issues/3636">#3636</a></li>
  <li>More sophisticated expression analysis and simplification is available</li>
</ul>

<h1 id="parquet">Parquet</h1>
<ul>
  <li>The parquet reader can now read directly from parquet files on remote object storage <a href="https://github.com/apache/arrow-datafusion/issues/2677">#2489</a> <a href="https://github.com/apache/arrow-datafusion/issues/3051">#3051</a></li>
  <li>Experimental support for “predicate pushdown” with late materialization after filtering during the scan (another blog post on this topic is coming soon).</li>
  <li>Support reading directly from AWS S3 and other object stores via <code class="language-plaintext highlighter-rouge">datafusion-cli </code> <a href="https://github.com/apache/arrow-datafusion/issues/3631">#3631</a></li>
</ul>

<h1 id="datatype-support">DataType Support</h1>
<ul>
  <li>Support for <code class="language-plaintext highlighter-rouge">TimestampTz</code> <a href="https://github.com/apache/arrow-datafusion/issues/3660">#3660</a></li>
  <li>Expanded support for the <code class="language-plaintext highlighter-rouge">Decimal</code> type, including  <code class="language-plaintext highlighter-rouge">IN</code> list and better built in coercion.</li>
  <li>Expanded support for date/time manipulation such as  <code class="language-plaintext highlighter-rouge">date_bin</code> built-in function , timestamp <code class="language-plaintext highlighter-rouge">+/-</code> interval, <code class="language-plaintext highlighter-rouge">TIME</code> literal values <a href="https://github.com/apache/arrow-datafusion/issues/3010">#3010</a>, <a href="https://github.com/apache/arrow-datafusion/issues/3110">#3110</a>, <a href="https://github.com/apache/arrow-datafusion/issues/3034">#3034</a></li>
  <li>Binary operations (<code class="language-plaintext highlighter-rouge">AND</code>, <code class="language-plaintext highlighter-rouge">XOR</code>, etc):  <a href="https://github.com/apache/arrow-datafusion/issues/1619">#3037</a> <a href="https://github.com/apache/arrow-datafusion/issues/3430">#3420</a></li>
  <li><code class="language-plaintext highlighter-rouge">IS TRUE/FALSE</code> and <code class="language-plaintext highlighter-rouge">IS [NOT] UNKNOWN</code> <a href="https://github.com/apache/arrow-datafusion/issues/3235">#3235</a>, <a href="https://github.com/apache/arrow-datafusion/issues/3246">#3246</a></li>
</ul>

<h2 id="upcoming-work">Upcoming Work</h2>
<p>With the community growing and code accelerating, there is so much great stuff on the horizon. Some features we expect to land in the next few months:</p>

<ul>
  <li><a href="https://github.com/apache/arrow-datafusion/issues/3462">Complete Parquet Pushdown</a></li>
  <li><a href="https://github.com/apache/arrow-datafusion/issues/3148">Additional date/time support</a></li>
  <li>Cost models, Nested Join Optimizations, analysis framework <a href="https://github.com/apache/arrow-datafusion/issues/128">#128</a>, <a href="https://github.com/apache/arrow-datafusion/issues/3843">#3843</a>, <a href="https://github.com/apache/arrow-datafusion/issues/3845">#3845</a></li>
</ul>

<h1 id="community-growth">Community Growth</h1>

<p>The DataFusion 9.0.0 and 13.0.0 releases consists of 433 PRs from 64 distinct contributors. This does not count all the work that goes into our dependencies such as <a href="https://crates.io/crates/arrow">arrow</a>,  <a href="https://crates.io/crates/parquet">parquet</a>, and <a href="https://crates.io/crates/object_store">object_store</a>, that much of the same community helps nurture.</p>

<!--
$ git log --pretty=oneline 9.0.0..13.0.0 . | wc -l
433

$ git shortlog -sn 9.0.0..13.0.0 . | wc -l
65
-->

<h1 id="how-to-get-involved">How to Get Involved</h1>

<p>Kudos to everyone in the community who contributed ideas, discussions, bug reports, documentation and code. It is exciting to be building something so cool together!</p>

<p>If you are interested in contributing to DataFusion, we would love to
have you join us on our journey to create the most advanced open
source query engine. You can try out DataFusion on some of your own
data and projects and let us know how it goes or contribute a PR with
documentation, tests or code. A list of open issues suitable for
beginners is
<a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a>.</p>

<p>Check out our <a href="https://arrow.apache.org/datafusion/community/communication.html">Communication Doc</a> on more
ways to engage with the community.</p>

<h2 id="appendix-contributor-shoutout">Appendix: Contributor Shoutout</h2>

<p>To give a sense of the number of people who contribute to this project regularly, we present for your consideration the following list derived from <code class="language-plaintext highlighter-rouge">git shortlog -sn 9.0.0..13.0.0 .</code> Thank you all again!</p>

<!-- Note: combined kmitchener and Kirk Mitchener -->

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    87	Andy Grove
    71	Andrew Lamb
    29	Kun Liu
    29	Kirk Mitchener
    17	Wei-Ting Kuo
    14	Yang Jiang
    12	Raphael Taylor-Davies
    11	Batuhan Taskaya
    10	Brent Gardner
    10	Remzi Yang
    10	comphead
    10	xudong.w
     8	AssHero
     7	Ruihang Xia
     6	Dan Harris
     6	Daniël Heres
     6	Ian Alexander Joiner
     6	Mike Roberts
     6	askoa
     4	BaymaxHWY
     4	gorkem
     4	jakevin
     3	George Andronchik
     3	Sarah Yurick
     3	Stuart Carnie
     2	Dalton Modlin
     2	Dmitry Patsura
     2	JasonLi
     2	Jon Mease
     2	Marco Neumann
     2	yahoNanJing
     1	Adilet Sarsembayev
     1	Ayush Dattagupta
     1	Dezhi Wu
     1	Dhamotharan Sritharan
     1	Eduard Karacharov
     1	Francis Du
     1	Harbour Zheng
     1	Ismaël Mejía
     1	Jack Klamer
     1	Jeremy Dyer
     1	Jiayu Liu
     1	Kamil Konior
     1	Liang-Chi Hsieh
     1	Martin Grigorov
     1	Matthijs Brobbel
     1	Mehmet Ozan Kabak
     1	Metehan Yıldırım
     1	Morgan Cassels
     1	Nitish Tiwari
     1	Renjie Liu
     1	Rito Takeuchi
     1	Robert Pack
     1	Thomas Cameron
     1	Vrishabh
     1	Xin Hao
     1	Yijie Shen
     1	byteink
     1	kamille
     1	mateuszkj
     1	nvartolomei
     1	yourenawo
     1	Özgür Akkurt
</code></pre></div></div>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[Introduction Apache Arrow DataFusion 13.0.0 is released, and this blog contains an update on the project for the 5 months since our last update in May 2022. DataFusion is an extensible and embeddable query engine, written in Rust used to create modern, fast and efficient data pipelines, ETL processes, and database systems. You may want to check out DataFusion to extend your Rust project to: Support SQL support Support DataFrame API Support a Domain Specific Query Language Easily and quickly read and process Parquet, JSON, Avro or CSV data. Read from remote object stores such as AWS S3, Azure Blob Storage, GCP. Even though DataFusion is 4 years “young,” it has seen significant community growth in the last few months and the momentum continues to accelerate. Background DataFusion is used as the engine in many open source and commercial projects and was one of the early open source projects to provide this capability. 2022 has validated our belief in the need for such a “LLVM for database and AI systems”(alternate link) with announcements such as the release of FaceBook’s Velox engine, the major investments in Acero as well as the continued popularity of Apache Calcite and other similar technologies. While Velox and Acero focus on execution engines, DataFusion provides the entire suite of components needed to build most analytic systems, including a SQL frontend, a dataframe API, and extension points for just about everything. Some DataFusion users use a subset of the features such as the frontend (e.g. dask-sql) or the execution engine, (e.g. Blaze), and some use many different components to build both SQL based and customized DSL based systems such as InfluxDB IOx and VegaFusion. One of DataFusion’s advantages is its implementation in Rust and thus its easy integration with the broader Rust ecosystem. Rust continues to be a major source of benefit, from the ease of parallelization with the high quality and standardized async ecosystem , as well as its modern dependency management system and wonderful performance. Summary We have increased the frequency of DataFusion releases to monthly instead of quarterly. This makes it easier for the increasing number of projects that now depend on DataFusion. We have also completed the “graduation” of Ballista to its own top-level arrow-ballista repository which decouples the two projects and allows each project to move even faster. Along with numerous other bug fixes and smaller improvements, here are some of the major advances: Improved Support for Cloud Object Stores DataFusion now supports many major cloud object stores (Amazon S3, Azure Blob Storage, and Google Cloud Storage) “out of the box” via the object_store crate. Using this integration, DataFusion optimizes reading parquet files by reading only the parts of the files that are needed. Advanced SQL DataFusion now supports correlated subqueries, by rewriting them as joins. See the Subquery page in the User Guide for more information. In addition to numerous other small improvements, the following SQL features are now supported: ROWS, RANGE, PRECEDING and FOLLOWING in OVER clauses #3570 ROLLUP and CUBE grouping set expressions #2446 SUM DISTINCT aggregate support #2405 IN and NOT IN Subqueries by rewriting them to SEMI / ANTI #2421 Non equality predicates in ON clause of LEFT, RIGHT, and FULL joins #2591 Exact MEDIAN #3009 GROUPING SETS/CUBE/ROLLUP #2716 More DDL Support Just as it is important to query, it is also important to give users the ability to define their data sources. We have added: CREATE VIEW #2279 DESCRIBE &lt;table&gt; #2642 Custom / Dynamic table provider factories #3311 SHOW CREATE TABLE for support for views #2830 Faster Execution Performance is always an important goal for DataFusion, and there are a number of significant new optimizations such as Optimizations of TopK (queries with a LIMIT or OFFSET clause): #3527, #2521 Reduce left/right/full joins to inner join #2750 Convert cross joins to inner joins when possible #3482 Sort preserving SortMergeJoin #2699 Improvements in group by and sort performance #2375 Adaptive regex_replace implementation #3518 Optimizer Enhancements Internally the optimizer has been significantly enhanced as well. Casting / coercion now happens during logical planning #3185 #3636 More sophisticated expression analysis and simplification is available Parquet The parquet reader can now read directly from parquet files on remote object storage #2489 #3051 Experimental support for “predicate pushdown” with late materialization after filtering during the scan (another blog post on this topic is coming soon). Support reading directly from AWS S3 and other object stores via datafusion-cli #3631 DataType Support Support for TimestampTz #3660 Expanded support for the Decimal type, including IN list and better built in coercion. Expanded support for date/time manipulation such as date_bin built-in function , timestamp +/- interval, TIME literal values #3010, #3110, #3034 Binary operations (AND, XOR, etc): #3037 #3420 IS TRUE/FALSE and IS [NOT] UNKNOWN #3235, #3246 Upcoming Work With the community growing and code accelerating, there is so much great stuff on the horizon. Some features we expect to land in the next few months: Complete Parquet Pushdown Additional date/time support Cost models, Nested Join Optimizations, analysis framework #128, #3843, #3845 Community Growth The DataFusion 9.0.0 and 13.0.0 releases consists of 433 PRs from 64 distinct contributors. This does not count all the work that goes into our dependencies such as arrow, parquet, and object_store, that much of the same community helps nurture. How to Get Involved Kudos to everyone in the community who contributed ideas, discussions, bug reports, documentation and code. It is exciting to be building something so cool together! If you are interested in contributing to DataFusion, we would love to have you join us on our journey to create the most advanced open source query engine. You can try out DataFusion on some of your own data and projects and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable for beginners is here. Check out our Communication Doc on more ways to engage with the community. Appendix: Contributor Shoutout To give a sense of the number of people who contribute to this project regularly, we present for your consideration the following list derived from git shortlog -sn 9.0.0..13.0.0 . Thank you all again! 87 Andy Grove 71 Andrew Lamb 29 Kun Liu 29 Kirk Mitchener 17 Wei-Ting Kuo 14 Yang Jiang 12 Raphael Taylor-Davies 11 Batuhan Taskaya 10 Brent Gardner 10 Remzi Yang 10 comphead 10 xudong.w 8 AssHero 7 Ruihang Xia 6 Dan Harris 6 Daniël Heres 6 Ian Alexander Joiner 6 Mike Roberts 6 askoa 4 BaymaxHWY 4 gorkem 4 jakevin 3 George Andronchik 3 Sarah Yurick 3 Stuart Carnie 2 Dalton Modlin 2 Dmitry Patsura 2 JasonLi 2 Jon Mease 2 Marco Neumann 2 yahoNanJing 1 Adilet Sarsembayev 1 Ayush Dattagupta 1 Dezhi Wu 1 Dhamotharan Sritharan 1 Eduard Karacharov 1 Francis Du 1 Harbour Zheng 1 Ismaël Mejía 1 Jack Klamer 1 Jeremy Dyer 1 Jiayu Liu 1 Kamil Konior 1 Liang-Chi Hsieh 1 Martin Grigorov 1 Matthijs Brobbel 1 Mehmet Ozan Kabak 1 Metehan Yıldırım 1 Morgan Cassels 1 Nitish Tiwari 1 Renjie Liu 1 Rito Takeuchi 1 Robert Pack 1 Thomas Cameron 1 Vrishabh 1 Xin Hao 1 Yijie Shen 1 byteink 1 kamille 1 mateuszkj 1 nvartolomei 1 yourenawo 1 Özgür Akkurt]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Arrow and Parquet Part 3: Arbitrary Nesting with Lists of Structs and Structs of Lists</title><link href="https://arrow.apache.org/blog/2022/10/17/arrow-parquet-encoding-part-3/" rel="alternate" type="text/html" title="Arrow and Parquet Part 3: Arbitrary Nesting with Lists of Structs and Structs of Lists" /><published>2022-10-17T00:00:00-04:00</published><updated>2022-10-17T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/10/17/arrow-parquet-encoding-part-3</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/10/17/arrow-parquet-encoding-part-3/"><![CDATA[<!--

-->

<h2 id="introduction">Introduction</h2>

<p>This is the third of a three part series exploring how projects such as <a href="https://github.com/apache/arrow-rs">Rust Apache Arrow</a> support conversion between <a href="https://arrow.apache.org/">Apache Arrow</a> for in memory processing and <a href="https://parquet.apache.org/">Apache Parquet</a> for efficient storage. <a href="https://arrow.apache.org/">Apache Arrow</a> is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. <a href="https://parquet.apache.org/">Apache Parquet</a> is an open, column-oriented data file format designed for very efficient data encoding and retrieval.</p>

<p><a href="https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/">Arrow and Parquet Part 1: Primitive Types and Nullability</a> covered the basics of primitive types.  <a href="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/">Arrow and Parquet Part 2: Nested and Hierarchical Data using Structs and Lists</a> covered the <code class="language-plaintext highlighter-rouge">Struct</code> and <code class="language-plaintext highlighter-rouge">List</code> types. This post builds on this foundation to show how both formats combine these to support arbitrary nesting.</p>

<p>Some libraries, such as Rust <a href="https://crates.io/crates/parquet">parquet</a> implementation, offer complete support for such combinations, and users of those libraries do not need to worry about these details except to satisfy their own curiosity. Other libraries may not handle some corner cases and this post gives some flavor of why it is so complicated to do so.</p>

<h1 id="structs-with-lists">Structs with Lists</h1>
<p>Consider the following three json documents</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>                     <span class="c1"># &lt;-- First record
</span>  <span class="s">"a"</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span>           <span class="c1"># &lt;-- top-level field a containing list of integers
</span>  <span class="s">"b"</span><span class="p">:</span> <span class="p">[</span>              <span class="c1"># &lt;-- top-level field b containing list of structures
</span>    <span class="p">{</span>                 <span class="c1"># &lt;-- list element of b containing two field b1 and b2
</span>      <span class="s">"b1"</span><span class="p">:</span> <span class="mi">1</span>         <span class="c1"># &lt;-- b1 is always provided (non nullable)
</span>    <span class="p">},</span>
    <span class="p">{</span>
      <span class="s">"b1"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
      <span class="s">"b2"</span><span class="p">:</span> <span class="p">[</span>         <span class="c1"># &lt;-- b2 contains list of integers
</span>        <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span>          <span class="c1"># &lt;-- list elements of b.b2 always provided (non nullable)
</span>      <span class="p">]</span>
    <span class="p">}</span>
  <span class="p">]</span>
<span class="p">}</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
  <span class="s">"b"</span><span class="p">:</span> <span class="p">[</span>              <span class="c1"># &lt;-- b is always provided (non nullable)
</span>    <span class="p">{</span>
      <span class="s">"b1"</span><span class="p">:</span> <span class="mi">2</span>
    <span class="p">},</span>
  <span class="p">]</span>
<span class="p">}</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
  <span class="s">"a"</span><span class="p">:</span> <span class="p">[</span><span class="n">null</span><span class="p">,</span> <span class="n">null</span><span class="p">],</span>  <span class="c1"># &lt;-- list elements of a are nullable
</span>  <span class="s">"b"</span><span class="p">:</span> <span class="p">[</span><span class="n">null</span><span class="p">]</span>         <span class="c1"># &lt;-- list elements of b are nullable
</span><span class="p">}</span>
</code></pre></div></div>

<p>Documents of this format could be stored in this Arrow schema</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"a"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">List</span><span class="p">(</span>
  <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"element"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Int32</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"b"</span><span class="p">),</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">false</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">List</span><span class="p">(</span>
  <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"element"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Struct</span><span class="p">[</span>
    <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"b1"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">false</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Int32</span><span class="p">),</span>
    <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"b2"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">List</span><span class="p">(</span>
      <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"element"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">false</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Int32</span><span class="p">)</span>
    <span class="p">))</span>
  <span class="p">])</span>
<span class="p">))</span>
</code></pre></div></div>

<p>As explained previously, Arrow chooses to represent this in a hierarchical fashion. <code class="language-plaintext highlighter-rouge">StructArray</code>s are stored as child arrays that contain each field of the struct.  <code class="language-plaintext highlighter-rouge">ListArray</code>s are stored as lists of monotonically increasing integers called offsets, with values stored in a single child array. Each consecutive pair of elements in the offset array identifies a slice of the child array for that array index.</p>

<p>The Arrow encoding of the example would be:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐
                     ┌──────────────────┐
│ ┌─────┐   ┌─────┐  │ ┌─────┐   ┌─────┐│ │
  │  1  │   │  0  │  │ │  1  │   │  1  ││
│ ├─────┤   ├─────┤  │ ├─────┤   ├─────┤│ │
  │  0  │   │  1  │  │ │  0  │   │ ??  ││
│ ├─────┤   ├─────┤  │ ├─────┤   ├─────┤│ │
  │  1  │   │  1  │  │ │  0  │   │ ??  ││
│ └─────┘   ├─────┤  │ └─────┘   └─────┘│ │
            │  3  │  │ Validity   Values│
│ Validity  └─────┘  │                  │ │
                     │ child[0]         │
│ "a"       Offsets  │ PrimitiveArray   │ │
  ListArray          └──────────────────┘
└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘

┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
           ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ │
│                    ┌──────────┐ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
  ┌─────┐  │ ┌─────┐ │ ┌─────┐  │   ┌─────┐ ┌─────┐ ┌──────────┐ │ │ │
│ │  0  │    │  1  │ │ │  1  │  │ │ │  0  │ │  0  │ │ ┌─────┐  │
  ├─────┤  │ ├─────┤ │ ├─────┤  │   ├─────┤ ├─────┤ │ │  3  │  │ │ │ │
│ │  2  │    │  1  │ │ │  1  │  │ │ │  1  │ │  0  │ │ ├─────┤  │
  ├─────┤  │ ├─────┤ │ ├─────┤  │   ├─────┤ ├─────┤ │ │  4  │  │ │ │ │
│ │  3  │    │  1  │ │ │  2  │  │ │ │  0  │ │  2  │ │ └─────┘  │
  ├─────┤  │ ├─────┤ │ ├─────┤  │   ├─────┤ ├─────┤ │          │ │ │ │
│ │  4  │    │  0  │ │ │ ??  │  │ │ │ ??  │ │  2  │ │  Values  │
  └─────┘  │ └─────┘ │ └─────┘  │   └─────┘ ├─────┤ │          │ │ │ │
│                    │          │ │         │  2  │ │          │
  Offsets  │ Validity│ Values   │           └─────┘ │          │ │ │ │
│                    │          │ │Validity         │ child[0] │
           │         │ "b1"     │           Offsets │ Primitive│ │ │ │
│                    │ Primitive│ │ "b2"            │ Array    │
           │         │ Array    │   ListArray       └──────────┘ │ │ │
│                    └──────────┘ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
           │ "element"                                             │ │
│            StructArray
  "b"      └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ │
│ ListArray
 ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘
</code></pre></div></div>

<p>Documents of this format could be stored in this Parquet schema</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">message</span> <span class="n">schema</span> <span class="p">{</span>
  <span class="n">optional</span> <span class="n">group</span> <span class="n">a</span> <span class="p">(</span><span class="n">LIST</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">repeated</span> <span class="n">group</span> <span class="nb">list</span> <span class="p">{</span>
      <span class="n">optional</span> <span class="n">int32</span> <span class="n">element</span><span class="p">;</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="n">required</span> <span class="n">group</span> <span class="n">b</span> <span class="p">(</span><span class="n">LIST</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">repeated</span> <span class="n">group</span> <span class="nb">list</span> <span class="p">{</span>
      <span class="n">optional</span> <span class="n">group</span> <span class="n">element</span> <span class="p">{</span>
        <span class="n">required</span> <span class="n">int32</span> <span class="n">b1</span><span class="p">;</span>
        <span class="n">optional</span> <span class="n">group</span> <span class="n">b2</span> <span class="p">(</span><span class="n">LIST</span><span class="p">)</span> <span class="p">{</span>
          <span class="n">repeated</span> <span class="n">group</span> <span class="nb">list</span> <span class="p">{</span>
            <span class="n">required</span> <span class="n">int32</span> <span class="n">element</span><span class="p">;</span>
          <span class="p">}</span>
        <span class="p">}</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>As explained in our previous posts, Parquet uses repetition levels and definition levels to encode nested structures and nullability.</p>

<p>Definition and repetition levels is a non trivial topic. For more detail, you can read the <a href="https://research.google/pubs/pub36632/">Google Dremel Paper</a> which offers an academic description of the algorithm. You can also explore this <a href="https://gist.github.com/alamb/acd653c49e318ff70672b61325ba3443">gist</a> to see Rust <a href="https://crates.io/crates/parquet">parquet</a> code which generates the example below.</p>

<p>The Parquet encoding of the example would be:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌───────────────────────────────┐ ┌────────────────────────────────┐
│ ┌─────┐    ┌─────┐    ┌─────┐ │ │  ┌─────┐    ┌─────┐    ┌─────┐ │
│ │  3  │    │  0  │    │  1  │ │ │  │  2  │    │  0  │    │  1  │ │
│ ├─────┤    ├─────┤    └─────┘ │ │  ├─────┤    ├─────┤    ├─────┤ │
│ │  0  │    │  0  │            │ │  │  2  │    │  1  │    │  1  │ │
│ ├─────┤    ├─────┤      Data  │ │  ├─────┤    ├─────┤    ├─────┤ │
│ │  2  │    │  0  │            │ │  │  2  │    │  0  │    │  2  │ │
│ ├─────┤    ├─────┤            │ │  ├─────┤    ├─────┤    └─────┘ │
│ │  2  │    │  1  │            │ │  │  1  │    │  0  │            │
│ └─────┘    └─────┘            │ │  └─────┘    └─────┘     Data   │
│                               │ │                                │
│Definition Repetition          │ │ Definition Repetition          │
│  Levels     Levels            │ │   Levels     Levels            │
│                               │ │                                │
│ "a"                           │ │  "b.b1"                        │
└───────────────────────────────┘ └────────────────────────────────┘

┌───────────────────────────────┐
│  ┌─────┐    ┌─────┐    ┌─────┐│
│  │  2  │    │  0  │    │  3  ││
│  ├─────┤    ├─────┤    ├─────┤│
│  │  4  │    │  1  │    │  4  ││
│  ├─────┤    ├─────┤    └─────┘│
│  │  4  │    │  2  │           │
│  ├─────┤    ├─────┤           │
│  │  2  │    │  0  │           │
│  ├─────┤    ├─────┤     Data  │
│  │  1  │    │  0  │           │
│  └─────┘    └─────┘           │
│Definition  Repetition         │
│  Levels      Levels           │
│                               │
│  "b.b2"                       │
└───────────────────────────────┘
</code></pre></div></div>

<h2 id="additional-complications">Additional Complications</h2>

<p>This series of posts has necessarily glossed over a number of details that further complicate actual implementations:</p>

<ul>
  <li>A <code class="language-plaintext highlighter-rouge">ListArray</code> may contain a non-empty offset range that is masked by a validity mask</li>
  <li>Reading a given number of rows from a nullable field requires reading the definition levels and determining the number of values to read based on the number of nulls present</li>
  <li>Reading a given number of rows from a repeated field requires reading the repetition levels and detecting a new row based on a repetition level of 0</li>
  <li>A Parquet file may contain multiple row groups, each containing multiple column chunks</li>
  <li>A column chunk may contain multiple pages, and there is no relationship between pages across columns</li>
  <li>Parquet has alternative schema for representing lists with varying degrees of nullability</li>
  <li>And more…</li>
</ul>

<h2 id="summary">Summary</h2>
<p>Both Parquet and Arrow are columnar formats and support nested structs and lists, however, the way they represent such nesting differs significantly and conversion between the two formats is complex.</p>

<p>Fortunately, with the Rust <a href="https://crates.io/crates/parquet">parquet</a> implementation, reading and writing nested data in Arrow, in Parquet or converting between the two is as simple as reading unnested data. The library handles all the complex record shredding and reconstruction automatically. With this and other exciting features, such as support for <a href="https://docs.rs/parquet/22.0.0/parquet/arrow/async_reader/index.html">reading asynchronously</a> from <a href="https://docs.rs/object_store">object storage</a>, it is the fastest and most feature complete Rust parquet implementation available. We look forward to seeing what you build with it!</p>]]></content><author><name>tustvold and alamb</name></author><category term="parquet" /><category term="arrow" /><summary type="html"><![CDATA[Introduction This is the third of a three part series exploring how projects such as Rust Apache Arrow support conversion between Apache Arrow for in memory processing and Apache Parquet for efficient storage. Apache Arrow is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. Apache Parquet is an open, column-oriented data file format designed for very efficient data encoding and retrieval. Arrow and Parquet Part 1: Primitive Types and Nullability covered the basics of primitive types. Arrow and Parquet Part 2: Nested and Hierarchical Data using Structs and Lists covered the Struct and List types. This post builds on this foundation to show how both formats combine these to support arbitrary nesting. Some libraries, such as Rust parquet implementation, offer complete support for such combinations, and users of those libraries do not need to worry about these details except to satisfy their own curiosity. Other libraries may not handle some corner cases and this post gives some flavor of why it is so complicated to do so. Structs with Lists Consider the following three json documents { # &lt;-- First record "a": [1], # &lt;-- top-level field a containing list of integers "b": [ # &lt;-- top-level field b containing list of structures { # &lt;-- list element of b containing two field b1 and b2 "b1": 1 # &lt;-- b1 is always provided (non nullable) }, { "b1": 1, "b2": [ # &lt;-- b2 contains list of integers 3, 4 # &lt;-- list elements of b.b2 always provided (non nullable) ] } ] } { "b": [ # &lt;-- b is always provided (non nullable) { "b1": 2 }, ] } { "a": [null, null], # &lt;-- list elements of a are nullable "b": [null] # &lt;-- list elements of b are nullable } Documents of this format could be stored in this Arrow schema Field(name: "a", nullable: true, datatype: List( Field(name: "element", nullable: true, datatype: Int32), ) Field(name: "b"), nullable: false, datatype: List( Field(name: "element", nullable: true, datatype: Struct[ Field(name: "b1", nullable: false, datatype: Int32), Field(name: "b2", nullable: true, datatype: List( Field(name: "element", nullable: false, datatype: Int32) )) ]) )) As explained previously, Arrow chooses to represent this in a hierarchical fashion. StructArrays are stored as child arrays that contain each field of the struct. ListArrays are stored as lists of monotonically increasing integers called offsets, with values stored in a single child array. Each consecutive pair of elements in the offset array identifies a slice of the child array for that array index. The Arrow encoding of the example would be: ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┌──────────────────┐ │ ┌─────┐ ┌─────┐ │ ┌─────┐ ┌─────┐│ │ │ 1 │ │ 0 │ │ │ 1 │ │ 1 ││ │ ├─────┤ ├─────┤ │ ├─────┤ ├─────┤│ │ │ 0 │ │ 1 │ │ │ 0 │ │ ?? ││ │ ├─────┤ ├─────┤ │ ├─────┤ ├─────┤│ │ │ 1 │ │ 1 │ │ │ 0 │ │ ?? ││ │ └─────┘ ├─────┤ │ └─────┘ └─────┘│ │ │ 3 │ │ Validity Values│ │ Validity └─────┘ │ │ │ │ child[0] │ │ "a" Offsets │ PrimitiveArray │ │ ListArray └──────────────────┘ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ │ │ ┌──────────┐ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌─────┐ │ ┌─────┐ │ ┌─────┐ │ ┌─────┐ ┌─────┐ ┌──────────┐ │ │ │ │ │ 0 │ │ 1 │ │ │ 1 │ │ │ │ 0 │ │ 0 │ │ ┌─────┐ │ ├─────┤ │ ├─────┤ │ ├─────┤ │ ├─────┤ ├─────┤ │ │ 3 │ │ │ │ │ │ │ 2 │ │ 1 │ │ │ 1 │ │ │ │ 1 │ │ 0 │ │ ├─────┤ │ ├─────┤ │ ├─────┤ │ ├─────┤ │ ├─────┤ ├─────┤ │ │ 4 │ │ │ │ │ │ │ 3 │ │ 1 │ │ │ 2 │ │ │ │ 0 │ │ 2 │ │ └─────┘ │ ├─────┤ │ ├─────┤ │ ├─────┤ │ ├─────┤ ├─────┤ │ │ │ │ │ │ │ 4 │ │ 0 │ │ │ ?? │ │ │ │ ?? │ │ 2 │ │ Values │ └─────┘ │ └─────┘ │ └─────┘ │ └─────┘ ├─────┤ │ │ │ │ │ │ │ │ │ │ 2 │ │ │ Offsets │ Validity│ Values │ └─────┘ │ │ │ │ │ │ │ │ │Validity │ child[0] │ │ │ "b1" │ Offsets │ Primitive│ │ │ │ │ │ Primitive│ │ "b2" │ Array │ │ │ Array │ ListArray └──────────┘ │ │ │ │ └──────────┘ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ "element" │ │ │ StructArray "b" └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ │ │ ListArray ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ Documents of this format could be stored in this Parquet schema message schema { optional group a (LIST) { repeated group list { optional int32 element; } } required group b (LIST) { repeated group list { optional group element { required int32 b1; optional group b2 (LIST) { repeated group list { required int32 element; } } } } } } As explained in our previous posts, Parquet uses repetition levels and definition levels to encode nested structures and nullability. Definition and repetition levels is a non trivial topic. For more detail, you can read the Google Dremel Paper which offers an academic description of the algorithm. You can also explore this gist to see Rust parquet code which generates the example below. The Parquet encoding of the example would be: ┌───────────────────────────────┐ ┌────────────────────────────────┐ │ ┌─────┐ ┌─────┐ ┌─────┐ │ │ ┌─────┐ ┌─────┐ ┌─────┐ │ │ │ 3 │ │ 0 │ │ 1 │ │ │ │ 2 │ │ 0 │ │ 1 │ │ │ ├─────┤ ├─────┤ └─────┘ │ │ ├─────┤ ├─────┤ ├─────┤ │ │ │ 0 │ │ 0 │ │ │ │ 2 │ │ 1 │ │ 1 │ │ │ ├─────┤ ├─────┤ Data │ │ ├─────┤ ├─────┤ ├─────┤ │ │ │ 2 │ │ 0 │ │ │ │ 2 │ │ 0 │ │ 2 │ │ │ ├─────┤ ├─────┤ │ │ ├─────┤ ├─────┤ └─────┘ │ │ │ 2 │ │ 1 │ │ │ │ 1 │ │ 0 │ │ │ └─────┘ └─────┘ │ │ └─────┘ └─────┘ Data │ │ │ │ │ │Definition Repetition │ │ Definition Repetition │ │ Levels Levels │ │ Levels Levels │ │ │ │ │ │ "a" │ │ "b.b1" │ └───────────────────────────────┘ └────────────────────────────────┘ ┌───────────────────────────────┐ │ ┌─────┐ ┌─────┐ ┌─────┐│ │ │ 2 │ │ 0 │ │ 3 ││ │ ├─────┤ ├─────┤ ├─────┤│ │ │ 4 │ │ 1 │ │ 4 ││ │ ├─────┤ ├─────┤ └─────┘│ │ │ 4 │ │ 2 │ │ │ ├─────┤ ├─────┤ │ │ │ 2 │ │ 0 │ │ │ ├─────┤ ├─────┤ Data │ │ │ 1 │ │ 0 │ │ │ └─────┘ └─────┘ │ │Definition Repetition │ │ Levels Levels │ │ │ │ "b.b2" │ └───────────────────────────────┘ Additional Complications This series of posts has necessarily glossed over a number of details that further complicate actual implementations: A ListArray may contain a non-empty offset range that is masked by a validity mask Reading a given number of rows from a nullable field requires reading the definition levels and determining the number of values to read based on the number of nulls present Reading a given number of rows from a repeated field requires reading the repetition levels and detecting a new row based on a repetition level of 0 A Parquet file may contain multiple row groups, each containing multiple column chunks A column chunk may contain multiple pages, and there is no relationship between pages across columns Parquet has alternative schema for representing lists with varying degrees of nullability And more… Summary Both Parquet and Arrow are columnar formats and support nested structs and lists, however, the way they represent such nesting differs significantly and conversion between the two formats is complex. Fortunately, with the Rust parquet implementation, reading and writing nested data in Arrow, in Parquet or converting between the two is as simple as reading unnested data. The library handles all the complex record shredding and reconstruction automatically. With this and other exciting features, such as support for reading asynchronously from object storage, it is the fastest and most feature complete Rust parquet implementation available. We look forward to seeing what you build with it!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Arrow and Parquet Part 2: Nested and Hierarchical Data using Structs and Lists</title><link href="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/" rel="alternate" type="text/html" title="Arrow and Parquet Part 2: Nested and Hierarchical Data using Structs and Lists" /><published>2022-10-08T00:00:00-04:00</published><updated>2022-10-08T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/"><![CDATA[<!--

-->

<h2 id="introduction">Introduction</h2>

<p>This is the second, in a three part series exploring how projects such as <a href="https://github.com/apache/arrow-rs">Rust Apache Arrow</a> support conversion between <a href="https://arrow.apache.org/">Apache Arrow</a> and <a href="https://parquet.apache.org/">Apache Parquet</a>. The <a href="https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/">first post</a> covered the basics of data storage and validity encoding, and this post will cover the more complex <code class="language-plaintext highlighter-rouge">Struct</code> and <code class="language-plaintext highlighter-rouge">List</code> types.</p>

<p><a href="https://arrow.apache.org/">Apache Arrow</a> is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. <a href="https://parquet.apache.org/">Apache Parquet</a> is an open, column-oriented data file format designed for very efficient data encoding and retrieval.</p>

<h2 id="struct--group-columns">Struct / Group Columns</h2>

<p>Both Parquet and Arrow have the concept of a <em>struct</em> column, which is a column containing one or more other columns in named fields and is analogous to a JSON object.</p>

<p>For example, consider the following three JSON documents</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>              <span class="c1"># &lt;-- First record
</span>  <span class="s">"a"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>      <span class="c1"># &lt;-- the top level fields are a, b, c, and d
</span>  <span class="s">"b"</span><span class="p">:</span> <span class="p">{</span>       <span class="c1"># &lt;-- b is always provided (not nullable)
</span>    <span class="s">"b1"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>   <span class="c1"># &lt;-- b1 and b2 are "nested" fields of "b"
</span>    <span class="s">"b2"</span><span class="p">:</span> <span class="mi">3</span>    <span class="c1"># &lt;-- b2 is always provided (not nullable)
</span>   <span class="p">},</span>
 <span class="s">"d"</span><span class="p">:</span> <span class="p">{</span>
   <span class="s">"d1"</span><span class="p">:</span>  <span class="mi">1</span>    <span class="c1"># &lt;-- d1 is a "nested" field of "d"
</span>  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>              <span class="c1"># &lt;-- Second record
</span>  <span class="s">"a"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
  <span class="s">"b"</span><span class="p">:</span> <span class="p">{</span>
    <span class="s">"b2"</span><span class="p">:</span> <span class="mi">4</span>    <span class="c1"># &lt;-- note "b1" is NULL in this record
</span>  <span class="p">},</span>
  <span class="s">"c"</span><span class="p">:</span> <span class="p">{</span>       <span class="c1"># &lt;-- note "c" was NULL in the first record
</span>    <span class="s">"c1"</span><span class="p">:</span> <span class="mi">6</span>        <span class="n">but</span> <span class="n">when</span> <span class="s">"c"</span> <span class="ow">is</span> <span class="n">provided</span><span class="p">,</span> <span class="n">c1</span> <span class="ow">is</span> <span class="n">also</span>
  <span class="p">},</span>               <span class="n">always</span> <span class="n">provided</span> <span class="p">(</span><span class="ow">not</span> <span class="n">nullable</span><span class="p">)</span>
  <span class="s">"d"</span><span class="p">:</span> <span class="p">{</span>
    <span class="s">"d1"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s">"d2"</span><span class="p">:</span> <span class="mi">1</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>              <span class="c1"># &lt;-- Third record
</span>  <span class="s">"b"</span><span class="p">:</span> <span class="p">{</span>
    <span class="s">"b1"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="s">"b2"</span><span class="p">:</span> <span class="mi">6</span>
  <span class="p">},</span>
  <span class="s">"c"</span><span class="p">:</span> <span class="p">{</span>
    <span class="s">"c1"</span><span class="p">:</span> <span class="mi">7</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>
<p>Documents of this format could be stored in an Arrow <code class="language-plaintext highlighter-rouge">StructArray</code> with this schema</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"a"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Int32</span><span class="p">)</span>
<span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"b"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">false</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Struct</span><span class="p">[</span>
  <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"b1"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Int32</span><span class="p">),</span>
  <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"b2"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">false</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Int32</span><span class="p">)</span>
<span class="p">])</span>
<span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"c"</span><span class="p">),</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Struct</span><span class="p">[</span>
  <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"c1"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">false</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Int32</span><span class="p">)</span>
<span class="p">])</span>
<span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"d"</span><span class="p">),</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Struct</span><span class="p">[</span>
  <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"d1"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">false</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Int32</span><span class="p">)</span>
  <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"d2"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Int32</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<p>Arrow represents each <code class="language-plaintext highlighter-rouge">StructArray</code> hierarchically using a parent child relationship, with separate validity masks on each of the individual nullable arrays</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  ┌───────────────────┐        ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐
  │                   │           ┌─────────────────┐ ┌────────────┐
  │ ┌─────┐   ┌─────┐ │        │  │┌─────┐   ┌─────┐│ │  ┌─────┐   │ │
  │ │  1  │   │  1  │ │           ││  1  │   │  1  ││ │  │  3  │   │
  │ ├─────┤   ├─────┤ │        │  │├─────┤   ├─────┤│ │  ├─────┤   │ │
  │ │  1  │   │  2  │ │           ││  0  │   │ ??  ││ │  │  4  │   │
  │ ├─────┤   ├─────┤ │        │  │├─────┤   ├─────┤│ │  ├─────┤   │ │
  │ │  0  │   │ ??  │ │           ││  1  │   │  5  ││ │  │  6  │   │
  │ └─────┘   └─────┘ │        │  │└─────┘   └─────┘│ │  └─────┘   │ │
  │ Validity   Values │           │Validity   Values│ │   Values   │
  │                   │        │  │                 │ │            │ │
  │ "a"               │           │"b.b1"           │ │  "b.b2"    │
  │ PrimitiveArray    │        │  │PrimitiveArray   │ │  Primitive │ │
  └───────────────────┘           │                 │ │  Array     │
                               │  └─────────────────┘ └────────────┘ │
                                    "b"
                               │    StructArray                      │
                                ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─

┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┌─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
            ┌───────────┐                ┌──────────┐┌─────────────────┐ │
│  ┌─────┐  │ ┌─────┐   │ │ │  ┌─────┐   │┌─────┐   ││ ┌─────┐  ┌─────┐│
   │  0  │  │ │ ??  │   │      │  1  │   ││  1  │   ││ │  0  │  │ ??  ││ │
│  ├─────┤  │ ├─────┤   │ │ │  ├─────┤   │├─────┤   ││ ├─────┤  ├─────┤│
   │  1  │  │ │  6  │   │      │  1  │   ││  2  │   ││ │  1  │  │  1  ││ │
│  ├─────┤  │ ├─────┤   │ │ │  ├─────┤   │├─────┤   ││ ├─────┤  ├─────┤│
   │  1  │  │ │  7  │   │      │  0  │   ││ ??  │   ││ │ ??  │  │ ??  ││ │
│  └─────┘  │ └─────┘   │ │ │  └─────┘   │└─────┘   ││ └─────┘  └─────┘│
   Validity │  Values   │      Validity  │ Values   ││ Validity  Values│ │
│           │           │ │ │            │          ││                 │
            │ "c.c1"    │                │"d.d1"    ││ "d.d2"          │ │
│           │ Primitive │ │ │            │Primitive ││ PrimitiveArray  │
            │ Array     │                │Array     ││                 │ │
│           └───────────┘ │ │            └──────────┘└─────────────────┘
    "c"                         "d"                                      │
│   StructArray           │ │   StructArray
  ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘
</code></pre></div></div>

<p>More technical detail is available in the <a href="https://arrow.apache.org/docs/format/Columnar.html#struct-layout">StructArray format specification</a>.</p>

<h3 id="definition-levels">Definition Levels</h3>
<p>Unlike Arrow, Parquet does not encode validity in a structured fashion, instead only storing definition levels for each of the primitive columns, i.e. those that don’t contain other columns. The definition level of a given element is the depth in the schema at which it is fully defined.</p>

<p>For example consider the case of <code class="language-plaintext highlighter-rouge">d.d2</code>, which contains two nullable levels <code class="language-plaintext highlighter-rouge">d</code> and <code class="language-plaintext highlighter-rouge">d2</code>.</p>

<p>A definition level of <code class="language-plaintext highlighter-rouge">0</code> would imply a null at the level of <code class="language-plaintext highlighter-rouge">d</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
<span class="p">}</span>
</code></pre></div></div>

<p>A definition level of <code class="language-plaintext highlighter-rouge">1</code> would imply a null at the level of <code class="language-plaintext highlighter-rouge">d.d2</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
  <span class="s">"d"</span><span class="p">:</span> <span class="p">{</span> <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>A definition level of <code class="language-plaintext highlighter-rouge">2</code> would imply a defined value for <code class="language-plaintext highlighter-rouge">d.d2</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
  <span class="s">"d"</span><span class="p">:</span> <span class="p">{</span> <span class="s">"d2"</span><span class="p">:</span> <span class="p">..</span> <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Going back to the three JSON documents above, they could be stored in Parquet with this schema</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">message</span> <span class="n">schema</span> <span class="p">{</span>
  <span class="n">optional</span> <span class="n">int32</span> <span class="n">a</span><span class="p">;</span>
  <span class="n">required</span> <span class="n">group</span> <span class="n">b</span> <span class="p">{</span>
    <span class="n">optional</span> <span class="n">int32</span> <span class="n">b1</span><span class="p">;</span>
    <span class="n">required</span> <span class="n">int32</span> <span class="n">b2</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="n">optional</span> <span class="n">group</span> <span class="n">c</span> <span class="p">{</span>
    <span class="n">required</span> <span class="n">int32</span> <span class="n">c1</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="n">optional</span> <span class="n">group</span> <span class="n">d</span> <span class="p">{</span>
    <span class="n">required</span> <span class="n">int32</span> <span class="n">d1</span><span class="p">;</span>
    <span class="n">optional</span> <span class="n">int32</span> <span class="n">d2</span><span class="p">;</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The Parquet encoding of the example would be:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code> ┌────────────────────────┐  ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
 │  ┌─────┐     ┌─────┐   │    ┌──────────────────────┐ ┌───────────┐ │
 │  │  1  │     │  1  │   │  │ │  ┌─────┐    ┌─────┐  │ │  ┌─────┐  │
 │  ├─────┤     ├─────┤   │    │  │  1  │    │  1  │  │ │  │  3  │  │ │
 │  │  1  │     │  2  │   │  │ │  ├─────┤    ├─────┤  │ │  ├─────┤  │
 │  ├─────┤     └─────┘   │    │  │  0  │    │  5  │  │ │  │  4  │  │ │
 │  │  0  │               │  │ │  ├─────┤    └─────┘  │ │  ├─────┤  │
 │  └─────┘               │    │  │  1  │             │ │  │  6  │  │ │
 │                        │  │ │  └─────┘             │ │  └─────┘  │
 │  Definition    Data    │    │                      │ │           │ │
 │    Levels              │  │ │  Definition   Data   │ │   Data    │
 │                        │    │    Levels            │ │           │ │
 │  "a"                   │  │ │                      │ │           │
 └────────────────────────┘    │  "b.b1"              │ │  "b.b2"   │ │
                             │ └──────────────────────┘ └───────────┘
                                  "b"                                 │
                             └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─


┌ ─ ─ ─ ─ ─ ── ─ ─ ─ ─ ─   ┌ ─ ─ ─ ─ ── ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
  ┌────────────────────┐ │   ┌────────────────────┐ ┌──────────────────┐ │
│ │  ┌─────┐   ┌─────┐ │   │ │  ┌─────┐   ┌─────┐ │ │ ┌─────┐  ┌─────┐ │
  │  │  0  │   │  6  │ │ │   │  │  1  │   │  1  │ │ │ │  1  │  │  1  │ │ │
│ │  ├─────┤   ├─────┤ │   │ │  ├─────┤   ├─────┤ │ │ ├─────┤  └─────┘ │
  │  │  1  │   │  7  │ │ │   │  │  1  │   │  2  │ │ │ │  2  │          │ │
│ │  ├─────┤   └─────┘ │   │ │  ├─────┤   └─────┘ │ │ ├─────┤          │
  │  │  1  │           │ │   │  │  0  │           │ │ │  0  │          │ │
│ │  └─────┘           │   │ │  └─────┘           │ │ └─────┘          │
  │                    │ │   │                    │ │                  │ │
│ │  Definition  Data  │   │ │  Definition  Data  │ │ Definition Data  │
  │    Levels          │ │   │    Levels          │ │   Levels         │ │
│ │                    │   │ │                    │ │                  │
  │  "c.1"             │ │   │  "d.1"             │ │  "d.d2"          │ │
│ └────────────────────┘   │ └────────────────────┘ └──────────────────┘
     "c"                 │      "d"                                      │
└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
</code></pre></div></div>

<h2 id="list--repeated-columns">List / Repeated Columns</h2>

<p>Closing out support for nested types are <em>lists</em>, which contain a variable number of other values. For example, the following four documents each have a (nullable) field <code class="language-plaintext highlighter-rouge">a</code> containing a list of integers</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>                     <span class="c1"># &lt;-- First record
</span>  <span class="s">"a"</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span>           <span class="c1"># &lt;-- top-level field a containing list of integers
</span><span class="p">}</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>                     <span class="c1"># &lt;-- "a" is not provided (is null)
</span><span class="p">}</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>                     <span class="c1"># &lt;-- "a" is non-null but empty
</span>  <span class="s">"a"</span><span class="p">:</span> <span class="p">[]</span>
<span class="p">}</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
  <span class="s">"a"</span><span class="p">:</span> <span class="p">[</span><span class="n">null</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>     <span class="c1"># &lt;-- "a" has a null and non-null elements
</span><span class="p">}</span>
</code></pre></div></div>

<p>Documents of this format could be stored in this Arrow schema</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"a"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">List</span><span class="p">(</span>
  <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"element"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Int32</span><span class="p">),</span>
<span class="p">)</span>
</code></pre></div></div>

<p>As before, Arrow chooses to represent this in a hierarchical fashion as a <code class="language-plaintext highlighter-rouge">ListArray</code>. A <code class="language-plaintext highlighter-rouge">ListArray</code> contains a list of monotonically increasing integers called <em>offsets</em>, a validity mask if the list is nullable, and a child array containing the list elements. Each consecutive pair of elements in the offset array identifies a slice of the child array for that index in the ListArray</p>

<p>For example, a list with offsets <code class="language-plaintext highlighter-rouge">[0, 2, 3, 3]</code> contains 3 pairs of offsets, <code class="language-plaintext highlighter-rouge">(0,2)</code>, <code class="language-plaintext highlighter-rouge">(2,3)</code>, and <code class="language-plaintext highlighter-rouge">(3,3)</code>, and therefore represents a <code class="language-plaintext highlighter-rouge">ListArray</code> of length 3 with the following values:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">0</span><span class="p">:</span> <span class="p">[</span><span class="n">child</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">child</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
<span class="mi">1</span><span class="p">:</span> <span class="p">[]</span>
<span class="mi">2</span><span class="p">:</span> <span class="p">[</span><span class="n">child</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span>
</code></pre></div></div>

<p>For the example above with 4 JSON documents, this would be encoded in Arrow as</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
                          ┌──────────────────┐ │
│    ┌─────┐   ┌─────┐    │ ┌─────┐   ┌─────┐│
     │  1  │   │  0  │    │ │  1  │   │  1  ││ │
│    ├─────┤   ├─────┤    │ ├─────┤   ├─────┤│
     │  0  │   │  1  │    │ │  0  │   │ ??  ││ │
│    ├─────┤   ├─────┤    │ ├─────┤   ├─────┤│
     │  1  │   │  1  │    │ │  1  │   │  2  ││ │
│    ├─────┤   ├─────┤    │ └─────┘   └─────┘│
     │  1  │   │  1  │    │ Validity   Values│ │
│    └─────┘   ├─────┤    │                  │
               │  3  │    │ child[0]         │ │
│    Validity  └─────┘    │ PrimitiveArray   │
                          │                  │ │
│              Offsets    └──────────────────┘
     "a"                                       │
│    ListArray
 ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘
</code></pre></div></div>

<p>More technical detail is available in the <a href="https://arrow.apache.org/docs/format/Columnar.html#variable-size-list-layout">ListArray format specification</a>.</p>

<h3 id="parquet-repetition-levels">Parquet Repetition Levels</h3>

<p>The example above with 4 JSON documents can be stored in this Parquet schema</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">message</span> <span class="n">schema</span> <span class="p">{</span>
  <span class="n">optional</span> <span class="n">group</span> <span class="n">a</span> <span class="p">(</span><span class="n">LIST</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">repeated</span> <span class="n">group</span> <span class="nb">list</span> <span class="p">{</span>
      <span class="n">optional</span> <span class="n">int32</span> <span class="n">element</span><span class="p">;</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>In order to encode lists, Parquet stores an integer <em>repetition level</em> in addition to a definition level. A repetition level identifies where in the hierarchy of repeated fields the current value is to be inserted. A value of <code class="language-plaintext highlighter-rouge">0</code> means a new list in the top-most repeated list, a value of <code class="language-plaintext highlighter-rouge">1</code> means a new element within the top-most repeated list, a value of <code class="language-plaintext highlighter-rouge">2</code> means a new element within the second top-most repeated list, and so on.</p>

<p>A consequence of this encoding is that the number of zeros in the <code class="language-plaintext highlighter-rouge">repetition</code> levels is the total number of rows in the column, and the first level in a column must be 0.</p>

<p>Each repeated field also has a corresponding definition level, however, in this case rather than indicating a null value, they indicate an empty array.</p>

<p>The example above would therefore be encoded as</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────────────────────────────────────┐
│  ┌─────┐      ┌─────┐               │
│  │  3  │      │  0  │               │
│  ├─────┤      ├─────┤               │
│  │  0  │      │  0  │               │
│  ├─────┤      ├─────┤      ┌─────┐  │
│  │  1  │      │  0  │      │  1  │  │
│  ├─────┤      ├─────┤      ├─────┤  │
│  │  2  │      │  0  │      │  2  │  │
│  ├─────┤      ├─────┤      └─────┘  │
│  │  3  │      │  1  │               │
│  └─────┘      └─────┘               │
│                                     │
│ Definition  Repetition      Values  │
│   Levels      Levels                │
│  "a"                                │
│                                     │
└─────────────────────────────────────┘
</code></pre></div></div>

<h2 id="next-up-arbitrary-nesting-lists-of-structs-and-structs-of-lists">Next up: Arbitrary Nesting: Lists of Structs and Structs of Lists</h2>

<p>In our <a href="https://arrow.apache.org/blog/2022/10/17/arrow-parquet-encoding-part-3/">final blog post</a>, we explain how Parquet and Arrow combine these concepts to support arbitrary nesting of potentially nullable data structures.</p>

<p>If you want to store and process structured types, you will be pleased to hear that the Rust <a href="https://crates.io/crates/parquet">parquet</a> implementation fully supports reading and writing directly into Arrow, as simply as any other type. All the complex record shredding and reconstruction is handled automatically. With this and other exciting features such as  <a href="https://docs.rs/parquet/22.0.0/parquet/arrow/async_reader/index.html">reading asynchronously</a> from <a href="https://docs.rs/object_store/0.5.0/object_store/">object storage</a>, and advanced row filter pushdown, it is the fastest and most feature complete Rust parquet implementation. We look forward to seeing what you build with it!</p>]]></content><author><name>tustvold and alamb</name></author><category term="parquet" /><category term="arrow" /><summary type="html"><![CDATA[Introduction This is the second, in a three part series exploring how projects such as Rust Apache Arrow support conversion between Apache Arrow and Apache Parquet. The first post covered the basics of data storage and validity encoding, and this post will cover the more complex Struct and List types. Apache Arrow is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. Apache Parquet is an open, column-oriented data file format designed for very efficient data encoding and retrieval. Struct / Group Columns Both Parquet and Arrow have the concept of a struct column, which is a column containing one or more other columns in named fields and is analogous to a JSON object. For example, consider the following three JSON documents { # &lt;-- First record "a": 1, # &lt;-- the top level fields are a, b, c, and d "b": { # &lt;-- b is always provided (not nullable) "b1": 1, # &lt;-- b1 and b2 are "nested" fields of "b" "b2": 3 # &lt;-- b2 is always provided (not nullable) }, "d": { "d1": 1 # &lt;-- d1 is a "nested" field of "d" } } { # &lt;-- Second record "a": 2, "b": { "b2": 4 # &lt;-- note "b1" is NULL in this record }, "c": { # &lt;-- note "c" was NULL in the first record "c1": 6 but when "c" is provided, c1 is also }, always provided (not nullable) "d": { "d1": 2, "d2": 1 } } { # &lt;-- Third record "b": { "b1": 5, "b2": 6 }, "c": { "c1": 7 } } Documents of this format could be stored in an Arrow StructArray with this schema Field(name: "a", nullable: true, datatype: Int32) Field(name: "b", nullable: false, datatype: Struct[ Field(name: "b1", nullable: true, datatype: Int32), Field(name: "b2", nullable: false, datatype: Int32) ]) Field(name: "c"), nullable: true, datatype: Struct[ Field(name: "c1", nullable: false, datatype: Int32) ]) Field(name: "d"), nullable: true, datatype: Struct[ Field(name: "d1", nullable: false, datatype: Int32) Field(name: "d2", nullable: true, datatype: Int32) ]) Arrow represents each StructArray hierarchically using a parent child relationship, with separate validity masks on each of the individual nullable arrays ┌───────────────────┐ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ │ │ ┌─────────────────┐ ┌────────────┐ │ ┌─────┐ ┌─────┐ │ │ │┌─────┐ ┌─────┐│ │ ┌─────┐ │ │ │ │ 1 │ │ 1 │ │ ││ 1 │ │ 1 ││ │ │ 3 │ │ │ ├─────┤ ├─────┤ │ │ │├─────┤ ├─────┤│ │ ├─────┤ │ │ │ │ 1 │ │ 2 │ │ ││ 0 │ │ ?? ││ │ │ 4 │ │ │ ├─────┤ ├─────┤ │ │ │├─────┤ ├─────┤│ │ ├─────┤ │ │ │ │ 0 │ │ ?? │ │ ││ 1 │ │ 5 ││ │ │ 6 │ │ │ └─────┘ └─────┘ │ │ │└─────┘ └─────┘│ │ └─────┘ │ │ │ Validity Values │ │Validity Values│ │ Values │ │ │ │ │ │ │ │ │ │ "a" │ │"b.b1" │ │ "b.b2" │ │ PrimitiveArray │ │ │PrimitiveArray │ │ Primitive │ │ └───────────────────┘ │ │ │ Array │ │ └─────────────────┘ └────────────┘ │ "b" │ StructArray │ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┌─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌───────────┐ ┌──────────┐┌─────────────────┐ │ │ ┌─────┐ │ ┌─────┐ │ │ │ ┌─────┐ │┌─────┐ ││ ┌─────┐ ┌─────┐│ │ 0 │ │ │ ?? │ │ │ 1 │ ││ 1 │ ││ │ 0 │ │ ?? ││ │ │ ├─────┤ │ ├─────┤ │ │ │ ├─────┤ │├─────┤ ││ ├─────┤ ├─────┤│ │ 1 │ │ │ 6 │ │ │ 1 │ ││ 2 │ ││ │ 1 │ │ 1 ││ │ │ ├─────┤ │ ├─────┤ │ │ │ ├─────┤ │├─────┤ ││ ├─────┤ ├─────┤│ │ 1 │ │ │ 7 │ │ │ 0 │ ││ ?? │ ││ │ ?? │ │ ?? ││ │ │ └─────┘ │ └─────┘ │ │ │ └─────┘ │└─────┘ ││ └─────┘ └─────┘│ Validity │ Values │ Validity │ Values ││ Validity Values│ │ │ │ │ │ │ │ ││ │ │ "c.c1" │ │"d.d1" ││ "d.d2" │ │ │ │ Primitive │ │ │ │Primitive ││ PrimitiveArray │ │ Array │ │Array ││ │ │ │ └───────────┘ │ │ └──────────┘└─────────────────┘ "c" "d" │ │ StructArray │ │ StructArray ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ More technical detail is available in the StructArray format specification. Definition Levels Unlike Arrow, Parquet does not encode validity in a structured fashion, instead only storing definition levels for each of the primitive columns, i.e. those that don’t contain other columns. The definition level of a given element is the depth in the schema at which it is fully defined. For example consider the case of d.d2, which contains two nullable levels d and d2. A definition level of 0 would imply a null at the level of d: { } A definition level of 1 would imply a null at the level of d.d2 { "d": { } } A definition level of 2 would imply a defined value for d.d2: { "d": { "d2": .. } } Going back to the three JSON documents above, they could be stored in Parquet with this schema message schema { optional int32 a; required group b { optional int32 b1; required int32 b2; } optional group c { required int32 c1; } optional group d { required int32 d1; optional int32 d2; } } The Parquet encoding of the example would be: ┌────────────────────────┐ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ ┌─────┐ ┌─────┐ │ ┌──────────────────────┐ ┌───────────┐ │ │ │ 1 │ │ 1 │ │ │ │ ┌─────┐ ┌─────┐ │ │ ┌─────┐ │ │ ├─────┤ ├─────┤ │ │ │ 1 │ │ 1 │ │ │ │ 3 │ │ │ │ │ 1 │ │ 2 │ │ │ │ ├─────┤ ├─────┤ │ │ ├─────┤ │ │ ├─────┤ └─────┘ │ │ │ 0 │ │ 5 │ │ │ │ 4 │ │ │ │ │ 0 │ │ │ │ ├─────┤ └─────┘ │ │ ├─────┤ │ │ └─────┘ │ │ │ 1 │ │ │ │ 6 │ │ │ │ │ │ │ └─────┘ │ │ └─────┘ │ │ Definition Data │ │ │ │ │ │ │ Levels │ │ │ Definition Data │ │ Data │ │ │ │ Levels │ │ │ │ │ "a" │ │ │ │ │ │ └────────────────────────┘ │ "b.b1" │ │ "b.b2" │ │ │ └──────────────────────┘ └───────────┘ "b" │ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ── ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ── ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌────────────────────┐ │ ┌────────────────────┐ ┌──────────────────┐ │ │ │ ┌─────┐ ┌─────┐ │ │ │ ┌─────┐ ┌─────┐ │ │ ┌─────┐ ┌─────┐ │ │ │ 0 │ │ 6 │ │ │ │ │ 1 │ │ 1 │ │ │ │ 1 │ │ 1 │ │ │ │ │ ├─────┤ ├─────┤ │ │ │ ├─────┤ ├─────┤ │ │ ├─────┤ └─────┘ │ │ │ 1 │ │ 7 │ │ │ │ │ 1 │ │ 2 │ │ │ │ 2 │ │ │ │ │ ├─────┤ └─────┘ │ │ │ ├─────┤ └─────┘ │ │ ├─────┤ │ │ │ 1 │ │ │ │ │ 0 │ │ │ │ 0 │ │ │ │ │ └─────┘ │ │ │ └─────┘ │ │ └─────┘ │ │ │ │ │ │ │ │ │ │ │ Definition Data │ │ │ Definition Data │ │ Definition Data │ │ Levels │ │ │ Levels │ │ Levels │ │ │ │ │ │ │ │ │ │ │ "c.1" │ │ │ "d.1" │ │ "d.d2" │ │ │ └────────────────────┘ │ └────────────────────┘ └──────────────────┘ "c" │ "d" │ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ List / Repeated Columns Closing out support for nested types are lists, which contain a variable number of other values. For example, the following four documents each have a (nullable) field a containing a list of integers { # &lt;-- First record "a": [1], # &lt;-- top-level field a containing list of integers } { # &lt;-- "a" is not provided (is null) } { # &lt;-- "a" is non-null but empty "a": [] } { "a": [null, 2], # &lt;-- "a" has a null and non-null elements } Documents of this format could be stored in this Arrow schema Field(name: "a", nullable: true, datatype: List( Field(name: "element", nullable: true, datatype: Int32), ) As before, Arrow chooses to represent this in a hierarchical fashion as a ListArray. A ListArray contains a list of monotonically increasing integers called offsets, a validity mask if the list is nullable, and a child array containing the list elements. Each consecutive pair of elements in the offset array identifies a slice of the child array for that index in the ListArray For example, a list with offsets [0, 2, 3, 3] contains 3 pairs of offsets, (0,2), (2,3), and (3,3), and therefore represents a ListArray of length 3 with the following values: 0: [child[0], child[1]] 1: [] 2: [child[2]] For the example above with 4 JSON documents, this would be encoded in Arrow as ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌──────────────────┐ │ │ ┌─────┐ ┌─────┐ │ ┌─────┐ ┌─────┐│ │ 1 │ │ 0 │ │ │ 1 │ │ 1 ││ │ │ ├─────┤ ├─────┤ │ ├─────┤ ├─────┤│ │ 0 │ │ 1 │ │ │ 0 │ │ ?? ││ │ │ ├─────┤ ├─────┤ │ ├─────┤ ├─────┤│ │ 1 │ │ 1 │ │ │ 1 │ │ 2 ││ │ │ ├─────┤ ├─────┤ │ └─────┘ └─────┘│ │ 1 │ │ 1 │ │ Validity Values│ │ │ └─────┘ ├─────┤ │ │ │ 3 │ │ child[0] │ │ │ Validity └─────┘ │ PrimitiveArray │ │ │ │ │ Offsets └──────────────────┘ "a" │ │ ListArray ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ More technical detail is available in the ListArray format specification. Parquet Repetition Levels The example above with 4 JSON documents can be stored in this Parquet schema message schema { optional group a (LIST) { repeated group list { optional int32 element; } } } In order to encode lists, Parquet stores an integer repetition level in addition to a definition level. A repetition level identifies where in the hierarchy of repeated fields the current value is to be inserted. A value of 0 means a new list in the top-most repeated list, a value of 1 means a new element within the top-most repeated list, a value of 2 means a new element within the second top-most repeated list, and so on. A consequence of this encoding is that the number of zeros in the repetition levels is the total number of rows in the column, and the first level in a column must be 0. Each repeated field also has a corresponding definition level, however, in this case rather than indicating a null value, they indicate an empty array. The example above would therefore be encoded as ┌─────────────────────────────────────┐ │ ┌─────┐ ┌─────┐ │ │ │ 3 │ │ 0 │ │ │ ├─────┤ ├─────┤ │ │ │ 0 │ │ 0 │ │ │ ├─────┤ ├─────┤ ┌─────┐ │ │ │ 1 │ │ 0 │ │ 1 │ │ │ ├─────┤ ├─────┤ ├─────┤ │ │ │ 2 │ │ 0 │ │ 2 │ │ │ ├─────┤ ├─────┤ └─────┘ │ │ │ 3 │ │ 1 │ │ │ └─────┘ └─────┘ │ │ │ │ Definition Repetition Values │ │ Levels Levels │ │ "a" │ │ │ └─────────────────────────────────────┘ Next up: Arbitrary Nesting: Lists of Structs and Structs of Lists In our final blog post, we explain how Parquet and Arrow combine these concepts to support arbitrary nesting of potentially nullable data structures. If you want to store and process structured types, you will be pleased to hear that the Rust parquet implementation fully supports reading and writing directly into Arrow, as simply as any other type. All the complex record shredding and reconstruction is handled automatically. With this and other exciting features such as reading asynchronously from object storage, and advanced row filter pushdown, it is the fastest and most feature complete Rust parquet implementation. We look forward to seeing what you build with it!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Arrow and Parquet Part 1: Primitive Types and Nullability</title><link href="https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/" rel="alternate" type="text/html" title="Arrow and Parquet Part 1: Primitive Types and Nullability" /><published>2022-10-05T00:00:00-04:00</published><updated>2022-10-05T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/"><![CDATA[<!--

-->

<h2 id="introduction">Introduction</h2>

<p>We recently completed a long-running project within <a href="https://github.com/apache/arrow-rs">Rust Apache Arrow</a> to complete support for reading and writing arbitrarily nested Parquet and Arrow schemas. This is a complex topic, and we encountered a lack of approachable technical information, and thus wrote this blog to share our learnings with the community.</p>

<p><a href="https://arrow.apache.org/">Apache Arrow</a> is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. <a href="https://parquet.apache.org/">Apache Parquet</a> is an open, column-oriented data file format designed for very efficient data encoding and retrieval.</p>

<p>It is increasingly common for analytic systems to use Arrow to process data stored in Parquet files, and therefore fast, efficient, and correct translation between them is a key building block.</p>

<p>Historically analytic processing primarily focused on querying data with a tabular schema, where there are a fixed number of columns, and each row contains a single value for each column. However, with the increasing adoption of structured document formats such as XML, JSON, etc…, only supporting tabular schema can be frustrating for users, as it necessitates often non-trivial data transformation to first flatten the document data.</p>

<p>As of version <a href="https://crates.io/crates/arrow/20.0.0">20.0.0</a>, released in August 2022, the Rust Arrow implementation for reading structured types is feature complete. Instructions for getting started can be found <a href="https://docs.rs/parquet/latest/parquet/arrow/index.html">here</a> and feel free to raise any issues on our <a href="https://github.com/apache/arrow-rs/issues">bugtracker</a>.</p>

<p>In this series we will explain how Parquet and Arrow represent nested data, highlighting the similarities and differences between them, and give a flavor of the practicalities of converting between the formats.</p>

<h2 id="columnar-vs-record-oriented">Columnar vs Record-Oriented</h2>

<p>First, it is necessary to take a step back and discuss the difference between columnar and record-oriented data formats. In a record oriented data format, such as newline-delimited JSON (NDJSON), all the values for a given record are stored contiguously.</p>

<p>For example</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="s">"Column1"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">"Column2"</span><span class="p">:</span> <span class="mi">2</span><span class="p">}</span>
<span class="p">{</span><span class="s">"Column1"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s">"Column2"</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s">"Column3"</span><span class="p">:</span> <span class="mi">5</span><span class="p">}</span>
<span class="p">{</span><span class="s">"Column1"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s">"Column2"</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s">"Column3"</span><span class="p">:</span> <span class="mi">5</span><span class="p">}</span>
</code></pre></div></div>

<p>In a columnar representation, the data for a given column is instead stored contiguously</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Column1</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">Column2</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">Column3</span><span class="p">:</span> <span class="p">[</span><span class="n">null</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
</code></pre></div></div>

<p>Aside from potentially yielding better data compression, a columnar layout can dramatically improve performance of certain queries. This is because laying data out contiguously in memory allows both the compiler and CPU to better exploit opportunities for parallelism. The specifics of <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data">SIMD</a> and <a href="https://en.wikipedia.org/wiki/Instruction-level_parallelism">ILP</a> are well beyond the scope of this post, but the important takeaway is that processing large blocks of data without intervening conditional branches has substantial performance benefits.</p>

<h2 id="parquet-vs-arrow">Parquet vs Arrow</h2>
<p>Parquet and Arrow are complementary technologies, and they make some different design tradeoffs. In particular, Parquet is a storage format designed for maximum space efficiency, whereas Arrow is an in-memory format intended for operation by vectorized computational kernels.</p>

<p>The major distinction is that Arrow provides <code class="language-plaintext highlighter-rouge">O(1)</code> random access lookups to any array index, whilst Parquet does not. In particular, Parquet uses <a href="https://akshays-blog.medium.com/wrapping-head-around-repetition-and-definition-levels-in-dremel-powering-bigquery-c1a33c9695da">dremel record shredding</a>, <a href="https://github.com/apache/parquet-format/blob/master/Encodings.md">variable length encoding schemes</a>, and <a href="https://github.com/apache/parquet-format/blob/master/Compression.md">block compression</a> to drastically reduce the data size, but these techniques come at the loss of performant random access lookups.</p>

<p>A common pattern that plays to each technologies strengths, is to stream data from a compressed representation, such as Parquet, in thousand row batches in the Arrow format, process these batches individually, and accumulate the results in a more compressed representation. This benefits from the ability to efficiently perform computations on Arrow data, whilst keeping memory requirements in check, and allowing the computation kernels to be agnostic to the encodings of the source and destination.</p>

<p><strong>Arrow is primarily an in-memory format, whereas Parquet is a storage format.</strong></p>

<h2 id="non-nullable-primitive-column">Non-Nullable Primitive Column</h2>

<p>Let us start with the simplest case of a non-nullable list of 32-bit signed integers.</p>

<p>In Arrow this would be represented as a <code class="language-plaintext highlighter-rouge">PrimitiveArray</code>, which would store them contiguously in memory</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────┐
│  1  │
├─────┤
│  2  │
├─────┤
│  3  │
├─────┤
│  4  │
└─────┘
Values
</code></pre></div></div>

<p>Parquet has multiple <a href="https://parquet.apache.org/docs/file-format/data-pages/encodings/">different encodings</a> that may be used for integer types, the exact details of which are beyond the scope of this post. Broadly speaking the data will be stored in one or more <a href="https://parquet.apache.org/docs/file-format/data-pages/"><em>DataPage</em></a>s containing the integers in an encoded form</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────┐
│  1  │
├─────┤
|  2  │
├─────┤
│  3  │
├─────┤
│  4  │
└─────┘
Values
</code></pre></div></div>

<h1 id="nullable-primitive-column">Nullable Primitive Column</h1>

<p>Now let us consider the case of a nullable column, where some of the values might have the special sentinel value <code class="language-plaintext highlighter-rouge">NULL</code> that designates “this value is unknown”.</p>

<p>In Arrow, nulls are stored separately from the values in the form of a <a href="https://arrow.apache.org/docs/format/Columnar.html#validity-bitmaps">validity bitmask</a>, with arbitrary data in the corresponding positions in the values buffer. This space efficient encoding means that the entire validity mask for the following example is stored using 5 bits</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────┐   ┌─────┐
│  1  │   │  1  │
├─────┤   ├─────┤
│  0  │   │ ??  │
├─────┤   ├─────┤
│  1  │   │  3  │
├─────┤   ├─────┤
│  1  │   │  4  │
├─────┤   ├─────┤
│  0  │   │ ??  │
└─────┘   └─────┘
Validity   Values
</code></pre></div></div>

<p>In Parquet the validity information is also stored separately from the values, however, instead of being encoded as a validity bitmask it is encoded as a list of 16-bit integers called <em>definition levels</em>. Like other data in Parquet, these integer definition levels are stored using high efficiency encoding, and will be expanded upon in the next post, but for now a definition level of <code class="language-plaintext highlighter-rouge">1</code> indicates a valid value, and <code class="language-plaintext highlighter-rouge">0</code> a null value. Unlike Arrow, nulls are not encoded in the list of values</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────┐    ┌─────┐
│  1  │    │  1  │
├─────┤    ├─────┤
│  0  │    │  3  │
├─────┤    ├─────┤
│  1  │    │  4  │
├─────┤    └─────┘
│  1  │
├─────┤
│  0  │
└─────┘
Definition  Values
 Levels
</code></pre></div></div>

<h2 id="next-up-nested-and-hierarchical-data">Next up: Nested and Hierarchical Data</h2>

<p>Armed with the foundational understanding of how Arrow and Parquet store nullability / definition differently we are ready to move on to more complex nested types, which you can read about in our <a href="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/">next blog post on the topic</a>.</p>]]></content><author><name>tustvold and alamb</name></author><category term="parquet" /><category term="arrow" /><summary type="html"><![CDATA[Introduction We recently completed a long-running project within Rust Apache Arrow to complete support for reading and writing arbitrarily nested Parquet and Arrow schemas. This is a complex topic, and we encountered a lack of approachable technical information, and thus wrote this blog to share our learnings with the community. Apache Arrow is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. Apache Parquet is an open, column-oriented data file format designed for very efficient data encoding and retrieval. It is increasingly common for analytic systems to use Arrow to process data stored in Parquet files, and therefore fast, efficient, and correct translation between them is a key building block. Historically analytic processing primarily focused on querying data with a tabular schema, where there are a fixed number of columns, and each row contains a single value for each column. However, with the increasing adoption of structured document formats such as XML, JSON, etc…, only supporting tabular schema can be frustrating for users, as it necessitates often non-trivial data transformation to first flatten the document data. As of version 20.0.0, released in August 2022, the Rust Arrow implementation for reading structured types is feature complete. Instructions for getting started can be found here and feel free to raise any issues on our bugtracker. In this series we will explain how Parquet and Arrow represent nested data, highlighting the similarities and differences between them, and give a flavor of the practicalities of converting between the formats. Columnar vs Record-Oriented First, it is necessary to take a step back and discuss the difference between columnar and record-oriented data formats. In a record oriented data format, such as newline-delimited JSON (NDJSON), all the values for a given record are stored contiguously. For example {"Column1": 1, "Column2": 2} {"Column1": 3, "Column2": 4, "Column3": 5} {"Column1": 5, "Column2": 4, "Column3": 5} In a columnar representation, the data for a given column is instead stored contiguously Column1: [1, 3, 5] Column2: [2, 4, 4] Column3: [null, 5, 5] Aside from potentially yielding better data compression, a columnar layout can dramatically improve performance of certain queries. This is because laying data out contiguously in memory allows both the compiler and CPU to better exploit opportunities for parallelism. The specifics of SIMD and ILP are well beyond the scope of this post, but the important takeaway is that processing large blocks of data without intervening conditional branches has substantial performance benefits. Parquet vs Arrow Parquet and Arrow are complementary technologies, and they make some different design tradeoffs. In particular, Parquet is a storage format designed for maximum space efficiency, whereas Arrow is an in-memory format intended for operation by vectorized computational kernels. The major distinction is that Arrow provides O(1) random access lookups to any array index, whilst Parquet does not. In particular, Parquet uses dremel record shredding, variable length encoding schemes, and block compression to drastically reduce the data size, but these techniques come at the loss of performant random access lookups. A common pattern that plays to each technologies strengths, is to stream data from a compressed representation, such as Parquet, in thousand row batches in the Arrow format, process these batches individually, and accumulate the results in a more compressed representation. This benefits from the ability to efficiently perform computations on Arrow data, whilst keeping memory requirements in check, and allowing the computation kernels to be agnostic to the encodings of the source and destination. Arrow is primarily an in-memory format, whereas Parquet is a storage format. Non-Nullable Primitive Column Let us start with the simplest case of a non-nullable list of 32-bit signed integers. In Arrow this would be represented as a PrimitiveArray, which would store them contiguously in memory ┌─────┐ │ 1 │ ├─────┤ │ 2 │ ├─────┤ │ 3 │ ├─────┤ │ 4 │ └─────┘ Values Parquet has multiple different encodings that may be used for integer types, the exact details of which are beyond the scope of this post. Broadly speaking the data will be stored in one or more DataPages containing the integers in an encoded form ┌─────┐ │ 1 │ ├─────┤ | 2 │ ├─────┤ │ 3 │ ├─────┤ │ 4 │ └─────┘ Values Nullable Primitive Column Now let us consider the case of a nullable column, where some of the values might have the special sentinel value NULL that designates “this value is unknown”. In Arrow, nulls are stored separately from the values in the form of a validity bitmask, with arbitrary data in the corresponding positions in the values buffer. This space efficient encoding means that the entire validity mask for the following example is stored using 5 bits ┌─────┐ ┌─────┐ │ 1 │ │ 1 │ ├─────┤ ├─────┤ │ 0 │ │ ?? │ ├─────┤ ├─────┤ │ 1 │ │ 3 │ ├─────┤ ├─────┤ │ 1 │ │ 4 │ ├─────┤ ├─────┤ │ 0 │ │ ?? │ └─────┘ └─────┘ Validity Values In Parquet the validity information is also stored separately from the values, however, instead of being encoded as a validity bitmask it is encoded as a list of 16-bit integers called definition levels. Like other data in Parquet, these integer definition levels are stored using high efficiency encoding, and will be expanded upon in the next post, but for now a definition level of 1 indicates a valid value, and 0 a null value. Unlike Arrow, nulls are not encoded in the list of values ┌─────┐ ┌─────┐ │ 1 │ │ 1 │ ├─────┤ ├─────┤ │ 0 │ │ 3 │ ├─────┤ ├─────┤ │ 1 │ │ 4 │ ├─────┤ └─────┘ │ 1 │ ├─────┤ │ 0 │ └─────┘ Definition Values Levels Next up: Nested and Hierarchical Data Armed with the foundational understanding of how Arrow and Parquet store nullability / definition differently we are ready to move on to more complex nested types, which you can read about in our next blog post on the topic.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 9.0.0 Release</title><link href="https://arrow.apache.org/blog/2022/08/16/9.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 9.0.0 Release" /><published>2022-08-16T00:00:00-04:00</published><updated>2022-08-16T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/08/16/9.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/08/16/9.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 9.0.0 release. This covers
over 3 months of development work and includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%209.0.0"><strong>509 resolved issues</strong></a>
from <a href="/release/9.0.0.html#contributors"><strong>114 distinct contributors</strong></a>. See the Install Page to learn how to
get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bug fixes and improvements have been made: we refer
you to the <a href="/release/9.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 8.0.0 release, Dewey Dunnington, Alenka Frim and Rok Mihevc
have been invited to be committers.
Thanks for your contributions and participation in the project!</p>

<h2 id="columnar-format-notes">Columnar Format Notes</h2>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>

<p>Arrow Flight is now available in MacOS M1 Python wheels (<a href="https://issues.apache.org/jira/browse/ARROW-16779">ARROW-16779</a>).
Arrow Flight SQL is now buildable on Windows (<a href="https://issues.apache.org/jira/browse/ARROW-16902">ARROW-16902</a>).
Ruby now exposes more of the Flight and Flight SQL APIs (various JIRAs).</p>

<h2 id="linux-packages-notes">Linux packages notes</h2>

<p>AlmaLinux 9 is now supported. (<a href="https://issues.apache.org/jira/browse/ARROW-16745">ARROW-16745</a>)</p>

<p>AmazonLinux 2 aarch64 is now supported. (<a href="https://issues.apache.org/jira/browse/ARROW-16477">ARROW-16477</a>)</p>

<h2 id="c-notes">C++ notes</h2>

<p>STL-like iteration is now provided over chunked arrays (<a href="https://issues.apache.org/jira/browse/ARROW-602">ARROW-602</a>).</p>

<h3 id="compute">Compute</h3>

<p>The C++ compute and execution engine is now officially named “Acero”, though
its C++ namespaces have not changed.</p>

<p>New light-weight data holder abstractions have been introduced in order
to reduce the overhead of invoking compute functions and kernels, especially
at the small data sizes desirable for efficient parallelization (typically
L1- or L2-sized).  Specifically, the non-owning <code class="language-plaintext highlighter-rouge">ArraySpan</code> and <code class="language-plaintext highlighter-rouge">ExecSpan</code>
structures have internally superseded the much heavier <code class="language-plaintext highlighter-rouge">ExecBatch</code>, which
is still supported for compatibility at the API level
(<a href="https://issues.apache.org/jira/browse/ARROW-16756">ARROW-16756</a>, <a href="https://issues.apache.org/jira/browse/ARROW-16824">ARROW-16824</a>, <a href="https://issues.apache.org/jira/browse/ARROW-16852">ARROW-16852</a>).</p>

<p>In a similar vein, the <code class="language-plaintext highlighter-rouge">ValueDescr</code> class was removed and <code class="language-plaintext highlighter-rouge">ScalarKernel</code>
implementations now always receive at least one non-scalar input, removing
the special case where a <code class="language-plaintext highlighter-rouge">ScalarKernel</code> needs to output a scalar rather than
an array. The higher-level compute APIs still allow executing a scalar function
over all-scalar inputs; but those scalars are internally broadcasted to
1-element arrays so as to simplify kernel implementation (<a href="https://issues.apache.org/jira/browse/ARROW-16757">ARROW-16757</a>).</p>

<p>Some performance improvements were made to the hash join node.  These changes
do not require additional configuration.  The hash join exec node has been
improved to more efficiently use CPU cache and make better use of available
vectorization hardware (<a href="https://issues.apache.org/jira/browse/ARROW-14182">ARROW-14182</a>).</p>

<p>Some plans containing a sequence of hash join operators will now use bloom
filters to eliminate rows earlier in the plan, reducing the overall CPU
cost of the plan (<a href="https://issues.apache.org/jira/browse/ARROW-15498">ARROW-15498</a>).</p>

<p>Timestamp comparison is now supported (<a href="https://issues.apache.org/jira/browse/ARROW-16425">ARROW-16425</a>).</p>

<p>A cumulative sum function is implemented over numeric inputs (<a href="https://issues.apache.org/jira/browse/ARROW-13530">ARROW-13530</a>). Note that this is a vector
function so cannot be used in an Acero ExecPlan.</p>

<p>A rank vector kernel has been added (<a href="https://issues.apache.org/jira/browse/ARROW-16234">ARROW-16234</a>).</p>

<p>Temporal rounding functions received additional options to control how
rounding is done (<a href="https://issues.apache.org/jira/browse/ARROW-14821">ARROW-14821</a>).</p>

<p>Improper computation of the “mode” function on boolean input was fixed
(<a href="https://issues.apache.org/jira/browse/ARROW-17096">ARROW-17096</a>).</p>

<p>Function registries can now be nested (<a href="https://issues.apache.org/jira/browse/ARROW-16677">ARROW-16677</a>).</p>

<h3 id="dataset">Dataset</h3>

<p>The <code class="language-plaintext highlighter-rouge">autogenerate_column_names</code> option for CSV reading is now handled correctly
(<a href="https://issues.apache.org/jira/browse/ARROW-16436">ARROW-16436</a>).</p>

<p>Fix <code class="language-plaintext highlighter-rouge">InMemoryDataset::ReplaceSchema</code> to actually replace the schema
(<a href="https://issues.apache.org/jira/browse/ARROW-16085">ARROW-16085</a>).</p>

<p>Fix <code class="language-plaintext highlighter-rouge">FilenamePartitioning</code> to properly support null values (<a href="https://issues.apache.org/jira/browse/ARROW-16302">ARROW-16302</a>).</p>

<h3 id="filesystem">Filesystem</h3>

<p>A number of bug fixes and improvements were made to the Google Cloud Storage
filesystem implementation (<a href="https://issues.apache.org/jira/browse/ARROW-14892">ARROW-14892</a>).</p>

<p>By default, the S3 filesystem implementation does not create or drop buckets
anymore (<a href="https://issues.apache.org/jira/browse/ARROW-15906">ARROW-15906</a>). This is a compatibility-breaking change intended
to prevent user errors from having potentially catastrophic consequences.
Options have been added to restore the previous behavior if necessary.</p>

<h3 id="parquet">Parquet</h3>

<p>The default Parquet version is now 2.4 for writing, enabling use of
more recent logical types by default (<a href="https://issues.apache.org/jira/browse/ARROW-12203">ARROW-12203</a>).</p>

<p>Non-nullable fields are now handled correctly by the Parquet reader
(<a href="https://issues.apache.org/jira/browse/ARROW-16116">ARROW-16116</a>).</p>

<p>Reading encrypted files should now be thread-safe (<a href="https://issues.apache.org/jira/browse/ARROW-14114">ARROW-14114</a>).</p>

<p>Statistics equality now works correctly with minmax (<a href="https://issues.apache.org/jira/browse/ARROW-16487">ARROW-16487</a>).</p>

<p>The minimum Thrift version required for building is now 0.13 (<a href="https://issues.apache.org/jira/browse/ARROW-16721">ARROW-16721</a>).</p>

<p>The Thrift deserialization limits can now be configured to accommodate for
data files with very large metadata (<a href="https://issues.apache.org/jira/browse/ARROW-16546">ARROW-16546</a>).</p>

<h3 id="substrait">Substrait</h3>

<p>The Substrait spec has been updated to 0.6.0 (<a href="https://issues.apache.org/jira/browse/ARROW-16816">ARROW-16816</a>). In addition, a
larger subset of the Substrait specification is now supported (<a href="https://issues.apache.org/jira/browse/ARROW-15587">ARROW-15587</a>,
<a href="https://issues.apache.org/jira/browse/ARROW-15590">ARROW-15590</a>,
<a href="https://issues.apache.org/jira/browse/ARROW-15901">ARROW-15901</a>,
<a href="https://issues.apache.org/jira/browse/ARROW-16657">ARROW-16657</a>,
<a href="https://issues.apache.org/jira/browse/ARROW-15591">ARROW-15591</a>).</p>

<h2 id="c-notes-1">C# notes</h2>

<h4 id="new-features">New Features</h4>

<ul>
  <li>Added support for Time32Array and Time64Array (<a href="https://github.com/apache/arrow/pull/13279">ARROW-16660</a>)</li>
</ul>

<h4 id="bug-fixes">Bug Fixes</h4>

<ul>
  <li>When using TableFromRecordBatches, the resulting table columns have no data array. (<a href="https://github.com/apache/arrow/pull/10562">ARROW-13129</a>)</li>
  <li>Fix intermittent test failures due to async memory management bug. (<a href="https://github.com/apache/arrow/pull/13573">ARROW-16978</a>)</li>
</ul>

<h2 id="go-notes">Go notes</h2>

<h3 id="security">Security</h3>

<ul>
  <li>Updated testify dependency to address CVE-2022-28948. (<a href="https://issues.apache.org/jira/browse/ARROW-16759">ARROW-16759</a>) (This was also backported to previous versions and released as patch versions v6.0.2, v7.0.1, and v8.0.1)</li>
</ul>

<h3 id="arrow">Arrow</h3>

<h4 id="new-features-1">New Features</h4>

<ul>
  <li>Dictionary Scalars are now available (<a href="https://issues.apache.org/jira/browse/ARROW-16323">ARROW-16323</a>)</li>
  <li>Introduced a DictionaryUnifier object along with functions for unifying Chunked Arrays and Tables (<a href="https://issues.apache.org/jira/browse/ARROW-16324">ARROW-16324</a>)</li>
  <li>New CSV examples added to documentation to demonstrate error handling (<a href="https://issues.apache.org/jira/browse/ARROW-16450">ARROW-16450</a>)</li>
  <li>CSV Reader now supports arrow.TimestampType (<a href="https://issues.apache.org/jira/browse/ARROW-16504">ARROW-16504</a>)</li>
  <li>JSON parsing for Temporal Types now allow passing numeric values in addition to strings for parsing. Timezones will be properly parsed if they exist in the string and a function was added to retrieve a time.Location object from a TimestampType (<a href="https://issues.apache.org/jira/browse/ARROW-16551">ARROW-16551</a>)</li>
  <li>New utilities added to decimal128 for rescaling and easy conversion to and from float32/float64 (<a href="https://issues.apache.org/jira/browse/ARROW-16552">ARROW-16552</a>)</li>
  <li>Arrow DataType interface now has a LayoutMethod which returns the physical layout of the given datatype such as the number of buffers, types, etc. This matches the behavior of the layout() methods in C++ for data types. (<a href="https://issues.apache.org/jira/browse/ARROW-16556">ARROW-16556</a>)</li>
  <li>Added a SliceBuffer function to the memory package to allow better re-using of memory across buffer objects (<a href="https://issues.apache.org/jira/browse/ARROW-16557">ARROW-16557</a>)</li>
  <li>Dictionary Arrays can now be concatenated using array.Concatenate (<a href="https://issues.apache.org/jira/browse/ARROW-17095">ARROW-17095</a>)</li>
</ul>

<h4 id="bug-fixes-1">Bug Fixes</h4>

<ul>
  <li>ipc.FileReader now properly uses the memory.Allocator interface (<a href="https://issues.apache.org/jira/browse/ARROW-16002">ARROW-16002</a>)</li>
  <li>Addressed issue with Integration tests between Go and Java (<a href="https://issues.apache.org/jira/browse/ARROW-16441">ARROW-16441</a>)</li>
  <li>RecordBuilder.UnmarshalJSON now properly ignores extra unknown fields rather than panicking (<a href="https://issues.apache.org/jira/browse/ARROW-16456">ARROW-16456</a>)</li>
  <li>StructBuilder.UnmarshalJSON will no longer fail and panic when Nullable fields are missing (<a href="https://issues.apache.org/jira/browse/ARROW-16502">ARROW-16502</a>)</li>
  <li>ipc.Reader no longer silently accepts string columns with invalid offsets, preventing unexpected panics later when writing or accessing the resulting arrays. (<a href="https://issues.apache.org/jira/browse/ARROW-16831">ARROW-16831</a>)</li>
  <li>Arrow CSV reader no longer clobbers its reported errors and properly surfaces them (<a href="https://issues.apache.org/jira/browse/ARROW-16926">ARROW-16926</a>)</li>
</ul>

<h3 id="parquet-1">Parquet</h3>

<h4 id="new-features-2">New Features</h4>

<ul>
  <li>The CreatedBy version string for the Parquet writer will now correctly reflect the library version, and will be updated by the release scripts (<a href="https://issues.apache.org/jira/browse/ARROW-16484">ARROW-16484</a>)</li>
  <li>Parquet bit_packing functions now have ARM64 NEON implementations for performance (<a href="https://issues.apache.org/jira/browse/ARROW-16486">ARROW-16486</a>)</li>
  <li>It is now possible to customize the root node in the Parquet writer instead of hardcoding it to be named “schema” with a repetition type of Repeated. This was needed to allow producing files similar to Apache Spark where the root node has a repetition type of Required. It still defaults to the spec definition of Repeated. (<a href="https://issues.apache.org/jira/browse/ARROW-16561">ARROW-16561</a>)</li>
  <li>parquet_reader CLI mainprog has been enhanced to dump values out as JSON and CSV along with setting an output file instead of just dumping to the terminal. (<a href="https://issues.apache.org/jira/browse/ARROW-16934">ARROW-16934</a>)</li>
</ul>

<h4 id="bug-fixes-2">Bug Fixes</h4>

<ul>
  <li>Fixed a memory leak with Parquet page reading (<a href="https://issues.apache.org/jira/browse/ARROW-16473">ARROW-16473</a>)</li>
  <li>Parquet Reader properly parallelizes column reads when the parallel option is set to true. (<a href="https://issues.apache.org/jira/browse/ARROW-16530">ARROW-16530</a>)</li>
  <li>Fixed bug in the Bool decoder for plain encoding (<a href="https://issues.apache.org/jira/browse/ARROW-16563">ARROW-16563</a>)</li>
  <li>Fixed a bug in the Parquet bool column reader where it failed to properly skip rows (<a href="https://issues.apache.org/jira/browse/ARROW-16638">ARROW-16638</a>)</li>
  <li>Fixed the flakey travis ARM64 builds by reducing the size of a test case in the pqarrow unit tests to reduce the memory usage for the tests. (<a href="https://issues.apache.org/jira/browse/ARROW-16669">ARROW-16669</a>)</li>
  <li>Parquet writer now properly handles writing arrow.NULL type arrays (<a href="https://issues.apache.org/jira/browse/ARROW-16749">ARROW-16749</a>)</li>
  <li>Column level dictionary encoding configuration for Parquet writing now correctly respects the input value (<a href="https://issues.apache.org/jira/browse/ARROW-16813">ARROW-16813</a>)</li>
  <li>Memory leak in DeltaByteArray encoding fixed (<a href="https://issues.apache.org/jira/browse/ARROW-16983">ARROW-16983</a>)</li>
</ul>

<h2 id="java-notes">Java notes</h2>
<h4 id="new-features-3">New Features</h4>
<ul>
  <li>Allow overriding column nullability in arrow-jdbc (<a href="https://github.com/apache/arrow/pull/13558">#13558</a>)</li>
  <li>Enable skip BOUNDS_CHECKING with setBytes and getBytes of ArrowBuf (<a href="https://github.com/apache/arrow/pull/13161">#13161</a>)</li>
  <li>Initialize JNI components on use instead of statically (<a href="https://github.com/apache/arrow/pull/13146">#13146</a>)</li>
  <li>Provide explicit JDBC column type mapping (<a href="https://github.com/apache/arrow/pull/13166">#13166</a>)</li>
  <li>Allow duplicated field names in Java C data interface (<a href="https://github.com/apache/arrow/pull/13247">#13247</a>)</li>
  <li>Improve and document StackTrace (<a href="https://github.com/apache/arrow/pull/12656">#12656</a>)</li>
  <li>Keep more context when marshaling errors through JNI (<a href="https://github.com/apache/arrow/pull/13246">#13246</a>)</li>
  <li>Make RoundingMode configurable to handle inconsistent scale in BigDecimals (<a href="https://github.com/apache/arrow/pull/13433">#13433</a>)</li>
  <li>Improve Java dev experience with IntelliJ (<a href="https://github.com/apache/arrow/pull/13017">#13017</a>)</li>
  <li>Implement ArrowArrayStream (<a href="https://github.com/apache/arrow/pull/13465">#13465</a>))</li>
</ul>

<h4 id="bug-fixes-3">Bug Fixes</h4>
<ul>
  <li>Fix variable-width vectors in integration JSON writer (<a href="https://github.com/apache/arrow/pull/13676">#13676</a>)</li>
  <li>Handle empty JDBC ResultSet (<a href="https://github.com/apache/arrow/pull/13049">#13049</a>)</li>
  <li>Fix hasNext() in ArrowVectorIterator (<a href="https://github.com/apache/arrow/pull/13107">#13107</a>)</li>
  <li>Fix ArrayConsumer when using ArrowVectorIterator (<a href="https://github.com/apache/arrow/pull/12692">#12692</a>)</li>
  <li>Update Gandiva Protobuf library to enable builds on Apple M1 (<a href="https://github.com/apache/arrow/pull/13121">#13121</a>)</li>
  <li>Patch dataset module testing failure with JSE11+ (<a href="https://github.com/apache/arrow/pull/13200">#13200</a>)</li>
  <li>Don’t duplicate generated Protobuf classes between flight-core and flight-sql (<a href="https://github.com/apache/arrow/pull/13596">#13596</a>)</li>
</ul>

<h2 id="javascript-notes">JavaScript notes</h2>

<ul>
  <li>Fix error iterating tables with no batches (<a href="https://issues.apache.org/jira/browse/ARROW-16371">ARROW-16371</a>)</li>
  <li>Handle case where <code class="language-plaintext highlighter-rouge">tableFromIPC</code> input is an async <code class="language-plaintext highlighter-rouge">RecordBatchReader</code> (<a href="https://issues.apache.org/jira/browse/ARROW-16704">ARROW-16704</a>)</li>
</ul>

<h2 id="python-notes">Python notes</h2>

<p>Compatibility notes:</p>

<ul>
  <li>
    <p>PyArrow now requires Python &gt;= 3.7 (<a href="https://issues.apache.org/jira/browse/ARROW-16474">ARROW-16474</a>).</p>
  </li>
  <li>
    <p>The default behaviour regarding memory mapping has changed in several APIs (reading of Feather or Parquet files, IPC RecordBatchFileReader and RecordBatchStreamReader) to disable memory mapping by default (<a href="https://issues.apache.org/jira/browse/ARROW-16382">ARROW-16382</a>).</p>
  </li>
  <li>
    <p>The default Parquet version is now 2.4 for writing, enabling use of
more recent logical types by default such as unsigned integers (<a href="https://issues.apache.org/jira/browse/ARROW-12203">ARROW-12203</a>). One can specify <code class="language-plaintext highlighter-rouge">version="2.6"</code> to also enable support for nanosecond timestamps. Use <code class="language-plaintext highlighter-rouge">version="1.0"</code> to restore the old behaviour and maximizes file compatibility.</p>
  </li>
  <li>
    <p>Some deprecated APIs (deprecated at least since pyarrow 1.0.0) have been removed: IPC methods in the top-level namespace, the <code class="language-plaintext highlighter-rouge">Value</code> scalar classes and the <code class="language-plaintext highlighter-rouge">pyarrow.compat</code> module (<a href="https://issues.apache.org/jira/browse/ARROW-17010">ARROW-17010</a>).</p>
  </li>
</ul>

<p>New features:</p>

<ul>
  <li>
    <p>Google Cloud Storage (GCS) File System support is now available in the Python bindings (<a href="https://issues.apache.org/jira/browse/ARROW-14892">ARROW-14892</a>).</p>
  </li>
  <li>
    <p>The <code class="language-plaintext highlighter-rouge">Table.filter()</code> method now supports passing an expression in addition to a boolean array (<a href="https://issues.apache.org/jira/browse/ARROW-16469">ARROW-16469</a>).</p>
  </li>
  <li>
    <p>When implementing extension types in Python, it is now possible to also customize which Python scalar gets returned (in <code class="language-plaintext highlighter-rouge">Array.to_pylist()</code> or <code class="language-plaintext highlighter-rouge">Scalar.as_py()</code>) by subclassing <code class="language-plaintext highlighter-rouge">ExtensionScalar</code> (<a href="https://issues.apache.org/jira/browse/ARROW-13612">ARROW-13612</a>, (<a href="https://issues.apache.org/jira/browse/ARROW-17065">ARROW-17065</a>)).</p>
  </li>
  <li>
    <p>It is now possible to register User Defined Functions (UDF) for scalar functions using <code class="language-plaintext highlighter-rouge">register_scalar_function</code> (<a href="https://issues.apache.org/jira/browse/ARROW-15639">ARROW-15639</a>).</p>
  </li>
  <li>
    <p>Basic support for consuming a Substrait plan has been exposed in Python as <code class="language-plaintext highlighter-rouge">pyarrow.substrait.run_query</code> (<a href="https://issues.apache.org/jira/browse/ARROW-15779">ARROW-15779</a>).</p>
  </li>
  <li>
    <p>The <code class="language-plaintext highlighter-rouge">cast</code> method and compute kernel now exposes the fine grained options in addition to safe/unsafe casting (<a href="https://issues.apache.org/jira/browse/ARROW-15365">ARROW-15365</a>).</p>
  </li>
</ul>

<p>In addition, this release includes several bug fixes and documention improvements (such as expanded examples in docstrings (<a href="https://issues.apache.org/jira/browse/ARROW-16091">ARROW-16091</a>)).</p>

<p>Further, the Python bindings benefit from improvements in the C++ library
(e.g. new compute functions); see the C++ notes above for additional details.</p>

<h2 id="r-notes">R notes</h2>

<p>Highlights include several new <code class="language-plaintext highlighter-rouge">dplyr</code> verbs, including <code class="language-plaintext highlighter-rouge">glimpse()</code> and <code class="language-plaintext highlighter-rouge">union_all()</code>, as well as many more datetime functions from <code class="language-plaintext highlighter-rouge">lubridate</code>. There is also experimental support for user-defined scalar functions in the query engine, and most packages include native support for datasets in Google Cloud Storage (opt-in in the Linux full source build).</p>

<p>For more on what’s in the 9.0.0 R package, see the <a href="/docs/r/news/">R changelog</a>.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<p>FlightSQL is now supported but there are minimum features for now.</p>

<p>More Flight features are now supported.</p>

<h3 id="ruby">Ruby</h3>

<p><code class="language-plaintext highlighter-rouge">Enumerable</code> compatible methods such as <code class="language-plaintext highlighter-rouge">#min</code> and <code class="language-plaintext highlighter-rouge">#max</code> on <code class="language-plaintext highlighter-rouge">Arrow::Array</code>, <code class="language-plaintext highlighter-rouge">Arrow::ChunkedArray</code> and <code class="language-plaintext highlighter-rouge">Arrow::Column</code> are implemented by C++’s <a href="/docs/cpp/compute.html">compute functions</a>. This improves performance. (<a href="https://issues.apache.org/jira/browse/ARROW-15222">ARROW-15222</a>)</p>

<p>This release fixed some memory leaks. (<a href="https://issues.apache.org/jira/browse/ARROW-14790">ARROW-14790</a>)</p>

<p>This release improved support for interval type arrays such as <code class="language-plaintext highlighter-rouge">Arrow::MonthIntervalArray</code>. (<a href="https://issues.apache.org/jira/browse/ARROW-16206">ARROW-16206</a>)</p>

<p>This release improved auto data type conversion. (<a href="https://issues.apache.org/jira/browse/ARROW-16874">ARROW-16874</a>)</p>

<h3 id="c-glib">C GLib</h3>

<p>Vala is now supported. (<a href="https://issues.apache.org/jira/browse/ARROW-15671">ARROW-15671</a>). See <a href="https://github.com/apache/arrow/tree/apache-arrow-9.0.0/c_glib/example/vala"><code class="language-plaintext highlighter-rouge">c_glib/example/vala/</code></a> for examples.</p>

<p><code class="language-plaintext highlighter-rouge">GArrowQuantil
eOptions</code> is added. (<a href="https://issues.apache.org/jira/browse/ARROW-16623">ARROW-16623</a>)</p>

<h2 id="rust-notes">Rust notes</h2>

<p>The Rust projects have moved to separate repositories outside the
main Arrow monorepo. For notes on the 19.0.0 release of the Rust
implementation, see the <a href="https://github.com/apache/arrow-rs/blob/19.0.0/CHANGELOG.md">Arrow Rust changelog</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 9.0.0 release. This covers over 3 months of development work and includes 509 resolved issues from 114 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bug fixes and improvements have been made: we refer you to the complete changelog. Community Since the 8.0.0 release, Dewey Dunnington, Alenka Frim and Rok Mihevc have been invited to be committers. Thanks for your contributions and participation in the project! Columnar Format Notes Arrow Flight RPC notes Arrow Flight is now available in MacOS M1 Python wheels (ARROW-16779). Arrow Flight SQL is now buildable on Windows (ARROW-16902). Ruby now exposes more of the Flight and Flight SQL APIs (various JIRAs). Linux packages notes AlmaLinux 9 is now supported. (ARROW-16745) AmazonLinux 2 aarch64 is now supported. (ARROW-16477) C++ notes STL-like iteration is now provided over chunked arrays (ARROW-602). Compute The C++ compute and execution engine is now officially named “Acero”, though its C++ namespaces have not changed. New light-weight data holder abstractions have been introduced in order to reduce the overhead of invoking compute functions and kernels, especially at the small data sizes desirable for efficient parallelization (typically L1- or L2-sized). Specifically, the non-owning ArraySpan and ExecSpan structures have internally superseded the much heavier ExecBatch, which is still supported for compatibility at the API level (ARROW-16756, ARROW-16824, ARROW-16852). In a similar vein, the ValueDescr class was removed and ScalarKernel implementations now always receive at least one non-scalar input, removing the special case where a ScalarKernel needs to output a scalar rather than an array. The higher-level compute APIs still allow executing a scalar function over all-scalar inputs; but those scalars are internally broadcasted to 1-element arrays so as to simplify kernel implementation (ARROW-16757). Some performance improvements were made to the hash join node. These changes do not require additional configuration. The hash join exec node has been improved to more efficiently use CPU cache and make better use of available vectorization hardware (ARROW-14182). Some plans containing a sequence of hash join operators will now use bloom filters to eliminate rows earlier in the plan, reducing the overall CPU cost of the plan (ARROW-15498). Timestamp comparison is now supported (ARROW-16425). A cumulative sum function is implemented over numeric inputs (ARROW-13530). Note that this is a vector function so cannot be used in an Acero ExecPlan. A rank vector kernel has been added (ARROW-16234). Temporal rounding functions received additional options to control how rounding is done (ARROW-14821). Improper computation of the “mode” function on boolean input was fixed (ARROW-17096). Function registries can now be nested (ARROW-16677). Dataset The autogenerate_column_names option for CSV reading is now handled correctly (ARROW-16436). Fix InMemoryDataset::ReplaceSchema to actually replace the schema (ARROW-16085). Fix FilenamePartitioning to properly support null values (ARROW-16302). Filesystem A number of bug fixes and improvements were made to the Google Cloud Storage filesystem implementation (ARROW-14892). By default, the S3 filesystem implementation does not create or drop buckets anymore (ARROW-15906). This is a compatibility-breaking change intended to prevent user errors from having potentially catastrophic consequences. Options have been added to restore the previous behavior if necessary. Parquet The default Parquet version is now 2.4 for writing, enabling use of more recent logical types by default (ARROW-12203). Non-nullable fields are now handled correctly by the Parquet reader (ARROW-16116). Reading encrypted files should now be thread-safe (ARROW-14114). Statistics equality now works correctly with minmax (ARROW-16487). The minimum Thrift version required for building is now 0.13 (ARROW-16721). The Thrift deserialization limits can now be configured to accommodate for data files with very large metadata (ARROW-16546). Substrait The Substrait spec has been updated to 0.6.0 (ARROW-16816). In addition, a larger subset of the Substrait specification is now supported (ARROW-15587, ARROW-15590, ARROW-15901, ARROW-16657, ARROW-15591). C# notes New Features Added support for Time32Array and Time64Array (ARROW-16660) Bug Fixes When using TableFromRecordBatches, the resulting table columns have no data array. (ARROW-13129) Fix intermittent test failures due to async memory management bug. (ARROW-16978) Go notes Security Updated testify dependency to address CVE-2022-28948. (ARROW-16759) (This was also backported to previous versions and released as patch versions v6.0.2, v7.0.1, and v8.0.1) Arrow New Features Dictionary Scalars are now available (ARROW-16323) Introduced a DictionaryUnifier object along with functions for unifying Chunked Arrays and Tables (ARROW-16324) New CSV examples added to documentation to demonstrate error handling (ARROW-16450) CSV Reader now supports arrow.TimestampType (ARROW-16504) JSON parsing for Temporal Types now allow passing numeric values in addition to strings for parsing. Timezones will be properly parsed if they exist in the string and a function was added to retrieve a time.Location object from a TimestampType (ARROW-16551) New utilities added to decimal128 for rescaling and easy conversion to and from float32/float64 (ARROW-16552) Arrow DataType interface now has a LayoutMethod which returns the physical layout of the given datatype such as the number of buffers, types, etc. This matches the behavior of the layout() methods in C++ for data types. (ARROW-16556) Added a SliceBuffer function to the memory package to allow better re-using of memory across buffer objects (ARROW-16557) Dictionary Arrays can now be concatenated using array.Concatenate (ARROW-17095) Bug Fixes ipc.FileReader now properly uses the memory.Allocator interface (ARROW-16002) Addressed issue with Integration tests between Go and Java (ARROW-16441) RecordBuilder.UnmarshalJSON now properly ignores extra unknown fields rather than panicking (ARROW-16456) StructBuilder.UnmarshalJSON will no longer fail and panic when Nullable fields are missing (ARROW-16502) ipc.Reader no longer silently accepts string columns with invalid offsets, preventing unexpected panics later when writing or accessing the resulting arrays. (ARROW-16831) Arrow CSV reader no longer clobbers its reported errors and properly surfaces them (ARROW-16926) Parquet New Features The CreatedBy version string for the Parquet writer will now correctly reflect the library version, and will be updated by the release scripts (ARROW-16484) Parquet bit_packing functions now have ARM64 NEON implementations for performance (ARROW-16486) It is now possible to customize the root node in the Parquet writer instead of hardcoding it to be named “schema” with a repetition type of Repeated. This was needed to allow producing files similar to Apache Spark where the root node has a repetition type of Required. It still defaults to the spec definition of Repeated. (ARROW-16561) parquet_reader CLI mainprog has been enhanced to dump values out as JSON and CSV along with setting an output file instead of just dumping to the terminal. (ARROW-16934) Bug Fixes Fixed a memory leak with Parquet page reading (ARROW-16473) Parquet Reader properly parallelizes column reads when the parallel option is set to true. (ARROW-16530) Fixed bug in the Bool decoder for plain encoding (ARROW-16563) Fixed a bug in the Parquet bool column reader where it failed to properly skip rows (ARROW-16638) Fixed the flakey travis ARM64 builds by reducing the size of a test case in the pqarrow unit tests to reduce the memory usage for the tests. (ARROW-16669) Parquet writer now properly handles writing arrow.NULL type arrays (ARROW-16749) Column level dictionary encoding configuration for Parquet writing now correctly respects the input value (ARROW-16813) Memory leak in DeltaByteArray encoding fixed (ARROW-16983) Java notes New Features Allow overriding column nullability in arrow-jdbc (#13558) Enable skip BOUNDS_CHECKING with setBytes and getBytes of ArrowBuf (#13161) Initialize JNI components on use instead of statically (#13146) Provide explicit JDBC column type mapping (#13166) Allow duplicated field names in Java C data interface (#13247) Improve and document StackTrace (#12656) Keep more context when marshaling errors through JNI (#13246) Make RoundingMode configurable to handle inconsistent scale in BigDecimals (#13433) Improve Java dev experience with IntelliJ (#13017) Implement ArrowArrayStream (#13465)) Bug Fixes Fix variable-width vectors in integration JSON writer (#13676) Handle empty JDBC ResultSet (#13049) Fix hasNext() in ArrowVectorIterator (#13107) Fix ArrayConsumer when using ArrowVectorIterator (#12692) Update Gandiva Protobuf library to enable builds on Apple M1 (#13121) Patch dataset module testing failure with JSE11+ (#13200) Don’t duplicate generated Protobuf classes between flight-core and flight-sql (#13596) JavaScript notes Fix error iterating tables with no batches (ARROW-16371) Handle case where tableFromIPC input is an async RecordBatchReader (ARROW-16704) Python notes Compatibility notes: PyArrow now requires Python &gt;= 3.7 (ARROW-16474). The default behaviour regarding memory mapping has changed in several APIs (reading of Feather or Parquet files, IPC RecordBatchFileReader and RecordBatchStreamReader) to disable memory mapping by default (ARROW-16382). The default Parquet version is now 2.4 for writing, enabling use of more recent logical types by default such as unsigned integers (ARROW-12203). One can specify version="2.6" to also enable support for nanosecond timestamps. Use version="1.0" to restore the old behaviour and maximizes file compatibility. Some deprecated APIs (deprecated at least since pyarrow 1.0.0) have been removed: IPC methods in the top-level namespace, the Value scalar classes and the pyarrow.compat module (ARROW-17010). New features: Google Cloud Storage (GCS) File System support is now available in the Python bindings (ARROW-14892). The Table.filter() method now supports passing an expression in addition to a boolean array (ARROW-16469). When implementing extension types in Python, it is now possible to also customize which Python scalar gets returned (in Array.to_pylist() or Scalar.as_py()) by subclassing ExtensionScalar (ARROW-13612, (ARROW-17065)). It is now possible to register User Defined Functions (UDF) for scalar functions using register_scalar_function (ARROW-15639). Basic support for consuming a Substrait plan has been exposed in Python as pyarrow.substrait.run_query (ARROW-15779). The cast method and compute kernel now exposes the fine grained options in addition to safe/unsafe casting (ARROW-15365). In addition, this release includes several bug fixes and documention improvements (such as expanded examples in docstrings (ARROW-16091)). Further, the Python bindings benefit from improvements in the C++ library (e.g. new compute functions); see the C++ notes above for additional details. R notes Highlights include several new dplyr verbs, including glimpse() and union_all(), as well as many more datetime functions from lubridate. There is also experimental support for user-defined scalar functions in the query engine, and most packages include native support for datasets in Google Cloud Storage (opt-in in the Linux full source build). For more on what’s in the 9.0.0 R package, see the R changelog. Ruby and C GLib notes FlightSQL is now supported but there are minimum features for now. More Flight features are now supported. Ruby Enumerable compatible methods such as #min and #max on Arrow::Array, Arrow::ChunkedArray and Arrow::Column are implemented by C++’s compute functions. This improves performance. (ARROW-15222) This release fixed some memory leaks. (ARROW-14790) This release improved support for interval type arrays such as Arrow::MonthIntervalArray. (ARROW-16206) This release improved auto data type conversion. (ARROW-16874) C GLib Vala is now supported. (ARROW-15671). See c_glib/example/vala/ for examples. GArrowQuantil eOptions is added. (ARROW-16623) Rust notes The Rust projects have moved to separate repositories outside the main Arrow monorepo. For notes on the 19.0.0 release of the Rust implementation, see the Arrow Rust changelog.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>