<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://arrow.apache.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arrow.apache.org/" rel="alternate" type="text/html" /><updated>2022-10-05T14:38:27-04:00</updated><id>https://arrow.apache.org/feed.xml</id><title type="html">Apache Arrow</title><subtitle>Apache Arrow is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, C++, C#, Go, Java, JavaScript, MATLAB, Python, R, Ruby, and Rust.</subtitle><entry><title type="html"></title><link href="https://arrow.apache.org/blog/2022/10/05/2022-10-01-arrow-parquet-encoding-part-1/" rel="alternate" type="text/html" title="" /><published>2022-10-05T14:38:27-04:00</published><updated>2022-10-05T14:38:27-04:00</updated><id>https://arrow.apache.org/blog/2022/10/05/2022-10-01-arrow-parquet-encoding-part-1</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/10/05/2022-10-01-arrow-parquet-encoding-part-1/"><![CDATA[<!--

-->

<h2 id="introduction">Introduction</h2>

<p>We recently completed a long-running project within <a href="https://github.com/apache/arrow-rs">Rust Apache Arrow</a> to complete support for reading and writing arbitrarily nested Parquet and Arrow schemas. This is a complex topic, and we encountered a lack of approachable technical information, and thus wrote this blog to share our learnings with the community.</p>

<p><a href="https://arrow.apache.org/">Apache Arrow</a> is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. <a href="https://parquet.apache.org/">Apache Parquet</a> is an open, column-oriented data file format designed for very efficient data encoding and retrieval.</p>

<p>It is increasingly common for analytic systems to use Arrow to process data stored in Parquet files, and therefore fast, efficient, and correct translation between them is a key building block.</p>

<p>Historically analytic processing primarily focused on querying data with a tabular schema, where there are a fixed number of columns, and each row contains a single value for each column. However, with the increasing adoption of structured document formats such as XML, JSON, etc…, only supporting tabular schema can be frustrating for users, as it necessitates often non-trivial data transformation to first flatten the document data.</p>

<p>As of version <a href="https://crates.io/crates/arrow/20.0.0">20.0.0</a>, released in August 2022, the Rust Arrow implementation for reading structured types is feature complete. Instructions for getting started can be found <a href="https://docs.rs/parquet/latest/parquet/arrow/index.html">here</a> and feel free to raise any issues on our <a href="https://github.com/apache/arrow-rs/issues">bugtracker</a>.</p>

<p>In this series we will explain how Parquet and Arrow represent nested data, highlighting the similarities and differences between them, and giving a flavor of the practicalities of converting between the formats.</p>

<h2 id="columnar-vs-record-oriented">Columnar vs Record-Oriented</h2>

<p>First, it is necessary to take a step back and discuss the difference between columnar and record-oriented data formats. In a record oriented data format, such as newline-delimited JSON (NDJSON), all the values for a given record are stored contiguously.</p>

<p>For example</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="nl">"Column1"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nl">"Column2"</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">}</span><span class="w">
</span><span class="p">{</span><span class="nl">"Column1"</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="nl">"Column2"</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="nl">"Column3"</span><span class="p">:</span><span class="w"> </span><span class="mi">5</span><span class="p">}</span><span class="w">
</span><span class="p">{</span><span class="nl">"Column1"</span><span class="p">:</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="nl">"Column2"</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="nl">"Column3"</span><span class="p">:</span><span class="w"> </span><span class="mi">5</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>In a columnar representation, the data for a given column is instead stored contiguously</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Column1: [1, 3, 5]
Column2: [2, 4, 4]
Column3: [null, 5, 5]
</code></pre></div></div>

<p>Aside from potentially yielding better data compression, a columnar layout can dramatically improve performance of certain queries. This is because laying data out contiguously in memory allows both the compiler and CPU to better exploit opportunities for parallelism. The specifics of <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data">SIMD</a> and <a href="https://en.wikipedia.org/wiki/Instruction-level_parallelism">ILP</a> are well beyond the scope of this post, but the important takeaway is that processing large blocks of data without intervening conditional branches has substantial performance benefits.</p>

<h2 id="parquet-vs-arrow">Parquet vs Arrow</h2>
<p>Parquet and Arrow are complementary technologies, and they make some different design tradeoffs. In particular, Parquet is a storage format designed for maximum space efficiency, whereas Arrow is an in-memory format intended for operation by vectorized computational kernels.</p>

<p>The major distinction is that Arrow provides <code class="language-plaintext highlighter-rouge">O(1)</code> random access lookups to any array index, whilst Parquet does not. In particular, Parquet uses <a href="https://akshays-blog.medium.com/wrapping-head-around-repetition-and-definition-levels-in-dremel-powering-bigquery-c1a33c9695da">dremel record shredding</a>, <a href="https://github.com/apache/parquet-format/blob/master/Encodings.md">variable length encoding schemes</a>, and <a href="https://github.com/apache/parquet-format/blob/master/Compression.md">block compression</a> to drastically reduce the data size, but these techniques come at the loss of performant random access lookups.</p>

<p>A common pattern that plays to each technologies strengths, is to stream data from a compressed representation, such as Parquet, in thousand row batches in the Arrow format, process these batches individually, and accumulate the results in a more compressed representation. This benefits from the ability to efficiently perform computations on Arrow data, whilst keeping memory requirements in check, and allowing the computation kernels to be agnostic to the encodings of the source and destination.</p>

<p><strong>Arrow is primarily an in-memory format, whereas Parquet is a storage format.</strong></p>

<h2 id="non-nullable-primitive-column">Non-Nullable Primitive Column</h2>

<p>Let us start with the simplest case of a non-nullable list of 32-bit signed integers.</p>

<p>In Arrow this would be represented as a <code class="language-plaintext highlighter-rouge">PrimitiveArray</code>, which would store them contiguously in memory</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────┐
│  1  │
├─────┤
│  2  │
├─────┤
│  3  │
├─────┤
│  4  │
└─────┘
Values
</code></pre></div></div>

<p>Parquet has multiple <a href="https://parquet.apache.org/docs/file-format/data-pages/encodings/">different encodings</a> that may be used for integer types, the exact details of which are beyond the scope of this post. Broadly speaking the data will be stored in one or more <a href="https://parquet.apache.org/docs/file-format/data-pages/"><em>DataPage</em></a>s containing the integers in an encoded form</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────┐
│  1  │
├─────┤
|  2  │
├─────┤
│  3  │
├─────┤
│  4  │
└─────┘
Values
</code></pre></div></div>

<h1 id="nullable-primitive-column">Nullable Primitive Column</h1>

<p>Now let us consider the case of a nullable column, where some of the values might have the special sentinel value <code class="language-plaintext highlighter-rouge">NULL</code> that designates “this value is unknown”.</p>

<p>In Arrow, nulls are stored separately from the values in the form of a <a href="https://arrow.apache.org/docs/format/Columnar.html#validity-bitmaps">validity bitmask</a>, with arbitrary data in the corresponding positions in the values buffer. This space efficient encoding means that the entire validity mask for the following example is stored using 5 bits</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────┐   ┌─────┐
│  1  │   │  1  │
├─────┤   ├─────┤
│  0  │   │ ??  │
├─────┤   ├─────┤
│  1  │   │  3  │
├─────┤   ├─────┤
│  1  │   │  4  │
├─────┤   ├─────┤
│  0  │   │ ??  │
└─────┘   └─────┘
Validity   Values
</code></pre></div></div>

<p>In Parquet the validity information is also stored separately from the values, however, instead of being encoded as a validity bitmask it is encoded as a list of 16-bit integers called <em>definition levels</em>. Like other data in Parquet, these integer definition levels are stored using high efficiency encoding, and will be expanded upon in the next post, but for now a definition level of <code class="language-plaintext highlighter-rouge">1</code> indicates a valid value, and <code class="language-plaintext highlighter-rouge">0</code> a null value. Unlike Arrow, nulls are not encoded in the list of values</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────┐    ┌─────┐
│  1  │    │  1  │
├─────┤    ├─────┤
│  0  │    │  3  │
├─────┤    ├─────┤
│  1  │    │  4  │
├─────┤    └─────┘
│  1  │
├─────┤
│  0  │
└─────┘
Definition  Values
 Levels
</code></pre></div></div>

<h2 id="next-up-nested-and-hierarchal-data">Next up: Nested and Hierarchal Data</h2>

<p>Armed with the foundational understanding of how Arrow and Parquet store nullability / definition differently we are ready to move on to more complex nested types, which you can read about in our upcoming blog post on the topic <!-- I propose to update this text with a link when when we have published the next blog -->.</p>]]></content><author><name></name></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 9.0.0 Release</title><link href="https://arrow.apache.org/blog/2022/08/16/9.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 9.0.0 Release" /><published>2022-08-16T00:00:00-04:00</published><updated>2022-08-16T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/08/16/9.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/08/16/9.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 9.0.0 release. This covers
over 3 months of development work and includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%209.0.0"><strong>509 resolved issues</strong></a>
from <a href="/release/9.0.0.html#contributors"><strong>114 distinct contributors</strong></a>. See the Install Page to learn how to
get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bug fixes and improvements have been made: we refer
you to the <a href="/release/9.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 8.0.0 release, Dewey Dunnington, Alenka Frim and Rok Mihevc
have been invited to be committers.
Thanks for your contributions and participation in the project!</p>

<h2 id="columnar-format-notes">Columnar Format Notes</h2>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>

<p>Arrow Flight is now available in MacOS M1 Python wheels (<a href="https://issues.apache.org/jira/browse/ARROW-16779">ARROW-16779</a>).
Arrow Flight SQL is now buildable on Windows (<a href="https://issues.apache.org/jira/browse/ARROW-16902">ARROW-16902</a>).
Ruby now exposes more of the Flight and Flight SQL APIs (various JIRAs).</p>

<h2 id="linux-packages-notes">Linux packages notes</h2>

<p>AlmaLinux 9 is now supported. (<a href="https://issues.apache.org/jira/browse/ARROW-16745">ARROW-16745</a>)</p>

<p>AmazonLinux 2 aarch64 is now supported. (<a href="https://issues.apache.org/jira/browse/ARROW-16477">ARROW-16477</a>)</p>

<h2 id="c-notes">C++ notes</h2>

<p>STL-like iteration is now provided over chunked arrays (<a href="https://issues.apache.org/jira/browse/ARROW-602">ARROW-602</a>).</p>

<h3 id="compute">Compute</h3>

<p>The C++ compute and execution engine is now officially named “Acero”, though
its C++ namespaces have not changed.</p>

<p>New light-weight data holder abstractions have been introduced in order
to reduce the overhead of invoking compute functions and kernels, especially
at the small data sizes desirable for efficient parallelization (typically
L1- or L2-sized).  Specifically, the non-owning <code class="language-plaintext highlighter-rouge">ArraySpan</code> and <code class="language-plaintext highlighter-rouge">ExecSpan</code>
structures have internally superseded the much heavier <code class="language-plaintext highlighter-rouge">ExecBatch</code>, which
is still supported for compatibility at the API level
(<a href="https://issues.apache.org/jira/browse/ARROW-16756">ARROW-16756</a>, <a href="https://issues.apache.org/jira/browse/ARROW-16824">ARROW-16824</a>, <a href="https://issues.apache.org/jira/browse/ARROW-16852">ARROW-16852</a>).</p>

<p>In a similar vein, the <code class="language-plaintext highlighter-rouge">ValueDescr</code> class was removed and <code class="language-plaintext highlighter-rouge">ScalarKernel</code>
implementations now always receive at least one non-scalar input, removing
the special case where a <code class="language-plaintext highlighter-rouge">ScalarKernel</code> needs to output a scalar rather than
an array. The higher-level compute APIs still allow executing a scalar function
over all-scalar inputs; but those scalars are internally broadcasted to
1-element arrays so as to simplify kernel implementation (<a href="https://issues.apache.org/jira/browse/ARROW-16757">ARROW-16757</a>).</p>

<p>Some performance improvements were made to the hash join node.  These changes
do not require additional configuration.  The hash join exec node has been
improved to more efficiently use CPU cache and make better use of available
vectorization hardware (<a href="https://issues.apache.org/jira/browse/ARROW-14182">ARROW-14182</a>).</p>

<p>Some plans containing a sequence of hash join operators will now use bloom
filters to eliminate rows earlier in the plan, reducing the overall CPU
cost of the plan (<a href="https://issues.apache.org/jira/browse/ARROW-15498">ARROW-15498</a>).</p>

<p>Timestamp comparison is now supported (<a href="https://issues.apache.org/jira/browse/ARROW-16425">ARROW-16425</a>).</p>

<p>A cumulative sum function is implemented over numeric inputs (<a href="https://issues.apache.org/jira/browse/ARROW-13530">ARROW-13530</a>). Note that this is a vector
function so cannot be used in an Acero ExecPlan.</p>

<p>A rank vector kernel has been added (<a href="https://issues.apache.org/jira/browse/ARROW-16234">ARROW-16234</a>).</p>

<p>Temporal rounding functions received additional options to control how
rounding is done (<a href="https://issues.apache.org/jira/browse/ARROW-14821">ARROW-14821</a>).</p>

<p>Improper computation of the “mode” function on boolean input was fixed
(<a href="https://issues.apache.org/jira/browse/ARROW-17096">ARROW-17096</a>).</p>

<p>Function registries can now be nested (<a href="https://issues.apache.org/jira/browse/ARROW-16677">ARROW-16677</a>).</p>

<h3 id="dataset">Dataset</h3>

<p>The <code class="language-plaintext highlighter-rouge">autogenerate_column_names</code> option for CSV reading is now handled correctly
(<a href="https://issues.apache.org/jira/browse/ARROW-16436">ARROW-16436</a>).</p>

<p>Fix <code class="language-plaintext highlighter-rouge">InMemoryDataset::ReplaceSchema</code> to actually replace the schema
(<a href="https://issues.apache.org/jira/browse/ARROW-16085">ARROW-16085</a>).</p>

<p>Fix <code class="language-plaintext highlighter-rouge">FilenamePartitioning</code> to properly support null values (<a href="https://issues.apache.org/jira/browse/ARROW-16302">ARROW-16302</a>).</p>

<h3 id="filesystem">Filesystem</h3>

<p>A number of bug fixes and improvements were made to the Google Cloud Storage
filesystem implementation (<a href="https://issues.apache.org/jira/browse/ARROW-14892">ARROW-14892</a>).</p>

<p>By default, the S3 filesystem implementation does not create or drop buckets
anymore (<a href="https://issues.apache.org/jira/browse/ARROW-15906">ARROW-15906</a>). This is a compatibility-breaking change intended
to prevent user errors from having potentially catastrophic consequences.
Options have been added to restore the previous behavior if necessary.</p>

<h3 id="parquet">Parquet</h3>

<p>The default Parquet version is now 2.4 for writing, enabling use of
more recent logical types by default (<a href="https://issues.apache.org/jira/browse/ARROW-12203">ARROW-12203</a>).</p>

<p>Non-nullable fields are now handled correctly by the Parquet reader
(<a href="https://issues.apache.org/jira/browse/ARROW-16116">ARROW-16116</a>).</p>

<p>Reading encrypted files should now be thread-safe (<a href="https://issues.apache.org/jira/browse/ARROW-14114">ARROW-14114</a>).</p>

<p>Statistics equality now works correctly with minmax (<a href="https://issues.apache.org/jira/browse/ARROW-16487">ARROW-16487</a>).</p>

<p>The minimum Thrift version required for building is now 0.13 (<a href="https://issues.apache.org/jira/browse/ARROW-16721">ARROW-16721</a>).</p>

<p>The Thrift deserialization limits can now be configured to accommodate for
data files with very large metadata (<a href="https://issues.apache.org/jira/browse/ARROW-16546">ARROW-16546</a>).</p>

<h3 id="substrait">Substrait</h3>

<p>The Substrait spec has been updated to 0.6.0 (<a href="https://issues.apache.org/jira/browse/ARROW-16816">ARROW-16816</a>). In addition, a
larger subset of the Substrait specification is now supported (<a href="https://issues.apache.org/jira/browse/ARROW-15587">ARROW-15587</a>,
<a href="https://issues.apache.org/jira/browse/ARROW-15590">ARROW-15590</a>,
<a href="https://issues.apache.org/jira/browse/ARROW-15901">ARROW-15901</a>,
<a href="https://issues.apache.org/jira/browse/ARROW-16657">ARROW-16657</a>,
<a href="https://issues.apache.org/jira/browse/ARROW-15591">ARROW-15591</a>).</p>

<h2 id="c-notes-1">C# notes</h2>

<h4 id="new-features">New Features</h4>

<ul>
  <li>Added support for Time32Array and Time64Array (<a href="https://github.com/apache/arrow/pull/13279">ARROW-16660</a>)</li>
</ul>

<h4 id="bug-fixes">Bug Fixes</h4>

<ul>
  <li>When using TableFromRecordBatches, the resulting table columns have no data array. (<a href="https://github.com/apache/arrow/pull/10562">ARROW-13129</a>)</li>
  <li>Fix intermittent test failures due to async memory management bug. (<a href="https://github.com/apache/arrow/pull/13573">ARROW-16978</a>)</li>
</ul>

<h2 id="go-notes">Go notes</h2>

<h3 id="security">Security</h3>

<ul>
  <li>Updated testify dependency to address CVE-2022-28948. (<a href="https://issues.apache.org/jira/browse/ARROW-16759">ARROW-16759</a>) (This was also backported to previous versions and released as patch versions v6.0.2, v7.0.1, and v8.0.1)</li>
</ul>

<h3 id="arrow">Arrow</h3>

<h4 id="new-features-1">New Features</h4>

<ul>
  <li>Dictionary Scalars are now available (<a href="https://issues.apache.org/jira/browse/ARROW-16323">ARROW-16323</a>)</li>
  <li>Introduced a DictionaryUnifier object along with functions for unifying Chunked Arrays and Tables (<a href="https://issues.apache.org/jira/browse/ARROW-16324">ARROW-16324</a>)</li>
  <li>New CSV examples added to documentation to demonstrate error handling (<a href="https://issues.apache.org/jira/browse/ARROW-16450">ARROW-16450</a>)</li>
  <li>CSV Reader now supports arrow.TimestampType (<a href="https://issues.apache.org/jira/browse/ARROW-16504">ARROW-16504</a>)</li>
  <li>JSON parsing for Temporal Types now allow passing numeric values in addition to strings for parsing. Timezones will be properly parsed if they exist in the string and a function was added to retrieve a time.Location object from a TimestampType (<a href="https://issues.apache.org/jira/browse/ARROW-16551">ARROW-16551</a>)</li>
  <li>New utilities added to decimal128 for rescaling and easy conversion to and from float32/float64 (<a href="https://issues.apache.org/jira/browse/ARROW-16552">ARROW-16552</a>)</li>
  <li>Arrow DataType interface now has a LayoutMethod which returns the physical layout of the given datatype such as the number of buffers, types, etc. This matches the behavior of the layout() methods in C++ for data types. (<a href="https://issues.apache.org/jira/browse/ARROW-16556">ARROW-16556</a>)</li>
  <li>Added a SliceBuffer function to the memory package to allow better re-using of memory across buffer objects (<a href="https://issues.apache.org/jira/browse/ARROW-16557">ARROW-16557</a>)</li>
  <li>Dictionary Arrays can now be concatenated using array.Concatenate (<a href="https://issues.apache.org/jira/browse/ARROW-17095">ARROW-17095</a>)</li>
</ul>

<h4 id="bug-fixes-1">Bug Fixes</h4>

<ul>
  <li>ipc.FileReader now properly uses the memory.Allocator interface (<a href="https://issues.apache.org/jira/browse/ARROW-16002">ARROW-16002</a>)</li>
  <li>Addressed issue with Integration tests between Go and Java (<a href="https://issues.apache.org/jira/browse/ARROW-16441">ARROW-16441</a>)</li>
  <li>RecordBuilder.UnmarshalJSON now properly ignores extra unknown fields rather than panicking (<a href="https://issues.apache.org/jira/browse/ARROW-16456">ARROW-16456</a>)</li>
  <li>StructBuilder.UnmarshalJSON will no longer fail and panic when Nullable fields are missing (<a href="https://issues.apache.org/jira/browse/ARROW-16502">ARROW-16502</a>)</li>
  <li>ipc.Reader no longer silently accepts string columns with invalid offsets, preventing unexpected panics later when writing or accessing the resulting arrays. (<a href="https://issues.apache.org/jira/browse/ARROW-16831">ARROW-16831</a>)</li>
  <li>Arrow CSV reader no longer clobbers its reported errors and properly surfaces them (<a href="https://issues.apache.org/jira/browse/ARROW-16926">ARROW-16926</a>)</li>
</ul>

<h3 id="parquet-1">Parquet</h3>

<h4 id="new-features-2">New Features</h4>

<ul>
  <li>The CreatedBy version string for the Parquet writer will now correctly reflect the library version, and will be updated by the release scripts (<a href="https://issues.apache.org/jira/browse/ARROW-16484">ARROW-16484</a>)</li>
  <li>Parquet bit_packing functions now have ARM64 NEON implementations for performance (<a href="https://issues.apache.org/jira/browse/ARROW-16486">ARROW-16486</a>)</li>
  <li>It is now possible to customize the root node in the Parquet writer instead of hardcoding it to be named “schema” with a repetition type of Repeated. This was needed to allow producing files similar to Apache Spark where the root node has a repetition type of Required. It still defaults to the spec definition of Repeated. (<a href="https://issues.apache.org/jira/browse/ARROW-16561">ARROW-16561</a>)</li>
  <li>parquet_reader CLI mainprog has been enhanced to dump values out as JSON and CSV along with setting an output file instead of just dumping to the terminal. (<a href="https://issues.apache.org/jira/browse/ARROW-16934">ARROW-16934</a>)</li>
</ul>

<h4 id="bug-fixes-2">Bug Fixes</h4>

<ul>
  <li>Fixed a memory leak with Parquet page reading (<a href="https://issues.apache.org/jira/browse/ARROW-16473">ARROW-16473</a>)</li>
  <li>Parquet Reader properly parallelizes column reads when the parallel option is set to true. (<a href="https://issues.apache.org/jira/browse/ARROW-16530">ARROW-16530</a>)</li>
  <li>Fixed bug in the Bool decoder for plain encoding (<a href="https://issues.apache.org/jira/browse/ARROW-16563">ARROW-16563</a>)</li>
  <li>Fixed a bug in the Parquet bool column reader where it failed to properly skip rows (<a href="https://issues.apache.org/jira/browse/ARROW-16638">ARROW-16638</a>)</li>
  <li>Fixed the flakey travis ARM64 builds by reducing the size of a test case in the pqarrow unit tests to reduce the memory usage for the tests. (<a href="https://issues.apache.org/jira/browse/ARROW-16669">ARROW-16669</a>)</li>
  <li>Parquet writer now properly handles writing arrow.NULL type arrays (<a href="https://issues.apache.org/jira/browse/ARROW-16749">ARROW-16749</a>)</li>
  <li>Column level dictionary encoding configuration for Parquet writing now correctly respects the input value (<a href="https://issues.apache.org/jira/browse/ARROW-16813">ARROW-16813</a>)</li>
  <li>Memory leak in DeltaByteArray encoding fixed (<a href="https://issues.apache.org/jira/browse/ARROW-16983">ARROW-16983</a>)</li>
</ul>

<h2 id="java-notes">Java notes</h2>
<h4 id="new-features-3">New Features</h4>
<ul>
  <li>Allow overriding column nullability in arrow-jdbc (<a href="https://github.com/apache/arrow/pull/13558">#13558</a>)</li>
  <li>Enable skip BOUNDS_CHECKING with setBytes and getBytes of ArrowBuf (<a href="https://github.com/apache/arrow/pull/13161">#13161</a>)</li>
  <li>Initialize JNI components on use instead of statically (<a href="https://github.com/apache/arrow/pull/13146">#13146</a>)</li>
  <li>Provide explicit JDBC column type mapping (<a href="https://github.com/apache/arrow/pull/13166">#13166</a>)</li>
  <li>Allow duplicated field names in Java C data interface (<a href="https://github.com/apache/arrow/pull/13247">#13247</a>)</li>
  <li>Improve and document StackTrace (<a href="https://github.com/apache/arrow/pull/12656">#12656</a>)</li>
  <li>Keep more context when marshaling errors through JNI (<a href="https://github.com/apache/arrow/pull/13246">#13246</a>)</li>
  <li>Make RoundingMode configurable to handle inconsistent scale in BigDecimals (<a href="https://github.com/apache/arrow/pull/13433">#13433</a>)</li>
  <li>Improve Java dev experience with IntelliJ (<a href="https://github.com/apache/arrow/pull/13017">#13017</a>)</li>
  <li>Implement ArrowArrayStream (<a href="https://github.com/apache/arrow/pull/13465">#13465</a>))</li>
</ul>

<h4 id="bug-fixes-3">Bug Fixes</h4>
<ul>
  <li>Fix variable-width vectors in integration JSON writer (<a href="https://github.com/apache/arrow/pull/13676">#13676</a>)</li>
  <li>Handle empty JDBC ResultSet (<a href="https://github.com/apache/arrow/pull/13049">#13049</a>)</li>
  <li>Fix hasNext() in ArrowVectorIterator (<a href="https://github.com/apache/arrow/pull/13107">#13107</a>)</li>
  <li>Fix ArrayConsumer when using ArrowVectorIterator (<a href="https://github.com/apache/arrow/pull/12692">#12692</a>)</li>
  <li>Update Gandiva Protobuf library to enable builds on Apple M1 (<a href="https://github.com/apache/arrow/pull/13121">#13121</a>)</li>
  <li>Patch dataset module testing failure with JSE11+ (<a href="https://github.com/apache/arrow/pull/13200">#13200</a>)</li>
  <li>Don’t duplicate generated Protobuf classes between flight-core and flight-sql (<a href="https://github.com/apache/arrow/pull/13596">#13596</a>)</li>
</ul>

<h2 id="javascript-notes">JavaScript notes</h2>

<ul>
  <li>Fix error iterating tables with no batches (<a href="https://issues.apache.org/jira/browse/ARROW-16371">ARROW-16371</a>)</li>
  <li>Handle case where <code class="language-plaintext highlighter-rouge">tableFromIPC</code> input is an async <code class="language-plaintext highlighter-rouge">RecordBatchReader</code> (<a href="https://issues.apache.org/jira/browse/ARROW-16704">ARROW-16704</a>)</li>
</ul>

<h2 id="python-notes">Python notes</h2>

<p>Compatibility notes:</p>

<ul>
  <li>
    <p>PyArrow now requires Python &gt;= 3.7 (<a href="https://issues.apache.org/jira/browse/ARROW-16474">ARROW-16474</a>).</p>
  </li>
  <li>
    <p>The default behaviour regarding memory mapping has changed in several APIs (reading of Feather or Parquet files, IPC RecordBatchFileReader and RecordBatchStreamReader) to disable memory mapping by default (<a href="https://issues.apache.org/jira/browse/ARROW-16382">ARROW-16382</a>).</p>
  </li>
  <li>
    <p>The default Parquet version is now 2.4 for writing, enabling use of
more recent logical types by default such as unsigned integers (<a href="https://issues.apache.org/jira/browse/ARROW-12203">ARROW-12203</a>). One can specify <code class="language-plaintext highlighter-rouge">version="2.6"</code> to also enable support for nanosecond timestamps. Use <code class="language-plaintext highlighter-rouge">version="1.0"</code> to restore the old behaviour and maximizes file compatibility.</p>
  </li>
  <li>
    <p>Some deprecated APIs (deprecated at least since pyarrow 1.0.0) have been removed: IPC methods in the top-level namespace, the <code class="language-plaintext highlighter-rouge">Value</code> scalar classes and the <code class="language-plaintext highlighter-rouge">pyarrow.compat</code> module (<a href="https://issues.apache.org/jira/browse/ARROW-17010">ARROW-17010</a>).</p>
  </li>
</ul>

<p>New features:</p>

<ul>
  <li>
    <p>Google Cloud Storage (GCS) File System support is now available in the Python bindings (<a href="https://issues.apache.org/jira/browse/ARROW-14892">ARROW-14892</a>).</p>
  </li>
  <li>
    <p>The <code class="language-plaintext highlighter-rouge">Table.filter()</code> method now supports passing an expression in addition to a boolean array (<a href="https://issues.apache.org/jira/browse/ARROW-16469">ARROW-16469</a>).</p>
  </li>
  <li>
    <p>When implementing extension types in Python, it is now possible to also customize which Python scalar gets returned (in <code class="language-plaintext highlighter-rouge">Array.to_pylist()</code> or <code class="language-plaintext highlighter-rouge">Scalar.as_py()</code>) by subclassing <code class="language-plaintext highlighter-rouge">ExtensionScalar</code> (<a href="https://issues.apache.org/jira/browse/ARROW-13612">ARROW-13612</a>, (<a href="https://issues.apache.org/jira/browse/ARROW-17065">ARROW-17065</a>)).</p>
  </li>
  <li>
    <p>It is now possible to register User Defined Functions (UDF) for scalar functions using <code class="language-plaintext highlighter-rouge">register_scalar_function</code> (<a href="https://issues.apache.org/jira/browse/ARROW-15639">ARROW-15639</a>).</p>
  </li>
  <li>
    <p>Basic support for consuming a Substrait plan has been exposed in Python as <code class="language-plaintext highlighter-rouge">pyarrow.substrait.run_query</code> (<a href="https://issues.apache.org/jira/browse/ARROW-15779">ARROW-15779</a>).</p>
  </li>
  <li>
    <p>The <code class="language-plaintext highlighter-rouge">cast</code> method and compute kernel now exposes the fine grained options in addition to safe/unsafe casting (<a href="https://issues.apache.org/jira/browse/ARROW-15365">ARROW-15365</a>).</p>
  </li>
</ul>

<p>In addition, this release includes several bug fixes and documention improvements (such as expanded examples in docstrings (<a href="https://issues.apache.org/jira/browse/ARROW-16091">ARROW-16091</a>)).</p>

<p>Further, the Python bindings benefit from improvements in the C++ library
(e.g. new compute functions); see the C++ notes above for additional details.</p>

<h2 id="r-notes">R notes</h2>

<p>Highlights include several new <code class="language-plaintext highlighter-rouge">dplyr</code> verbs, including <code class="language-plaintext highlighter-rouge">glimpse()</code> and <code class="language-plaintext highlighter-rouge">union_all()</code>, as well as many more datetime functions from <code class="language-plaintext highlighter-rouge">lubridate</code>. There is also experimental support for user-defined scalar functions in the query engine, and most packages include native support for datasets in Google Cloud Storage (opt-in in the Linux full source build).</p>

<p>For more on what’s in the 9.0.0 R package, see the <a href="/docs/r/news/">R changelog</a>.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<p>FlightSQL is now supported but there are minimum features for now.</p>

<p>More Flight features are now supported.</p>

<h3 id="ruby">Ruby</h3>

<p><code class="language-plaintext highlighter-rouge">Enumerable</code> compatible methods such as <code class="language-plaintext highlighter-rouge">#min</code> and <code class="language-plaintext highlighter-rouge">#max</code> on <code class="language-plaintext highlighter-rouge">Arrow::Array</code>, <code class="language-plaintext highlighter-rouge">Arrow::ChunkedArray</code> and <code class="language-plaintext highlighter-rouge">Arrow::Column</code> are implemented by C++’s <a href="/docs/cpp/compute.html">compute functions</a>. This improves performance. (<a href="https://issues.apache.org/jira/browse/ARROW-15222">ARROW-15222</a>)</p>

<p>This release fixed some memory leaks. (<a href="https://issues.apache.org/jira/browse/ARROW-14790">ARROW-14790</a>)</p>

<p>This release improved support for interval type arrays such as <code class="language-plaintext highlighter-rouge">Arrow::MonthIntervalArray</code>. (<a href="https://issues.apache.org/jira/browse/ARROW-16206">ARROW-16206</a>)</p>

<p>This release improved auto data type conversion. (<a href="https://issues.apache.org/jira/browse/ARROW-16874">ARROW-16874</a>)</p>

<h3 id="c-glib">C GLib</h3>

<p>Vala is now supported. (<a href="https://issues.apache.org/jira/browse/ARROW-15671">ARROW-15671</a>). See <a href="https://github.com/apache/arrow/tree/apache-arrow-9.0.0/c_glib/example/vala"><code class="language-plaintext highlighter-rouge">c_glib/example/vala/</code></a> for examples.</p>

<p><code class="language-plaintext highlighter-rouge">GArrowQuantil
eOptions</code> is added. (<a href="https://issues.apache.org/jira/browse/ARROW-16623">ARROW-16623</a>)</p>

<h2 id="rust-notes">Rust notes</h2>

<p>The Rust projects have moved to separate repositories outside the
main Arrow monorepo. For notes on the 19.0.0 release of the Rust
implementation, see the <a href="https://github.com/apache/arrow-rs/blob/19.0.0/CHANGELOG.md">Arrow Rust changelog</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 9.0.0 release. This covers over 3 months of development work and includes 509 resolved issues from 114 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bug fixes and improvements have been made: we refer you to the complete changelog. Community Since the 8.0.0 release, Dewey Dunnington, Alenka Frim and Rok Mihevc have been invited to be committers. Thanks for your contributions and participation in the project! Columnar Format Notes Arrow Flight RPC notes Arrow Flight is now available in MacOS M1 Python wheels (ARROW-16779). Arrow Flight SQL is now buildable on Windows (ARROW-16902). Ruby now exposes more of the Flight and Flight SQL APIs (various JIRAs). Linux packages notes AlmaLinux 9 is now supported. (ARROW-16745) AmazonLinux 2 aarch64 is now supported. (ARROW-16477) C++ notes STL-like iteration is now provided over chunked arrays (ARROW-602). Compute The C++ compute and execution engine is now officially named “Acero”, though its C++ namespaces have not changed. New light-weight data holder abstractions have been introduced in order to reduce the overhead of invoking compute functions and kernels, especially at the small data sizes desirable for efficient parallelization (typically L1- or L2-sized). Specifically, the non-owning ArraySpan and ExecSpan structures have internally superseded the much heavier ExecBatch, which is still supported for compatibility at the API level (ARROW-16756, ARROW-16824, ARROW-16852). In a similar vein, the ValueDescr class was removed and ScalarKernel implementations now always receive at least one non-scalar input, removing the special case where a ScalarKernel needs to output a scalar rather than an array. The higher-level compute APIs still allow executing a scalar function over all-scalar inputs; but those scalars are internally broadcasted to 1-element arrays so as to simplify kernel implementation (ARROW-16757). Some performance improvements were made to the hash join node. These changes do not require additional configuration. The hash join exec node has been improved to more efficiently use CPU cache and make better use of available vectorization hardware (ARROW-14182). Some plans containing a sequence of hash join operators will now use bloom filters to eliminate rows earlier in the plan, reducing the overall CPU cost of the plan (ARROW-15498). Timestamp comparison is now supported (ARROW-16425). A cumulative sum function is implemented over numeric inputs (ARROW-13530). Note that this is a vector function so cannot be used in an Acero ExecPlan. A rank vector kernel has been added (ARROW-16234). Temporal rounding functions received additional options to control how rounding is done (ARROW-14821). Improper computation of the “mode” function on boolean input was fixed (ARROW-17096). Function registries can now be nested (ARROW-16677). Dataset The autogenerate_column_names option for CSV reading is now handled correctly (ARROW-16436). Fix InMemoryDataset::ReplaceSchema to actually replace the schema (ARROW-16085). Fix FilenamePartitioning to properly support null values (ARROW-16302). Filesystem A number of bug fixes and improvements were made to the Google Cloud Storage filesystem implementation (ARROW-14892). By default, the S3 filesystem implementation does not create or drop buckets anymore (ARROW-15906). This is a compatibility-breaking change intended to prevent user errors from having potentially catastrophic consequences. Options have been added to restore the previous behavior if necessary. Parquet The default Parquet version is now 2.4 for writing, enabling use of more recent logical types by default (ARROW-12203). Non-nullable fields are now handled correctly by the Parquet reader (ARROW-16116). Reading encrypted files should now be thread-safe (ARROW-14114). Statistics equality now works correctly with minmax (ARROW-16487). The minimum Thrift version required for building is now 0.13 (ARROW-16721). The Thrift deserialization limits can now be configured to accommodate for data files with very large metadata (ARROW-16546). Substrait The Substrait spec has been updated to 0.6.0 (ARROW-16816). In addition, a larger subset of the Substrait specification is now supported (ARROW-15587, ARROW-15590, ARROW-15901, ARROW-16657, ARROW-15591). C# notes New Features Added support for Time32Array and Time64Array (ARROW-16660) Bug Fixes When using TableFromRecordBatches, the resulting table columns have no data array. (ARROW-13129) Fix intermittent test failures due to async memory management bug. (ARROW-16978) Go notes Security Updated testify dependency to address CVE-2022-28948. (ARROW-16759) (This was also backported to previous versions and released as patch versions v6.0.2, v7.0.1, and v8.0.1) Arrow New Features Dictionary Scalars are now available (ARROW-16323) Introduced a DictionaryUnifier object along with functions for unifying Chunked Arrays and Tables (ARROW-16324) New CSV examples added to documentation to demonstrate error handling (ARROW-16450) CSV Reader now supports arrow.TimestampType (ARROW-16504) JSON parsing for Temporal Types now allow passing numeric values in addition to strings for parsing. Timezones will be properly parsed if they exist in the string and a function was added to retrieve a time.Location object from a TimestampType (ARROW-16551) New utilities added to decimal128 for rescaling and easy conversion to and from float32/float64 (ARROW-16552) Arrow DataType interface now has a LayoutMethod which returns the physical layout of the given datatype such as the number of buffers, types, etc. This matches the behavior of the layout() methods in C++ for data types. (ARROW-16556) Added a SliceBuffer function to the memory package to allow better re-using of memory across buffer objects (ARROW-16557) Dictionary Arrays can now be concatenated using array.Concatenate (ARROW-17095) Bug Fixes ipc.FileReader now properly uses the memory.Allocator interface (ARROW-16002) Addressed issue with Integration tests between Go and Java (ARROW-16441) RecordBuilder.UnmarshalJSON now properly ignores extra unknown fields rather than panicking (ARROW-16456) StructBuilder.UnmarshalJSON will no longer fail and panic when Nullable fields are missing (ARROW-16502) ipc.Reader no longer silently accepts string columns with invalid offsets, preventing unexpected panics later when writing or accessing the resulting arrays. (ARROW-16831) Arrow CSV reader no longer clobbers its reported errors and properly surfaces them (ARROW-16926) Parquet New Features The CreatedBy version string for the Parquet writer will now correctly reflect the library version, and will be updated by the release scripts (ARROW-16484) Parquet bit_packing functions now have ARM64 NEON implementations for performance (ARROW-16486) It is now possible to customize the root node in the Parquet writer instead of hardcoding it to be named “schema” with a repetition type of Repeated. This was needed to allow producing files similar to Apache Spark where the root node has a repetition type of Required. It still defaults to the spec definition of Repeated. (ARROW-16561) parquet_reader CLI mainprog has been enhanced to dump values out as JSON and CSV along with setting an output file instead of just dumping to the terminal. (ARROW-16934) Bug Fixes Fixed a memory leak with Parquet page reading (ARROW-16473) Parquet Reader properly parallelizes column reads when the parallel option is set to true. (ARROW-16530) Fixed bug in the Bool decoder for plain encoding (ARROW-16563) Fixed a bug in the Parquet bool column reader where it failed to properly skip rows (ARROW-16638) Fixed the flakey travis ARM64 builds by reducing the size of a test case in the pqarrow unit tests to reduce the memory usage for the tests. (ARROW-16669) Parquet writer now properly handles writing arrow.NULL type arrays (ARROW-16749) Column level dictionary encoding configuration for Parquet writing now correctly respects the input value (ARROW-16813) Memory leak in DeltaByteArray encoding fixed (ARROW-16983) Java notes New Features Allow overriding column nullability in arrow-jdbc (#13558) Enable skip BOUNDS_CHECKING with setBytes and getBytes of ArrowBuf (#13161) Initialize JNI components on use instead of statically (#13146) Provide explicit JDBC column type mapping (#13166) Allow duplicated field names in Java C data interface (#13247) Improve and document StackTrace (#12656) Keep more context when marshaling errors through JNI (#13246) Make RoundingMode configurable to handle inconsistent scale in BigDecimals (#13433) Improve Java dev experience with IntelliJ (#13017) Implement ArrowArrayStream (#13465)) Bug Fixes Fix variable-width vectors in integration JSON writer (#13676) Handle empty JDBC ResultSet (#13049) Fix hasNext() in ArrowVectorIterator (#13107) Fix ArrayConsumer when using ArrowVectorIterator (#12692) Update Gandiva Protobuf library to enable builds on Apple M1 (#13121) Patch dataset module testing failure with JSE11+ (#13200) Don’t duplicate generated Protobuf classes between flight-core and flight-sql (#13596) JavaScript notes Fix error iterating tables with no batches (ARROW-16371) Handle case where tableFromIPC input is an async RecordBatchReader (ARROW-16704) Python notes Compatibility notes: PyArrow now requires Python &gt;= 3.7 (ARROW-16474). The default behaviour regarding memory mapping has changed in several APIs (reading of Feather or Parquet files, IPC RecordBatchFileReader and RecordBatchStreamReader) to disable memory mapping by default (ARROW-16382). The default Parquet version is now 2.4 for writing, enabling use of more recent logical types by default such as unsigned integers (ARROW-12203). One can specify version="2.6" to also enable support for nanosecond timestamps. Use version="1.0" to restore the old behaviour and maximizes file compatibility. Some deprecated APIs (deprecated at least since pyarrow 1.0.0) have been removed: IPC methods in the top-level namespace, the Value scalar classes and the pyarrow.compat module (ARROW-17010). New features: Google Cloud Storage (GCS) File System support is now available in the Python bindings (ARROW-14892). The Table.filter() method now supports passing an expression in addition to a boolean array (ARROW-16469). When implementing extension types in Python, it is now possible to also customize which Python scalar gets returned (in Array.to_pylist() or Scalar.as_py()) by subclassing ExtensionScalar (ARROW-13612, (ARROW-17065)). It is now possible to register User Defined Functions (UDF) for scalar functions using register_scalar_function (ARROW-15639). Basic support for consuming a Substrait plan has been exposed in Python as pyarrow.substrait.run_query (ARROW-15779). The cast method and compute kernel now exposes the fine grained options in addition to safe/unsafe casting (ARROW-15365). In addition, this release includes several bug fixes and documention improvements (such as expanded examples in docstrings (ARROW-16091)). Further, the Python bindings benefit from improvements in the C++ library (e.g. new compute functions); see the C++ notes above for additional details. R notes Highlights include several new dplyr verbs, including glimpse() and union_all(), as well as many more datetime functions from lubridate. There is also experimental support for user-defined scalar functions in the query engine, and most packages include native support for datasets in Google Cloud Storage (opt-in in the Linux full source build). For more on what’s in the 9.0.0 R package, see the R changelog. Ruby and C GLib notes FlightSQL is now supported but there are minimum features for now. More Flight features are now supported. Ruby Enumerable compatible methods such as #min and #max on Arrow::Array, Arrow::ChunkedArray and Arrow::Column are implemented by C++’s compute functions. This improves performance. (ARROW-15222) This release fixed some memory leaks. (ARROW-14790) This release improved support for interval type arrays such as Arrow::MonthIntervalArray. (ARROW-16206) This release improved auto data type conversion. (ARROW-16874) C GLib Vala is now supported. (ARROW-15671). See c_glib/example/vala/ for examples. GArrowQuantil eOptions is added. (ARROW-16623) Rust notes The Rust projects have moved to separate repositories outside the main Arrow monorepo. For notes on the 19.0.0 release of the Rust implementation, see the Arrow Rust changelog.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">June 2022 Rust Apache Arrow and Parquet 16.0.0 Highlights</title><link href="https://arrow.apache.org/blog/2022/06/16/rust-16.0.0/" rel="alternate" type="text/html" title="June 2022 Rust Apache Arrow and Parquet 16.0.0 Highlights" /><published>2022-06-16T02:00:00-04:00</published><updated>2022-06-16T02:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/06/16/rust-16.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/06/16/rust-16.0.0/"><![CDATA[<!--

-->

<h1 id="introduction">Introduction</h1>

<p>We recently celebrated releasing version 16.0.0 of the Rust implementation of <a href="https://arrow.apache.org/">Apache Arrow</a>. While we still get a few comments on “most rust libraries use versions 0.x.0, why are you at 16.0.0?”, our versioning scheme appears to be working well, and permits quick releases of new features and API evolution in a semver compatible way without breaking downstream projects.</p>

<p>This post contains highlights from the last four months (versions
10.0.0 to 16.0.0) of
<a href="https://github.com/apache/arrow-rs/arrow">arrow-rs</a> and
<a href="https://github.com/apache/arrow-rs/parquet">parquet-rs</a> development
as well as a roadmap of future work. The full list of awesomeness can
be found in the
<a href="https://github.com/apache/arrow-rs/blob/master/CHANGELOG.md">CHANGELOG</a>.</p>

<p>As you <a href="https://github.com/apache/arrow-rs/issues/1715">may remember</a>, the arrow and parquet implementations are in the same crate, on the same release schedule, and in this same blog. This is not for technical reasons, but helps to keep the maintenance burden for delivering great Apache software reasonable, and allows easier development of optimized conversion between Arrow &lt;–&gt; Parquet formats.</p>

<h1 id="parquet">Parquet</h1>
<p>The <a href="https://crates.io/crates/parquet">parquet crate</a> has seen a return to substantial improvements after being relatively dormant for several years. The current major areas of focus are</p>

<ol>
  <li><strong>Performance</strong>: Improving the raw performance for reading and writing mirroring the efforts that went into the C++ version a few years ago.</li>
  <li><strong>API Ease of Use</strong>: Improving the API so it is easy to use efficiently with modern Rust for two preeminent use cases: 1) reading from local disk and 2) reading <code class="language-plaintext highlighter-rouge">async</code>hronously from remote object stores.</li>
</ol>

<p>Some Major Highlights:</p>

<ul>
  <li><strong>Advanced Metadata Access</strong>: API access to advanced parquet metadata, such as <a href="https://github.com/apache/arrow-rs/pull/1322">PageEncoding</a>, <a href="https://github.com/apache/arrow-rs/pull/1309">BloomFilters</a> and <a href="https://github.com/apache/arrow-rs/pull/1762">PageIndex</a>.</li>
  <li><strong>Improved API Usability</strong>: For example, the <a href="https://github.com/apache/arrow-rs/pull/1719">parquet writer now uses <code class="language-plaintext highlighter-rouge">std:io::Write</code></a> rather than a custom <code class="language-plaintext highlighter-rouge">ParquetWriter</code> trait, making it more interoperable with the rest of the Rust ecosystem and the <a href="https://github.com/apache/arrow-rs/pull/1716">projection API is easier to use with nested types</a>.</li>
  <li><strong>Rewritten support for nested types (e.g. struct, lists)</strong> : @tustvold has revamped / rewritten support for <a href="https://github.com/apache/arrow-rs/pull/1682">reading</a> and <a href="https://github.com/apache/arrow-rs/pull/1746">writing</a> structured types, which both improved the support for arbitrary nested schemas, and is 30% faster.</li>
</ul>

<p>Looking Forward:</p>

<ul>
  <li><strong>Even Faster</strong>: We are actively working to make <a href="https://github.com/apache/arrow-rs/issues/1764">writing even faster</a> and expect to see some major improvements over the next few releases.</li>
  <li><strong>Object Store Integration</strong>: Support for easily and efficiently reading/writing to/from object storage is improving, and we expect it will soon work well out of the box, fetching the minimal bytes, etc… More on this to follow in a separate blog post.</li>
  <li><strong>Parallel Decode</strong>: We intend to transparently support high performance parallel decoding of parquet to arrow arrays, when invoked from a <a href="https://crates.io/crates/rayon">rayon</a> threadpool.</li>
</ul>

<h1 id="arrow">Arrow</h1>

<p>The Rust arrow implementation has also had substantial improvements, in addition to bug fixes and performance improvements.</p>

<p>Some Major Highlights:</p>

<ul>
  <li><strong>Ecosystem Compatibility</strong>: <a href="https://github.com/viirya">@viriya</a> has put in a massive effort to improve (and prove) compatibility with other Arrow implementations via the Rust IPC integration tests. There have been major improvements for corner cases involving nested structures, nullability, nested dictionaries, etc.</li>
  <li><strong>Safety</strong>: We continue to improve the safety of arrow, and it is not possible to trigger undefined behavior using <code class="language-plaintext highlighter-rouge">safe</code> apis – checkout the <a href="https://github.com/apache/arrow-rs/tree/master/arrow#safety">README</a> and the <a href="https://docs.rs/arrow/16.0.0/arrow/#safety-and-security">module level rustdocs</a> for more details. Among other things, we have added additional validation checking to <a href="https://github.com/apache/arrow-rs/issues/1575">string kernels</a> and <a href="https://github.com/apache/arrow-rs/pull/1767"><code class="language-plaintext highlighter-rouge">DecimalArrays</code></a> and <a href="https://github.com/apache/arrow-rs/pull/1819">sealed some sensitive traits</a>.</li>
  <li><strong>Performance</strong>: There have been several major performance improvements such as <a href="https://github.com/apache/arrow-rs/issues/1288">much faster filter kernels</a>, thanks to <a href="https://github.com/tustvold">@tustvold</a>.</li>
  <li><strong>Easier to Use APIs</strong>: Several of the APIs are now easier to use (e.g. <a href="https://github.com/apache/arrow-rs/pull/1645">#1645</a> and <a href="https://github.com/apache/arrow-rs/pull/1739">#1739</a> which lowers the barrier to entry of using <code class="language-plaintext highlighter-rouge">arrow-rs</code>, thanks to <a href="https://github.com/HaoYang670">@HaoYang670</a>.</li>
  <li><strong>DataType::Null support</strong>: is much improved, such as <a href="https://github.com/apache/arrow-rs/pull/1572">in the cast kernels</a>, thanks to <a href="https://github.com/WinkerDu">@WinkerDu</a>.</li>
  <li><strong>Improved JSON reader</strong>: The JSON reader is <a href="https://github.com/apache/arrow-rs/pull/1451">now easier to use</a> thanks to <a href="https://github.com/sum12">@sum12</a>.</li>
</ul>

<p>Looking Forward:</p>
<ul>
  <li><strong>Make ArrayData Easier to use Safely</strong>: Some amount of <code class="language-plaintext highlighter-rouge">unsafe</code> will likely always be required in arrow (for fast IPC, for example), but we are also working to improve the underlying <code class="language-plaintext highlighter-rouge">ArrayData</code> structure to make it more compatible with the ecosystem (e.g. use <code class="language-plaintext highlighter-rouge">Bytes</code>), support faster to decode from parquet, and to avoid bugs related to offsets (slicing) which are a <a href="https://github.com/apache/arrow-rs/issues/1799">frequent pain point</a>.</li>
  <li><strong>FlightSQL</strong> – we have some <a href="https://github.com/apache/arrow-rs/pulls?q=flightsql">initial support for Flight SQL</a> thank to <a href="https://github.com/wangfenjin">@wangfenjin</a> and <a href="https://github.com/timvw">@timvw</a>, though we would love to see some additional contributors. Such help can include a basic FlightSQL server, and starting work on clients.</li>
</ul>

<p>Some areas looking for help include:</p>

<ul>
  <li><strong>Decimal 256 support</strong>: <a href="https://github.com/apache/arrow-rs/issues/131">https://github.com/apache/arrow-rs/issues/131</a></li>
  <li><strong>Support for negative Decimal scale</strong>: <a href="https://github.com/apache/arrow-rs/issues/1785">https://github.com/apache/arrow-rs/issues/1785</a></li>
  <li><strong>Support IPC file compression</strong>: <a href="https://github.com/apache/arrow-rs/issues/1709">https://github.com/apache/arrow-rs/issues/1709</a></li>
  <li><strong>Zero-copy bitmap slicing</strong>: <a href="https://github.com/apache/arrow-rs/issues/1802">https://github.com/apache/arrow-rs/issues/1802</a></li>
</ul>

<h1 id="contributors">Contributors:</h1>

<p>While some open source software can be created mostly by a single contributor, we believe the greatest software with the largest impact and reach is built around its community. Thus, Arrow is part of the <a href="https://www.apache.org/">Apache Software Foundation</a> and our releases both past and present are a result of our amazing community’s effort.</p>

<p>We would like to thank everyone who has contributed to the arrow-rs repository since the <code class="language-plaintext highlighter-rouge">9.0.2</code> release. Keep up the great work and we look forward to continued improvements:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">git shortlog -sn 9.0.0..16.0.0
    47  Liang-Chi Hsieh
    45  Raphael Taylor-Davies
    43  Andrew Lamb
    40  Remzi Yang
     8  Sergey Glushchenko
     7  Jörn Horstmann
     6  Shani Solomon
     6  dependabot[bot]
     5  Yang Jiang
     4  jakevin
     4  Chao Sun
     4  Yijie Shen
     3  kazuhiko kikuchi
     2  Sumit
     2  Ismail-Maj
     2  Kamil Konior
     2  tfeda
     2  Matthew Turner
     1  iyupeng
     1  ryan-jacobs1
     1  Alex Qyoun-ae
     1  tjwilson90
     1  Andy Grove
     1  Atef Sawaed
     1  Daniël Heres
     1  DuRipeng
     1  Helgi Kristvin Sigurbjarnarson
     1  Kun Liu
     1  Kyle Barron
     1  Marc Garcia
     1  Peter C. Jentsch
     1  Remco Verhoef
     1  Sven Cattell
     1  Thomas Peiselt
     1  Tiphaine Ruy
     1  Trent Feda
     1  Wang Fenjin
     1  Ze'ev Maor
     1  diana
</span></code></pre></div></div>

<h1 id="join-the-community">Join the community</h1>

<p>If you are interested in contributing to the Rust subproject in Apache Arrow, you can find a list of open issues
suitable for beginners <a href="https://github.com/apache/arrow-rs/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a>
and the full list <a href="https://github.com/apache/arrow-rs/issues">here</a>.</p>

<p>Other ways to get involved include trying out Arrow on some of your data and filing bug reports, and helping to
improve the documentation.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[Introduction We recently celebrated releasing version 16.0.0 of the Rust implementation of Apache Arrow. While we still get a few comments on “most rust libraries use versions 0.x.0, why are you at 16.0.0?”, our versioning scheme appears to be working well, and permits quick releases of new features and API evolution in a semver compatible way without breaking downstream projects. This post contains highlights from the last four months (versions 10.0.0 to 16.0.0) of arrow-rs and parquet-rs development as well as a roadmap of future work. The full list of awesomeness can be found in the CHANGELOG. As you may remember, the arrow and parquet implementations are in the same crate, on the same release schedule, and in this same blog. This is not for technical reasons, but helps to keep the maintenance burden for delivering great Apache software reasonable, and allows easier development of optimized conversion between Arrow &lt;–&gt; Parquet formats. Parquet The parquet crate has seen a return to substantial improvements after being relatively dormant for several years. The current major areas of focus are Performance: Improving the raw performance for reading and writing mirroring the efforts that went into the C++ version a few years ago. API Ease of Use: Improving the API so it is easy to use efficiently with modern Rust for two preeminent use cases: 1) reading from local disk and 2) reading asynchronously from remote object stores. Some Major Highlights: Advanced Metadata Access: API access to advanced parquet metadata, such as PageEncoding, BloomFilters and PageIndex. Improved API Usability: For example, the parquet writer now uses std:io::Write rather than a custom ParquetWriter trait, making it more interoperable with the rest of the Rust ecosystem and the projection API is easier to use with nested types. Rewritten support for nested types (e.g. struct, lists) : @tustvold has revamped / rewritten support for reading and writing structured types, which both improved the support for arbitrary nested schemas, and is 30% faster. Looking Forward: Even Faster: We are actively working to make writing even faster and expect to see some major improvements over the next few releases. Object Store Integration: Support for easily and efficiently reading/writing to/from object storage is improving, and we expect it will soon work well out of the box, fetching the minimal bytes, etc… More on this to follow in a separate blog post. Parallel Decode: We intend to transparently support high performance parallel decoding of parquet to arrow arrays, when invoked from a rayon threadpool. Arrow The Rust arrow implementation has also had substantial improvements, in addition to bug fixes and performance improvements. Some Major Highlights: Ecosystem Compatibility: @viriya has put in a massive effort to improve (and prove) compatibility with other Arrow implementations via the Rust IPC integration tests. There have been major improvements for corner cases involving nested structures, nullability, nested dictionaries, etc. Safety: We continue to improve the safety of arrow, and it is not possible to trigger undefined behavior using safe apis – checkout the README and the module level rustdocs for more details. Among other things, we have added additional validation checking to string kernels and DecimalArrays and sealed some sensitive traits. Performance: There have been several major performance improvements such as much faster filter kernels, thanks to @tustvold. Easier to Use APIs: Several of the APIs are now easier to use (e.g. #1645 and #1739 which lowers the barrier to entry of using arrow-rs, thanks to @HaoYang670. DataType::Null support: is much improved, such as in the cast kernels, thanks to @WinkerDu. Improved JSON reader: The JSON reader is now easier to use thanks to @sum12. Looking Forward: Make ArrayData Easier to use Safely: Some amount of unsafe will likely always be required in arrow (for fast IPC, for example), but we are also working to improve the underlying ArrayData structure to make it more compatible with the ecosystem (e.g. use Bytes), support faster to decode from parquet, and to avoid bugs related to offsets (slicing) which are a frequent pain point. FlightSQL – we have some initial support for Flight SQL thank to @wangfenjin and @timvw, though we would love to see some additional contributors. Such help can include a basic FlightSQL server, and starting work on clients. Some areas looking for help include: Decimal 256 support: https://github.com/apache/arrow-rs/issues/131 Support for negative Decimal scale: https://github.com/apache/arrow-rs/issues/1785 Support IPC file compression: https://github.com/apache/arrow-rs/issues/1709 Zero-copy bitmap slicing: https://github.com/apache/arrow-rs/issues/1802 Contributors: While some open source software can be created mostly by a single contributor, we believe the greatest software with the largest impact and reach is built around its community. Thus, Arrow is part of the Apache Software Foundation and our releases both past and present are a result of our amazing community’s effort. We would like to thank everyone who has contributed to the arrow-rs repository since the 9.0.2 release. Keep up the great work and we look forward to continued improvements: git shortlog -sn 9.0.0..16.0.0 47 Liang-Chi Hsieh 45 Raphael Taylor-Davies 43 Andrew Lamb 40 Remzi Yang 8 Sergey Glushchenko 7 Jörn Horstmann 6 Shani Solomon 6 dependabot[bot] 5 Yang Jiang 4 jakevin 4 Chao Sun 4 Yijie Shen 3 kazuhiko kikuchi 2 Sumit 2 Ismail-Maj 2 Kamil Konior 2 tfeda 2 Matthew Turner 1 iyupeng 1 ryan-jacobs1 1 Alex Qyoun-ae 1 tjwilson90 1 Andy Grove 1 Atef Sawaed 1 Daniël Heres 1 DuRipeng 1 Helgi Kristvin Sigurbjarnarson 1 Kun Liu 1 Kyle Barron 1 Marc Garcia 1 Peter C. Jentsch 1 Remco Verhoef 1 Sven Cattell 1 Thomas Peiselt 1 Tiphaine Ruy 1 Trent Feda 1 Wang Fenjin 1 Ze'ev Maor 1 diana Join the community If you are interested in contributing to the Rust subproject in Apache Arrow, you can find a list of open issues suitable for beginners here and the full list here. Other ways to get involved include trying out Arrow on some of your data and filing bug reports, and helping to improve the documentation.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow DataFusion 8.0.0 Release</title><link href="https://arrow.apache.org/blog/2022/05/16/datafusion-8.0.0/" rel="alternate" type="text/html" title="Apache Arrow DataFusion 8.0.0 Release" /><published>2022-05-16T00:00:00-04:00</published><updated>2022-05-16T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/05/16/datafusion-8.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/05/16/datafusion-8.0.0/"><![CDATA[<!--

-->

<h1 id="introduction">Introduction</h1>

<p><a href="https://arrow.apache.org/datafusion/">DataFusion</a> is an extensible query execution framework, written in Rust, that
uses Apache Arrow as its in-memory format.</p>

<p>When you want to extend your Rust project with <a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html">SQL support</a>,
a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is definitely worth
checking out.</p>

<p>DataFusion’s SQL, <code class="language-plaintext highlighter-rouge">DataFrame</code>, and manual <code class="language-plaintext highlighter-rouge">PlanBuilder</code> API let users access a sophisticated query optimizer and
execution engine capable of fast, resource efficient, and parallel execution that takes optimal advantage of
today’s multicore hardware. Being written in Rust means DataFusion can offer <em>both</em> the safety of a dynamic language and
the resource efficiency of a compiled language.</p>

<p>The Apache Arrow team is pleased to announce the DataFusion 8.0.0 release (and also the release of version 0.7.0 of
the Ballista subproject). This covers 3 months of development work and includes 279 commits from the following 49
distinct contributors.</p>

<!--
$ git log --pretty=oneline 7.0.0..8.0.0 datafusion datafusion-cli datafusion-examples ballista ballista-cli ballista-examples | wc -l
279

$ git shortlog -sn 7.0.0..8.0.0 datafusion datafusion-cli datafusion-examples ballista ballista-cli ballista-examples | wc -l
49

(feynman han, feynman.h, Feynman Han were assumed to be the same person)
-->

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    39  Andy Grove
    33  Andrew Lamb
    21  DuRipeng
    20  Yijie Shen
    19  Yang Jiang
    17  Raphael Taylor-Davies
    11  Dan Harris
    11  Matthew Turner
    11  yahoNanJing
     9  dependabot[bot]
     8  jakevin
     6  Kun Liu
     5  Jiayu Liu
     4  Daniël Heres
     4  mingmwang
     4  xudong.w
     3  Carol (Nichols || Goulding)
     3  Dmitry Patsura
     3  Eduard Karacharov
     3  Jeremy Dyer
     3  Kaushik
     3  Rich
     3  comphead
     3  gaojun2048
     3  Feynman Han
     2  Jie Han
     2  Jon Mease
     2  Tim Van Wassenhove
     2  Yt
     2  Zhang Li
     2  silence-coding
     1  Alexander Spies
     1  George Andronchik
     1  Guillaume Balaine
     1  Hao Xin
     1  Jiacai Liu
     1  Jörn Horstmann
     1  Liang-Chi Hsieh
     1  Max Burke
     1  NaincyKumariKnoldus
     1  Nga Tran
     1  Patrick More
     1  Pierre Zemb
     1  Remzi Yang
     1  Sergey Melnychuk
     1  Stephen Carman
     1  doki
</code></pre></div></div>

<p>The following sections highlight some of the changes in this release. Of course, many other bug fixes and
improvements have been made and we encourage you to check out the
<a href="https://github.com/apache/arrow-datafusion/blob/8.0.0/datafusion/CHANGELOG.md">changelog</a> for full details.</p>

<h1 id="summary">Summary</h1>

<h2 id="ddl-support">DDL Support</h2>

<p>DDL support has been expanded to include the following commands for creating databases, schemas, and views. This
allows DataFusion to be used more effectively from the CLI.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">CREATE DATABASE</code></li>
  <li><code class="language-plaintext highlighter-rouge">CREATE VIEW</code></li>
  <li><code class="language-plaintext highlighter-rouge">CREATE SCHEMA</code></li>
  <li><code class="language-plaintext highlighter-rouge">CREATE EXTERNAL TABLE</code> now supports JSON files, <code class="language-plaintext highlighter-rouge">IF NOT EXISTS</code>, and partition columns</li>
</ul>

<h2 id="sql-support">SQL Support</h2>

<p>The SQL query planner now supports a number of new SQL features, including:</p>

<ul>
  <li><em>Subqueries</em>: when used via <code class="language-plaintext highlighter-rouge">IN</code>, <code class="language-plaintext highlighter-rouge">EXISTS</code>, and as scalars</li>
  <li><em>Grouping Sets</em>: <code class="language-plaintext highlighter-rouge">CUBE</code> and <code class="language-plaintext highlighter-rouge">ROLLUP</code> grouping sets.</li>
  <li><em>Aggregate functions</em>: <code class="language-plaintext highlighter-rouge">approx_percentile</code>, <code class="language-plaintext highlighter-rouge">approx_percentile_cont</code>, <code class="language-plaintext highlighter-rouge">approx_percentile_cont_with_weight</code>, <code class="language-plaintext highlighter-rouge">approx_distinct</code>, <code class="language-plaintext highlighter-rouge">approx_median</code> and <code class="language-plaintext highlighter-rouge">array</code></li>
  <li><em><code class="language-plaintext highlighter-rouge">null</code> literals</em></li>
  <li><em>bitwise operations</em>: for example ‘<code class="language-plaintext highlighter-rouge">|</code>’</li>
</ul>

<p>There are also many bug fixes and improvements around normalizing identifiers consistently.</p>

<p>We continue our tradition of incrementally releasing support for new
features as they are developed. Thus, while the physical plan may not yet
support all new features, it gets more complete each release. These
changes also make DataFusion an increasingly compelling choice for
projects looking for a SQL parser and query planner that can produce
optimized logical plans that can be translated to
their own execution engine.</p>

<h2 id="query-execution--internals">Query Execution &amp; Internals</h2>

<p>There are several notable improvements and new features in the query execution engine:</p>

<ul>
  <li>The <code class="language-plaintext highlighter-rouge">ExecutionContext</code> has been renamed to <code class="language-plaintext highlighter-rouge">SessionContext</code> and now supports multi-tenancy</li>
  <li>The <code class="language-plaintext highlighter-rouge">ExecutionPlan</code> trait is no longer <code class="language-plaintext highlighter-rouge">async</code></li>
  <li>A new serialization API for serializing plans to bytes (based on protobuf)</li>
</ul>

<p>In addition, we have added several foundational features to drive even
more advanced query processing into DataFusion, focusing on running
arbitrary queries larger than available memory, and pushing the
envelope for performance of sorting, grouping, and joining even
further:</p>

<ul>
  <li>Morsel-Driven Scheduler based on <a href="https://15721.courses.cs.cmu.edu/spring2016/papers/p743-leis.pdf">“Morsel-Driven Parallelism: A NUMA-Aware Query
Evaluation Framework for the Many-Core Age”</a></li>
  <li>Consolidated object store implementation and integration with parquet decoding</li>
  <li>Memory Limited Spilling sort operator</li>
  <li>Memory Limited Sort-Merge join operator</li>
  <li>High performance JIT code generation for tuple comparisons</li>
  <li>Memory efficient Row Format</li>
</ul>

<h2 id="improved-file-support">Improved file support</h2>

<p>DataFusion now supports JSON, both for reading and writing. There are also new DataFrame methods for writing query
results to files in CSV, Parquet, and JSON format.</p>

<h2 id="ballista">Ballista</h2>

<p>Ballista continues to mature and now supports a wider range of operators and expressions. There are also improvements
to the scheduler to support UDFs, and there are some robustness improvements, such as cleaning up work directories
and persisting session configs to allow schedulers to restart and continue processing in-flight jobs.</p>

<h2 id="upcoming-work">Upcoming Work</h2>

<p>Here are some of the initiatives that the community plans on working on prior to the next release.</p>

<ul>
  <li>There is a <a href="https://docs.google.com/document/d/1jNRbadyStSrV5kifwn0khufAwq6OnzGczG4z8oTQJP4/edit?usp=sharing">proposal to move Ballista to its own top-level arrow-ballista repository</a>
 to decouple DataFusion and Ballista releases and to allow each project to have documentation better targeted at
its particular audience.</li>
  <li>We plan on increasing the frequency of DataFusion releases, with monthly releases now instead of quarterly. This
is driven by requests from the increasing number of projects that now depend on DataFusion.</li>
  <li>There is ongoing work to implement new optimizer rules to rewrite queries containing subquery expressions as
joins, to support a wider range of queries.</li>
  <li>The new scheduler based on morsel-driven execution will continue to evolve in this next release, with work to
refine IO abstractions to improve performance and integration with the new scheduler.</li>
  <li>Improved performance for Sort, Grouping and Joins</li>
</ul>

<h1 id="how-to-get-involved">How to Get Involved</h1>

<p>If you are interested in contributing to DataFusion, and learning about state-of-the-art query processing, we would
love to have you join us on the journey! You can help by trying out DataFusion on some of your own data and projects
and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable
for beginners is <a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a></p>

<p>Check out our new <a href="https://arrow.apache.org/datafusion/community/communication.html">Communication Doc</a> on more
ways to engage with the community.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[Introduction DataFusion is an extensible query execution framework, written in Rust, that uses Apache Arrow as its in-memory format. When you want to extend your Rust project with SQL support, a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is definitely worth checking out. DataFusion’s SQL, DataFrame, and manual PlanBuilder API let users access a sophisticated query optimizer and execution engine capable of fast, resource efficient, and parallel execution that takes optimal advantage of today’s multicore hardware. Being written in Rust means DataFusion can offer both the safety of a dynamic language and the resource efficiency of a compiled language. The Apache Arrow team is pleased to announce the DataFusion 8.0.0 release (and also the release of version 0.7.0 of the Ballista subproject). This covers 3 months of development work and includes 279 commits from the following 49 distinct contributors. 39 Andy Grove 33 Andrew Lamb 21 DuRipeng 20 Yijie Shen 19 Yang Jiang 17 Raphael Taylor-Davies 11 Dan Harris 11 Matthew Turner 11 yahoNanJing 9 dependabot[bot] 8 jakevin 6 Kun Liu 5 Jiayu Liu 4 Daniël Heres 4 mingmwang 4 xudong.w 3 Carol (Nichols || Goulding) 3 Dmitry Patsura 3 Eduard Karacharov 3 Jeremy Dyer 3 Kaushik 3 Rich 3 comphead 3 gaojun2048 3 Feynman Han 2 Jie Han 2 Jon Mease 2 Tim Van Wassenhove 2 Yt 2 Zhang Li 2 silence-coding 1 Alexander Spies 1 George Andronchik 1 Guillaume Balaine 1 Hao Xin 1 Jiacai Liu 1 Jörn Horstmann 1 Liang-Chi Hsieh 1 Max Burke 1 NaincyKumariKnoldus 1 Nga Tran 1 Patrick More 1 Pierre Zemb 1 Remzi Yang 1 Sergey Melnychuk 1 Stephen Carman 1 doki The following sections highlight some of the changes in this release. Of course, many other bug fixes and improvements have been made and we encourage you to check out the changelog for full details. Summary DDL Support DDL support has been expanded to include the following commands for creating databases, schemas, and views. This allows DataFusion to be used more effectively from the CLI. CREATE DATABASE CREATE VIEW CREATE SCHEMA CREATE EXTERNAL TABLE now supports JSON files, IF NOT EXISTS, and partition columns SQL Support The SQL query planner now supports a number of new SQL features, including: Subqueries: when used via IN, EXISTS, and as scalars Grouping Sets: CUBE and ROLLUP grouping sets. Aggregate functions: approx_percentile, approx_percentile_cont, approx_percentile_cont_with_weight, approx_distinct, approx_median and array null literals bitwise operations: for example ‘|’ There are also many bug fixes and improvements around normalizing identifiers consistently. We continue our tradition of incrementally releasing support for new features as they are developed. Thus, while the physical plan may not yet support all new features, it gets more complete each release. These changes also make DataFusion an increasingly compelling choice for projects looking for a SQL parser and query planner that can produce optimized logical plans that can be translated to their own execution engine. Query Execution &amp; Internals There are several notable improvements and new features in the query execution engine: The ExecutionContext has been renamed to SessionContext and now supports multi-tenancy The ExecutionPlan trait is no longer async A new serialization API for serializing plans to bytes (based on protobuf) In addition, we have added several foundational features to drive even more advanced query processing into DataFusion, focusing on running arbitrary queries larger than available memory, and pushing the envelope for performance of sorting, grouping, and joining even further: Morsel-Driven Scheduler based on “Morsel-Driven Parallelism: A NUMA-Aware Query Evaluation Framework for the Many-Core Age” Consolidated object store implementation and integration with parquet decoding Memory Limited Spilling sort operator Memory Limited Sort-Merge join operator High performance JIT code generation for tuple comparisons Memory efficient Row Format Improved file support DataFusion now supports JSON, both for reading and writing. There are also new DataFrame methods for writing query results to files in CSV, Parquet, and JSON format. Ballista Ballista continues to mature and now supports a wider range of operators and expressions. There are also improvements to the scheduler to support UDFs, and there are some robustness improvements, such as cleaning up work directories and persisting session configs to allow schedulers to restart and continue processing in-flight jobs. Upcoming Work Here are some of the initiatives that the community plans on working on prior to the next release. There is a proposal to move Ballista to its own top-level arrow-ballista repository to decouple DataFusion and Ballista releases and to allow each project to have documentation better targeted at its particular audience. We plan on increasing the frequency of DataFusion releases, with monthly releases now instead of quarterly. This is driven by requests from the increasing number of projects that now depend on DataFusion. There is ongoing work to implement new optimizer rules to rewrite queries containing subquery expressions as joins, to support a wider range of queries. The new scheduler based on morsel-driven execution will continue to evolve in this next release, with work to refine IO abstractions to improve performance and integration with the new scheduler. Improved performance for Sort, Grouping and Joins How to Get Involved If you are interested in contributing to DataFusion, and learning about state-of-the-art query processing, we would love to have you join us on the journey! You can help by trying out DataFusion on some of your own data and projects and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable for beginners is here Check out our new Communication Doc on more ways to engage with the community.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 8.0.0 Release</title><link href="https://arrow.apache.org/blog/2022/05/15/8.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 8.0.0 Release" /><published>2022-05-15T00:00:00-04:00</published><updated>2022-05-15T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/05/15/8.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/05/15/8.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 8.0.0 release. This covers
over 3 months of development work and includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%208.0.0"><strong>586 resolved issues</strong></a>
from <a href="/release/8.0.0.html#contributors"><strong>127 distinct contributors</strong></a>. See the Install Page to learn how to
get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/8.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 7.0.0 release, Kun Liu, Raphael Taylor-Davies Xudong Wang, Yijie Shen
and Liang-Chi Hsieh have been invited to be committers.
Thanks for your contributions and participation in the project!</p>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>

<p>Flight SQL has been extended with a method to get type metadata (<a href="https://issues.apache.org/jira/browse/ARROW-15313">ARROW-15313</a>) and column metadata in returned schemas (<a href="https://issues.apache.org/jira/browse/ARROW-15314">ARROW-15314</a>, <a href="https://issues.apache.org/jira/browse/ARROW-16064">ARROW-16064</a>) New documentation is available describing Flight and Flight SQL, along with several Cookbook recipes (<a href="https://issues.apache.org/jira/browse/ARROW-14698">ARROW-14698</a>, <a href="https://issues.apache.org/jira/browse/ARROW-16065">ARROW-16065</a>).</p>

<p>The C++ libraries now support UCX as a network transport (<a href="https://issues.apache.org/jira/browse/ARROW-15706">ARROW-15706</a>), and the APIs have been refactored to allow other transports to be implemented (<a href="https://issues.apache.org/jira/browse/ARROW-15282">ARROW-15282</a>). UCX support is experimental and still subject to change. Many of the APIs have been refactored to use the <code class="language-plaintext highlighter-rouge">arrow::Result</code> type, and the original variants have been deprecated (<a href="https://issues.apache.org/jira/browse/ARROW-16032">ARROW-16032</a>). Support for gRPC &gt;= 1.43 has been added (<a href="https://issues.apache.org/jira/browse/ARROW-15551">ARROW-15551</a>).</p>

<h2 id="c-notes">C++ notes</h2>

<h3 id="compute">Compute</h3>

<p>Arrow C++ can now optionally build with support for the experimental
<a href="https://substrait.io/">Substrait</a> query representation format (<a href="https://issues.apache.org/jira/browse/ARROW-15238">ARROW-15238</a>).</p>

<p>A number of compute kernels operating on temporal data have been added:</p>

<ul>
  <li>addition, subtraction and multiplication between various temporal types;</li>
  <li>a new “is_dst” function to compute whether the input timestamps fall within
daylight saving time (DST);</li>
  <li>a new “is_leap_year” function to compute whether the input timestamps fall
within a leap year.</li>
</ul>

<p>It is possible to enable a timezone database on Windows at runtime
by calling the <code class="language-plaintext highlighter-rouge">arrow::Initialize()</code> function (<a href="https://issues.apache.org/jira/browse/ARROW-13168">ARROW-13168</a>).</p>

<p>New hash aggregations are available: “hash_one” to return one value from each
group (<a href="https://issues.apache.org/jira/browse/ARROW-13993">ARROW-13993</a>), and “hash_list” to return all values from each group
(<a href="https://issues.apache.org/jira/browse/ARROW-15152">ARROW-15152</a>).  Null columns are now supported on the sum, mean and product
hash aggregates (<a href="https://issues.apache.org/jira/browse/ARROW-15506">ARROW-15506</a>).  Also, it is now possible to execute hash
“aggregations” with only key columns (<a href="https://issues.apache.org/jira/browse/ARROW-15609">ARROW-15609</a>).</p>

<p>A new compute function “map_lookup” allows looking up a given key in a map
array (<a href="https://issues.apache.org/jira/browse/ARROW-15089">ARROW-15089</a>).</p>

<p>New compute functions “sqrt” and “sqrt_checked” allow extracting the square
root of their input (<a href="https://issues.apache.org/jira/browse/ARROW-15614">ARROW-15614</a>).</p>

<p>Casting between two struct types is now possible, assuming the destination field
names all exist in the source struct type (<a href="https://issues.apache.org/jira/browse/ARROW-1888">ARROW-1888</a>, <a href="https://issues.apache.org/jira/browse/ARROW-15643">ARROW-15643</a>).</p>

<p>Optional OpenTelemetry tracing has been added to kernel functions and execution
plan nodes (<a href="https://issues.apache.org/jira/browse/ARROW-15061">ARROW-15061</a>).</p>

<p>The CMake build option <code class="language-plaintext highlighter-rouge">ARROW_ENGINE</code> has been renamed to <code class="language-plaintext highlighter-rouge">ARROW_SUBSTRAIT</code>,
to better reflect its actual effect (<a href="https://issues.apache.org/jira/browse/ARROW-16158">ARROW-16158</a>).</p>

<h3 id="csv">CSV</h3>

<p>It is now possible to change the field delimiter when writing a CSV file
(<a href="https://issues.apache.org/jira/browse/ARROW-15672">ARROW-15672</a>).</p>

<h3 id="dataset">Dataset</h3>

<p>The ORC dataset scanner now observes the batch size parameter (<a href="https://issues.apache.org/jira/browse/ARROW-14153">ARROW-14153</a>).</p>

<p>The dataset layer now supports filename-based partitioning, where the data
files are all laid out in the dataset’s base directory, their names prefixed
with the partition values separated by underscore characters (<a href="https://issues.apache.org/jira/browse/ARROW-14612">ARROW-14612</a>).</p>

<p>Optional OpenTelemetry tracing has been added to the dataset scanner (<a href="https://issues.apache.org/jira/browse/ARROW-15067">ARROW-15067</a>).</p>

<h3 id="filesystem">Filesystem</h3>

<p>It is possible to instantiate a Google Cloud Storage (GCS) filesystem
from a URI, making GCS implicitly usable in the datasets layer (<a href="https://issues.apache.org/jira/browse/ARROW-14893">ARROW-14893</a>).
Recognized URI schemes are <code class="language-plaintext highlighter-rouge">gs</code> and <code class="language-plaintext highlighter-rouge">gcs</code>.</p>

<p><code class="language-plaintext highlighter-rouge">FileSystem::DeleteDirContents</code> can now optionally succeed when the directory
doesn’t exist (<a href="https://issues.apache.org/jira/browse/ARROW-16159">ARROW-16159</a>).</p>

<h3 id="io">IO</h3>

<p>It is possible to override the number of IO threads using the
environment variable <code class="language-plaintext highlighter-rouge">ARROW_IO_THREADS</code> (<a href="https://issues.apache.org/jira/browse/ARROW-15941">ARROW-15941</a>).</p>

<h3 id="ipc">IPC</h3>

<p>The IPC file reader and writer now allow accessing the custom metadata
associated with record batches (<a href="https://issues.apache.org/jira/browse/ARROW-16131">ARROW-16131</a>).</p>

<h3 id="miscellaneous">Miscellaneous</h3>

<p>It is possible to enable lightweight memory checks on the standard memory pools
using a dedicated environment variable <code class="language-plaintext highlighter-rouge">ARROW_DEBUG_MEMORY_POOL</code> (<a href="https://issues.apache.org/jira/browse/ARROW-15550">ARROW-15550</a>).
These checks are not a replacement for sophisticated checkers such as Address
Sanitizer or Valgrind, but might come up useful if those tools are not
available.</p>

<p>Temporal data is now validated when doing full array validation (<a href="https://issues.apache.org/jira/browse/ARROW-10924">ARROW-10924</a>).
The validation catches values not matching the specification (for example,
a time value being outside of the span of one day).</p>

<p>The GDB plugin now attempts to print the data of an array, in addition to its
metadata (<a href="https://issues.apache.org/jira/browse/ARROW-15389">ARROW-15389</a>).  This only works for primitive datatypes.</p>

<p>Pretty-printing is now shorter and more customizable for nested datatypes
(<a href="https://issues.apache.org/jira/browse/ARROW-14798">ARROW-14798</a>).</p>

<h2 id="c-notes-1">C# notes</h2>

<p>With <a href="https://devblogs.microsoft.com/dotnet/net-core-2-1-will-reach-end-of-support-on-august-21-2021">.NET Core 2.1 reaching end-of-life</a> in August 2021, the Apache.Arrow library has been updated to target <code class="language-plaintext highlighter-rouge">netcoreapp3.1</code> and higher. It still supports <code class="language-plaintext highlighter-rouge">netstandard1.3</code>, so the library works on .NET Framework. But to get the best performance, using .NET Core 3.1, .NET 5, or later is recommended.</p>

<h2 id="go-notes">Go notes</h2>

<h3 id="bug-fixes">Bug fixes</h3>

<ul>
  <li>parquet_reader / parquet_schema no longer crash (<a href="https://github.com/apache/arrow/pull/12303">ARROW-15509</a>)</li>
  <li>Base64 encoding of origin Arrow schema properly uses padding and decodes both with or without the padding in pqarrow.getOriginSchema (<a href="https://github.com/apache/arrow/pull/12679">ARROW-15544</a>)</li>
  <li>ipc.Writer no longer includes unnecessary offsets when encoding sliced arrays (<a href="https://github.com/apache/arrow/pull/12453">ARROW-15715</a>)</li>
  <li>Use base64.StdEncoding for Arrow Flight Basic Auth middleware for proper encoding/decoding (<a href="https://github.com/apache/arrow/pull/12503">ARROW-15772</a>)</li>
  <li>Fix panic during concurrent compression of ipc body buffers due to negative WaitGroup counter (<a href="https://github.com/apache/arrow/pull/12518">ARROW-15792</a>)</li>
  <li>Fix memory leak in pqarrow.NewColumnWriter with nested structures (<a href="https://github.com/apache/arrow/pull/12641">ARROW-15946</a>)</li>
  <li>ipc.FileReader no longer leaks memory when using ZSTD compression (<a href="https://github.com/apache/arrow/pull/12857">ARROW-16163</a>)</li>
</ul>

<h3 id="enhancements">Enhancements</h3>

<h4 id="flight-rpc">Flight RPC</h4>

<ul>
  <li><em>Breaking Change</em> gRPC version is updated and Flight Server creation has been simplified. Flight servers must now embed flight.BaseFlightServer (<a href="https://github.com/apache/arrow/pull/12233">ARROW-15418</a>)</li>
  <li>You can now provide a full net.Listener as an alternative to just providing an address to bind to when creating a Flight server (<a href="https://github.com/apache/arrow/pull/12768">ARROW-16082</a>)</li>
</ul>

<h4 id="parquet">Parquet</h4>

<ul>
  <li>‘unpack_bool’, sum_float64, and bitmap handling functions have been given optimized assembly implementation for Arm64 NEON (<a href="https://github.com/apache/arrow/pull/12398">ARROW-15440</a>, <a href="https://github.com/apache/arrow/pull/12502">ARROW-15742</a>, <a href="https://github.com/apache/arrow/pull/12687">ARROW-15995</a>)</li>
  <li>Go Parquet handling has been simplified to only need io.ReadSeeker instead of a ReadAtSeeker interface (<a href="https://github.com/apache/arrow/pull/12658">ARROW-15963</a>)</li>
  <li>BitSetRunReader and helper functions have been lifted to internal/bitutils package to share between arrow and parquet implementations (<a href="https://github.com/apache/arrow/pull/12926">ARROW-15950</a>)</li>
  <li>Parquet Reader now properly obeys the buffer size read property for buffered streams (<a href="https://github.com/apache/arrow/pull/12876">ARROW-16187</a>)</li>
  <li>parquet NewBufferedReader no longer panics (<a href="https://github.com/apache/arrow/pull/12960">ARROW-16283</a>)</li>
</ul>

<h4 id="arrow">Arrow</h4>

<ul>
  <li>Go Arrow library now supports Dictionary Arrays (<a href="https://issues.apache.org/jira/browse/ARROW-3039">ARROW-3039</a>, <a href="https://issues.apache.org/jira/browse/ARROW-9378">ARROW-9378</a>, <a href="https://github.com/apache/arrow/pull/12158">GH-12158</a>)</li>
  <li>array.ArrayEqual, array.ArrayApproxEqual have been renamed to array.Equal and array.ApproxEqual. Aliases are provided to avoid breaking existing code which will be removed in v9. (<a href="https://github.com/apache/arrow/pull/12877">ARROW-5598</a>)</li>
  <li>Custom cpu discovery package replaced with using golang.org/x/sys/cpu (<a href="https://github.com/apache/arrow/pull/12764">ARROW-16193</a>)</li>
  <li><em>Breaking Change</em> array.Interface, array.Record, array.Table, etc. were deprecated in v7 in favor of arrow.Array, arrow.Record, etc. The deprecated aliases have been removed in v8 (<a href="https://github.com/apache/arrow/pull/12960">ARROW-16192</a>)</li>
</ul>

<h4 id="ci">CI</h4>

<ul>
  <li>staticcheck is now run as part of CI to lint the Go code (<a href="https://github.com/apache/arrow/pull/12540">ARROW-15296</a>)</li>
  <li>Travis builds on Arm64 for Go are no longer allowed to fail (<a href="https://github.com/apache/arrow/pull/12214">ARROW-15400</a>)</li>
</ul>

<h2 id="java-notes">Java notes</h2>

<ul>
  <li>Java 17 is now supported as a target and has been added to tested platforms</li>
  <li>When scanning datasets an <code class="language-plaintext highlighter-rouge">ArrowReader</code> is now returned, which makes easier to create <code class="language-plaintext highlighter-rouge">VectorSchemaRoot</code> from it.</li>
  <li>Java Documentation had a overall improvement, with few sections added and most sections rewritten as more clear tutorials</li>
  <li>Overall improvements to FlightSQL support in Java</li>
  <li><a href="https://arrow.apache.org/cookbook/java/index.html">Java Cookbook</a> is now available</li>
</ul>

<h2 id="javascript-notes">JavaScript notes</h2>

<ul>
  <li>Tables now allow setting arbitrary symbols, which enables support for passing Arrow tables to Vega. <a href="https://github.com/apache/arrow/pull/12907">ARROW-16209</a></li>
  <li>Arrow now supports <code class="language-plaintext highlighter-rouge">tableFromJSON</code> and struct vectors in <code class="language-plaintext highlighter-rouge">vectorFromArray</code>. <a href="https://github.com/apache/arrow/pull/12908">ARROW-16210</a></li>
  <li>Fixed support for appending null children in a StructBuilder. <a href="https://github.com/apache/arrow/pull/12451">ARROW-15705</a></li>
</ul>

<h2 id="python-notes">Python notes</h2>

<p>In general, the Python bindings benefit from improvements in the C++ library
(e.g. new compute functions); see the C++ notes above for additional details.
In addition:</p>

<ul>
  <li>Tables and Datasets now support the <code class="language-plaintext highlighter-rouge">join</code> operation to perform <code class="language-plaintext highlighter-rouge">left</code>, <code class="language-plaintext highlighter-rouge">right</code>, <code class="language-plaintext highlighter-rouge">full</code> joins of <code class="language-plaintext highlighter-rouge">inner</code> or <code class="language-plaintext highlighter-rouge">outer</code> types. The result of the join operation will be a new table (<a href="https://issues.apache.org/jira/browse/ARROW-14293">ARROW-14293</a>). See https://arrow.apache.org/docs/dev/python/compute.html#table-and-dataset-joins for examples.</li>
  <li>Additional legacy keywords and properties of the <code class="language-plaintext highlighter-rouge">ParquetDataset</code> class have been deprecated and will issue a warning, in favor of functionality based on the <code class="language-plaintext highlighter-rouge">pyarrow.dataset</code> functionality (<a href="https://issues.apache.org/jira/browse/ARROW-16119">ARROW-16119</a>).</li>
  <li>It is now possible to create references to nested fields in a Table or Dataset using <code class="language-plaintext highlighter-rouge">py.field("a", "b")</code> (<a href="https://issues.apache.org/jira/browse/ARROW-11259">ARROW-11259</a>).</li>
  <li>Docstrings in <code class="language-plaintext highlighter-rouge">Schema</code>, <code class="language-plaintext highlighter-rouge">ChunkedArray</code>, <code class="language-plaintext highlighter-rouge">Tensor</code>, <code class="language-plaintext highlighter-rouge">RecordBatch</code>, <code class="language-plaintext highlighter-rouge">parquet</code> and <code class="language-plaintext highlighter-rouge">Table</code> now include examples on how to use the methods and classes (<a href="https://issues.apache.org/jira/browse/ARROW-15367">ARROW-15367</a>).</li>
  <li>Reading and writing Parquet files now supports encryption (<a href="https://issues.apache.org/jira/browse/ARROW-9947">ARROW-9947</a>). See the <a href="https://arrow.apache.org/docs/python/parquet.html#parquet-modular-encryption-columnar-encryption">docs</a> for more details.</li>
  <li>Support for <code class="language-plaintext highlighter-rouge">zoneinfo</code> (Python 3.9+) and <code class="language-plaintext highlighter-rouge">dateutil</code> timezones in conversion to Arrow data structures (<a href="https://issues.apache.org/jira/browse/">ARROW-5248</a>).</li>
  <li>Multiple bugfixes and segfaults resolved.</li>
</ul>

<h2 id="r-notes">R notes</h2>

<p>This release includes:</p>

<ul>
  <li>Support for over 20 additional <code class="language-plaintext highlighter-rouge">lubridate</code> and <code class="language-plaintext highlighter-rouge">base</code> date and time functions in Arrow dpylr queries,</li>
  <li>An API to allow external packages to define custom extension array types and custom conversions into Arrow types,</li>
  <li>Support for concatenating Arrays, RecordBatches, and Tables, including with <code class="language-plaintext highlighter-rouge">c()</code>, <code class="language-plaintext highlighter-rouge">rbind()</code> and <code class="language-plaintext highlighter-rouge">cbind()</code>.</li>
</ul>

<p>For more on what’s in the 8.0.0 R package, see the <a href="/docs/r/news/">R changelog</a>.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<h3 id="ruby">Ruby</h3>

<ul>
  <li>Add support for <code class="language-plaintext highlighter-rouge">#values</code> of <code class="language-plaintext highlighter-rouge">MonthInterval</code> Type (<a href="https://issues.apache.org/jira/browse/ARROW-15749">ARROW-15749</a>)</li>
  <li>Add support for <code class="language-plaintext highlighter-rouge">#raw_records</code> of <code class="language-plaintext highlighter-rouge">MonthInterval</code> type (<a href="https://issues.apache.org/jira/browse/ARROW-15750">ARROW-15750</a>)</li>
  <li>Add support for <code class="language-plaintext highlighter-rouge">#values</code> of <code class="language-plaintext highlighter-rouge">DayTimeInterval</code> type (<a href="https://issues.apache.org/jira/browse/ARROW-15885">ARROW-15885</a>)</li>
  <li>Add <code class="language-plaintext highlighter-rouge">DayTimeIntervalArrayBuilder</code> to support to make <code class="language-plaintext highlighter-rouge">DayTimeIntervalArray</code> by a Hash with <code class="language-plaintext highlighter-rouge">:day</code> and <code class="language-plaintext highlighter-rouge">:millisecond</code> keys (<a href="https://issues.apache.org/jira/browse/ARROW-15918">ARROW-15918</a>)</li>
  <li>Add support for <code class="language-plaintext highlighter-rouge">#raw_records</code> of <code class="language-plaintext highlighter-rouge">DayTimeInterval</code> type (<a href="https://issues.apache.org/jira/browse/ARROW-15886">ARROW-15886</a>)</li>
  <li>Add support for <code class="language-plaintext highlighter-rouge">#values</code> of <code class="language-plaintext highlighter-rouge">MonthDayNanoInterval</code> type (<a href="https://issues.apache.org/jira/browse/ARROW-15924">ARROW-15924</a>)
    <ul>
      <li>Also add <code class="language-plaintext highlighter-rouge">MonthDayNanoIntervalArrayBuilder</code> to support to make <code class="language-plaintext highlighter-rouge">MonthDayNanoIntervalArray</code> by a Hash with <code class="language-plaintext highlighter-rouge">:month</code>, <code class="language-plaintext highlighter-rouge">:day</code>, and <code class="language-plaintext highlighter-rouge">:nanosecond</code> keys</li>
    </ul>
  </li>
  <li>Add support for <code class="language-plaintext highlighter-rouge">#raw_records</code> of <code class="language-plaintext highlighter-rouge">MonthDayNanoInterval</code> type (<a href="https://issues.apache.org/jira/browse/ARROW-15925">ARROW-15925</a>)</li>
  <li>Add Ruby-ish interfaces for <code class="language-plaintext highlighter-rouge">Parquet::BooleanStatistics</code> (<a href="https://issues.apache.org/jira/browse/ARROW-16251">ARROW-16251</a>)</li>
</ul>

<h3 id="c-glib">C GLib</h3>

<ul>
  <li>Add <code class="language-plaintext highlighter-rouge">gaflight_client_close</code> (<a href="https://issues.apache.org/jira/browse/ARROW-15487">ARROW-15487</a>)</li>
  <li>Add <code class="language-plaintext highlighter-rouge">GParquetFileMetadata</code> and <code class="language-plaintext highlighter-rouge">gparquet_arrow_file_reader_get_metadata</code> (<a href="https://issues.apache.org/jira/browse/ARROW-16214">ARROW-16214</a>)</li>
  <li>Fix <code class="language-plaintext highlighter-rouge">GArrowGIOInputStream</code> so that all the data is completely read (<a href="https://issues.apache.org/jira/browse/ARROW-15626">ARROW-15626</a>)</li>
  <li>Add <code class="language-plaintext highlighter-rouge">garrow_string_array_builder_append_string_len</code> and <code class="language-plaintext highlighter-rouge">garrow_large_string_array_builder_append_string_len</code> (<a href="https://issues.apache.org/jira/browse/ARROW-15629">ARROW-15629</a>)</li>
  <li>Add <code class="language-plaintext highlighter-rouge">GParquetRowGroupMetadata</code> (<a href="https://issues.apache.org/jira/browse/ARROW-16245">ARROW-16245</a>)</li>
  <li>Add <code class="language-plaintext highlighter-rouge">GParquetColumnChunkMetadata</code> (<a href="https://issues.apache.org/jira/browse/ARROW-16250">ARROW-16250</a>)</li>
  <li>Add <code class="language-plaintext highlighter-rouge">GArrowGCSFileSystem</code> (<a href="https://issues.apache.org/jira/browse/ARROW-16247">ARROW-16247</a>)</li>
  <li>Add <code class="language-plaintext highlighter-rouge">GParquetStatistics</code> and its family (<a href="https://issues.apache.org/jira/browse/ARROW-16251">ARROW-16251</a>)
    <ul>
      <li><code class="language-plaintext highlighter-rouge">GParquetBooleanStatistics</code></li>
      <li><code class="language-plaintext highlighter-rouge">GParquetInt32Statistics</code></li>
      <li><code class="language-plaintext highlighter-rouge">GParquetInt64Statistics</code></li>
      <li><code class="language-plaintext highlighter-rouge">GParquetFloatStatistics</code></li>
      <li><code class="language-plaintext highlighter-rouge">GParquetDoubleStatistics</code></li>
      <li><code class="language-plaintext highlighter-rouge">GParquetByteArrayStatistics</code></li>
      <li><code class="language-plaintext highlighter-rouge">GParquetFixedLengthByteArrayStatistics</code></li>
    </ul>
  </li>
  <li>Add missing casts for <code class="language-plaintext highlighter-rouge">GArrowRoundMode</code> (<a href="https://issues.apache.org/jira/browse/ARROW-16296">ARROW-16296</a>)</li>
</ul>

<h2 id="rust-notes">Rust notes</h2>

<p>The Rust projects have moved to separate repositories outside the
main Arrow monorepo. For notes on the 13.0.0 release of the Rust
implementation, see the <a href="https://github.com/apache/arrow-rs/blob/13.0.0/CHANGELOG.md">Arrow Rust changelog</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 8.0.0 release. This covers over 3 months of development work and includes 586 resolved issues from 127 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 7.0.0 release, Kun Liu, Raphael Taylor-Davies Xudong Wang, Yijie Shen and Liang-Chi Hsieh have been invited to be committers. Thanks for your contributions and participation in the project! Arrow Flight RPC notes Flight SQL has been extended with a method to get type metadata (ARROW-15313) and column metadata in returned schemas (ARROW-15314, ARROW-16064) New documentation is available describing Flight and Flight SQL, along with several Cookbook recipes (ARROW-14698, ARROW-16065). The C++ libraries now support UCX as a network transport (ARROW-15706), and the APIs have been refactored to allow other transports to be implemented (ARROW-15282). UCX support is experimental and still subject to change. Many of the APIs have been refactored to use the arrow::Result type, and the original variants have been deprecated (ARROW-16032). Support for gRPC &gt;= 1.43 has been added (ARROW-15551). C++ notes Compute Arrow C++ can now optionally build with support for the experimental Substrait query representation format (ARROW-15238). A number of compute kernels operating on temporal data have been added: addition, subtraction and multiplication between various temporal types; a new “is_dst” function to compute whether the input timestamps fall within daylight saving time (DST); a new “is_leap_year” function to compute whether the input timestamps fall within a leap year. It is possible to enable a timezone database on Windows at runtime by calling the arrow::Initialize() function (ARROW-13168). New hash aggregations are available: “hash_one” to return one value from each group (ARROW-13993), and “hash_list” to return all values from each group (ARROW-15152). Null columns are now supported on the sum, mean and product hash aggregates (ARROW-15506). Also, it is now possible to execute hash “aggregations” with only key columns (ARROW-15609). A new compute function “map_lookup” allows looking up a given key in a map array (ARROW-15089). New compute functions “sqrt” and “sqrt_checked” allow extracting the square root of their input (ARROW-15614). Casting between two struct types is now possible, assuming the destination field names all exist in the source struct type (ARROW-1888, ARROW-15643). Optional OpenTelemetry tracing has been added to kernel functions and execution plan nodes (ARROW-15061). The CMake build option ARROW_ENGINE has been renamed to ARROW_SUBSTRAIT, to better reflect its actual effect (ARROW-16158). CSV It is now possible to change the field delimiter when writing a CSV file (ARROW-15672). Dataset The ORC dataset scanner now observes the batch size parameter (ARROW-14153). The dataset layer now supports filename-based partitioning, where the data files are all laid out in the dataset’s base directory, their names prefixed with the partition values separated by underscore characters (ARROW-14612). Optional OpenTelemetry tracing has been added to the dataset scanner (ARROW-15067). Filesystem It is possible to instantiate a Google Cloud Storage (GCS) filesystem from a URI, making GCS implicitly usable in the datasets layer (ARROW-14893). Recognized URI schemes are gs and gcs. FileSystem::DeleteDirContents can now optionally succeed when the directory doesn’t exist (ARROW-16159). IO It is possible to override the number of IO threads using the environment variable ARROW_IO_THREADS (ARROW-15941). IPC The IPC file reader and writer now allow accessing the custom metadata associated with record batches (ARROW-16131). Miscellaneous It is possible to enable lightweight memory checks on the standard memory pools using a dedicated environment variable ARROW_DEBUG_MEMORY_POOL (ARROW-15550). These checks are not a replacement for sophisticated checkers such as Address Sanitizer or Valgrind, but might come up useful if those tools are not available. Temporal data is now validated when doing full array validation (ARROW-10924). The validation catches values not matching the specification (for example, a time value being outside of the span of one day). The GDB plugin now attempts to print the data of an array, in addition to its metadata (ARROW-15389). This only works for primitive datatypes. Pretty-printing is now shorter and more customizable for nested datatypes (ARROW-14798). C# notes With .NET Core 2.1 reaching end-of-life in August 2021, the Apache.Arrow library has been updated to target netcoreapp3.1 and higher. It still supports netstandard1.3, so the library works on .NET Framework. But to get the best performance, using .NET Core 3.1, .NET 5, or later is recommended. Go notes Bug fixes parquet_reader / parquet_schema no longer crash (ARROW-15509) Base64 encoding of origin Arrow schema properly uses padding and decodes both with or without the padding in pqarrow.getOriginSchema (ARROW-15544) ipc.Writer no longer includes unnecessary offsets when encoding sliced arrays (ARROW-15715) Use base64.StdEncoding for Arrow Flight Basic Auth middleware for proper encoding/decoding (ARROW-15772) Fix panic during concurrent compression of ipc body buffers due to negative WaitGroup counter (ARROW-15792) Fix memory leak in pqarrow.NewColumnWriter with nested structures (ARROW-15946) ipc.FileReader no longer leaks memory when using ZSTD compression (ARROW-16163) Enhancements Flight RPC Breaking Change gRPC version is updated and Flight Server creation has been simplified. Flight servers must now embed flight.BaseFlightServer (ARROW-15418) You can now provide a full net.Listener as an alternative to just providing an address to bind to when creating a Flight server (ARROW-16082) Parquet ‘unpack_bool’, sum_float64, and bitmap handling functions have been given optimized assembly implementation for Arm64 NEON (ARROW-15440, ARROW-15742, ARROW-15995) Go Parquet handling has been simplified to only need io.ReadSeeker instead of a ReadAtSeeker interface (ARROW-15963) BitSetRunReader and helper functions have been lifted to internal/bitutils package to share between arrow and parquet implementations (ARROW-15950) Parquet Reader now properly obeys the buffer size read property for buffered streams (ARROW-16187) parquet NewBufferedReader no longer panics (ARROW-16283) Arrow Go Arrow library now supports Dictionary Arrays (ARROW-3039, ARROW-9378, GH-12158) array.ArrayEqual, array.ArrayApproxEqual have been renamed to array.Equal and array.ApproxEqual. Aliases are provided to avoid breaking existing code which will be removed in v9. (ARROW-5598) Custom cpu discovery package replaced with using golang.org/x/sys/cpu (ARROW-16193) Breaking Change array.Interface, array.Record, array.Table, etc. were deprecated in v7 in favor of arrow.Array, arrow.Record, etc. The deprecated aliases have been removed in v8 (ARROW-16192) CI staticcheck is now run as part of CI to lint the Go code (ARROW-15296) Travis builds on Arm64 for Go are no longer allowed to fail (ARROW-15400) Java notes Java 17 is now supported as a target and has been added to tested platforms When scanning datasets an ArrowReader is now returned, which makes easier to create VectorSchemaRoot from it. Java Documentation had a overall improvement, with few sections added and most sections rewritten as more clear tutorials Overall improvements to FlightSQL support in Java Java Cookbook is now available JavaScript notes Tables now allow setting arbitrary symbols, which enables support for passing Arrow tables to Vega. ARROW-16209 Arrow now supports tableFromJSON and struct vectors in vectorFromArray. ARROW-16210 Fixed support for appending null children in a StructBuilder. ARROW-15705 Python notes In general, the Python bindings benefit from improvements in the C++ library (e.g. new compute functions); see the C++ notes above for additional details. In addition: Tables and Datasets now support the join operation to perform left, right, full joins of inner or outer types. The result of the join operation will be a new table (ARROW-14293). See https://arrow.apache.org/docs/dev/python/compute.html#table-and-dataset-joins for examples. Additional legacy keywords and properties of the ParquetDataset class have been deprecated and will issue a warning, in favor of functionality based on the pyarrow.dataset functionality (ARROW-16119). It is now possible to create references to nested fields in a Table or Dataset using py.field("a", "b") (ARROW-11259). Docstrings in Schema, ChunkedArray, Tensor, RecordBatch, parquet and Table now include examples on how to use the methods and classes (ARROW-15367). Reading and writing Parquet files now supports encryption (ARROW-9947). See the docs for more details. Support for zoneinfo (Python 3.9+) and dateutil timezones in conversion to Arrow data structures (ARROW-5248). Multiple bugfixes and segfaults resolved. R notes This release includes: Support for over 20 additional lubridate and base date and time functions in Arrow dpylr queries, An API to allow external packages to define custom extension array types and custom conversions into Arrow types, Support for concatenating Arrays, RecordBatches, and Tables, including with c(), rbind() and cbind(). For more on what’s in the 8.0.0 R package, see the R changelog. Ruby and C GLib notes Ruby Add support for #values of MonthInterval Type (ARROW-15749) Add support for #raw_records of MonthInterval type (ARROW-15750) Add support for #values of DayTimeInterval type (ARROW-15885) Add DayTimeIntervalArrayBuilder to support to make DayTimeIntervalArray by a Hash with :day and :millisecond keys (ARROW-15918) Add support for #raw_records of DayTimeInterval type (ARROW-15886) Add support for #values of MonthDayNanoInterval type (ARROW-15924) Also add MonthDayNanoIntervalArrayBuilder to support to make MonthDayNanoIntervalArray by a Hash with :month, :day, and :nanosecond keys Add support for #raw_records of MonthDayNanoInterval type (ARROW-15925) Add Ruby-ish interfaces for Parquet::BooleanStatistics (ARROW-16251) C GLib Add gaflight_client_close (ARROW-15487) Add GParquetFileMetadata and gparquet_arrow_file_reader_get_metadata (ARROW-16214) Fix GArrowGIOInputStream so that all the data is completely read (ARROW-15626) Add garrow_string_array_builder_append_string_len and garrow_large_string_array_builder_append_string_len (ARROW-15629) Add GParquetRowGroupMetadata (ARROW-16245) Add GParquetColumnChunkMetadata (ARROW-16250) Add GArrowGCSFileSystem (ARROW-16247) Add GParquetStatistics and its family (ARROW-16251) GParquetBooleanStatistics GParquetInt32Statistics GParquetInt64Statistics GParquetFloatStatistics GParquetDoubleStatistics GParquetByteArrayStatistics GParquetFixedLengthByteArrayStatistics Add missing casts for GArrowRoundMode (ARROW-16296) Rust notes The Rust projects have moved to separate repositories outside the main Arrow monorepo. For notes on the 13.0.0 release of the Rust implementation, see the Arrow Rust changelog.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow for R Cheatsheet</title><link href="https://arrow.apache.org/blog/2022/04/27/arrow-r-cheatsheet/" rel="alternate" type="text/html" title="Apache Arrow for R Cheatsheet" /><published>2022-04-27T00:00:00-04:00</published><updated>2022-04-27T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/04/27/arrow-r-cheatsheet</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/04/27/arrow-r-cheatsheet/"><![CDATA[<!--

-->

<p>We are excited to introduce the new <a href="https://github.com/apache/arrow/blob/master/r/cheatsheet/arrow-cheatsheet.pdf">Apache Arrow for R Cheatsheet</a>.</p>

<div align="center">
<a href="https://github.com/apache/arrow/blob/master/r/cheatsheet/arrow-cheatsheet.pdf">
<img src="/img/20220427-arrow-r-cheatsheet-thumbnail.png" alt="Thumbnail image of the first page of the Arrow for R cheatsheet." width="70%" height="70%" />
</a>
</div>

<h2 id="helping-not-cheating">Helping (Not Cheating)</h2>

<p>While <a href="https://en.wikipedia.org/wiki/Cheat_sheet">cheatsheets</a> may have started as a set of notes used without an instructor’s knowledge—so, ummm, cheating—using the Arrow for R cheatsheet is definitely not cheating! Today, cheatsheets are a common tool to provide users an introduction to software’s functionality and a quick reference guide to help users get started.</p>

<p>The Arrow for R cheatsheet is intended to be an easy-to-scan introduction to the Arrow R package and Arrow data structures, with getting started sections on some of the package’s main functionality. The cheatsheet includes introductory snippets on using Arrow to read and work with larger-than-memory multi-file data sets, sending and receiving data with Flight, reading data from cloud storage without downloading the data first, and more. The Arrow for R cheatsheet also directs users to the full <a href="https://arrow.apache.org/docs/r/index.html">Arrow for R package documentation and articles</a> and the <a href="https://arrow.apache.org/cookbook/r/">Arrow Cookbook</a>, both full of code examples and recipes to support users build their Arrow-based data workflows. Finally, the cheatsheet debuts one of the first uses of the hot-off-the-presses Arrow hex sticker, recently made available as part of the <a href="https://arrow.apache.org/visual_identity/">Apache Arrow visual identity guidance</a>.</p>

<h2 id="cheatsheet-maintenance">Cheatsheet Maintenance</h2>

<p>See something that needs updating? Or want to suggest a change? Like software itself, a package cheatsheet needs maintenance to keep pace with new features or user-facing changes. Contributions can be made by downloading and making changes to the <a href="https://github.com/apache/arrow/tree/master/r/cheatsheet"><code class="language-plaintext highlighter-rouge">arrow-cheatsheet.pptx</code> file</a> (in Microsoft PowerPoint or Google Slides), and offering the revised <code class="language-plaintext highlighter-rouge">.pptx</code> and rendered PDF back to the project following the <em>new</em> <a href="https://arrow.apache.org/docs/developers/guide/step_by_step/set_up.html">New Contributors Guide</a>. Since a cheatsheet contribution does not touch the Arrow codebase, cheatsheet contributors don’t need to build the package or worry about running (or writing!) code tests. The New Contributors Guide will walk you through how to get set up with git, fork the Arrow GitHub repository, make a branch, replace the <code class="language-plaintext highlighter-rouge">.pptx</code> and <code class="language-plaintext highlighter-rouge">.pdf</code> files with your editions, and contribute the changes with a <a href="https://arrow.apache.org/docs/developers/guide/step_by_step/pr_and_github.html">Pull Request</a>. Questions and support are always available through the <a href="https://arrow.apache.org/community/">community mailing list</a>.</p>

<h2 id="by-the-community-for-the-community">By the Community For the Community</h2>

<p>The Arrow for R cheatsheet was initiated by Mauricio (Pachá) Vargas Sepúlveda (<a href="https://issues.apache.org/jira/browse/ARROW-13616">ARROW-13616</a>) and was co-developed and reviewed by many Apache Arrow community members. The cheatsheet was created by the community for the community, and anyone in the Arrow community is welcome and encouraged to help with maintenance and offer improvements. Thank you for your support!</p>]]></content><author><name>stephhazlitt</name></author><category term="application" /><summary type="html"><![CDATA[We are excited to introduce the new Apache Arrow for R Cheatsheet. Helping (Not Cheating) While cheatsheets may have started as a set of notes used without an instructor’s knowledge—so, ummm, cheating—using the Arrow for R cheatsheet is definitely not cheating! Today, cheatsheets are a common tool to provide users an introduction to software’s functionality and a quick reference guide to help users get started. The Arrow for R cheatsheet is intended to be an easy-to-scan introduction to the Arrow R package and Arrow data structures, with getting started sections on some of the package’s main functionality. The cheatsheet includes introductory snippets on using Arrow to read and work with larger-than-memory multi-file data sets, sending and receiving data with Flight, reading data from cloud storage without downloading the data first, and more. The Arrow for R cheatsheet also directs users to the full Arrow for R package documentation and articles and the Arrow Cookbook, both full of code examples and recipes to support users build their Arrow-based data workflows. Finally, the cheatsheet debuts one of the first uses of the hot-off-the-presses Arrow hex sticker, recently made available as part of the Apache Arrow visual identity guidance. Cheatsheet Maintenance See something that needs updating? Or want to suggest a change? Like software itself, a package cheatsheet needs maintenance to keep pace with new features or user-facing changes. Contributions can be made by downloading and making changes to the arrow-cheatsheet.pptx file (in Microsoft PowerPoint or Google Slides), and offering the revised .pptx and rendered PDF back to the project following the new New Contributors Guide. Since a cheatsheet contribution does not touch the Arrow codebase, cheatsheet contributors don’t need to build the package or worry about running (or writing!) code tests. The New Contributors Guide will walk you through how to get set up with git, fork the Arrow GitHub repository, make a branch, replace the .pptx and .pdf files with your editions, and contribute the changes with a Pull Request. Questions and support are always available through the community mailing list. By the Community For the Community The Arrow for R cheatsheet was initiated by Mauricio (Pachá) Vargas Sepúlveda (ARROW-13616) and was co-developed and reviewed by many Apache Arrow community members. The cheatsheet was created by the community for the community, and anyone in the Arrow community is welcome and encouraged to help with maintenance and offer improvements. Thank you for your support!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Introducing Apache Arrow DataFusion Contrib</title><link href="https://arrow.apache.org/blog/2022/03/21/datafusion-contrib/" rel="alternate" type="text/html" title="Introducing Apache Arrow DataFusion Contrib" /><published>2022-03-21T00:00:00-04:00</published><updated>2022-03-21T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/03/21/datafusion-contrib</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/03/21/datafusion-contrib/"><![CDATA[<!--

-->

<h1 id="introduction">Introduction</h1>

<p>Apache Arrow <a href="https://arrow.apache.org/datafusion/">DataFusion</a> is an extensible query execution framework, written in Rust, that uses <a href="https://arrow.apache.org">Apache Arrow</a> as its in-memory format.</p>

<p>When you want to extend your Rust project with <a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html">SQL support</a>, a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is definitely worth checking out. DataFusion’s pluggable design makes creating extensions at various points particular easy to build.</p>

<p>DataFusion’s  SQL, <code class="language-plaintext highlighter-rouge">DataFrame</code>, and manual <code class="language-plaintext highlighter-rouge">PlanBuilder</code> API let users access a sophisticated query optimizer and execution engine capable of fast, resource efficient, and parallel execution that takes optimal advantage of todays multicore hardware. Being written in Rust means DataFusion can offer <em>both</em> the safety of dynamic languages as well as the resource efficiency of a compiled language.</p>

<p>The DataFusion team is pleased to announce the creation of the <a href="https://github.com/datafusion-contrib">DataFusion-Contrib</a> GitHub organization to support and accelerate other projects.  While the core DataFusion library remains under Apache governance, the contrib organization provides a more flexible testing ground for new DataFusion features and a home for DataFusion extensions.  With this announcement, we are pleased to introduce the following inaugural DataFusion-Contrib repositories.</p>

<h2 id="datafusion-python">DataFusion-Python</h2>

<p>This <a href="https://github.com/datafusion-contrib/datafusion-python">project</a> provides Python bindings to the core Rust implementation of DataFusion, which allows users to:</p>

<ul>
  <li>Work with familiar SQL or DataFrame APIs to run queries in a safe, multi-threaded environment, returning results in Python</li>
  <li>Create User Defined Functions and User Defined Aggregate Functions for complex operations</li>
  <li>Pay no overhead to copy between Python and underlying Rust execution engine (by way of Apache Arrow arrays)</li>
</ul>

<h3 id="upcoming-enhancements">Upcoming enhancements</h3>

<p>The team is focusing on exposing more features from the underlying Rust implementation of DataFusion and improving documentation.</p>

<h3 id="how-to-install">How to install</h3>

<p>From <code class="language-plaintext highlighter-rouge">pip</code></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>datafusion
</code></pre></div></div>

<p>Or</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> pip <span class="nb">install </span>datafusion
</code></pre></div></div>

<h2 id="datafusion-objectstore-s3">DataFusion-ObjectStore-S3</h2>

<p>This <a href="https://github.com/datafusion-contrib/datafusion-objectstore-s3">crate</a> provides an <code class="language-plaintext highlighter-rouge">ObjectStore</code> implementation for querying data stored in S3 or S3 compatible storage. This makes it almost as easy to query data that lives on S3 as lives in local files</p>

<ul>
  <li>Ability to create <code class="language-plaintext highlighter-rouge">S3FileSystem</code> to register as part of DataFusion <code class="language-plaintext highlighter-rouge">ExecutionContext</code></li>
  <li>Register files or directories stored on S3 with <code class="language-plaintext highlighter-rouge">ctx.register_listing_table</code></li>
</ul>

<h3 id="upcoming-enhancements-1">Upcoming enhancements</h3>

<p>The current priority is adding python bindings for <code class="language-plaintext highlighter-rouge">S3FileSystem</code>.  After that there will be async improvements as DataFusion adopts more of that functionality and we are looking into S3 Select functionality.</p>

<h3 id="how-to-install-1">How to Install</h3>

<p>Add the below to your <code class="language-plaintext highlighter-rouge">Cargo.toml</code> in your Rust Project with DataFusion.</p>

<div class="language-toml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="py">datafusion-objectstore-s3</span> <span class="p">=</span> <span class="s">"0.1.0"</span>
</code></pre></div></div>

<h2 id="datafusion-substrait">DataFusion-Substrait</h2>

<p><a href="https://substrait.io/">Substrait</a> is an emerging standard that provides a cross-language serialization format for relational algebra (e.g. expressions and query plans).</p>

<p>This <a href="https://github.com/datafusion-contrib/datafusion-substrait">crate</a> provides a Substrait producer and consumer for DataFusion.  A producer converts a DataFusion logical plan into a Substrait protobuf and a consumer does the reverse.</p>

<p>Examples of how to use this crate can be found <a href="https://github.com/datafusion-contrib/datafusion-substrait/blob/main/src/lib.rs">here</a>.</p>

<h3 id="potential-use-cases">Potential Use Cases</h3>

<ul>
  <li>Replace custom DataFusion protobuf serialization.</li>
  <li>Make it easier to pass query plans over FFI boundaries, such as from Python to Rust</li>
  <li>Allow Apache Calcite query plans to be executed in DataFusion</li>
</ul>

<h2 id="datafusion-bigtable">DataFusion-BigTable</h2>

<p>This <a href="https://github.com/datafusion-contrib/datafusion-bigtable">crate</a> implements <a href="https://cloud.google.com/bigtable">Bigtable</a> as a data source and physical executor for DataFusion queries.  It currently supports both UTF-8 string and 64-bit big-endian signed integers in Bigtable.  From a SQL perspective it supports both simple and composite row keys with <code class="language-plaintext highlighter-rouge">=</code>, <code class="language-plaintext highlighter-rouge">IN</code>, and <code class="language-plaintext highlighter-rouge">BETWEEN</code> operators as well as projection pushdown.  The physical execution for queries is handled by this crate while any subsequent aggregation, group bys, or joins are handled in DataFusion.</p>

<h3 id="upcoming-enhancements-2">Upcoming Enhancements</h3>

<ul>
  <li>Predicate pushdown
    <ul>
      <li>Value range</li>
      <li>Value Regex</li>
      <li>Timestamp range</li>
    </ul>
  </li>
  <li>Multithreaded</li>
  <li>Partition aware execution</li>
  <li>Production ready</li>
</ul>

<h3 id="how-to-install-2">How to Install</h3>

<p>Add the below to your <code class="language-plaintext highlighter-rouge">Cargo.toml</code> in your Rust Project with DataFusion.</p>

<div class="language-toml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="py">datafusion-bigtable</span> <span class="p">=</span> <span class="s">"0.1.0"</span>
</code></pre></div></div>

<h2 id="datafusion-hdfs">DataFusion-HDFS</h2>

<p>This <a href="https://github.com/datafusion-contrib/datafusion-objectstore-hdfs">crate</a> introduces <code class="language-plaintext highlighter-rouge">HadoopFileSystem</code> as a remote <code class="language-plaintext highlighter-rouge">ObjectStore</code> which provides the ability to query HDFS files.  For HDFS access the <a href="https://github.com/yahoNanJing/fs-hdfs">fs-hdfs</a> library is used.</p>

<h2 id="datafusion-tokomak">DataFusion-Tokomak</h2>

<p>This <a href="https://github.com/datafusion-contrib/datafusion-tokomak">crate</a> provides an e-graph based DataFusion optimization framework based on the Rust <a href="https://egraphs-good.github.io">egg</a> library.  An e-graph is a data structure that powers the equality saturation optimization technique.</p>

<p>As context, the optimizer framework within DataFusion is currently <a href="https://github.com/apache/arrow-datafusion/issues/1972">under review</a> with the objective of implementing a more strategic long term solution that is more efficient and simpler to develop.</p>

<p>Some of the benefits of using <code class="language-plaintext highlighter-rouge">egg</code> within DataFusion are:</p>

<ul>
  <li>Implements optimized algorithms that are hard to match with manually written optimization passes</li>
  <li>Makes it easy and less verbose to add optimization rules</li>
  <li>Plugin framework to add more complex optimizations</li>
  <li>Egg does not depend on rule order and can lead to a higher level of optimization by being able to apply multiple rules at the same time until it converges</li>
  <li>Allows for cost-based optimizations</li>
</ul>

<p>This is an exciting new area for DataFusion with lots of opportunity for community involvement!</p>

<h2 id="datafusion-tui">DataFusion-Tui</h2>

<p><a href="https://github.com/datafusion-contrib/datafusion-tui">DataFusion-tui</a> aka <code class="language-plaintext highlighter-rouge">dft</code> provides a feature rich terminal application for using DataFusion.  It has drawn inspiration and several features from <code class="language-plaintext highlighter-rouge">datafusion-cli</code>.  In contrast to <code class="language-plaintext highlighter-rouge">datafusion-cli</code> the objective of this tool is to provide a light SQL IDE experience for querying data with DataFusion.  This includes features such as the following which are currently implemented:</p>

<ul>
  <li>Tab Management to provide clean and structured organization of DataFusion queries, results, <code class="language-plaintext highlighter-rouge">ExecutionContext</code> information, and logs
    <ul>
      <li>SQL Editor
        <ul>
          <li>Text editor for writing SQL queries</li>
        </ul>
      </li>
      <li>Query History
        <ul>
          <li>History of executed queries, their execution time, and the number of returned rows</li>
        </ul>
      </li>
      <li><code class="language-plaintext highlighter-rouge">ExecutionContext</code> information
        <ul>
          <li>Expose information on which physical optimizers are used and which <code class="language-plaintext highlighter-rouge">ExecutionConfig</code> settings are set</li>
        </ul>
      </li>
      <li>Logs
        <ul>
          <li>Logs from <code class="language-plaintext highlighter-rouge">dft</code>, DataFusion, and any dependent libraries</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Support for custom <code class="language-plaintext highlighter-rouge">ObjectStore</code>s
    <ul>
      <li>S3</li>
    </ul>
  </li>
  <li>Preload DDL from <code class="language-plaintext highlighter-rouge">~/.datafusionrc</code> to enable having local “database” available at startup</li>
</ul>

<h3 id="upcoming-enhancements-3">Upcoming Enhancements</h3>

<ul>
  <li>SQL Editor
    <ul>
      <li>Command to write query results to file</li>
      <li>Multiple SQL editor tabs</li>
    </ul>
  </li>
  <li>Expose more information from <code class="language-plaintext highlighter-rouge">ExecutionContext</code></li>
  <li>A help tab that provides information on functions</li>
  <li>Query custom <code class="language-plaintext highlighter-rouge">TableProvider</code>s such as <a href="https://github.com/delta-io/delta-rs">DeltaTable</a> or <a href="https://github.com/datafusion-contrib/datafusion-bigtable">BigTable</a></li>
</ul>

<h2 id="datafusion-streams">DataFusion-Streams</h2>

<p><a href="https://github.com/datafusion-contrib/datafusion-streams">DataFusion-Stream</a> is a new testing ground for creating a <code class="language-plaintext highlighter-rouge">StreamProvider</code> in DataFusion that will enable querying streaming data sources such as Apache Kafka.  The implementation for this feature is currently being designed and is under active review.  Once the design is finalized the trait and attendant data structures will be added back to the core DataFusion crate.</p>

<h2 id="datafusion-java">DataFusion-Java</h2>

<p>This <a href="https://github.com/datafusion-contrib/datafusion-java">project</a> created an initial set of Java bindings to DataFusion.  The project is currently in maintenance mode and is looking for maintainers to drive future development.</p>

<h1 id="how-to-get-involved">How to Get Involved</h1>

<p>If you are interested in contributing to DataFusion, and learning about state of
the art query processing, we would love to have you join us on the journey! You
can help by trying out DataFusion on some of your own data and projects and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable for beginners is <a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a></p>

<p>The best way to find out about creating new extensions within DataFusion-Contrib is reaching out on the <code class="language-plaintext highlighter-rouge">#arrow-rust</code> channel of the Apache Software Foundation <a href="https://join.slack.com/t/the-asf/shared_invite/zt-vlfbf7ch-HkbNHiU_uDlcH_RvaHv9gQ">Slack</a> workspace.</p>

<p>You can also check out our new <a href="https://arrow.apache.org/datafusion/community/communication.html">Communication Doc</a> on more ways to engage with the community.</p>

<p>Links for each DataFusion-Contrib repository are provided above if you would like to contribute to those.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[Introduction Apache Arrow DataFusion is an extensible query execution framework, written in Rust, that uses Apache Arrow as its in-memory format. When you want to extend your Rust project with SQL support, a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is definitely worth checking out. DataFusion’s pluggable design makes creating extensions at various points particular easy to build. DataFusion’s SQL, DataFrame, and manual PlanBuilder API let users access a sophisticated query optimizer and execution engine capable of fast, resource efficient, and parallel execution that takes optimal advantage of todays multicore hardware. Being written in Rust means DataFusion can offer both the safety of dynamic languages as well as the resource efficiency of a compiled language. The DataFusion team is pleased to announce the creation of the DataFusion-Contrib GitHub organization to support and accelerate other projects. While the core DataFusion library remains under Apache governance, the contrib organization provides a more flexible testing ground for new DataFusion features and a home for DataFusion extensions. With this announcement, we are pleased to introduce the following inaugural DataFusion-Contrib repositories. DataFusion-Python This project provides Python bindings to the core Rust implementation of DataFusion, which allows users to: Work with familiar SQL or DataFrame APIs to run queries in a safe, multi-threaded environment, returning results in Python Create User Defined Functions and User Defined Aggregate Functions for complex operations Pay no overhead to copy between Python and underlying Rust execution engine (by way of Apache Arrow arrays) Upcoming enhancements The team is focusing on exposing more features from the underlying Rust implementation of DataFusion and improving documentation. How to install From pip pip install datafusion Or python -m pip install datafusion DataFusion-ObjectStore-S3 This crate provides an ObjectStore implementation for querying data stored in S3 or S3 compatible storage. This makes it almost as easy to query data that lives on S3 as lives in local files Ability to create S3FileSystem to register as part of DataFusion ExecutionContext Register files or directories stored on S3 with ctx.register_listing_table Upcoming enhancements The current priority is adding python bindings for S3FileSystem. After that there will be async improvements as DataFusion adopts more of that functionality and we are looking into S3 Select functionality. How to Install Add the below to your Cargo.toml in your Rust Project with DataFusion. datafusion-objectstore-s3 = "0.1.0" DataFusion-Substrait Substrait is an emerging standard that provides a cross-language serialization format for relational algebra (e.g. expressions and query plans). This crate provides a Substrait producer and consumer for DataFusion. A producer converts a DataFusion logical plan into a Substrait protobuf and a consumer does the reverse. Examples of how to use this crate can be found here. Potential Use Cases Replace custom DataFusion protobuf serialization. Make it easier to pass query plans over FFI boundaries, such as from Python to Rust Allow Apache Calcite query plans to be executed in DataFusion DataFusion-BigTable This crate implements Bigtable as a data source and physical executor for DataFusion queries. It currently supports both UTF-8 string and 64-bit big-endian signed integers in Bigtable. From a SQL perspective it supports both simple and composite row keys with =, IN, and BETWEEN operators as well as projection pushdown. The physical execution for queries is handled by this crate while any subsequent aggregation, group bys, or joins are handled in DataFusion. Upcoming Enhancements Predicate pushdown Value range Value Regex Timestamp range Multithreaded Partition aware execution Production ready How to Install Add the below to your Cargo.toml in your Rust Project with DataFusion. datafusion-bigtable = "0.1.0" DataFusion-HDFS This crate introduces HadoopFileSystem as a remote ObjectStore which provides the ability to query HDFS files. For HDFS access the fs-hdfs library is used. DataFusion-Tokomak This crate provides an e-graph based DataFusion optimization framework based on the Rust egg library. An e-graph is a data structure that powers the equality saturation optimization technique. As context, the optimizer framework within DataFusion is currently under review with the objective of implementing a more strategic long term solution that is more efficient and simpler to develop. Some of the benefits of using egg within DataFusion are: Implements optimized algorithms that are hard to match with manually written optimization passes Makes it easy and less verbose to add optimization rules Plugin framework to add more complex optimizations Egg does not depend on rule order and can lead to a higher level of optimization by being able to apply multiple rules at the same time until it converges Allows for cost-based optimizations This is an exciting new area for DataFusion with lots of opportunity for community involvement! DataFusion-Tui DataFusion-tui aka dft provides a feature rich terminal application for using DataFusion. It has drawn inspiration and several features from datafusion-cli. In contrast to datafusion-cli the objective of this tool is to provide a light SQL IDE experience for querying data with DataFusion. This includes features such as the following which are currently implemented: Tab Management to provide clean and structured organization of DataFusion queries, results, ExecutionContext information, and logs SQL Editor Text editor for writing SQL queries Query History History of executed queries, their execution time, and the number of returned rows ExecutionContext information Expose information on which physical optimizers are used and which ExecutionConfig settings are set Logs Logs from dft, DataFusion, and any dependent libraries Support for custom ObjectStores S3 Preload DDL from ~/.datafusionrc to enable having local “database” available at startup Upcoming Enhancements SQL Editor Command to write query results to file Multiple SQL editor tabs Expose more information from ExecutionContext A help tab that provides information on functions Query custom TableProviders such as DeltaTable or BigTable DataFusion-Streams DataFusion-Stream is a new testing ground for creating a StreamProvider in DataFusion that will enable querying streaming data sources such as Apache Kafka. The implementation for this feature is currently being designed and is under active review. Once the design is finalized the trait and attendant data structures will be added back to the core DataFusion crate. DataFusion-Java This project created an initial set of Java bindings to DataFusion. The project is currently in maintenance mode and is looking for maintainers to drive future development. How to Get Involved If you are interested in contributing to DataFusion, and learning about state of the art query processing, we would love to have you join us on the journey! You can help by trying out DataFusion on some of your own data and projects and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable for beginners is here The best way to find out about creating new extensions within DataFusion-Contrib is reaching out on the #arrow-rust channel of the Apache Software Foundation Slack workspace. You can also check out our new Communication Doc on more ways to engage with the community. Links for each DataFusion-Contrib repository are provided above if you would like to contribute to those.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow DataFusion 7.0.0 Release</title><link href="https://arrow.apache.org/blog/2022/02/28/datafusion-7.0.0/" rel="alternate" type="text/html" title="Apache Arrow DataFusion 7.0.0 Release" /><published>2022-02-28T00:00:00-05:00</published><updated>2022-02-28T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2022/02/28/datafusion-7.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/02/28/datafusion-7.0.0/"><![CDATA[<!--

-->

<h1 id="introduction">Introduction</h1>

<p><a href="https://arrow.apache.org/datafusion/">DataFusion</a> is an extensible query execution framework, written in Rust, that uses Apache Arrow as its in-memory format.</p>

<p>When you want to extend your Rust project with <a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html">SQL support</a>, a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is definitely worth checking out.</p>

<p>DataFusion’s  SQL, <code class="language-plaintext highlighter-rouge">DataFrame</code>, and manual <code class="language-plaintext highlighter-rouge">PlanBuilder</code> API let users access a sophisticated query optimizer and execution engine capable of fast, resource efficient, and parallel execution that takes optimal advantage of todays multicore hardware. Being written in Rust means DataFusion can offer <em>both</em> the safety of dynamic languages as well as the resource efficiency of a compiled language.</p>

<p>The Apache Arrow team is pleased to announce the DataFusion 7.0.0 release. This covers 4 months of development work
and includes 195 commits from the following 37 distinct contributors.</p>

<!--
git log --pretty=oneline 5.0.0..6.0.0 datafusion datafusion-cli datafusion-examples | wc -l
     134

git shortlog -sn 5.0.0..6.0.0 datafusion datafusion-cli datafusion-examples | wc -l
      29

      Carlos and xudong963 are same individual
-->

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    44  Andrew Lamb
    24  Kun Liu
    23  Jiayu Liu
    17  xudong.w
    11  Yijie Shen
     9  Matthew Turner
     7  Liang-Chi Hsieh
     5  Lin Ma
     4  Stephen Carman
     4  James Katz
     4  Dmitry Patsura
     4  QP Hou
     3  dependabot[bot]
     3  Remzi Yang
     3  Yang
     3  ic4y
     3  Daniël Heres
     2  Andy Grove
     2  Raphael Taylor-Davies
     2  Jason Tianyi Wang
     2  Dan Harris
     2  Sergey Melnychuk
     1  Nitish Tiwari
     1  Dom
     1  Eduard Karacharov
     1  Javier Goday
     1  Boaz
     1  Marko Mikulicic
     1  Max Burke
     1  Carol (Nichols || Goulding)
     1  Phillip Cloud
     1  Rich
     1  Toby Hede
     1  Will Jones
     1  r.4ntix
     1  rdettai
</code></pre></div></div>

<p>The following section highlights some of the improvements in this release. Of course, many other bug fixes and improvements have also been made and we refer you to the complete <a href="https://github.com/apache/arrow-datafusion/blob/7.0.0/datafusion/CHANGELOG.md">changelog</a> for the full detail.</p>

<h1 id="summary">Summary</h1>

<ul>
  <li>DataFusion Crate
    <ul>
      <li>The DataFusion crate is being split into multiple crates to decrease compilation times and improve the development experience. Initially, <code class="language-plaintext highlighter-rouge">datafusion-common</code> (the core DataFusion components) and <code class="language-plaintext highlighter-rouge">datafusion-expr</code> (DataFusion expressions, functions, and operators) have been split out. There will be additional splits after the 7.0 release.</li>
    </ul>
  </li>
  <li>Performance Improvements and Optimizations
    <ul>
      <li>Arrow’s dyn scalar kernels are now used to enable efficient operations on <code class="language-plaintext highlighter-rouge">DictionaryArray</code>s <a href="https://github.com/apache/arrow-datafusion/pull/1685">#1685</a></li>
      <li>Switch from <code class="language-plaintext highlighter-rouge">std::sync::Mutex</code> to <code class="language-plaintext highlighter-rouge">parking_lot::Mutex</code> <a href="https://github.com/apache/arrow-datafusion/pull/1720">#1720</a></li>
    </ul>
  </li>
  <li>New Features
    <ul>
      <li>Support for memory tracking and spilling to disk
        <ul>
          <li>MemoryMananger and DiskManager <a href="https://github.com/apache/arrow-datafusion/pull/1526">#1526</a></li>
          <li>Out of core sort <a href="https://github.com/apache/arrow-datafusion/pull/1526">#1526</a></li>
          <li>New metrics
            <ul>
              <li><code class="language-plaintext highlighter-rouge">Gauge</code> and <code class="language-plaintext highlighter-rouge">CurrentMemoryUsage</code> <a href="https://github.com/apache/arrow-datafusion/pull/1682">#1682</a></li>
              <li><code class="language-plaintext highlighter-rouge">Spill_count</code> and <code class="language-plaintext highlighter-rouge">spilled_bytes</code> <a href="https://github.com/apache/arrow-datafusion/pull/1641">#1641</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li>New math functions
        <ul>
          <li><code class="language-plaintext highlighter-rouge">Approx_quantile</code> <a href="https://github.com/apache/arrow-datafusion/pull/1539">#1529</a></li>
          <li><code class="language-plaintext highlighter-rouge">stddev</code> and <code class="language-plaintext highlighter-rouge">variance</code> (sample and population) <a href="https://github.com/apache/arrow-datafusion/pull/1525">#1525</a></li>
          <li><code class="language-plaintext highlighter-rouge">corr</code> <a href="https://github.com/apache/arrow-datafusion/pull/1561">#1561</a></li>
        </ul>
      </li>
      <li>Support decimal type <a href="https://github.com/apache/arrow-datafusion/pull/1394">#1394</a><a href="https://github.com/apache/arrow-datafusion/pull/1407">#1407</a><a href="https://github.com/apache/arrow-datafusion/pull/1408">#1408</a><a href="https://github.com/apache/arrow-datafusion/pull/1431">#1431</a><a href="https://github.com/apache/arrow-datafusion/pull/1483">#1483</a><a href="https://github.com/apache/arrow-datafusion/pull/1554">#1554</a><a href="https://github.com/apache/arrow-datafusion/pull/1640">#1640</a></li>
      <li>Support for reading Parquet files with evolved schemas <a href="https://github.com/apache/arrow-datafusion/pull/1622">#1622</a><a href="https://github.com/apache/arrow-datafusion/pull/1709">#1709</a></li>
      <li>Support for registering <code class="language-plaintext highlighter-rouge">DataFrame</code> as table <a href="https://github.com/apache/arrow-datafusion/pull/1699">#1699</a></li>
      <li>Support for the <code class="language-plaintext highlighter-rouge">substring</code> function <a href="https://github.com/apache/arrow-datafusion/pull/1621">#1621</a></li>
      <li>Support <code class="language-plaintext highlighter-rouge">array_agg(distinct ...)</code> <a href="https://github.com/apache/arrow-datafusion/pull/1579">#1579</a></li>
      <li>Support <code class="language-plaintext highlighter-rouge">sort</code> on unprojected columns <a href="https://github.com/apache/arrow-datafusion/pull/1415">#1415</a></li>
    </ul>
  </li>
  <li>Additional Integration Points
    <ul>
      <li>A new public Expression simplification API <a href="https://github.com/apache/arrow-datafusion/pull/1717">#1717</a></li>
    </ul>
  </li>
  <li><a href="https://github.com/datafusion-contrib">DataFusion-Contrib</a>
    <ul>
      <li>A new GitHub organization created as a home for both <code class="language-plaintext highlighter-rouge">DataFusion</code> extensions and as a testing ground for new features.
        <ul>
          <li>Extensions
            <ul>
              <li><a href="https://github.com/datafusion-contrib/datafusion-python">DataFusion-Python</a></li>
              <li><a href="https://github.com/datafusion-contrib/datafusion-java">DataFusion-Java</a></li>
              <li><a href="https://github.com/datafusion-contrib/datafusion-hdfs-native">DataFusion-hdsfs-native</a></li>
              <li><a href="https://github.com/datafusion-contrib/datafusion-objectstore-s3">DataFusion-ObjectStore-s3</a></li>
            </ul>
          </li>
          <li>New Features
            <ul>
              <li><a href="https://github.com/datafusion-contrib/datafusion-streams">DataFusion-Streams</a></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="https://github.com/jorgecarleitao/arrow2">Arrow2</a>
    <ul>
      <li>An <a href="https://github.com/apache/arrow-datafusion/tree/arrow2">Arrow2 Branch</a> has been created.  There are ongoing discussions in <a href="https://github.com/apache/arrow-datafusion/issues/1532">DataFusion</a> and <a href="https://github.com/apache/arrow-rs/issues/1176">arrow-rs</a> about migrating <code class="language-plaintext highlighter-rouge">DataFusion</code> to <code class="language-plaintext highlighter-rouge">Arrow2</code></li>
    </ul>
  </li>
</ul>

<h1 id="documentation-and-roadmap">Documentation and Roadmap</h1>

<p>We are working to consolidate the documentation into the <a href="https://arrow.apache.org/datafusion">official site</a>.  You can find more details there on topics such as the <a href="https://arrow.apache.org/datafusion/user-guide/sql/index.html">SQL status</a>  and a <a href="https://arrow.apache.org/datafusion/user-guide/introduction.html#introduction">user guide</a>. This is also an area we would love to get help from the broader community <a href="https://github.com/apache/arrow-datafusion/issues/1821">#1821</a>.</p>

<p>To provide transparency on DataFusion’s priorities to users and developers a three month roadmap will be published at the beginning of each quarter.  This can be found here <a href="https://arrow.apache.org/datafusion/specification/roadmap.html">here</a>.</p>

<h1 id="upcoming-attractions">Upcoming Attractions</h1>

<ul>
  <li>Ballista is gaining momentum, and several groups are now evaluating and contributing to the project.
    <ul>
      <li>Some of the proposed improvements
        <ul>
          <li><a href="https://github.com/apache/arrow-datafusion/issues/1701">Improvements Overview</a></li>
          <li><a href="https://github.com/apache/arrow-datafusion/issues/1675">Extensibility</a></li>
          <li><a href="https://github.com/apache/arrow-datafusion/issues/1702">File system access</a></li>
          <li><a href="https://github.com/apache/arrow-datafusion/issues/1704">Cluster state</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Continued improvements for working with limited resources and large datasets
    <ul>
      <li>Memory limited joins<a href="https://github.com/apache/arrow-datafusion/issues/1599">#1599</a></li>
      <li>Sort-merge join<a href="https://github.com/apache/arrow-datafusion/issues/141">#141</a><a href="https://github.com/apache/arrow-datafusion/pull/1776">#1776</a></li>
      <li>Introduce row based bytes representation <a href="https://github.com/apache/arrow-datafusion/pull/1708">#1708</a></li>
    </ul>
  </li>
</ul>

<h1 id="how-to-get-involved">How to Get Involved</h1>

<p>If you are interested in contributing to DataFusion, and learning about state of
the art query processing, we would love to have you join us on the journey! You
can help by trying out DataFusion on some of your own data and projects and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable for beginners is <a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a></p>

<p>Check out our new <a href="https://arrow.apache.org/datafusion/community/communication.html">Communication Doc</a> on more
ways to engage with the community.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[Introduction DataFusion is an extensible query execution framework, written in Rust, that uses Apache Arrow as its in-memory format. When you want to extend your Rust project with SQL support, a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is definitely worth checking out. DataFusion’s SQL, DataFrame, and manual PlanBuilder API let users access a sophisticated query optimizer and execution engine capable of fast, resource efficient, and parallel execution that takes optimal advantage of todays multicore hardware. Being written in Rust means DataFusion can offer both the safety of dynamic languages as well as the resource efficiency of a compiled language. The Apache Arrow team is pleased to announce the DataFusion 7.0.0 release. This covers 4 months of development work and includes 195 commits from the following 37 distinct contributors. 44 Andrew Lamb 24 Kun Liu 23 Jiayu Liu 17 xudong.w 11 Yijie Shen 9 Matthew Turner 7 Liang-Chi Hsieh 5 Lin Ma 4 Stephen Carman 4 James Katz 4 Dmitry Patsura 4 QP Hou 3 dependabot[bot] 3 Remzi Yang 3 Yang 3 ic4y 3 Daniël Heres 2 Andy Grove 2 Raphael Taylor-Davies 2 Jason Tianyi Wang 2 Dan Harris 2 Sergey Melnychuk 1 Nitish Tiwari 1 Dom 1 Eduard Karacharov 1 Javier Goday 1 Boaz 1 Marko Mikulicic 1 Max Burke 1 Carol (Nichols || Goulding) 1 Phillip Cloud 1 Rich 1 Toby Hede 1 Will Jones 1 r.4ntix 1 rdettai The following section highlights some of the improvements in this release. Of course, many other bug fixes and improvements have also been made and we refer you to the complete changelog for the full detail. Summary DataFusion Crate The DataFusion crate is being split into multiple crates to decrease compilation times and improve the development experience. Initially, datafusion-common (the core DataFusion components) and datafusion-expr (DataFusion expressions, functions, and operators) have been split out. There will be additional splits after the 7.0 release. Performance Improvements and Optimizations Arrow’s dyn scalar kernels are now used to enable efficient operations on DictionaryArrays #1685 Switch from std::sync::Mutex to parking_lot::Mutex #1720 New Features Support for memory tracking and spilling to disk MemoryMananger and DiskManager #1526 Out of core sort #1526 New metrics Gauge and CurrentMemoryUsage #1682 Spill_count and spilled_bytes #1641 New math functions Approx_quantile #1529 stddev and variance (sample and population) #1525 corr #1561 Support decimal type #1394#1407#1408#1431#1483#1554#1640 Support for reading Parquet files with evolved schemas #1622#1709 Support for registering DataFrame as table #1699 Support for the substring function #1621 Support array_agg(distinct ...) #1579 Support sort on unprojected columns #1415 Additional Integration Points A new public Expression simplification API #1717 DataFusion-Contrib A new GitHub organization created as a home for both DataFusion extensions and as a testing ground for new features. Extensions DataFusion-Python DataFusion-Java DataFusion-hdsfs-native DataFusion-ObjectStore-s3 New Features DataFusion-Streams Arrow2 An Arrow2 Branch has been created. There are ongoing discussions in DataFusion and arrow-rs about migrating DataFusion to Arrow2 Documentation and Roadmap We are working to consolidate the documentation into the official site. You can find more details there on topics such as the SQL status and a user guide. This is also an area we would love to get help from the broader community #1821. To provide transparency on DataFusion’s priorities to users and developers a three month roadmap will be published at the beginning of each quarter. This can be found here here. Upcoming Attractions Ballista is gaining momentum, and several groups are now evaluating and contributing to the project. Some of the proposed improvements Improvements Overview Extensibility File system access Cluster state Continued improvements for working with limited resources and large datasets Memory limited joins#1599 Sort-merge join#141#1776 Introduce row based bytes representation #1708 How to Get Involved If you are interested in contributing to DataFusion, and learning about state of the art query processing, we would love to have you join us on the journey! You can help by trying out DataFusion on some of your own data and projects and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable for beginners is here Check out our new Communication Doc on more ways to engage with the community.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Introducing Apache Arrow Flight SQL: Accelerating Database Access</title><link href="https://arrow.apache.org/blog/2022/02/16/introducing-arrow-flight-sql/" rel="alternate" type="text/html" title="Introducing Apache Arrow Flight SQL: Accelerating Database Access" /><published>2022-02-16T00:00:00-05:00</published><updated>2022-02-16T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2022/02/16/introducing-arrow-flight-sql</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/02/16/introducing-arrow-flight-sql/"><![CDATA[<!--

-->

<p>We would like to introduce Flight SQL, a new client-server protocol developed by the Apache Arrow community for interacting with SQL databases that makes use of the Arrow in-memory columnar format and the Flight RPC framework.</p>

<p>Flight SQL aims to provide broadly similar functionality to existing APIs like JDBC and ODBC, including executing queries; creating prepared statements; and fetching metadata about the supported SQL dialect, available types, defined tables, and so on.
By building on Apache Arrow, however, Flight SQL makes it easy for clients to talk to Arrow-native databases without converting data.
And by using <a href="/blog/2019/10/13/introducing-arrow-flight/">Flight</a>, it provides an efficient implementation of a wire format that supports features like encryption and authentication out of the box, while allowing for further optimizations like parallel data access.</p>

<p>While it can be directly used for database access, it is not a direct replacement for JDBC/ODBC. Instead, Flight SQL serves as a concrete wire protocol/driver implementation that can support a JDBC/ODBC driver and reduces implementation burden on databases.</p>

<!-- mermaidjs:

graph LR
    JDBC[JDBC]
    ODBC
    FlightSQL[Flight SQL<br>libraries]
    ANA[Arrow-native app]
    DB[(Database with<br>Flight SQL endpoint)]

    JDBC --&gt; FlightSQL
    ODBC --&gt; FlightSQL
    ANA --&gt; FlightSQL

    FlightSQL --&gt;|Flight RPC| DB

-->

<div align="center">
<img src="/img/20220216-flight-sql-jdbc-odbc.svg" alt="Illustration of where Flight SQL sits in the stack. JDBC and ODBC drivers can wrap Flight SQL, or an Arrow-native application can directly use the Flight SQL libraries. Flight SQL in turn talks over Arrow Flight to a database exposing a Flight SQL endpoint." width="90%" class="img-responsive" />
</div>

<h2 id="motivation">Motivation</h2>

<p>While standards like <a href="https://docs.oracle.com/javase/8/docs/technotes/guides/jdbc/">JDBC</a> and <a href="https://docs.microsoft.com/en-us/sql/odbc/reference/odbc-overview?view=sql-server-ver15">ODBC</a> have served users well for decades, they fall short for databases and clients which wish to use Apache Arrow or columnar data in general.
Row-based APIs like JDBC or <a href="https://www.python.org/dev/peps/pep-0249/">PEP 249</a> require transposing data in this case, and for a database which is itself columnar, this means that data has to be transposed twice—once to present it in rows for the API, and once to get it back into columns for the consumer.
Meanwhile, while APIs like ODBC do provide bulk access to result buffers, this data must still be copied into Arrow arrays for use with the broader Arrow ecosystem, as implemented by projects like <a href="https://turbodbc.readthedocs.io/en/latest/">Turbodbc</a>.
Flight SQL aims to get rid of these intermediate steps.</p>

<p>Flight SQL means database servers can implement a standard interface that is designed around Apache Arrow and columnar data from the start.
Just like how Arrow provides a standard in-memory format, Flight SQL saves developers from having to design and implement an entirely new wire protocol.
As mentioned, Flight already implements features like encryption on the wire and authentication of requests, which databases do not need to re-implement.</p>

<p>For clients, Flight SQL provides bulk access to query results without having to convert data from another API or format.
Additionally, by pushing the work of implementing the wire protocol into the Flight and Flight SQL libraries, less code has to be written for each client language or driver.
And by using Flight underneath, clients and servers can cooperate to implement optimizations like parallel data access, <a href="/blog/2019/10/13/introducing-arrow-flight/#horizontal-scalability-parallel-and-partitioned-data-access">one of the original goals of Flight itself</a>.
Databases can return multiple “endpoints” to a Flight SQL client, which can then pull data from all of them in parallel, enabling the database backend to horizontally scale.</p>

<h2 id="flight-sql-basics">Flight SQL Basics</h2>

<p>Flight SQL makes full use of the Flight RPC framework and its extensibility, defining additional request/response messages via <a href="https://developers.google.com/protocol-buffers/">Protobuf</a>.
We’ll go over the Flight SQL protocol briefly, but C++ and Java already implement clients that manage much of this work.
The full <a href="https://github.com/apache/arrow/blob/release-7.0.0/format/FlightSql.proto">protocol</a> can be found on GitHub.</p>

<p>Most requests follow this pattern:</p>
<ol>
  <li>The client constructs a request using one of the defined Protobuf messages.</li>
  <li>The client sends the request via the GetSchema RPC method (to get the schema of the response) or the GetFlightInfo RPC method (to execute the request).</li>
  <li>The client makes request(s) to the endpoints returned from GetFlightInfo to get the response.</li>
</ol>

<p>Flight SQL defines methods to query database metadata, execute queries, or manipulate prepared statements.</p>

<p>Metadata requests:</p>
<ul>
  <li>CommandGetCatalogs: list catalogs in a database.</li>
  <li>CommandGetCrossReference: list foreign key columns that reference a particular other table.</li>
  <li>CommandGetDbSchemas: list schemas in a catalog.</li>
  <li>CommandGetExportedKeys: list foreign keys referencing a table.</li>
  <li>CommandGetImportedKeys: list foreign keys of a table.</li>
  <li>CommandGetPrimaryKeys: list primary keys of a table.</li>
  <li>CommandGetSqlInfo: get information about the database itself and its supported SQL dialect.</li>
  <li>CommandGetTables: list tables in a catalog/schema.</li>
  <li>CommandGetTableTypes: list table types supported (e.g. table, view, system table).</li>
</ul>

<p>Queries:</p>
<ul>
  <li>CommandStatementQuery: execute a one-off SQL query.</li>
  <li>CommandStatementUpdate: execute a one-off SQL update query.</li>
</ul>

<p>Prepared statements:</p>
<ul>
  <li>ActionClosePreparedStatementRequest: close a prepared statement.</li>
  <li>ActionCreatePreparedStatementRequest: create a new prepared statement.</li>
  <li>CommandPreparedStatementQuery: execute a prepared statement.</li>
  <li>CommandPreparedStatementUpdate: execute a prepared statement that updates data.</li>
</ul>

<p>For example, to list all tables:</p>

<!-- mermaidjs:

sequenceDiagram
    Client->>Server: GetFlightInfo(CommandGetTables)
    Server->>Client: FlightInfo{..., Ticket, ...}
    Client->>Server: DoGet(Ticket)
    Server->>Client: list of tables as Arrow data

-->

<div align="center">
<img src="/img/20220216-flight-sql-gettables.svg" alt="Sequence diagram showing how to use CommandGetTables. First, the client calls the GetFlightInfo RPC method with a serialized CommandGetTables message as the argument. The server returns a FlightInfo message containing a Ticket message. The client then calls the DoGet RPC method with the Ticket as the argument, and gets back a stream of Arrow record batches containing the tables in the database." height="363" class="img-responsive" />
</div>

<p>To execute a query:</p>

<!-- mermaidjs:

sequenceDiagram
    Client->>Server: GetFlightInfo(CommandStatementQuery)
    Server->>Client: FlightInfo{..., Ticket, ...}
    Client->>Server: DoGet(Ticket)
    Server->>Client: query results as Arrow data

-->

<div align="center">
<img src="/img/20220216-flight-sql-query.svg" alt="Sequence diagram showing how to use CommandStatementQuery. First, the client calls the GetFlightInfo RPC method with a serialized CommandStatementQuery message as the argument. This message contains the SQL query. The server returns a FlightInfo message containing a Ticket message. The client then calls the DoGet RPC method with the Ticket as the argument, and gets back a stream of Arrow record batches containing the query results." height="363" class="img-responsive" />
</div>

<p>To create and execute a prepared statement to insert rows:</p>

<!-- mermaidjs:

sequenceDiagram
    Client->>Server: DoAction(ActionCreatePreparedStatementRequest)
    Server->>Client: ActionCreatePreparedStatementResult
    Client->>Server: DoPut(CommandPreparedStatementUpdate)
    Client--&gt;>Server: Arrow data representing parameter values
    Server->>Client: DoPutUpdateResult
    Client->>Server: DoAction(ActionClosePreparedStatementRequest)
-->

<div align="center">
<img src="/img/20220216-flight-sql-prepared.svg" alt="Sequence diagram showing how to use ActionCreatePreparedStatementResult. First, the client calls the DoAction RPC method with a serialized ActionCreatePreparedStatementResult message as the argument. This message contains the SQL query. The server returns a serialized ActionCreatePreparedStatementResult message containing an opaque handle for the prepared statement. The client then calls the DoPut RPC method with a CommandPreparedStatementUpdate message, containing the opaque handle, as the argument, and uploads a stream of Arrow record batches containing query parameters. The server responds with a serialized DoPutUpdateResult message containing the number of affected rows. Finally, the client calls DoAction again with ActionClosePreparedStatementRequest to clean up the prepared statement." height="459" class="img-responsive" />
</div>

<h2 id="getting-started">Getting Started</h2>

<p>Note that while Flight SQL is shipping as part of Apache Arrow 7.0.0, it is still under development, and detailed documentation is forthcoming.
However, implementations are already available in C++ and Java, which provide a low-level client that can be used as well as a server skeleton that can be implemented.</p>

<p>For those interested, a <a href="https://github.com/apache/arrow/blob/release-7.0.0/java/flight/flight-sql/src/test/java/org/apache/arrow/flight/sql/example/FlightSqlExample.java">server implementation wrapping Apache Derby</a> and <a href="https://github.com/apache/arrow/blob/release-7.0.0/cpp/src/arrow/flight/sql/example/sqlite_server.h">one wrapping SQLite</a> are available in the source.
A <a href="https://github.com/apache/arrow/blob/release-7.0.0/cpp/src/arrow/flight/sql/test_app_cli.cc">simple CLI demonstrating the client</a> is also available. Finally, we can look at a brief example of executing a query and fetching results:</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">flight</span><span class="o">::</span><span class="n">FlightCallOptions</span> <span class="n">call_options</span><span class="p">;</span>

<span class="c1">// Execute the query, getting a FlightInfo describing how to fetch the results</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Executing query: '"</span> <span class="o">&lt;&lt;</span> <span class="n">FLAGS_query</span> <span class="o">&lt;&lt;</span> <span class="s">"'"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="n">ARROW_ASSIGN_OR_RAISE</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">flight</span><span class="o">::</span><span class="n">FlightInfo</span><span class="o">&gt;</span> <span class="n">flight_info</span><span class="p">,</span>
                      <span class="n">client</span><span class="o">-&gt;</span><span class="n">Execute</span><span class="p">(</span><span class="n">call_options</span><span class="p">,</span> <span class="n">FLAGS_query</span><span class="p">));</span>

<span class="c1">// Fetch each partition sequentially (though this can be done in parallel)</span>
<span class="k">for</span> <span class="p">(</span><span class="k">const</span> <span class="n">flight</span><span class="o">::</span><span class="n">FlightEndpoint</span><span class="o">&amp;</span> <span class="n">endpoint</span> <span class="o">:</span> <span class="n">flight_info</span><span class="o">-&gt;</span><span class="n">endpoints</span><span class="p">())</span> <span class="p">{</span>
  <span class="c1">// Here we assume each partition is on the same server we originally queried, but this</span>
  <span class="c1">// isn't true in general: the server may split the query results between multiple</span>
  <span class="c1">// other servers, which we would have to connect to.</span>

  <span class="c1">// The "ticket" in the endpoint is opaque to the client. The server uses it to</span>
  <span class="c1">// identify which part of the query results to return.</span>
  <span class="n">ARROW_ASSIGN_OR_RAISE</span><span class="p">(</span><span class="k">auto</span> <span class="n">stream</span><span class="p">,</span> <span class="n">client</span><span class="o">-&gt;</span><span class="n">DoGet</span><span class="p">(</span><span class="n">call_options</span><span class="p">,</span> <span class="n">endpoint</span><span class="p">.</span><span class="n">ticket</span><span class="p">));</span>
  <span class="c1">// Read all results into an Arrow Table, though we can iteratively process record</span>
  <span class="c1">// batches as they arrive as well</span>
  <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">arrow</span><span class="o">::</span><span class="n">Table</span><span class="o">&gt;</span> <span class="n">table</span><span class="p">;</span>
  <span class="n">ARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">stream</span><span class="o">-&gt;</span><span class="n">ReadAll</span><span class="p">(</span><span class="o">&amp;</span><span class="n">table</span><span class="p">));</span>
  <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Read one partition:"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
  <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">table</span><span class="o">-&gt;</span><span class="n">ToString</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The full source is <a href="https://github.com/apache/arrow/blob/master/cpp/examples/arrow/flight_sql_example.cc">available on GitHub</a>.</p>

<h2 id="whats-next--getting-involved">What’s Next &amp; Getting Involved</h2>

<p>Compared to existing libraries like PyODBC, <a href="https://www.dremio.com/subsurface/arrow-flight-and-flight-sql-accelerating-data-movement/">Arrow Flight is already as much as 20x faster</a> (~00:21:00).
Flight SQL will package these performance advantages into a standard interface, ready for clients and databases to implement.</p>

<p>Further protocol refinements and extensions are expected.
Some of this work is to make it possible to implement APIs like JDBC on top of Flight SQL; a JDBC driver is being actively worked on.
While this again introduces the overhead of data conversion, it means a database can make itself accessible to both Arrow-native clients and traditional clients by implementing Flight SQL.
Other improvements in the future may include Python bindings, an ODBC driver, and more.</p>

<p>For anyone interested in getting involved, either as a contributor or adopter, please reach out on the <a href="/community/#mailing-lists">mailing list</a> or join the discussion on <a href="https://github.com/apache/arrow">GitHub</a>.</p>]]></content><author><name>José Almeida, James Duong, Vinicius Fraga, Juscelino Junior, David Li, Kyle Porter, Rafael Telles</name></author><category term="application" /><summary type="html"><![CDATA[This post introduces Arrow Flight SQL, a protocol for interacting with SQL databases over Arrow Flight. We have been working on this protocol over the last six months, and are looking for feedback, interested contributors, and early adopters.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">February 2022 Rust Apache Arrow and Parquet Highlights</title><link href="https://arrow.apache.org/blog/2022/02/13/rust-9.0/" rel="alternate" type="text/html" title="February 2022 Rust Apache Arrow and Parquet Highlights" /><published>2022-02-13T01:00:00-05:00</published><updated>2022-02-13T01:00:00-05:00</updated><id>https://arrow.apache.org/blog/2022/02/13/rust-9.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/02/13/rust-9.0/"><![CDATA[<!--

-->

<p>The Rust implementation of <a href="https://arrow.apache.org/">Apache Arrow</a> has just released version <code class="language-plaintext highlighter-rouge">9.0.2</code>.</p>

<p>While a major version of this magnitude may shock some in the Rust
community to whom it implies a slow moving 20 year old piece of
software, nothing could be further from the truth!</p>

<p>With regular and predictable bi-weekly releases, the library continues
to evolve rapidly, and <code class="language-plaintext highlighter-rouge">9.0.2</code> is no exception. Some recent highlights:</p>

<h1 id="parquet-async-performance-safety-and-nested-types"><code class="language-plaintext highlighter-rouge">parquet</code>: async, performance, safety and nested types</h1>

<p>The <a href="https://crates.io/crates/arrow/9.0.2">parquet <code class="language-plaintext highlighter-rouge">9.0.2</code></a> release includes an <a href="https://github.com/apache/arrow-rs/blob/9.0.2/parquet/src/arrow/async_reader.rs#L21-L75"><code class="language-plaintext highlighter-rouge">async</code> reader</a>, a long time requested feature. Using the <code class="language-plaintext highlighter-rouge">async</code>
reader it is now possible to read only the relevant parts of a parquet
file from a networked source such as object storage. Previously the
entire file had to be buffered locally. We are hoping to add an <code class="language-plaintext highlighter-rouge">async</code>
writer in a future release and would love some
<a href="https://github.com/apache/arrow-rs/issues/1269">help</a>.</p>

<p>It is also significantly faster to read parquet data (up to
<a href="https://github.com/apache/arrow-rs/pull/1180#issuecomment-1018518863">60x</a>
in some cases) than with previous versions of the <code class="language-plaintext highlighter-rouge">parquet</code>
crate. Kudos to <a href="https://github.com/tustvold">tustvold</a> and
<a href="https://github.com/yordan-pavlov">yordan-pavlov</a> for their
contributions in these areas.</p>

<p>With <code class="language-plaintext highlighter-rouge">8.0.0</code> and later, the code that reads and writes <code class="language-plaintext highlighter-rouge">RecordBatch</code>es
to and from Parquet now supports all types, including deeply nested
structs and lists. Thanks <a href="https://github.com/helgikrs">helgikrs</a> for
cleaning up the last corner cases!</p>

<p>Other notable recent additions to parquet are <code class="language-plaintext highlighter-rouge">UTF-8</code> validation on
string data for improved security against malicious inputs.</p>

<p>Planned upcoming work includes <a href="https://github.com/apache/arrow-rs/issues/1191">pushing more
filtering</a> directly
into the parquet scan as well as an <code class="language-plaintext highlighter-rouge">async</code> writer.</p>

<h1 id="arrow-performance-dyn-kernels-and-decimalarray"><code class="language-plaintext highlighter-rouge">arrow</code>: performance, dyn kernels, and DecimalArray</h1>

<p>The <a href="https://docs.rs/arrow/latest/arrow/compute/index.html">compute</a>
kernels have been improved significantly in <a href="https://crates.io/crates/parquet/9.0.2">arrow <code class="language-plaintext highlighter-rouge">9.0.2</code></a>. Some <a href="https://github.com/apache/arrow-rs/pull/1228#issue-1111889246">filter
benchmarks</a>
are twice as fast and the SIMD kernels are also <a href="https://github.com/apache/arrow-rs/pull/1221">significantly
faster</a>. Many thanks to
<a href="https://github.com/tustvold">tustvold</a> and
<a href="https://github.com/jhorstmann">jhorstmann</a>.
<a href="https://github.com/apache/arrow-rs/pull/1248">Additional substantial</a>
improvements are likely to land in arrow <code class="language-plaintext highlighter-rouge">10.0.0</code>.</p>

<p>We are working on new set of “dynamic” <code class="language-plaintext highlighter-rouge">dyn_</code> kernels (for example,
<a href="https://docs.rs/arrow/8.0.0/arrow/compute/kernels/comparison/fn.eq_dyn.html"><code class="language-plaintext highlighter-rouge">eq_dyn</code></a>)
that make it easier to invoke the heavily optimized kernels provided
by the <code class="language-plaintext highlighter-rouge">arrow</code> crate. Work is underway to expand the breadth of types
supported by these new kernels to make them even more useful. Thanks
to <a href="https://github.com/matthewmturner">matthewmturner</a> and
<a href="https://github.com/viirya">viirya</a> for their help in this
effort.</p>

<p>While <code class="language-plaintext highlighter-rouge">arrow</code> has had basic support for <code class="language-plaintext highlighter-rouge">DecimalArray</code> since version
<code class="language-plaintext highlighter-rouge">3.0.0</code>, support has been expanded for <code class="language-plaintext highlighter-rouge">Decimal</code> type in calculation
kernels such as <code class="language-plaintext highlighter-rouge">sort</code>, <code class="language-plaintext highlighter-rouge">take</code> and <code class="language-plaintext highlighter-rouge">filter</code> thanks to some great
contributions from <a href="https://github.com/liukun4515">liukun4515</a>. There
is <a href="https://github.com/apache/arrow-rs/pull/1223">ongoing work</a> to
improve the API ergonomics and performance of <code class="language-plaintext highlighter-rouge">DecimalArray</code> as well.</p>

<h1 id="security">Security</h1>

<p>The <code class="language-plaintext highlighter-rouge">6.4.0</code> release resolved the last outstanding
<a href="https://rustsec.org/">RUSTSEC</a>
<a href="https://github.com/rustsec/advisory-db/pull/1131">advisory</a> on the
arrow crate and the <code class="language-plaintext highlighter-rouge">8.0.0</code> release resolved the last outstanding
known security issues. While these security issues were mostly limited
misuse of the low level “power user” APIs which most users do not (and
should not) be using, it was good to tighten up that area.</p>

<p>Now that <code class="language-plaintext highlighter-rouge">arrow-rs</code> is releasing major versions every other week, we
are also able to update dependencies at the same pace, helping to
ensure that security fixes upstream can flow more quickly to
downstream projects.</p>

<h1 id="final-shoutout">Final shoutout</h1>
<p>It takes a community to build great software, and we would like to
thank everyone who has contributed to the arrow-rs repository since
the <code class="language-plaintext highlighter-rouge">7.0.0</code> release:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">git shortlog -sn 7.0.0..9.0.0
    22  Raphael Taylor-Davies
    18  Andrew Lamb
     6  Helgi Kristvin Sigurbjarnarson
     6  Remzi Yang
     5  Jörn Horstmann
     4  Liang-Chi Hsieh
     3  Jiayu Liu
     2  dependabot[bot]
     2  Yijie Shen
     1  Matthew Turner
     1  Kun Liu
     1  Yang
     1  Edd Robinson
     1  Patrick More
</span></code></pre></div></div>

<h1 id="how-to-get-involved">How to Get Involved</h1>

<p>If you are interested in contributing to the Rust subproject in Apache Arrow, you can find a list of open issues
suitable for beginners <a href="https://github.com/apache/arrow-rs/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a>
and the full list <a href="https://github.com/apache/arrow-rs/issues">here</a>.</p>

<p>Other ways to get involved include trying out Arrow on some of your data and filing bug reports, and helping to
improve the documentation.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Rust implementation of Apache Arrow has just released version 9.0.2. While a major version of this magnitude may shock some in the Rust community to whom it implies a slow moving 20 year old piece of software, nothing could be further from the truth! With regular and predictable bi-weekly releases, the library continues to evolve rapidly, and 9.0.2 is no exception. Some recent highlights: parquet: async, performance, safety and nested types The parquet 9.0.2 release includes an async reader, a long time requested feature. Using the async reader it is now possible to read only the relevant parts of a parquet file from a networked source such as object storage. Previously the entire file had to be buffered locally. We are hoping to add an async writer in a future release and would love some help. It is also significantly faster to read parquet data (up to 60x in some cases) than with previous versions of the parquet crate. Kudos to tustvold and yordan-pavlov for their contributions in these areas. With 8.0.0 and later, the code that reads and writes RecordBatches to and from Parquet now supports all types, including deeply nested structs and lists. Thanks helgikrs for cleaning up the last corner cases! Other notable recent additions to parquet are UTF-8 validation on string data for improved security against malicious inputs. Planned upcoming work includes pushing more filtering directly into the parquet scan as well as an async writer. arrow: performance, dyn kernels, and DecimalArray The compute kernels have been improved significantly in arrow 9.0.2. Some filter benchmarks are twice as fast and the SIMD kernels are also significantly faster. Many thanks to tustvold and jhorstmann. Additional substantial improvements are likely to land in arrow 10.0.0. We are working on new set of “dynamic” dyn_ kernels (for example, eq_dyn) that make it easier to invoke the heavily optimized kernels provided by the arrow crate. Work is underway to expand the breadth of types supported by these new kernels to make them even more useful. Thanks to matthewmturner and viirya for their help in this effort. While arrow has had basic support for DecimalArray since version 3.0.0, support has been expanded for Decimal type in calculation kernels such as sort, take and filter thanks to some great contributions from liukun4515. There is ongoing work to improve the API ergonomics and performance of DecimalArray as well. Security The 6.4.0 release resolved the last outstanding RUSTSEC advisory on the arrow crate and the 8.0.0 release resolved the last outstanding known security issues. While these security issues were mostly limited misuse of the low level “power user” APIs which most users do not (and should not) be using, it was good to tighten up that area. Now that arrow-rs is releasing major versions every other week, we are also able to update dependencies at the same pace, helping to ensure that security fixes upstream can flow more quickly to downstream projects. Final shoutout It takes a community to build great software, and we would like to thank everyone who has contributed to the arrow-rs repository since the 7.0.0 release: git shortlog -sn 7.0.0..9.0.0 22 Raphael Taylor-Davies 18 Andrew Lamb 6 Helgi Kristvin Sigurbjarnarson 6 Remzi Yang 5 Jörn Horstmann 4 Liang-Chi Hsieh 3 Jiayu Liu 2 dependabot[bot] 2 Yijie Shen 1 Matthew Turner 1 Kun Liu 1 Yang 1 Edd Robinson 1 Patrick More How to Get Involved If you are interested in contributing to the Rust subproject in Apache Arrow, you can find a list of open issues suitable for beginners here and the full list here. Other ways to get involved include trying out Arrow on some of your data and filing bug reports, and helping to improve the documentation.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>