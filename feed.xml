<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://arrow.apache.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arrow.apache.org/" rel="alternate" type="text/html" /><updated>2021-08-24T03:26:54-04:00</updated><id>https://arrow.apache.org/feed.xml</id><title type="html">Apache Arrow</title><subtitle>Apache Arrow is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, C++, C#, Go, Java, JavaScript, MATLAB, Python, R, Ruby, and Rust.</subtitle><entry><title type="html">Apache Arrow Ballista 0.5.0 Release</title><link href="https://arrow.apache.org/blog/2021/08/18/ballista-0.5.0/" rel="alternate" type="text/html" title="Apache Arrow Ballista 0.5.0 Release" /><published>2021-08-18T00:00:00-04:00</published><updated>2021-08-18T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2021/08/18/ballista-0.5.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2021/08/18/ballista-0.5.0/">&lt;!--

--&gt;

&lt;p&gt;Ballista extends DataFusion to provide support for distributed queries. This is the first release of Ballista since 
the project was &lt;a href=&quot;https://arrow.apache.org/blog/2021/04/12/ballista-donation/&quot;&gt;donated&lt;/a&gt; to the Apache Arrow project 
and includes 80 commits from 11 contributors.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git shortlog -sn 4.0.0..5.0.0 ballista/rust/client ballista/rust/core ballista/rust/executor ballista/rust/scheduler
  27  Andy Grove
  15  Jiayu Liu
  12  Andrew Lamb
   8  Ximo Guanter
   6  Daniël Heres
   5  QP Hou
   2  Jorge Leitao
   1  Javier Goday
   1  K.I. (Dennis) Jung
   1  Mike Seddon
   1  sathis
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;!--
$ git log --pretty=oneline 4.0.0..5.0.0 ballista/rust/client ballista/rust/core ballista/rust/executor ballista/rust/scheduler ballista-examples/ | wc -l
80
--&gt;

&lt;p&gt;The release notes below are not exhaustive and only expose selected highlights of the release. Many other bug fixes 
and improvements have been made: we refer you to the &lt;a href=&quot;https://github.com/apache/arrow-datafusion/blob/5.0.0/ballista/CHANGELOG.md&quot;&gt;complete changelog&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;performance-and-scalability&quot;&gt;Performance and Scalability&lt;/h1&gt;

&lt;p&gt;Ballista is now capable of running complex SQL queries at scale and supports scalable distributed joins. We have been 
benchmarking using individual queries from the TPC-H benchmark at scale factors up to 1000 (1 TB). When running against 
CSV files, performance is generally very close to DataFusion, and significantly faster in some cases due to the fact 
that the scheduler limits the number of concurrent tasks that run at any given time. Performance against large Parquet 
datasets is currently non ideal due to some issues (&lt;a href=&quot;https://github.com/apache/arrow-datafusion/issues/867&quot;&gt;#867&lt;/a&gt;, 
&lt;a href=&quot;https://github.com/apache/arrow-datafusion/issues/868&quot;&gt;#868&lt;/a&gt;) that we hope to resolve for the next release.&lt;/p&gt;

&lt;h1 id=&quot;new-features&quot;&gt;New Features&lt;/h1&gt;

&lt;p&gt;The main new features in this release are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ballista queries can now be executed by calling DataFrame.collect()&lt;/li&gt;
  &lt;li&gt;The shuffle mechanism has been re-implemented&lt;/li&gt;
  &lt;li&gt;Distributed hash-partitioned joins are now supported&lt;/li&gt;
  &lt;li&gt;Keda autoscaling is supported&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To get started with Ballista, refer to the &lt;a href=&quot;https://docs.rs/ballista/0.5.0/ballista/&quot;&gt;crate documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now that the basic functionality is in place, the focus for the next release will be to improve the performance and
scalability as well as improving the documentation.&lt;/p&gt;

&lt;h1 id=&quot;how-to-get-involved&quot;&gt;How to Get Involved&lt;/h1&gt;

&lt;p&gt;If you are interested in contributing to Ballista, we would love to have you! You
can help by trying out Ballista on some of your own data and projects and filing bug reports and helping to
improve the documentation, or contribute to the documentation, tests or code. A list of open issues suitable for
beginners is &lt;a href=&quot;https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22&quot;&gt;here&lt;/a&gt;
and the full list is &lt;a href=&quot;https://github.com/apache/arrow-datafusion/issues&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>pmc</name></author><category term="release" /><summary type="html">Ballista extends DataFusion to provide support for distributed queries. This is the first release of Ballista since the project was donated to the Apache Arrow project and includes 80 commits from 11 contributors. git shortlog -sn 4.0.0..5.0.0 ballista/rust/client ballista/rust/core ballista/rust/executor ballista/rust/scheduler 27 Andy Grove 15 Jiayu Liu 12 Andrew Lamb 8 Ximo Guanter 6 Daniël Heres 5 QP Hou 2 Jorge Leitao 1 Javier Goday 1 K.I. (Dennis) Jung 1 Mike Seddon 1 sathis The release notes below are not exhaustive and only expose selected highlights of the release. Many other bug fixes and improvements have been made: we refer you to the complete changelog. Performance and Scalability Ballista is now capable of running complex SQL queries at scale and supports scalable distributed joins. We have been benchmarking using individual queries from the TPC-H benchmark at scale factors up to 1000 (1 TB). When running against CSV files, performance is generally very close to DataFusion, and significantly faster in some cases due to the fact that the scheduler limits the number of concurrent tasks that run at any given time. Performance against large Parquet datasets is currently non ideal due to some issues (#867, #868) that we hope to resolve for the next release. New Features The main new features in this release are: Ballista queries can now be executed by calling DataFrame.collect() The shuffle mechanism has been re-implemented Distributed hash-partitioned joins are now supported Keda autoscaling is supported To get started with Ballista, refer to the crate documentation. Now that the basic functionality is in place, the focus for the next release will be to improve the performance and scalability as well as improving the documentation. How to Get Involved If you are interested in contributing to Ballista, we would love to have you! You can help by trying out Ballista on some of your own data and projects and filing bug reports and helping to improve the documentation, or contribute to the documentation, tests or code. A list of open issues suitable for beginners is here and the full list is here.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow DataFusion 5.0.0 Release</title><link href="https://arrow.apache.org/blog/2021/08/18/datafusion-5.0.0/" rel="alternate" type="text/html" title="Apache Arrow DataFusion 5.0.0 Release" /><published>2021-08-18T00:00:00-04:00</published><updated>2021-08-18T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2021/08/18/datafusion-5.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2021/08/18/datafusion-5.0.0/">&lt;!--

--&gt;

&lt;p&gt;The Apache Arrow team is pleased to announce the DataFusion 5.0.0 release. This covers 4 months of development work 
and includes 211 commits from the following 31 distinct contributors.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ git shortlog -sn 4.0.0..5.0.0 datafusion datafusion-cli datafusion-examples
    61  Jiayu Liu
    47  Andrew Lamb
    27  Daniël Heres
    13  QP Hou
    13  Andy Grove
     4  Javier Goday
     4  sathis
     3  Ruan Pearce-Authers
     3  Raphael Taylor-Davies
     3  Jorge Leitao
     3  Cui Wenzheng
     3  Mike Seddon
     3  Edd Robinson
     2  思维
     2  Liang-Chi Hsieh
     2  Michael Lu
     2  Parth Sarthy
     2  Patrick More
     2  Rich
     1  Charlie Evans
     1  Gang Liao
     1  Agata Naomichi
     1  Ritchie Vink
     1  Evan Chan
     1  Ruihang Xia
     1  Todd Treece
     1  Yichen Wang
     1  baishen
     1  Nga Tran
     1  rdettai
     1  Marco Neumann
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;!--
$ git log --pretty=oneline 4.0.0..5.0.0 datafusion datafusion-cli datafusion-examples | wc -l
     211
--&gt;

&lt;p&gt;The release notes below are not exhaustive and only expose selected highlights of the release. Many other bug fixes 
and improvements have been made: we refer you to the complete 
&lt;a href=&quot;https://github.com/apache/arrow-datafusion/blob/5.0.0/datafusion/CHANGELOG.md&quot;&gt;changelog&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;performance&quot;&gt;Performance&lt;/h1&gt;

&lt;p&gt;There have been numerous performance improvements in this release. The following chart shows the relative 
performance of individual TPC-H queries compared to the previous release.&lt;/p&gt;

&lt;p&gt;&lt;i&gt;TPC-H @ scale factor 100, in parquet format. Concurrency 24.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/2021-08-18-datafusion500perf.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We also extended support for more TPC-H queries: q7, q8, q9 and q13 are running successfully in DataFusion 5.0.&lt;/p&gt;

&lt;h1 id=&quot;new-features&quot;&gt;New Features&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Initial support for SQL-99 Analytics (WINDOW functions)&lt;/li&gt;
  &lt;li&gt;Improved JOIN support: cross join, semi-join, anti join, and fixes to null handling&lt;/li&gt;
  &lt;li&gt;Improved EXPLAIN support&lt;/li&gt;
  &lt;li&gt;Initial implementation of metrics in the physical plan&lt;/li&gt;
  &lt;li&gt;Support for SELECT DISTINCT&lt;/li&gt;
  &lt;li&gt;Support for Json and NDJson formatted inputs&lt;/li&gt;
  &lt;li&gt;Query column with relations&lt;/li&gt;
  &lt;li&gt;Added more datetime related functions: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;now&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;date_trunc&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;to_timestamp_millis&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;to_timestamp_micros&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;to_timestamp_seconds&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Streaming Dataframe.collect&lt;/li&gt;
  &lt;li&gt;Support table column aliases&lt;/li&gt;
  &lt;li&gt;Answer count(*), min() and max() queries using only statistics&lt;/li&gt;
  &lt;li&gt;Non-equi-join filters in JOIN conditions&lt;/li&gt;
  &lt;li&gt;Modulus operation&lt;/li&gt;
  &lt;li&gt;Support group by column positions&lt;/li&gt;
  &lt;li&gt;Added constant folding query optimizer&lt;/li&gt;
  &lt;li&gt;Hash partitioned aggregation&lt;/li&gt;
  &lt;li&gt;Added &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;random&lt;/code&gt; SQL function&lt;/li&gt;
  &lt;li&gt;Implemented count distinct for floats and dictionary types&lt;/li&gt;
  &lt;li&gt;Re-exported arrow and parquet crates in Datafusion&lt;/li&gt;
  &lt;li&gt;General row group pruning logic that’s agnostic to storage format&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;how-to-get-involved&quot;&gt;How to Get Involved&lt;/h1&gt;

&lt;p&gt;If you are interested in contributing to DataFusion, we would love to have you! You 
can help by trying out DataFusion on some of your own data and projects and filing bug reports and helping to 
improve the documentation, or contribute to the documentation, tests or code. A list of open issues suitable for 
beginners is &lt;a href=&quot;https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22&quot;&gt;here&lt;/a&gt; 
and the full list is &lt;a href=&quot;https://github.com/apache/arrow-datafusion/issues&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>pmc</name></author><category term="release" /><summary type="html">The Apache Arrow team is pleased to announce the DataFusion 5.0.0 release. This covers 4 months of development work and includes 211 commits from the following 31 distinct contributors. $ git shortlog -sn 4.0.0..5.0.0 datafusion datafusion-cli datafusion-examples 61 Jiayu Liu 47 Andrew Lamb 27 Daniël Heres 13 QP Hou 13 Andy Grove 4 Javier Goday 4 sathis 3 Ruan Pearce-Authers 3 Raphael Taylor-Davies 3 Jorge Leitao 3 Cui Wenzheng 3 Mike Seddon 3 Edd Robinson 2 思维 2 Liang-Chi Hsieh 2 Michael Lu 2 Parth Sarthy 2 Patrick More 2 Rich 1 Charlie Evans 1 Gang Liao 1 Agata Naomichi 1 Ritchie Vink 1 Evan Chan 1 Ruihang Xia 1 Todd Treece 1 Yichen Wang 1 baishen 1 Nga Tran 1 rdettai 1 Marco Neumann The release notes below are not exhaustive and only expose selected highlights of the release. Many other bug fixes and improvements have been made: we refer you to the complete changelog. Performance There have been numerous performance improvements in this release. The following chart shows the relative performance of individual TPC-H queries compared to the previous release. TPC-H @ scale factor 100, in parquet format. Concurrency 24. We also extended support for more TPC-H queries: q7, q8, q9 and q13 are running successfully in DataFusion 5.0. New Features Initial support for SQL-99 Analytics (WINDOW functions) Improved JOIN support: cross join, semi-join, anti join, and fixes to null handling Improved EXPLAIN support Initial implementation of metrics in the physical plan Support for SELECT DISTINCT Support for Json and NDJson formatted inputs Query column with relations Added more datetime related functions: now, date_trunc, to_timestamp_millis, to_timestamp_micros, to_timestamp_seconds Streaming Dataframe.collect Support table column aliases Answer count(*), min() and max() queries using only statistics Non-equi-join filters in JOIN conditions Modulus operation Support group by column positions Added constant folding query optimizer Hash partitioned aggregation Added random SQL function Implemented count distinct for floats and dictionary types Re-exported arrow and parquet crates in Datafusion General row group pruning logic that’s agnostic to storage format How to Get Involved If you are interested in contributing to DataFusion, we would love to have you! You can help by trying out DataFusion on some of your own data and projects and filing bug reports and helping to improve the documentation, or contribute to the documentation, tests or code. A list of open issues suitable for beginners is here and the full list is here.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 5.0.0 Release</title><link href="https://arrow.apache.org/blog/2021/07/29/5.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 5.0.0 Release" /><published>2021-07-29T02:00:00-04:00</published><updated>2021-07-29T02:00:00-04:00</updated><id>https://arrow.apache.org/blog/2021/07/29/5.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2021/07/29/5.0.0-release/">&lt;!--

--&gt;

&lt;p&gt;The Apache Arrow team is pleased to announce the 5.0.0 release. This covers
3 months of development work and includes &lt;strong&gt;684 commits&lt;/strong&gt; from
&lt;a href=&quot;/release/5.0.0.html#contributors&quot;&gt;&lt;strong&gt;99 distinct contributors&lt;/strong&gt;&lt;/a&gt; in 2 repositories. See the Install Page to
learn how to get the libraries for your platform.&lt;/p&gt;

&lt;p&gt;The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the complete changelogs for the &lt;a href=&quot;/release/5.0.0.html#changelog&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apache/arrow&lt;/code&gt;&lt;/a&gt; and
&lt;a href=&quot;https://github.com/apache/arrow-rs/blob/5.0.0/CHANGELOG.md&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apache/arrow-rs&lt;/code&gt;&lt;/a&gt; repositories.&lt;/p&gt;

&lt;h2 id=&quot;community&quot;&gt;Community&lt;/h2&gt;

&lt;p&gt;Since the 4.0.0 release, Daniël Heres, Kazuaki Ishizaki, Dominik Moritz, and Weston Pace
have been invited as committers to Arrow,
and Benjamin Kietzman and David Li have joined the Project Management Committee
(PMC). Thank you for all of your contributions!&lt;/p&gt;

&lt;h2 id=&quot;columnar-format-notes&quot;&gt;Columnar Format Notes&lt;/h2&gt;

&lt;p&gt;Official IANA Media types (MIME types) have been registered for Apache
Arrow IPC protocol data, both &lt;a href=&quot;/docs/format/Columnar.html#ipc-streaming-format&quot;&gt;stream&lt;/a&gt;
and &lt;a href=&quot;/docs/format/Columnar.html#ipc-file-format&quot;&gt;file&lt;/a&gt; variants:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.iana.org/assignments/media-types/application/vnd.apache.arrow.stream&quot;&gt;https://www.iana.org/assignments/media-types/application/vnd.apache.arrow.stream&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.iana.org/assignments/media-types/application/vnd.apache.arrow.file&quot;&gt;https://www.iana.org/assignments/media-types/application/vnd.apache.arrow.file&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We recommend &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.arrow&lt;/code&gt; as the IPC file format file extension and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.arrows&lt;/code&gt; for
the IPC streaming format file extension.&lt;/p&gt;

&lt;h2 id=&quot;arrow-flight-rpc-notes&quot;&gt;Arrow Flight RPC notes&lt;/h2&gt;

&lt;p&gt;The Go implementation now supports custom metadata and middleware, and has
been added to integration testing.&lt;/p&gt;

&lt;p&gt;In Python, some operations can now be interrupted via Control-C.&lt;/p&gt;

&lt;h2 id=&quot;c-notes&quot;&gt;C++ notes&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MakeArrayFromScalar&lt;/code&gt; now works for fixed-size binary types (&lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-13321&quot;&gt;ARROW-13321&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&quot;compute-layer&quot;&gt;Compute layer&lt;/h3&gt;

&lt;p&gt;The following &lt;a href=&quot;/docs/cpp/compute.html&quot;&gt;compute functions&lt;/a&gt;
were added:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;aggregations: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;scalar arithmetic and math functions: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;abs&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;abs_checked&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;acos&lt;/code&gt;,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;acos_checked&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asin&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asin_checked&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;atan&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;atan2&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ceil&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cos&lt;/code&gt;,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cos_checked&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;floor&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ln&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ln_checked&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log10&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log10_checked&lt;/code&gt;,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log1p&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log1p_checked&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log2&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log2_checked&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;negate&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;negate_checked&lt;/code&gt;,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sign&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sin&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sin_checked&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tan&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tan_checked&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;trunc&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;scalar bitwise functions: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bit_wise_and&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bit_wise_not&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bit_wise_or&lt;/code&gt;,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bit_wise_xor&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;shift_left&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;shift_left_checked&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;shift_right&lt;/code&gt;,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;shift_right_checked&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;scalar string functions: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ascii_center&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ascii_lpad&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ascii_reverse&lt;/code&gt;,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ascii_rpad&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;binary_join&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;binary_join_element_wise&lt;/code&gt;,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;binary_replace_slice&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;count_substring&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;count_substring_regex&lt;/code&gt;,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ends_with&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;find_substring&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;find_substring_regex&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;match_like&lt;/code&gt;,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;split_pattern_regex&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;starts_with&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;utf8_center&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;utf8_lpad&lt;/code&gt;,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;utf8_replace_slice&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;utf8_rpad&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;utf8_reverse&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;utf8_slice_codepoints&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;scalar temporal functions: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;day&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;day_of_week&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;day_of_year&lt;/code&gt;,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;iso_calendar&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;iso_week&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;iso_year&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hour&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;microsecond&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;millisecond&lt;/code&gt;,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;minute&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;month&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nanosecond&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;quarter&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;second&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;subsecond&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;year&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;other scalar functions: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;case_when&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;coalesce&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;if_else&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_finite&lt;/code&gt;,
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_inf&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_nan&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_element_wise&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;min_element_wise&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make_struct&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;vector functions: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;replace_with_mask&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Duplicates are now allowed in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SetLookupOptions::value_set&lt;/code&gt; (&lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-12554&quot;&gt;ARROW-12554&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Decimal types are now supported by some basic arithmetic functions (&lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-12074&quot;&gt;ARROW-12074&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;take&lt;/code&gt; function now supports dense unions (&lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-13005&quot;&gt;ARROW-13005&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;It is now possible to cast between dictionary types with different index
types (&lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-11673&quot;&gt;ARROW-11673&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Sorting is now implemented for boolean input (&lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-12016&quot;&gt;ARROW-12016&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&quot;csv&quot;&gt;CSV&lt;/h3&gt;

&lt;p&gt;The streaming CSV reader can now take some advantage of multiple threads (&lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-11889&quot;&gt;ARROW-11889&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The CSV reader tries to make its errors more informative by adding the
row number when it is known, i.e. when parallel reading is disabled (&lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-12675&quot;&gt;ARROW-12675&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;A new option &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ReaderOptions::skip_rows_after_names&lt;/code&gt; allows skipping a number
of rows &lt;em&gt;after&lt;/em&gt; reading the column names (as opposed to
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ReaderOptions::skip_rows&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Quoted strings can now be treated as always non-null (&lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-10115&quot;&gt;ARROW-10115&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&quot;dataset-layer&quot;&gt;Dataset layer&lt;/h3&gt;

&lt;p&gt;The asynchronous scanner introduced in 4.0.0 has been improved with truly 
asynchronous readers implemented for CSV, Parquet, and IPC file formats and 
file-level parallelism added.  This mode is controlled by a flag &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;use_async&lt;/code&gt; that
can be passed into methods which scan a dataset.  Setting this flag to True
will have significant improvements on filesystems with high latency or parallel
reads (e.g. S3).&lt;/p&gt;

&lt;p&gt;A CountRows method has been added to count rows matching a predicate; where
possible, this will use metadata in files instead of reading the data itself.&lt;/p&gt;

&lt;p&gt;CSV datasets can now be written, and when reading a CSV dataset, explicit types can
now be specified for a subset of columns while allowing the rest to still be inferred.&lt;/p&gt;

&lt;h3 id=&quot;io-and-filesystem-layer&quot;&gt;IO and Filesystem layer&lt;/h3&gt;

&lt;p&gt;The I/O thread pool size can now be adjusted at runtime (&lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-12760&quot;&gt;ARROW-12760&lt;/a&gt;).
The default size remains 8 threads.&lt;/p&gt;

&lt;p&gt;Streams now can have auxiliary metadata, depending on the backend.  This
has been implemented for the S3 filesystems, where a couple metadata
keys are supported such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Content-Type&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ACL&lt;/code&gt; (&lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-11161&quot;&gt;ARROW-11161&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-12719&quot;&gt;ARROW-12719&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The HadoopFileSystem implementation now implements the FileSystem abstraction
more faithfully (&lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-12790&quot;&gt;ARROW-12790&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&quot;parquet&quot;&gt;Parquet&lt;/h3&gt;

&lt;p&gt;The new LZ4_RAW compression scheme was implemented (PARQUET-1998).
Unlike the legacy LZ4 compression scheme, it is defined unambiguously
and should provide better portability once other Parquet implementations
catch up.&lt;/p&gt;

&lt;h2 id=&quot;go-notes&quot;&gt;Go notes&lt;/h2&gt;

&lt;h3 id=&quot;flight&quot;&gt;Flight&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Flight Client and Server now support Custom Metadata through the functions &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flight.NewClientWithMiddleware&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flight.NewServerWithMiddleware&lt;/code&gt;. Functions
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flight.NewFlightClient&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flight.NewFlightServer&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flight.CreateServerBearerTokenAuthInterceptors&lt;/code&gt; have been deprecated in favor of using the new middleware. &lt;a href=&quot;https://github.com/apache/arrow/pull/10633&quot;&gt;#10633&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Flight Client &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AuthHandler&lt;/code&gt; no longer overwrites outgoing metadata, correctly appending new metadata without overwriting existing metadata &lt;a href=&quot;https://github.com/apache/arrow/pull/10297&quot;&gt;#10297&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Flight AppMetadata field is now exposed both for Reading and Writing via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flight.Reader#LatestAppMetadata()&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flight.Writer#WriteWithAppMetadata&lt;/code&gt; functions &lt;a href=&quot;https://github.com/apache/arrow/pull/10142&quot;&gt;#10142&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;other-enhancements&quot;&gt;Other enhancements&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/arrow/pull/10106&quot;&gt;Map&lt;/a&gt; and &lt;a href=&quot;https://github.com/apache/arrow/pull/10203&quot;&gt;Extension&lt;/a&gt; Datatypes are now implemented for Arrow Arrays&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/arrow/pull/10071&quot;&gt;Schema package&lt;/a&gt; and first part of &lt;a href=&quot;https://github.com/apache/arrow/pull/10379&quot;&gt;Encoding package&lt;/a&gt; added for Golang Parquet Implementation&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;java-notes&quot;&gt;Java notes&lt;/h2&gt;

&lt;p&gt;Highlighted improvements and fixes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Improved support for extension types using a complex storage type, e.g. struct, map or union. These can now extend
the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ExtensionTypeVector&lt;/code&gt; base class.&lt;/li&gt;
  &lt;li&gt;Union vectors now extend &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AbstractContainerVector&lt;/code&gt; to be consistent with other vectors.&lt;/li&gt;
  &lt;li&gt;Guava dependency updated to 30.1.1&lt;/li&gt;
  &lt;li&gt;Memory leak fixed if an exception occurs when reading IPC messages from a channel. &lt;a href=&quot;https://github.com/apache/arrow/pull/10423&quot;&gt;#10423&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Flight error metadata is now propagated to the client. &lt;a href=&quot;https://github.com/apache/arrow/pull/10370&quot;&gt;#10370&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;JDBC adapter now preserves nullability. &lt;a href=&quot;https://github.com/apache/arrow/pull/10285&quot;&gt;#10285&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The memory rounding policy is respected when allocating vector buffers. This helps saving memory space. &lt;a href=&quot;https://github.com/apache/arrow/pull/10576&quot;&gt;#10576&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;API compatibility changes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Complex vectors now return covariant types from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;getObject(int)&lt;/code&gt;. &lt;a href=&quot;https://github.com/apache/arrow/pull/9964&quot;&gt;#9964&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;javascript-notes&quot;&gt;JavaScript notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Tables do not extend DataFrames anymore. This enables smaller bundles. &lt;a href=&quot;https://github.com/apache/arrow/pull/10277&quot;&gt;#10277&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Arrow uses closure compiler for all UMD bundles, making them smaller. &lt;a href=&quot;https://github.com/apache/arrow/pull/10281&quot;&gt;#10281&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The npm package now comes with declaration maps for better navigation from types to source code. &lt;a href=&quot;https://github.com/apache/arrow/pull/10673&quot;&gt;#10673&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Updated dependencies and improvements to the code.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;python-notes&quot;&gt;Python notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Datasets can now scan files asynchronously when the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;use_async=True&lt;/code&gt; option is provided to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dataset.scanner&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dataset.to_table&lt;/code&gt;, or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dataset.to_batches&lt;/code&gt; methods. This should provide better performance in environments where I/O can be slow, such as with remote sources.&lt;/li&gt;
  &lt;li&gt;Arrow now provides builtin support for writing CSV files through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pyarrow.csv.write_csv&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Wheels for Apple M1 Macs are now provided.&lt;/li&gt;
  &lt;li&gt;Many new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pyarrow.compute&lt;/code&gt; functions are available (see the C++ notes above
for more details), and introspection of the functions was improved so that
they look more like standard Python functions.&lt;/li&gt;
  &lt;li&gt;It is now possible to access ORC file metadata from Python &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ORCFile&lt;/code&gt; objects&lt;/li&gt;
  &lt;li&gt;Building a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;StructArray&lt;/code&gt; now accepts a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask&lt;/code&gt; like other arrays&lt;/li&gt;
  &lt;li&gt;Many updates and fixes for the documentation&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;r-notes&quot;&gt;R notes&lt;/h2&gt;

&lt;p&gt;In this release, we’ve more than doubled the number of functions you can call on Arrow Datasets inside &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dplyr::filter()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mutate()&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;arrange()&lt;/code&gt;, including many more string, datetime, and math functions. You can also write Datasets to CSV files, in addition to Parquet and Feather. We’ve also deepened support for the Arrow C interface, which is used in the Python interface and allows integration with other projects, such as DuckDB.&lt;/p&gt;

&lt;p&gt;For more on what’s in the 5.0.0 R package, see the &lt;a href=&quot;/docs/r/news/&quot;&gt;R changelog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;ruby-and-c-glib-notes&quot;&gt;Ruby and C GLib notes&lt;/h2&gt;

&lt;p&gt;Apache Arrow Flight support is started. But &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ListFlights&lt;/code&gt; is only supported for now. More features will be implemented in the next major release.&lt;/p&gt;

&lt;h3 id=&quot;ruby&quot;&gt;Ruby&lt;/h3&gt;

&lt;p&gt;You need gobject-introspection gem 3.4.5 or later to implement your Apache Arrow Flight server. If you only use Apache Arrow Flight client, gobject-introspection gem 3.4.5 or later isn’t required.&lt;/p&gt;

&lt;p&gt;Here are highlighted improvements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Compute functions accept raw Ruby objects such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Integer&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Array&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;String&lt;/code&gt;:&lt;/p&gt;

    &lt;div class=&quot;language-ruby highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;add_function&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Arrow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;Function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;find&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;add&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Not shortcut version&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;augend&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Arrow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;Int8Array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;addend&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Arrow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;Int8Scalar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
  &lt;span class=&quot;no&quot;&gt;Arrow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;ArrayDatum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;augend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;no&quot;&gt;Arrow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;ScalarDatum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;addend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;add_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;to_a&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# =&amp;gt; [6, 7, 8]&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Shortcut version&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;add_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;to_a&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# =&amp;gt; [6, 7, 8]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Arrow::PrimaryArray&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Arrow::Buffer&lt;/code&gt; can be used as MemoryView that is added in Ruby 3.0.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are some backward incompatible changes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Arrow::CountOptions&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Arrow::CountMode&lt;/code&gt; are removed. Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Arrow::ScalarAggregateOptions&lt;/code&gt; instead.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;c-glib&quot;&gt;C GLib&lt;/h3&gt;

&lt;p&gt;There are some backward incompatible changes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GArrowCountOptions&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GArrowCountMode&lt;/code&gt; are removed. Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GArrowScalarAggregateOptions&lt;/code&gt; instead.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;garrow_array_equal_range()&lt;/code&gt; requires &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GArrowEqualOptions&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Prefix in arrow-dataset-glib is changed to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gadataset_&lt;/code&gt;/&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GADATASET_&lt;/code&gt; from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gad_&lt;/code&gt;/&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GAD_&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GADScanOptions&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GADScanTask&lt;/code&gt;  and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GADInMemoryScanTask&lt;/code&gt; are removed. Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gadataset_begin_scan()&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gadataset_to_table()&lt;/code&gt; instead.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GArrowCompareOptions&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GArrowCompareOperator&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;garrow_*_array_compare()&lt;/code&gt; are removed. Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;equal&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;not_equal&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;less_than&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;less_than_equal&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;greater_than&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;greater_than_equal&lt;/code&gt; compute functions directly instead.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;rust-notes&quot;&gt;Rust notes&lt;/h2&gt;

&lt;p&gt;The Rust projects have moved to separate repositories outside the
main Arrow monorepo. For notes on the 5.0.0 release of the Rust
implementation, see the &lt;a href=&quot;https://github.com/apache/arrow-rs/blob/5.0.0/CHANGELOG.md&quot;&gt;Arrow Rust changelog&lt;/a&gt; and the
&lt;a href=&quot;/blog/2021/07/29/5.0.0-rs-release/&quot;&gt;Apache Arrow Rust 5.0.0 Release blog post&lt;/a&gt;.&lt;/p&gt;</content><author><name>pmc</name></author><category term="release" /><summary type="html">The Apache Arrow team is pleased to announce the 5.0.0 release. This covers 3 months of development work and includes 684 commits from 99 distinct contributors in 2 repositories. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelogs for the apache/arrow and apache/arrow-rs repositories. Community Since the 4.0.0 release, Daniël Heres, Kazuaki Ishizaki, Dominik Moritz, and Weston Pace have been invited as committers to Arrow, and Benjamin Kietzman and David Li have joined the Project Management Committee (PMC). Thank you for all of your contributions! Columnar Format Notes Official IANA Media types (MIME types) have been registered for Apache Arrow IPC protocol data, both stream and file variants: https://www.iana.org/assignments/media-types/application/vnd.apache.arrow.stream https://www.iana.org/assignments/media-types/application/vnd.apache.arrow.file We recommend .arrow as the IPC file format file extension and .arrows for the IPC streaming format file extension. Arrow Flight RPC notes The Go implementation now supports custom metadata and middleware, and has been added to integration testing. In Python, some operations can now be interrupted via Control-C. C++ notes MakeArrayFromScalar now works for fixed-size binary types (ARROW-13321). Compute layer The following compute functions were added: aggregations: index scalar arithmetic and math functions: abs, abs_checked, acos, acos_checked, asin, asin_checked, atan, atan2, ceil, cos, cos_checked, floor, ln, ln_checked, log10, log10_checked, log1p, log1p_checked, log2, log2_checked, negate, negate_checked, sign, sin, sin_checked, tan, tan_checked, trunc scalar bitwise functions: bit_wise_and, bit_wise_not, bit_wise_or, bit_wise_xor, shift_left, shift_left_checked, shift_right, shift_right_checked scalar string functions: ascii_center, ascii_lpad, ascii_reverse, ascii_rpad, binary_join, binary_join_element_wise, binary_replace_slice, count_substring, count_substring_regex, ends_with, find_substring, find_substring_regex, match_like, split_pattern_regex, starts_with, utf8_center, utf8_lpad, utf8_replace_slice, utf8_rpad, utf8_reverse, utf8_slice_codepoints scalar temporal functions: day, day_of_week, day_of_year, iso_calendar, iso_week, iso_year, hour, microsecond, millisecond, minute, month, nanosecond, quarter, second, subsecond, year other scalar functions: case_when, coalesce, if_else, is_finite, is_inf, is_nan, max_element_wise, min_element_wise, make_struct vector functions: replace_with_mask Duplicates are now allowed in SetLookupOptions::value_set (ARROW-12554). Decimal types are now supported by some basic arithmetic functions (ARROW-12074). The take function now supports dense unions (ARROW-13005). It is now possible to cast between dictionary types with different index types (ARROW-11673). Sorting is now implemented for boolean input (ARROW-12016). CSV The streaming CSV reader can now take some advantage of multiple threads (ARROW-11889). The CSV reader tries to make its errors more informative by adding the row number when it is known, i.e. when parallel reading is disabled (ARROW-12675). A new option ReaderOptions::skip_rows_after_names allows skipping a number of rows after reading the column names (as opposed to ReaderOptions::skip_rows). Quoted strings can now be treated as always non-null (ARROW-10115). Dataset layer The asynchronous scanner introduced in 4.0.0 has been improved with truly asynchronous readers implemented for CSV, Parquet, and IPC file formats and file-level parallelism added. This mode is controlled by a flag use_async that can be passed into methods which scan a dataset. Setting this flag to True will have significant improvements on filesystems with high latency or parallel reads (e.g. S3). A CountRows method has been added to count rows matching a predicate; where possible, this will use metadata in files instead of reading the data itself. CSV datasets can now be written, and when reading a CSV dataset, explicit types can now be specified for a subset of columns while allowing the rest to still be inferred. IO and Filesystem layer The I/O thread pool size can now be adjusted at runtime (ARROW-12760). The default size remains 8 threads. Streams now can have auxiliary metadata, depending on the backend. This has been implemented for the S3 filesystems, where a couple metadata keys are supported such as Content-Type and ACL (ARROW-11161, ARROW-12719). The HadoopFileSystem implementation now implements the FileSystem abstraction more faithfully (ARROW-12790). Parquet The new LZ4_RAW compression scheme was implemented (PARQUET-1998). Unlike the legacy LZ4 compression scheme, it is defined unambiguously and should provide better portability once other Parquet implementations catch up. Go notes Flight Flight Client and Server now support Custom Metadata through the functions flight.NewClientWithMiddleware and flight.NewServerWithMiddleware. Functions flight.NewFlightClient, flight.NewFlightServer, flight.CreateServerBearerTokenAuthInterceptors have been deprecated in favor of using the new middleware. #10633 Flight Client AuthHandler no longer overwrites outgoing metadata, correctly appending new metadata without overwriting existing metadata #10297 Flight AppMetadata field is now exposed both for Reading and Writing via flight.Reader#LatestAppMetadata() and flight.Writer#WriteWithAppMetadata functions #10142 Other enhancements Map and Extension Datatypes are now implemented for Arrow Arrays Schema package and first part of Encoding package added for Golang Parquet Implementation Java notes Highlighted improvements and fixes: Improved support for extension types using a complex storage type, e.g. struct, map or union. These can now extend the ExtensionTypeVector base class. Union vectors now extend AbstractContainerVector to be consistent with other vectors. Guava dependency updated to 30.1.1 Memory leak fixed if an exception occurs when reading IPC messages from a channel. #10423 Flight error metadata is now propagated to the client. #10370 JDBC adapter now preserves nullability. #10285 The memory rounding policy is respected when allocating vector buffers. This helps saving memory space. #10576 API compatibility changes: Complex vectors now return covariant types from getObject(int). #9964 JavaScript notes Tables do not extend DataFrames anymore. This enables smaller bundles. #10277 Arrow uses closure compiler for all UMD bundles, making them smaller. #10281 The npm package now comes with declaration maps for better navigation from types to source code. #10673 Updated dependencies and improvements to the code. Python notes Datasets can now scan files asynchronously when the use_async=True option is provided to Dataset.scanner, Dataset.to_table, or Dataset.to_batches methods. This should provide better performance in environments where I/O can be slow, such as with remote sources. Arrow now provides builtin support for writing CSV files through pyarrow.csv.write_csv Wheels for Apple M1 Macs are now provided. Many new pyarrow.compute functions are available (see the C++ notes above for more details), and introspection of the functions was improved so that they look more like standard Python functions. It is now possible to access ORC file metadata from Python ORCFile objects Building a StructArray now accepts a mask like other arrays Many updates and fixes for the documentation R notes In this release, we’ve more than doubled the number of functions you can call on Arrow Datasets inside dplyr::filter(), mutate(), and arrange(), including many more string, datetime, and math functions. You can also write Datasets to CSV files, in addition to Parquet and Feather. We’ve also deepened support for the Arrow C interface, which is used in the Python interface and allows integration with other projects, such as DuckDB. For more on what’s in the 5.0.0 R package, see the R changelog. Ruby and C GLib notes Apache Arrow Flight support is started. But ListFlights is only supported for now. More features will be implemented in the next major release. Ruby You need gobject-introspection gem 3.4.5 or later to implement your Apache Arrow Flight server. If you only use Apache Arrow Flight client, gobject-introspection gem 3.4.5 or later isn’t required. Here are highlighted improvements: Compute functions accept raw Ruby objects such as true, Integer, Array and String: add_function = Arrow::Function.find(&quot;add&quot;) # Not shortcut version augend = Arrow::Int8Array.new([1, 2, 3]) addend = Arrow::Int8Scalar.new(5) args = [ Arrow::ArrayDatum.new(augend), Arrow::ScalarDatum.new(addend), ] add_function.execute(args).value.to_a # =&amp;gt; [6, 7, 8] # Shortcut version add_function.execute([[1, 2, 3], 5]).value.to_a # =&amp;gt; [6, 7, 8] Arrow::PrimaryArray and Arrow::Buffer can be used as MemoryView that is added in Ruby 3.0. There are some backward incompatible changes: Arrow::CountOptions and Arrow::CountMode are removed. Use Arrow::ScalarAggregateOptions instead. C GLib There are some backward incompatible changes: GArrowCountOptions and GArrowCountMode are removed. Use GArrowScalarAggregateOptions instead. garrow_array_equal_range() requires GArrowEqualOptions. Prefix in arrow-dataset-glib is changed to gadataset_/GADATASET_ from gad_/GAD_. GADScanOptions, GADScanTask and GADInMemoryScanTask are removed. Use gadataset_begin_scan() or gadataset_to_table() instead. GArrowCompareOptions, GArrowCompareOperator and garrow_*_array_compare() are removed. Use equal, not_equal, less_than, less_than_equal, greater_than and greater_than_equal compute functions directly instead. Rust notes The Rust projects have moved to separate repositories outside the main Arrow monorepo. For notes on the 5.0.0 release of the Rust implementation, see the Arrow Rust changelog and the Apache Arrow Rust 5.0.0 Release blog post.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow Rust 5.0.0 Release</title><link href="https://arrow.apache.org/blog/2021/07/29/5.0.0-rs-release/" rel="alternate" type="text/html" title="Apache Arrow Rust 5.0.0 Release" /><published>2021-07-29T00:00:00-04:00</published><updated>2021-07-29T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2021/07/29/5.0.0-rs-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2021/07/29/5.0.0-rs-release/">&lt;!--

--&gt;

&lt;p&gt;We recently released the 5.0.0 Rust version of &lt;a href=&quot;https://arrow.apache.org/&quot;&gt;Apache Arrow&lt;/a&gt; which coincides with the &lt;a href=&quot;https://arrow.apache.org/release/5.0.0.html&quot;&gt;Arrow 5.0.0 release&lt;/a&gt;. This post highlights some of the improvements in the Rust implementation. The full changelog can be found &lt;a href=&quot;https://github.com/apache/arrow-rs/blob/5.0.0/CHANGELOG.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;!--
(arrow_dev) alamb@MacBook-Pro:~/Software/arrow-rs$ git log --pretty=oneline 4.0.0..5.0.0 | wc -l
     161
(arrow_dev) alamb@MacBook-Pro:~/Software/arrow-rs$ git shortlog -sn 4.0.0..5.0.0 | wc -l
      35 // but Jorge is double counted
--&gt;

&lt;p&gt;The Rust Arrow implementation would not be possible without the wonderful work and support of our community, and the 5.0.0 release is no exception. It includes 161 commits from 34 individual contributors, many of them with their first contribution. Thank you all very much.&lt;/p&gt;

&lt;h1 id=&quot;arrow&quot;&gt;Arrow&lt;/h1&gt;
&lt;p&gt;Feature-wise, this release adds:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;A &lt;a href=&quot;https://github.com/apache/arrow-rs/pull/424&quot;&gt;new kernel&lt;/a&gt; which lexicographically partitions points.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/arrow-rs/pull/439&quot;&gt;Expanded support&lt;/a&gt; for the FFI/&lt;a href=&quot;https://arrow.apache.org/docs/format/CDataInterface.html&quot;&gt;C data interface&lt;/a&gt;, easing integration with the broader Arrow ecosystem&lt;/li&gt;
  &lt;li&gt;Usability enhancements for creating and manipulating &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RecordBatch&lt;/code&gt;es.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/arrow-rs/pull/377&quot;&gt;Improved usability&lt;/a&gt; for Arrow Flight’s API.&lt;/li&gt;
  &lt;li&gt;Slimmer dependency stack when default features are disabled&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We continue to leverage the Rust ecosystem to deliver reliable and performant code. We made significant progress towards running the Rust test suite under the &lt;a href=&quot;https://github.com/rust-lang/miri&quot;&gt;MIRI checker&lt;/a&gt; (a sort of valgrind for Rust) for memory access violations, and we expect it to be fully enabled in CI for the 5.1.0 release.&lt;/p&gt;

&lt;p&gt;Of course, this release also contains bug fixes, performance improvements, and improved documentation examples. For the full list of changes, please consult the &lt;a href=&quot;https://github.com/apache/arrow-rs/blob/5.0.0/CHANGELOG.md&quot;&gt;changelog&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;parquet&quot;&gt;Parquet&lt;/h1&gt;
&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;parquet-derive&lt;/code&gt; crate now automatically derives the required parquet schema, and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;parquet&lt;/code&gt; crate had several bug fixes and enhancements.&lt;/p&gt;

&lt;h1 id=&quot;more-frequent-releases&quot;&gt;More Frequent Releases&lt;/h1&gt;
&lt;p&gt;Arrow releases major versions every three months. The Rust implementation has been experimenting with releasing minor version updates to speed the flow of new features and fixes. By implementing a new development process, as described in &lt;a href=&quot;https://arrow.apache.org/blog/2021/05/04/rust-dev-workflow/&quot;&gt;A New Development Workflow for Arrow’s Rust Implementation&lt;/a&gt; we have successfully created 4 minor releases on the 4.x.x line every other week without any reports of breakage.&lt;/p&gt;

&lt;p&gt;You can always find the latest releases on crates.io: &lt;a href=&quot;https://crates.io/crates/arrow&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;arrow&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;https://crates.io/crates/parquet&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;parquet&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;https://crates.io/crates/arrow-flight&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;arrow-flight&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;https://crates.io/crates/parquet-derive&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;parquet-derive&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;datafusion--ballista&quot;&gt;DataFusion &amp;amp; Ballista&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.rs/datafusion/4.0.0/datafusion/&quot;&gt;DataFusion&lt;/a&gt; is an in-memory query engine with DataFrame and SQL APIs, built on top of Arrow. Ballista is a distributed compute platform. These projects are now in their &lt;a href=&quot;https://github.com/apache/arrow-datafusion&quot;&gt;own repository&lt;/a&gt;, and are no longer released in lock-step with Arrow. Expect further news in this area soon.&lt;/p&gt;

&lt;h1 id=&quot;roadmap-for-600-and-beyond&quot;&gt;Roadmap for 6.0.0 and Beyond&lt;/h1&gt;
&lt;p&gt;Here are some of the initiatives that contributors are currently working on for future releases:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Improved performance of compute kernels&lt;/li&gt;
  &lt;li&gt;Date/time/timestamp/interval compute kernels&lt;/li&gt;
  &lt;li&gt;MapArray support&lt;/li&gt;
  &lt;li&gt;Preparations for removing the use of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsafe&lt;/code&gt; to make arrow faster and more secure – see the &lt;a href=&quot;https://lists.apache.org/thread.html/recac1f6dc982bab2923f8fb6992e2d4c927f46daff5f03ed6c4de19c%40%3Cdev.arrow.apache.org%3E&quot;&gt;mailing list&lt;/a&gt; discussion for more details.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;contributors-to-500&quot;&gt;Contributors to 5.0.0:&lt;/h1&gt;
&lt;p&gt;Again, thank you to all the contributors for this release. Here is the raw git listing:&lt;/p&gt;

&lt;!--
(arrow_dev) alamb@MacBook-Pro:~/Software/arrow-rs$ git shortlog -sn 4.0.0..5.0.0
.. list below ..

Note I combined two distinct names for Jorge
--&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    28  Jorge Leitao
    27  Andrew Lamb
    15  Jiayu Liu
    12  Ritchie Vink
    10  Wakahisa
     8  Raphael Taylor-Davies
     6  Daniël Heres
     5  Andy Grove
     5  Navin
     5  Jörn Horstmann
     4  Ádám Lippai
     4  Dominik Moritz
     4  Marco Neumann
     3  Roee Shlomo
     3  Michael Edwards
     2  Steven
     2  Krisztián Szűcs
     2  Gary Pennington
     1  Ben Chambers
     1  Max Meldrum
     1  Edd Robinson
     1  Gang Liao
     1  Chojan Shang
     1  Boaz
     1  Wes McKinney
     1  Yordan Pavlov
     1  baishen
     1  hulunbier
     1  kazuhiko kikuchi
     1  Dmitry Patsura
     1  Kornelijus Survila
     1  Laurent Mazare
     1  Manish Gill
     1  Marc van Heerden
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;how-to-get-involved&quot;&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;If you are interested in contributing to the Rust implementation of Apache Arrow, we would love to have you! You can help by trying out Arrow on some of your own data and projects and filing bug reports and helping to improve the documentation, or contribute to the documentation, tests or code. A list of open issues suitable for beginners is &lt;a href=&quot;https://github.com/apache/arrow-rs/labels/good%20first%20issue&quot;&gt;here&lt;/a&gt; and the full list is &lt;a href=&quot;https://github.com/apache/arrow-rs/issues&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>pmc</name></author><category term="release" /><summary type="html">We recently released the 5.0.0 Rust version of Apache Arrow which coincides with the Arrow 5.0.0 release. This post highlights some of the improvements in the Rust implementation. The full changelog can be found here. The Rust Arrow implementation would not be possible without the wonderful work and support of our community, and the 5.0.0 release is no exception. It includes 161 commits from 34 individual contributors, many of them with their first contribution. Thank you all very much. Arrow Feature-wise, this release adds: A new kernel which lexicographically partitions points. Expanded support for the FFI/C data interface, easing integration with the broader Arrow ecosystem Usability enhancements for creating and manipulating RecordBatches. Improved usability for Arrow Flight’s API. Slimmer dependency stack when default features are disabled We continue to leverage the Rust ecosystem to deliver reliable and performant code. We made significant progress towards running the Rust test suite under the MIRI checker (a sort of valgrind for Rust) for memory access violations, and we expect it to be fully enabled in CI for the 5.1.0 release. Of course, this release also contains bug fixes, performance improvements, and improved documentation examples. For the full list of changes, please consult the changelog. Parquet The parquet-derive crate now automatically derives the required parquet schema, and the parquet crate had several bug fixes and enhancements. More Frequent Releases Arrow releases major versions every three months. The Rust implementation has been experimenting with releasing minor version updates to speed the flow of new features and fixes. By implementing a new development process, as described in A New Development Workflow for Arrow’s Rust Implementation we have successfully created 4 minor releases on the 4.x.x line every other week without any reports of breakage. You can always find the latest releases on crates.io: arrow, parquet, arrow-flight, and parquet-derive. DataFusion &amp;amp; Ballista DataFusion is an in-memory query engine with DataFrame and SQL APIs, built on top of Arrow. Ballista is a distributed compute platform. These projects are now in their own repository, and are no longer released in lock-step with Arrow. Expect further news in this area soon. Roadmap for 6.0.0 and Beyond Here are some of the initiatives that contributors are currently working on for future releases: Improved performance of compute kernels Date/time/timestamp/interval compute kernels MapArray support Preparations for removing the use of unsafe to make arrow faster and more secure – see the mailing list discussion for more details. Contributors to 5.0.0: Again, thank you to all the contributors for this release. Here is the raw git listing: 28 Jorge Leitao 27 Andrew Lamb 15 Jiayu Liu 12 Ritchie Vink 10 Wakahisa 8 Raphael Taylor-Davies 6 Daniël Heres 5 Andy Grove 5 Navin 5 Jörn Horstmann 4 Ádám Lippai 4 Dominik Moritz 4 Marco Neumann 3 Roee Shlomo 3 Michael Edwards 2 Steven 2 Krisztián Szűcs 2 Gary Pennington 1 Ben Chambers 1 Max Meldrum 1 Edd Robinson 1 Gang Liao 1 Chojan Shang 1 Boaz 1 Wes McKinney 1 Yordan Pavlov 1 baishen 1 hulunbier 1 kazuhiko kikuchi 1 Dmitry Patsura 1 Kornelijus Survila 1 Laurent Mazare 1 Manish Gill 1 Marc van Heerden How to Get Involved If you are interested in contributing to the Rust implementation of Apache Arrow, we would love to have you! You can help by trying out Arrow on some of your own data and projects and filing bug reports and helping to improve the documentation, or contribute to the documentation, tests or code. A list of open issues suitable for beginners is here and the full list is here.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 4.0.1 Release</title><link href="https://arrow.apache.org/blog/2021/06/19/4.0.1-release/" rel="alternate" type="text/html" title="Apache Arrow 4.0.1 Release" /><published>2021-06-19T00:00:00-04:00</published><updated>2021-06-19T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2021/06/19/4.0.1-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2021/06/19/4.0.1-release/">&lt;!--

--&gt;

&lt;p&gt;The Apache Arrow team is pleased to announce the 4.0.1 release. This
release covers general bug fixes on the different implementations, notably
C++, R, Python and JavaScript.
The list is available &lt;a href=&quot;https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%204.0.1&quot;&gt;here&lt;/a&gt;, with the list of contributors &lt;a href=&quot;/release/4.0.1.html#contributors&quot;&gt;here&lt;/a&gt;
and changelog &lt;a href=&quot;/release/4.0.1.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As usual, see &lt;a href=&quot;/install/&quot;&gt;the install page&lt;/a&gt; for instructions on how to install it.&lt;/p&gt;</content><author><name>pmc</name></author><category term="release" /><summary type="html">The Apache Arrow team is pleased to announce the 4.0.1 release. This release covers general bug fixes on the different implementations, notably C++, R, Python and JavaScript. The list is available here, with the list of contributors here and changelog here. As usual, see the install page for instructions on how to install it.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A New Development Workflow for Arrow’s Rust Implementation</title><link href="https://arrow.apache.org/blog/2021/05/04/rust-dev-workflow/" rel="alternate" type="text/html" title="A New Development Workflow for Arrow’s Rust Implementation" /><published>2021-05-04T00:00:00-04:00</published><updated>2021-05-04T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2021/05/04/rust-dev-workflow</id><content type="html" xml:base="https://arrow.apache.org/blog/2021/05/04/rust-dev-workflow/">&lt;!--

--&gt;

&lt;p&gt;The Apache Arrow Rust community is excited to announce that its migration to a new development workflow is now complete! If you’re considering Rust as a language for working with columnar data, read on and see how your use case might benefit from our new and improved project setup.&lt;/p&gt;

&lt;p&gt;In recent months, members of the community have been working closely with Arrow’s &lt;a href=&quot;https://arrow.apache.org/committers/&quot;&gt;Project Management Committee&lt;/a&gt; and other contributors to expand the set of available workflows for Arrow implementations. The goal was to define a new development process that ultimately:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Enables a faster release cadence that adheres to &lt;a href=&quot;https://semver.org/&quot;&gt;SemVer&lt;/a&gt; where appropriate&lt;/li&gt;
  &lt;li&gt;Encourages maximum participation from the wider community with unified tooling&lt;/li&gt;
  &lt;li&gt;Ensures that we continue to uphold the tenets of &lt;a href=&quot;https://www.apache.org/theapacheway/&quot;&gt;The Apache Way&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you’re just here for the highlights, the major outcomes of these discussions are as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The Rust projects have moved to separate repositories, outside the main Arrow &lt;a href=&quot;https://en.wikipedia.org/wiki/Monorepo&quot;&gt;monorepo&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/apache/arrow-rs&quot;&gt;arrow-rs&lt;/a&gt; for the core Arrow, Arrow Flight, and Parquet implementations in Rust&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/apache/arrow-datafusion&quot;&gt;arrow-datafusion&lt;/a&gt; for DataFusion and Ballista (more on these projects below!)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The Rust community will use GitHub Issues for tracking feature development and issues, replacing the Jira instance maintained by the Apache Software Foundation (ASF)&lt;/li&gt;
  &lt;li&gt;DataFusion and Ballista will follow a new release cycle, independent of the main Arrow releases&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But why, as a community, have we decided to change our processes? Let’s take a slightly more in-depth look at the Rust implementation’s needs.&lt;/p&gt;

&lt;h2 id=&quot;project-structure&quot;&gt;Project Structure&lt;/h2&gt;
&lt;p&gt;The Rust implementation of Arrow actually consists of several distinct projects, or in Rust parlance, &lt;a href=&quot;https://doc.rust-lang.org/book/ch07-01-packages-and-crates.html&quot;&gt;“crates”&lt;/a&gt;. In addition to the core crates, namely &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;arrow&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;arrow-flight&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;parquet&lt;/code&gt;, we also maintain:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/arrow-datafusion/datafusion&quot;&gt;DataFusion&lt;/a&gt;: an extensible in-memory query execution engine using Arrow as its format&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/arrow-datafusion/ballista&quot;&gt;Ballista&lt;/a&gt;: a distributed compute platform, powered by Apache Arrow and DataFusion&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Whilst these projects are all closely related, with many shared contributors, they’re very much at different stages in their respective lifecycles. The core Arrow crate, as an implementation of a spec, has strict compatibility requirements with other versions of Arrow, and this is tested via rigorous cross-language integration tests.&lt;/p&gt;

&lt;p&gt;However, at the other end of the spectrum, DataFusion and Ballista are still nascent projects in their own right that undergo frequent backwards-incompatible changes. In the old workflow, DataFusion was released in lockstep with Arrow; because DataFusion users often need newly-contributed features or bugfixes on a tighter schedule than Arrow releases, we observed that many people in the community simply resorted to referencing our GitHub repository directly, rather than properly versioned builds on &lt;a href=&quot;https://crates.io/&quot;&gt;crates.io&lt;/a&gt;, Rust’s package registry.&lt;/p&gt;

&lt;p&gt;Ultimately, the decision was made to split the Rust crates into two separate repositories: &lt;a href=&quot;https://github.com/apache/arrow-rs&quot;&gt;arrow-rs&lt;/a&gt; for the core Arrow functionality, and &lt;a href=&quot;https://github.com/apache/arrow-datafusion&quot;&gt;arrow-datafusion&lt;/a&gt; for DataFusion and Ballista. There’s still work to be done on determining the exact release workflows for the latter, but this leaves us in a much better position to meet the broader Rust community’s expectations of crate versioning and stability.&lt;/p&gt;

&lt;h2 id=&quot;community-participation&quot;&gt;Community Participation&lt;/h2&gt;
&lt;p&gt;All Apache projects are built on volunteer contribution; it’s a core principle of both the ASF and open-source software development more broadly. One point of friction that was observed in the previous workflow for the Rust community in particular was the requirement for issues to be logged in Arrow’s Jira project. This step required would-be contributors to first register an account, and then receive a permissions grant to manage tickets.&lt;/p&gt;

&lt;p&gt;To streamline this process for new community members, we’ve taken the decision to migrate to GitHub Issues for tracking both new development work and known bugs that need addressing, and bootstrapped our new repositories by importing their respective tickets from Jira. Creating issues to track non-trivial proposed features and enhancements is still required; this creates an opportunity for community review and helps ensure that feedback is delivered as early in the process as possible. We hope that this strikes a better balance between organization and accessibility for prospective contributors.&lt;/p&gt;

&lt;h2 id=&quot;get-involved&quot;&gt;Get Involved&lt;/h2&gt;
&lt;p&gt;To further improve the onboarding flow for new Arrow contributors, we have started the process of labeling select issues as “good first issue” in &lt;a href=&quot;https://github.com/apache/arrow-rs/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22&quot;&gt;arrow-rs&lt;/a&gt; and &lt;a href=&quot;https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22&quot;&gt;arrow-datafusion&lt;/a&gt;. These issues are small in scope but still serve as valuable contributions to the project, and help new community members to get familiar with our development workflows and tools.&lt;/p&gt;

&lt;p&gt;Not quite sure where to start with a particular issue, or curious about the status of one of our projects? Join the Arrow &lt;a href=&quot;https://arrow.apache.org/community&quot;&gt;mailing lists&lt;/a&gt; or the #arrow-rust channel on the &lt;a href=&quot;https://s.apache.org/slack-invite&quot;&gt;ASF Slack&lt;/a&gt; server.&lt;/p&gt;

&lt;h2 id=&quot;in-closing&quot;&gt;In Closing&lt;/h2&gt;
&lt;p&gt;As a final note: nothing here is intended as prescriptive advice. As a community, we’ve decided that these processes are the best fit for the current status of our projects, but this may change over time! There is, after all, &lt;a href=&quot;https://en.wikipedia.org/wiki/No_Silver_Bullet&quot;&gt;no silver bullet&lt;/a&gt; for software engineering.&lt;/p&gt;</content><author><name>ruanpa</name></author><category term="application" /><summary type="html">The Apache Arrow Rust community is excited to announce that its migration to a new development workflow is now complete! If you’re considering Rust as a language for working with columnar data, read on and see how your use case might benefit from our new and improved project setup. In recent months, members of the community have been working closely with Arrow’s Project Management Committee and other contributors to expand the set of available workflows for Arrow implementations. The goal was to define a new development process that ultimately: Enables a faster release cadence that adheres to SemVer where appropriate Encourages maximum participation from the wider community with unified tooling Ensures that we continue to uphold the tenets of The Apache Way If you’re just here for the highlights, the major outcomes of these discussions are as follows: The Rust projects have moved to separate repositories, outside the main Arrow monorepo arrow-rs for the core Arrow, Arrow Flight, and Parquet implementations in Rust arrow-datafusion for DataFusion and Ballista (more on these projects below!) The Rust community will use GitHub Issues for tracking feature development and issues, replacing the Jira instance maintained by the Apache Software Foundation (ASF) DataFusion and Ballista will follow a new release cycle, independent of the main Arrow releases But why, as a community, have we decided to change our processes? Let’s take a slightly more in-depth look at the Rust implementation’s needs. Project Structure The Rust implementation of Arrow actually consists of several distinct projects, or in Rust parlance, “crates”. In addition to the core crates, namely arrow, arrow-flight, and parquet, we also maintain: DataFusion: an extensible in-memory query execution engine using Arrow as its format Ballista: a distributed compute platform, powered by Apache Arrow and DataFusion Whilst these projects are all closely related, with many shared contributors, they’re very much at different stages in their respective lifecycles. The core Arrow crate, as an implementation of a spec, has strict compatibility requirements with other versions of Arrow, and this is tested via rigorous cross-language integration tests. However, at the other end of the spectrum, DataFusion and Ballista are still nascent projects in their own right that undergo frequent backwards-incompatible changes. In the old workflow, DataFusion was released in lockstep with Arrow; because DataFusion users often need newly-contributed features or bugfixes on a tighter schedule than Arrow releases, we observed that many people in the community simply resorted to referencing our GitHub repository directly, rather than properly versioned builds on crates.io, Rust’s package registry. Ultimately, the decision was made to split the Rust crates into two separate repositories: arrow-rs for the core Arrow functionality, and arrow-datafusion for DataFusion and Ballista. There’s still work to be done on determining the exact release workflows for the latter, but this leaves us in a much better position to meet the broader Rust community’s expectations of crate versioning and stability. Community Participation All Apache projects are built on volunteer contribution; it’s a core principle of both the ASF and open-source software development more broadly. One point of friction that was observed in the previous workflow for the Rust community in particular was the requirement for issues to be logged in Arrow’s Jira project. This step required would-be contributors to first register an account, and then receive a permissions grant to manage tickets. To streamline this process for new community members, we’ve taken the decision to migrate to GitHub Issues for tracking both new development work and known bugs that need addressing, and bootstrapped our new repositories by importing their respective tickets from Jira. Creating issues to track non-trivial proposed features and enhancements is still required; this creates an opportunity for community review and helps ensure that feedback is delivered as early in the process as possible. We hope that this strikes a better balance between organization and accessibility for prospective contributors. Get Involved To further improve the onboarding flow for new Arrow contributors, we have started the process of labeling select issues as “good first issue” in arrow-rs and arrow-datafusion. These issues are small in scope but still serve as valuable contributions to the project, and help new community members to get familiar with our development workflows and tools. Not quite sure where to start with a particular issue, or curious about the status of one of our projects? Join the Arrow mailing lists or the #arrow-rust channel on the ASF Slack server. In Closing As a final note: nothing here is intended as prescriptive advice. As a community, we’ve decided that these processes are the best fit for the current status of our projects, but this may change over time! There is, after all, no silver bullet for software engineering.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 4.0.0 Release</title><link href="https://arrow.apache.org/blog/2021/05/03/4.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 4.0.0 Release" /><published>2021-05-03T02:00:00-04:00</published><updated>2021-05-03T02:00:00-04:00</updated><id>https://arrow.apache.org/blog/2021/05/03/4.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2021/05/03/4.0.0-release/">&lt;!--

--&gt;

&lt;p&gt;The Apache Arrow team is pleased to announce the 4.0.0 release. This covers
3 months of development work and includes &lt;a href=&quot;https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%204.0.0&quot;&gt;&lt;strong&gt;711 resolved issues&lt;/strong&gt;&lt;/a&gt;
from &lt;a href=&quot;/release/4.0.0.html#contributors&quot;&gt;&lt;strong&gt;114 distinct contributors&lt;/strong&gt;&lt;/a&gt;. See the Install Page to learn how to
get the libraries for your platform.&lt;/p&gt;

&lt;p&gt;The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the &lt;a href=&quot;/release/4.0.0.html&quot;&gt;complete changelog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;community&quot;&gt;Community&lt;/h2&gt;

&lt;p&gt;Since the 3.0.0 release, Yibo Cai, Ian Cook, and Jonathan Keane
have been invited as committers to Arrow,
and Andrew Lamb and Jorge Leitão have joined the Project Management Committee
(PMC). Thank you for all of your contributions!&lt;/p&gt;

&lt;h2 id=&quot;arrow-flight-rpc-notes&quot;&gt;Arrow Flight RPC notes&lt;/h2&gt;

&lt;p&gt;In Java, applications can now enable zero-copy optimizations when writing
data (ARROW-11066). This potentially breaks source compatibility, so it is
not enabled by default.&lt;/p&gt;

&lt;p&gt;Arrow Flight is now packaged for C#/.NET.&lt;/p&gt;

&lt;h2 id=&quot;linux-packages-notes&quot;&gt;Linux packages notes&lt;/h2&gt;

&lt;p&gt;There are Linux packages for C++ and C GLib. They were provided by Bintray
but &lt;a href=&quot;https://jfrog.com/blog/into-the-sunset-bintray-jcenter-gocenter-and-chartcenter/&quot;&gt;Bintray is no longer available as of 2021-05-01&lt;/a&gt;. They are provided
by Artifactory now. Users needs to change the install instructions because the URL
has changed. See &lt;a href=&quot;/install/&quot;&gt;the install page&lt;/a&gt; for new instructions. Here is a
summary of needed changes.&lt;/p&gt;

&lt;p&gt;For Debian GNU Linux and Ubuntu users:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Users need to change the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apache-arrow-archive-keyring&lt;/code&gt; install instruction:
    &lt;ul&gt;
      &lt;li&gt;Package name is changed to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apache-arrow-apt-source&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;Download URL is changed to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://apache.jfrog.io/artifactory/arrow/...&lt;/code&gt; from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://apache.bintray.com/arrow/...&lt;/code&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For CentOS and Red Hat Enterprise Linux users:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Users need to change the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apache-arrow-release&lt;/code&gt; install instruction:
    &lt;ul&gt;
      &lt;li&gt;Download URL is changed to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://apache.jfrog.io/artifactory/arrow/...&lt;/code&gt; from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://apache.bintray.com/arrow/...&lt;/code&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;c-notes&quot;&gt;C++ notes&lt;/h2&gt;

&lt;p&gt;The Arrow C++ library now includes a &lt;a href=&quot;https://github.com/apache/arrow/blob/master/cpp/vcpkg.json&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vcpkg.json&lt;/code&gt;&lt;/a&gt;
manifest file and a new CMake option &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-DARROW_DEPENDENCY_SOURCE=VCPKG&lt;/code&gt; to
simplify installation of dependencies using the &lt;a href=&quot;https://github.com/microsoft/vcpkg&quot;&gt;vcpkg&lt;/a&gt;
package manager. This provides an alternative means of installing C++ library
dependencies on Linux, macOS, and Windows. See the
&lt;a href=&quot;/docs/developers/cpp/building.html&quot;&gt;Building Arrow C++&lt;/a&gt;
and &lt;a href=&quot;/docs/developers/cpp/windows.html&quot;&gt;Developing on Windows&lt;/a&gt;
docs pages for details.&lt;/p&gt;

&lt;p&gt;The default memory allocator on macOS has been changed from jemalloc to mimalloc,
yielding performance benefits on a range of macro-benchmarks (ARROW-12316).&lt;/p&gt;

&lt;p&gt;Non-monotonic dense union offsets are now disallowed as per the Arrow format
specification, and return an error in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Array::ValidateFull&lt;/code&gt; (ARROW-10580).&lt;/p&gt;

&lt;h3 id=&quot;compute-layer&quot;&gt;Compute layer&lt;/h3&gt;

&lt;p&gt;Automatic implicit casting in compute kernels (ARROW-8919). For example, for
the addition of two arrays, the arrays are first cast to their common numeric
type instead of erroring when the types are not equal.&lt;/p&gt;

&lt;p&gt;Compute functions &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;quantile&lt;/code&gt; (ARROW-10831) and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;power&lt;/code&gt; (ARROW-11070) have been
added for numeric data.&lt;/p&gt;

&lt;p&gt;Compute functions for string processing have been added for:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Trimming characters (ARROW-9128).&lt;/li&gt;
  &lt;li&gt;Extracting substrings captured by a regex pattern (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;extract_regex&lt;/code&gt;, ARROW-10195).&lt;/li&gt;
  &lt;li&gt;Computing UTF8 string lengths (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;utf8_length&lt;/code&gt;, ARROW-11693).&lt;/li&gt;
  &lt;li&gt;Matching strings against regex pattern (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;match_substring_regex&lt;/code&gt;, ARROW-12134).&lt;/li&gt;
  &lt;li&gt;Replacing non-overlapping substrings that match a literal pattern or regular
expression (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;replace_substring&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;replace_substring_regex&lt;/code&gt;, ARROW-10306).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is now possible to sort decimal and fixed-width binary data (ARROW-11662).&lt;/p&gt;

&lt;p&gt;The precision of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sum&lt;/code&gt; kernel was improved (ARROW-11758).&lt;/p&gt;

&lt;h3 id=&quot;csv&quot;&gt;CSV&lt;/h3&gt;

&lt;p&gt;A CSV writer has been added (ARROW-2229).&lt;/p&gt;

&lt;p&gt;The CSV reader can now infer timestamp columns with fractional seconds (ARROW-12031).&lt;/p&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;Arrow Datasets received various performance improvements and new
features. Some highlights:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;New columns can be projected from arbitrary expressions at scan time
(ARROW-11174)&lt;/li&gt;
  &lt;li&gt;Read performance was improved for Parquet on high-latency
filesystems (ARROW-11601) and in general when there are thousands of
files or more (ARROW-8658)&lt;/li&gt;
  &lt;li&gt;Null partition keys can be written (ARROW-10438)&lt;/li&gt;
  &lt;li&gt;Compressed CSV files can be read (ARROW-10372)&lt;/li&gt;
  &lt;li&gt;Filesystems support async operations (ARROW-10846)&lt;/li&gt;
  &lt;li&gt;Usage and API documentation were added (ARROW-11677)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;files-and-filesystems&quot;&gt;Files and filesystems&lt;/h3&gt;

&lt;p&gt;Fixed some rare instances of GZip files could not be read properly (ARROW-12169).&lt;/p&gt;

&lt;p&gt;Support for setting S3 proxy parameters has been added (ARROW-8900).&lt;/p&gt;

&lt;p&gt;The HDFS filesystem is now able to write more than 2GB of data at a time
(ARROW-11391).&lt;/p&gt;

&lt;h3 id=&quot;ipc&quot;&gt;IPC&lt;/h3&gt;

&lt;p&gt;The IPC reader now supports reading data with dictionaries shared between
different schema fields (ARROW-11838).&lt;/p&gt;

&lt;p&gt;The IPC reader now supports optional endian conversion when receiving IPC
data represented with a different endianness. It is therefore possible to
exchange Arrow data between systems with different endiannesses (ARROW-8797).&lt;/p&gt;

&lt;p&gt;The IPC file writer now optionally unifies dictionaries when writing a
file in a single shot, instead of erroring out if unequal dictionaries are
encountered (ARROW-10406).&lt;/p&gt;

&lt;p&gt;An interoperability issue with the C# implementation was fixed (ARROW-12100).&lt;/p&gt;

&lt;h3 id=&quot;json&quot;&gt;JSON&lt;/h3&gt;

&lt;p&gt;A possible crash when reading a line-separated JSON file has been fixed (ARROW-12065).&lt;/p&gt;

&lt;h3 id=&quot;orc&quot;&gt;ORC&lt;/h3&gt;

&lt;p&gt;The Arrow C++ library now includes an ORC file writer. Hence it is possible
to both read and write ORC files from/to Arrow data.&lt;/p&gt;

&lt;h3 id=&quot;parquet&quot;&gt;Parquet&lt;/h3&gt;

&lt;p&gt;The Parquet C++ library version is now synced with the Arrow version (ARROW-7830).&lt;/p&gt;

&lt;p&gt;Parquet DECIMAL statistics were previously calculated incorrectly, this
has now been fixed (PARQUET-1655).&lt;/p&gt;

&lt;p&gt;Initial support for high-level Parquet encryption APIs similar to those
in parquet-mr is available (ARROW-9318).&lt;/p&gt;

&lt;h2 id=&quot;c-notes-1&quot;&gt;C# notes&lt;/h2&gt;

&lt;p&gt;Arrow Flight is now packaged for C#/.NET.&lt;/p&gt;

&lt;h2 id=&quot;go-notes&quot;&gt;Go notes&lt;/h2&gt;

&lt;p&gt;The go implementation now supports IPC buffer compression&lt;/p&gt;

&lt;h2 id=&quot;java-notes&quot;&gt;Java notes&lt;/h2&gt;

&lt;p&gt;Java now supports IPC buffer compression (ZSTD is recommended as the current performance of LZ4 is quite slow).&lt;/p&gt;

&lt;h2 id=&quot;javascript-notes&quot;&gt;JavaScript notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The Arrow JS module is now tree-shakeable.&lt;/li&gt;
  &lt;li&gt;Iterating over Tables or Vectors is ~2X faster. &lt;a href=&quot;https://observablehq.com/@domoritz/arrow-js-3-vs-4-iterator&quot;&gt;Demo&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The default bundles use modern JS.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;python-notes&quot;&gt;Python notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Limited support for writing out CSV files (only types that have cast implementation to String) is now available.&lt;/li&gt;
  &lt;li&gt;Writing parquet list types now has the option of enabling the canonical group naming according to the Parquet specification.&lt;/li&gt;
  &lt;li&gt;The ORC Writer is now available.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Creating a dataset with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pyarrow.dataset.write_dataset&lt;/code&gt; is now possible from a
Python iterator of record batches (ARROW-10882).
The Dataset interface can now use custom projections using expressions when
scanning (ARROW-11750). The expressions gained basic support for arithmetic
operations (e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ds.field(&apos;a&apos;) / ds.field(&apos;b&apos;)&lt;/code&gt;) (ARROW-12058). See
the &lt;a href=&quot;https://arrow.apache.org/docs/python/dataset.html#projecting-columns&quot;&gt;Dataset docs&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;See the C++ notes above for additional details.&lt;/p&gt;

&lt;h2 id=&quot;r-notes&quot;&gt;R notes&lt;/h2&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dplyr&lt;/code&gt; interface to Arrow data gained many new features in this release, including support for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mutate()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;relocate()&lt;/code&gt;, and more. You can also call in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;filter()&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mutate()&lt;/code&gt; over 100 functions supported by the Arrow C++ library, and many string functions are available both by their base R (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grepl()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsub()&lt;/code&gt;, etc.) and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stringr&lt;/code&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;str_detect()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;str_replace()&lt;/code&gt;) spellings.&lt;/p&gt;

&lt;p&gt;Datasets can now read compressed CSVs automatically, and you can also open a dataset that is based on a single file, enabling you to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;write_dataset()&lt;/code&gt; to partition a very large file without having to read the whole file into memory.&lt;/p&gt;

&lt;p&gt;For more on what’s in the 4.0.0 R package, see the &lt;a href=&quot;/docs/r/news/&quot;&gt;R changelog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;c-glib-and-ruby-notes&quot;&gt;C GLib and Ruby notes&lt;/h2&gt;

&lt;h3 id=&quot;c-glib&quot;&gt;C GLib&lt;/h3&gt;

&lt;p&gt;In Arrow GLib version 4.0.0, the following changes are introduced in addition to the changes by Arrow C++.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;gandiva-glib supports filtering by using the newly introduced &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GGandivaFilter&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GGandivaCondition&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GGandivaSelectableProjector&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;input&lt;/code&gt; property is added in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GArrowCSVReader&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GArrowJSONReader&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;GNU Autotools, namely &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;configure&lt;/code&gt; script, support is dropped&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GADScanContext&lt;/code&gt; is removed, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;use_threads&lt;/code&gt; property is moved to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GADScanOptions&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;garrow_chunked_array_combine&lt;/code&gt; function is added&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;garrow_array_concatenate&lt;/code&gt; function is added&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GADFragment&lt;/code&gt; and its subclass &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GADInMemoryFragment&lt;/code&gt; are added&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GADScanTask&lt;/code&gt; now holds the corresponding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GADFragment&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gad_scan_options_replace_schema&lt;/code&gt; function is removed&lt;/li&gt;
  &lt;li&gt;The name of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Decimal128DataType&lt;/code&gt; is changed to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;decimal128&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ruby&quot;&gt;Ruby&lt;/h3&gt;

&lt;p&gt;In Red Arrow version 4.0.0, the following changes are introduced in addition to the changes by Arrow GLib.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ArrowDataset::ScanContext&lt;/code&gt; is removed, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;use_threads&lt;/code&gt; attribute is moved to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ArrowDataset::ScanOptions&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Arrow::Array#concatenate&lt;/code&gt; is added; it can concatenate not only an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Arrow::Array&lt;/code&gt; but also a normal &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Array&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Arrow::SortKey&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Arrow::SortOptions&lt;/code&gt; are added for accepting Ruby objects as sort key and options&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ArrowDataset::InMemoryFragment&lt;/code&gt; is added&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;rust-notes&quot;&gt;Rust notes&lt;/h2&gt;

&lt;p&gt;This release of Arrow continues to add new features and performance improvements. Much of our time this release was spent hammering out the necessary details so we can release the Rust versions to cargo at a more regular rate. In addition, we welcomed the &lt;a href=&quot;https://ballistacompute.org/&quot;&gt;Ballista distributed compute project&lt;/a&gt; officially to the fold.&lt;/p&gt;

&lt;h3 id=&quot;arrow&quot;&gt;Arrow&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Improved LargeUtf8 support&lt;/li&gt;
  &lt;li&gt;Improved null handling in AND/OR kernels&lt;/li&gt;
  &lt;li&gt;Added JSON writer support (ARROW-11310)&lt;/li&gt;
  &lt;li&gt;JSON reader improvements&lt;/li&gt;
  &lt;li&gt;LargeUTF8
    &lt;ul&gt;
      &lt;li&gt;Improved schema inference for nested list and struct types&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Various performance improvements&lt;/li&gt;
  &lt;li&gt;IPC writer no longer calls finish() implicitly on drop&lt;/li&gt;
  &lt;li&gt;Compute kernels
    &lt;ul&gt;
      &lt;li&gt;Support for optional &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;limit&lt;/code&gt; in sort kernel&lt;/li&gt;
      &lt;li&gt;Divide by a single scalar&lt;/li&gt;
      &lt;li&gt;Support for casting to timestamps&lt;/li&gt;
      &lt;li&gt;Cast: Improved support between casting List, LargeList, Int32, Int64, Date64&lt;/li&gt;
      &lt;li&gt;Kernel to combine two arrays based on boolean mask&lt;/li&gt;
      &lt;li&gt;Pow kernel&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;new_null_array&lt;/code&gt; for creating Arrays full of nulls.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;parquet-1&quot;&gt;Parquet&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Added support for filtering row groups (used by DataFusion to implement filter push-down)&lt;/li&gt;
  &lt;li&gt;Added support for Parquet v 2.0 logical types&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;datafusion&quot;&gt;DataFusion&lt;/h3&gt;

&lt;p&gt;New Features&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SQL Support&lt;/li&gt;
  &lt;li&gt;
    &lt;ul&gt;
      &lt;li&gt;CTEs&lt;/li&gt;
      &lt;li&gt;UNION&lt;/li&gt;
      &lt;li&gt;HAVING&lt;/li&gt;
      &lt;li&gt;EXTRACT&lt;/li&gt;
      &lt;li&gt;SHOW TABLES&lt;/li&gt;
      &lt;li&gt;SHOW COLUMNS&lt;/li&gt;
      &lt;li&gt;INTERVAL&lt;/li&gt;
      &lt;li&gt;SQL Information schema&lt;/li&gt;
      &lt;li&gt;Support GROUP BY for more data types, including dictionary columns, boolean, Date32&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Extensibility API
    &lt;ul&gt;
      &lt;li&gt;Catalogs and schemas support&lt;/li&gt;
      &lt;li&gt;Table deregistration&lt;/li&gt;
      &lt;li&gt;Better support for multiple optimizers&lt;/li&gt;
      &lt;li&gt;User defined functions can now provide specialized implementations for scalar values&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Physical Plans&lt;/li&gt;
  &lt;li&gt;Hash Repartitioning&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SQL Metrics&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Additional Postgres compatible function library:
    &lt;ul&gt;
      &lt;li&gt;Length functions&lt;/li&gt;
      &lt;li&gt;Pad/trim functions&lt;/li&gt;
      &lt;li&gt;Concat functions&lt;/li&gt;
      &lt;li&gt;Ascii/Unicode functions&lt;/li&gt;
      &lt;li&gt;Regex&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Proper identifier case identification (e.g. “Foo” vs Foo vs foo)&lt;/li&gt;
  &lt;li&gt;Upgraded to Tokio 1.x&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Performance Improvements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;LIMIT pushdown&lt;/li&gt;
  &lt;li&gt;Constant folding&lt;/li&gt;
  &lt;li&gt;Partitioned hash join&lt;/li&gt;
  &lt;li&gt;Create hashes vectorized in hash join&lt;/li&gt;
  &lt;li&gt;Improve parallelism using repartitioning pass&lt;/li&gt;
  &lt;li&gt;Improved hash aggregate performance with large number of grouping values&lt;/li&gt;
  &lt;li&gt;Predicate pushdown support for table scans&lt;/li&gt;
  &lt;li&gt;Predicate push-down to parquet enables DataFusion to quickly eliminate entire parquet row-groups based on query filter expressions and parquet row group min/max statistics&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;API Changes&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;DataFrame methods now take &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Vec&amp;lt;Expr&amp;gt;&lt;/code&gt; rather than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;amp;[Expr]&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;TableProvider now consistently uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Arc&amp;lt;TableProvider&amp;gt;&lt;/code&gt; rather than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Box&amp;lt;TableProvider&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ballista&quot;&gt;Ballista&lt;/h3&gt;

&lt;p&gt;Ballista was donated shortly before the Arrow 4.0.0 release and there is no new release of Ballista as part of Arrow 4.0.0&lt;/p&gt;</content><author><name>pmc</name></author><category term="release" /><summary type="html">The Apache Arrow team is pleased to announce the 4.0.0 release. This covers 3 months of development work and includes 711 resolved issues from 114 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 3.0.0 release, Yibo Cai, Ian Cook, and Jonathan Keane have been invited as committers to Arrow, and Andrew Lamb and Jorge Leitão have joined the Project Management Committee (PMC). Thank you for all of your contributions! Arrow Flight RPC notes In Java, applications can now enable zero-copy optimizations when writing data (ARROW-11066). This potentially breaks source compatibility, so it is not enabled by default. Arrow Flight is now packaged for C#/.NET. Linux packages notes There are Linux packages for C++ and C GLib. They were provided by Bintray but Bintray is no longer available as of 2021-05-01. They are provided by Artifactory now. Users needs to change the install instructions because the URL has changed. See the install page for new instructions. Here is a summary of needed changes. For Debian GNU Linux and Ubuntu users: Users need to change the apache-arrow-archive-keyring install instruction: Package name is changed to apache-arrow-apt-source. Download URL is changed to https://apache.jfrog.io/artifactory/arrow/... from https://apache.bintray.com/arrow/.... For CentOS and Red Hat Enterprise Linux users: Users need to change the apache-arrow-release install instruction: Download URL is changed to https://apache.jfrog.io/artifactory/arrow/... from https://apache.bintray.com/arrow/.... C++ notes The Arrow C++ library now includes a vcpkg.json manifest file and a new CMake option -DARROW_DEPENDENCY_SOURCE=VCPKG to simplify installation of dependencies using the vcpkg package manager. This provides an alternative means of installing C++ library dependencies on Linux, macOS, and Windows. See the Building Arrow C++ and Developing on Windows docs pages for details. The default memory allocator on macOS has been changed from jemalloc to mimalloc, yielding performance benefits on a range of macro-benchmarks (ARROW-12316). Non-monotonic dense union offsets are now disallowed as per the Arrow format specification, and return an error in Array::ValidateFull (ARROW-10580). Compute layer Automatic implicit casting in compute kernels (ARROW-8919). For example, for the addition of two arrays, the arrays are first cast to their common numeric type instead of erroring when the types are not equal. Compute functions quantile (ARROW-10831) and power (ARROW-11070) have been added for numeric data. Compute functions for string processing have been added for: Trimming characters (ARROW-9128). Extracting substrings captured by a regex pattern (extract_regex, ARROW-10195). Computing UTF8 string lengths (utf8_length, ARROW-11693). Matching strings against regex pattern (match_substring_regex, ARROW-12134). Replacing non-overlapping substrings that match a literal pattern or regular expression (replace_substring and replace_substring_regex, ARROW-10306). It is now possible to sort decimal and fixed-width binary data (ARROW-11662). The precision of the sum kernel was improved (ARROW-11758). CSV A CSV writer has been added (ARROW-2229). The CSV reader can now infer timestamp columns with fractional seconds (ARROW-12031). Dataset Arrow Datasets received various performance improvements and new features. Some highlights: New columns can be projected from arbitrary expressions at scan time (ARROW-11174) Read performance was improved for Parquet on high-latency filesystems (ARROW-11601) and in general when there are thousands of files or more (ARROW-8658) Null partition keys can be written (ARROW-10438) Compressed CSV files can be read (ARROW-10372) Filesystems support async operations (ARROW-10846) Usage and API documentation were added (ARROW-11677) Files and filesystems Fixed some rare instances of GZip files could not be read properly (ARROW-12169). Support for setting S3 proxy parameters has been added (ARROW-8900). The HDFS filesystem is now able to write more than 2GB of data at a time (ARROW-11391). IPC The IPC reader now supports reading data with dictionaries shared between different schema fields (ARROW-11838). The IPC reader now supports optional endian conversion when receiving IPC data represented with a different endianness. It is therefore possible to exchange Arrow data between systems with different endiannesses (ARROW-8797). The IPC file writer now optionally unifies dictionaries when writing a file in a single shot, instead of erroring out if unequal dictionaries are encountered (ARROW-10406). An interoperability issue with the C# implementation was fixed (ARROW-12100). JSON A possible crash when reading a line-separated JSON file has been fixed (ARROW-12065). ORC The Arrow C++ library now includes an ORC file writer. Hence it is possible to both read and write ORC files from/to Arrow data. Parquet The Parquet C++ library version is now synced with the Arrow version (ARROW-7830). Parquet DECIMAL statistics were previously calculated incorrectly, this has now been fixed (PARQUET-1655). Initial support for high-level Parquet encryption APIs similar to those in parquet-mr is available (ARROW-9318). C# notes Arrow Flight is now packaged for C#/.NET. Go notes The go implementation now supports IPC buffer compression Java notes Java now supports IPC buffer compression (ZSTD is recommended as the current performance of LZ4 is quite slow). JavaScript notes The Arrow JS module is now tree-shakeable. Iterating over Tables or Vectors is ~2X faster. Demo The default bundles use modern JS. Python notes Limited support for writing out CSV files (only types that have cast implementation to String) is now available. Writing parquet list types now has the option of enabling the canonical group naming according to the Parquet specification. The ORC Writer is now available. Creating a dataset with pyarrow.dataset.write_dataset is now possible from a Python iterator of record batches (ARROW-10882). The Dataset interface can now use custom projections using expressions when scanning (ARROW-11750). The expressions gained basic support for arithmetic operations (e.g. ds.field(&apos;a&apos;) / ds.field(&apos;b&apos;)) (ARROW-12058). See the Dataset docs for more details. See the C++ notes above for additional details. R notes The dplyr interface to Arrow data gained many new features in this release, including support for mutate(), relocate(), and more. You can also call in filter() or mutate() over 100 functions supported by the Arrow C++ library, and many string functions are available both by their base R (grepl(), gsub(), etc.) and stringr (str_detect(), str_replace()) spellings. Datasets can now read compressed CSVs automatically, and you can also open a dataset that is based on a single file, enabling you to use write_dataset() to partition a very large file without having to read the whole file into memory. For more on what’s in the 4.0.0 R package, see the R changelog. C GLib and Ruby notes C GLib In Arrow GLib version 4.0.0, the following changes are introduced in addition to the changes by Arrow C++. gandiva-glib supports filtering by using the newly introduced GGandivaFilter, GGandivaCondition, and GGandivaSelectableProjector The input property is added in GArrowCSVReader and GArrowJSONReader GNU Autotools, namely configure script, support is dropped GADScanContext is removed, and use_threads property is moved to GADScanOptions garrow_chunked_array_combine function is added garrow_array_concatenate function is added GADFragment and its subclass GADInMemoryFragment are added GADScanTask now holds the corresponding GADFragment gad_scan_options_replace_schema function is removed The name of Decimal128DataType is changed to decimal128 Ruby In Red Arrow version 4.0.0, the following changes are introduced in addition to the changes by Arrow GLib. ArrowDataset::ScanContext is removed, and use_threads attribute is moved to ArrowDataset::ScanOptions Arrow::Array#concatenate is added; it can concatenate not only an Arrow::Array but also a normal Array Arrow::SortKey and Arrow::SortOptions are added for accepting Ruby objects as sort key and options ArrowDataset::InMemoryFragment is added Rust notes This release of Arrow continues to add new features and performance improvements. Much of our time this release was spent hammering out the necessary details so we can release the Rust versions to cargo at a more regular rate. In addition, we welcomed the Ballista distributed compute project officially to the fold. Arrow Improved LargeUtf8 support Improved null handling in AND/OR kernels Added JSON writer support (ARROW-11310) JSON reader improvements LargeUTF8 Improved schema inference for nested list and struct types Various performance improvements IPC writer no longer calls finish() implicitly on drop Compute kernels Support for optional limit in sort kernel Divide by a single scalar Support for casting to timestamps Cast: Improved support between casting List, LargeList, Int32, Int64, Date64 Kernel to combine two arrays based on boolean mask Pow kernel new_null_array for creating Arrays full of nulls. Parquet Added support for filtering row groups (used by DataFusion to implement filter push-down) Added support for Parquet v 2.0 logical types DataFusion New Features SQL Support CTEs UNION HAVING EXTRACT SHOW TABLES SHOW COLUMNS INTERVAL SQL Information schema Support GROUP BY for more data types, including dictionary columns, boolean, Date32 Extensibility API Catalogs and schemas support Table deregistration Better support for multiple optimizers User defined functions can now provide specialized implementations for scalar values Physical Plans Hash Repartitioning SQL Metrics Additional Postgres compatible function library: Length functions Pad/trim functions Concat functions Ascii/Unicode functions Regex Proper identifier case identification (e.g. “Foo” vs Foo vs foo) Upgraded to Tokio 1.x Performance Improvements: LIMIT pushdown Constant folding Partitioned hash join Create hashes vectorized in hash join Improve parallelism using repartitioning pass Improved hash aggregate performance with large number of grouping values Predicate pushdown support for table scans Predicate push-down to parquet enables DataFusion to quickly eliminate entire parquet row-groups based on query filter expressions and parquet row group min/max statistics API Changes DataFrame methods now take Vec&amp;lt;Expr&amp;gt; rather than &amp;amp;[Expr] TableProvider now consistently uses Arc&amp;lt;TableProvider&amp;gt; rather than Box&amp;lt;TableProvider&amp;gt; Ballista Ballista was donated shortly before the Arrow 4.0.0 release and there is no new release of Ballista as part of Arrow 4.0.0</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Ballista: A Distributed Scheduler for Apache Arrow</title><link href="https://arrow.apache.org/blog/2021/04/12/ballista-donation/" rel="alternate" type="text/html" title="Ballista: A Distributed Scheduler for Apache Arrow" /><published>2021-04-12T02:00:00-04:00</published><updated>2021-04-12T02:00:00-04:00</updated><id>https://arrow.apache.org/blog/2021/04/12/ballista-donation</id><content type="html" xml:base="https://arrow.apache.org/blog/2021/04/12/ballista-donation/">&lt;!--

--&gt;

&lt;p&gt;We are excited to announce that &lt;a href=&quot;https://github.com/apache/arrow-datafusion/tree/master/ballista&quot;&gt;Ballista&lt;/a&gt; has been donated 
to the Apache Arrow project.&lt;/p&gt;

&lt;p&gt;Ballista is a distributed compute platform primarily implemented in Rust, and powered by Apache Arrow. It is built
on an architecture that allows other programming languages (such as Python, C++, and Java) to be supported as
first-class citizens without paying a penalty for serialization costs.&lt;/p&gt;

&lt;p&gt;The foundational technologies in Ballista are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arrow.apache.org/&quot;&gt;Apache Arrow&lt;/a&gt; memory model and compute kernels for efficient processing of data.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/arrow-datafusion&quot;&gt;Apache Arrow DataFusion&lt;/a&gt; query planning and 
execution framework, extended by Ballista to provide distributed planning and execution.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arrow.apache.org/blog/2019/10/13/introducing-arrow-flight/&quot;&gt;Apache Arrow Flight Protocol&lt;/a&gt; for efficient
data transfer between processes.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developers.google.com/protocol-buffers&quot;&gt;Google Protocol Buffers&lt;/a&gt; for serializing query plans.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt; for packaging up executors along with user-defined code.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ballista can be deployed as a standalone cluster and also supports &lt;a href=&quot;https://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt;. In either
case, the scheduler can be configured to use &lt;a href=&quot;https://etcd.io/&quot;&gt;etcd&lt;/a&gt; as a backing store to (eventually) provide
redundancy in the case of a scheduler failing.&lt;/p&gt;

&lt;h2 id=&quot;status&quot;&gt;Status&lt;/h2&gt;

&lt;p&gt;The Ballista project is at an early stage of development. However, it is capable of running complex analytics queries 
in a distributed cluster with reasonable performance (comparable to more established distributed query frameworks).&lt;/p&gt;

&lt;p&gt;One of the benefits of Ballista being part of the Arrow codebase is that there is now an opportunity to push parts of 
the scheduler down to DataFusion so that is possible to seamlessly scale across cores in DataFusion, and across nodes 
in Ballista, using the same unified query scheduler.&lt;/p&gt;

&lt;h2 id=&quot;contributors-welcome&quot;&gt;Contributors Welcome!&lt;/h2&gt;

&lt;p&gt;If you are excited about being able to use Rust for distributed compute and ETL and would like to contribute to this 
work then there are many ways to get involved. The simplest way to get started is to try out Ballista against your own 
datasets and file bug reports for any issues that you find. You could also check out the current 
&lt;a href=&quot;https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20component%20%3D%20%22Rust%20-%20Ballista%22&quot;&gt;list of issues&lt;/a&gt; and have a go at fixing one.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/apache/arrow/blob/master/rust/README.md#arrow-rust-community&quot;&gt;Arrow Rust Community&lt;/a&gt;
section of the Rust README provides information on other ways to interact with the Ballista contributors and 
maintainers.&lt;/p&gt;</content><author><name>agrove</name></author><category term="application" /><summary type="html">We are excited to announce that Ballista has been donated to the Apache Arrow project. Ballista is a distributed compute platform primarily implemented in Rust, and powered by Apache Arrow. It is built on an architecture that allows other programming languages (such as Python, C++, and Java) to be supported as first-class citizens without paying a penalty for serialization costs. The foundational technologies in Ballista are: Apache Arrow memory model and compute kernels for efficient processing of data. Apache Arrow DataFusion query planning and execution framework, extended by Ballista to provide distributed planning and execution. Apache Arrow Flight Protocol for efficient data transfer between processes. Google Protocol Buffers for serializing query plans. Docker for packaging up executors along with user-defined code. Ballista can be deployed as a standalone cluster and also supports Kubernetes. In either case, the scheduler can be configured to use etcd as a backing store to (eventually) provide redundancy in the case of a scheduler failing. Status The Ballista project is at an early stage of development. However, it is capable of running complex analytics queries in a distributed cluster with reasonable performance (comparable to more established distributed query frameworks). One of the benefits of Ballista being part of the Arrow codebase is that there is now an opportunity to push parts of the scheduler down to DataFusion so that is possible to seamlessly scale across cores in DataFusion, and across nodes in Ballista, using the same unified query scheduler. Contributors Welcome! If you are excited about being able to use Rust for distributed compute and ETL and would like to contribute to this work then there are many ways to get involved. The simplest way to get started is to try out Ballista against your own datasets and file bug reports for any issues that you find. You could also check out the current list of issues and have a go at fixing one. The Arrow Rust Community section of the Rust README provides information on other ways to interact with the Ballista contributors and maintainers.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 3.0.0 Release</title><link href="https://arrow.apache.org/blog/2021/01/25/3.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 3.0.0 Release" /><published>2021-01-25T01:00:00-05:00</published><updated>2021-01-25T01:00:00-05:00</updated><id>https://arrow.apache.org/blog/2021/01/25/3.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2021/01/25/3.0.0-release/">&lt;!--

--&gt;

&lt;p&gt;The Apache Arrow team is pleased to announce the 3.0.0 release. This covers
over 3 months of development work and includes &lt;a href=&quot;https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%203.0.0&quot;&gt;&lt;strong&gt;666 resolved issues&lt;/strong&gt;&lt;/a&gt;
from &lt;a href=&quot;/release/3.0.0.html#contributors&quot;&gt;&lt;strong&gt;106 distinct contributors&lt;/strong&gt;&lt;/a&gt;. See the Install Page to learn how to
get the libraries for your platform.&lt;/p&gt;

&lt;p&gt;The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the &lt;a href=&quot;/release/3.0.0.html&quot;&gt;complete changelog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;columnar-format-notes&quot;&gt;Columnar Format Notes&lt;/h2&gt;

&lt;p&gt;The Decimal256 data type, which was already supported by the Arrow columnar
format specification, is now implemented in C++ and Java (ARROW-9747).&lt;/p&gt;

&lt;h2 id=&quot;arrow-flight-rpc-notes&quot;&gt;Arrow Flight RPC notes&lt;/h2&gt;

&lt;p&gt;Authentication in C++/Java/Python has been overhauled, allowing more flexible authentication methods and use of standard headers.
Support for cookies has also been added.
The C++/Java implementations are now more permissive when parsing messages in order to interoperate better with other Flight implementations.&lt;/p&gt;

&lt;p&gt;A basic Flight implementation for C#/.NET has been added.
See the &lt;a href=&quot;https://arrow.apache.org/docs/status.html#flight-rpc&quot;&gt;implementation status matrix&lt;/a&gt; for details.&lt;/p&gt;

&lt;h2 id=&quot;c-notes&quot;&gt;C++ notes&lt;/h2&gt;

&lt;p&gt;The default memory pool can now be changed at runtime using the environment
variable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ARROW_DEFAULT_MEMORY_POOL&lt;/code&gt; (ARROW-11009).  The environment variable
is inspected at process startup.  This is useful when trying to diagnose memory
consumption issues with Arrow.&lt;/p&gt;

&lt;p&gt;STL-like iterators are now provided over concrete arrays. Those are useful for
non-performance critical tasks, for example testing (ARROW-10776).&lt;/p&gt;

&lt;p&gt;It is now possible to concatenate dictionary arrays with unequal dictionaries.
The dictionaries are unified when concatenating, for supported data types
(ARROW-5336).&lt;/p&gt;

&lt;p&gt;Threads in a thread pool are now spawned lazily as needed for enqueued
tasks, up to the configured capacity. They used to be spawned upfront on
creation of the thread pool (ARROW-10038).&lt;/p&gt;

&lt;h3 id=&quot;compute-layer&quot;&gt;Compute layer&lt;/h3&gt;

&lt;p&gt;Comprehensive documentation for compute functions is now available:
https://arrow.apache.org/docs/cpp/compute.html&lt;/p&gt;

&lt;p&gt;Compute functions for string processing have been added for:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;splitting on whitespace (ASCII and Unicode flavors) and splitting on a
pattern (ARROW-9991);&lt;/li&gt;
  &lt;li&gt;trimming characters (ARROW-9128).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Behavior of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index_in&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_in&lt;/code&gt; compute functions with nulls has been
changed for consistency (ARROW-10663).&lt;/p&gt;

&lt;p&gt;Multiple-column sort kernels are now available for tables and record batches
(ARROW-8199, ARROW-10796, ARROW-10790).&lt;/p&gt;

&lt;p&gt;Performance of table filtering has been vastly improved (ARROW-10569).&lt;/p&gt;

&lt;p&gt;Scalar arguments are now accepted for more compute functions.&lt;/p&gt;

&lt;p&gt;Compute functions &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;quantile&lt;/code&gt; (ARROW-10831) and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_nan&lt;/code&gt; (ARROW-11043) have been
added for numeric data.&lt;/p&gt;

&lt;p&gt;Aggregation functions &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;any&lt;/code&gt; (ARROW-1846) and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;all&lt;/code&gt; (ARROW-10301) have been
added for boolean data.&lt;/p&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Expression&lt;/code&gt; hierarchy has simplified to a wrapper around literals, field references,
or calls to named functions. This enables usage of any compute function while filtering
with no boilerplate.&lt;/p&gt;

&lt;p&gt;Parquet statistics are lazily parsed in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ParquetDatasetFactory&lt;/code&gt; and
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ParquetFileFragment&lt;/code&gt; for shorter construction time.&lt;/p&gt;

&lt;h3 id=&quot;csv&quot;&gt;CSV&lt;/h3&gt;

&lt;p&gt;Conversion of string columns is now faster thanks to faster UTF-8 validation
of small strings (ARROW-10313).&lt;/p&gt;

&lt;p&gt;Conversion of floating-point columns is now faster thanks to optimized
string-to-double conversion routines (ARROW-10328).&lt;/p&gt;

&lt;p&gt;Parsing of ISO8601 timestamps is now more liberal: trailing zeros can
be omitted in the fractional part (ARROW-10337).&lt;/p&gt;

&lt;p&gt;Fixed a bug where null detection could give the wrong results on some platforms
(ARROW-11067).&lt;/p&gt;

&lt;p&gt;Added type inference for Date32 columns for values in the form &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YYYY-MM-DD&lt;/code&gt;
(ARROW-11247).&lt;/p&gt;

&lt;h3 id=&quot;feather&quot;&gt;Feather&lt;/h3&gt;

&lt;p&gt;Fixed reading of compressed Feather files written with Arrow 0.17 (ARROW-11163).&lt;/p&gt;

&lt;h3 id=&quot;filesystem-layer&quot;&gt;Filesystem layer&lt;/h3&gt;

&lt;p&gt;S3 recursive tree walks now benefit from a parallel implementation, where reads
of multiple child directories are now issued concurrently (ARROW-10788).&lt;/p&gt;

&lt;p&gt;Improved empty directory detection to be mindful of differences between Amazon
and Minio S3 implementations (ARROW-10942).&lt;/p&gt;

&lt;h3 id=&quot;flight-rpc&quot;&gt;Flight RPC&lt;/h3&gt;

&lt;p&gt;IPv6 host addresses are now supported (ARROW-10475).&lt;/p&gt;

&lt;h3 id=&quot;ipc&quot;&gt;IPC&lt;/h3&gt;

&lt;p&gt;It is now possible to emit dictionary deltas where possible using the IPC
stream writer. This is governed by a new variable in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IpcWriteOptions&lt;/code&gt; class
(ARROW-6883).&lt;/p&gt;

&lt;p&gt;It is now possible to read wider tables, which used to fail due to reaching a
limit during Flatbuffers verification (ARROW-10056).&lt;/p&gt;

&lt;h3 id=&quot;parquet&quot;&gt;Parquet&lt;/h3&gt;

&lt;p&gt;Fixed reading of LZ4-compressed Parquet columns emitted by the Java Parquet
implementation (ARROW-11301).&lt;/p&gt;

&lt;p&gt;Fixed a bug where writing multiple batches of nullable nested strings to Parquet
would not write any data in batches after the first one (ARROW-10493)&lt;/p&gt;

&lt;p&gt;The Decimal256 data type can be read from and written to Parquet (ARROW-10607).&lt;/p&gt;

&lt;p&gt;LargeString and LargeBinary data can now be written to Parquet (ARROW-10426).&lt;/p&gt;

&lt;h2 id=&quot;c-notes-1&quot;&gt;C# notes&lt;/h2&gt;

&lt;p&gt;The .NET package added initial support for Arrow Flight clients and servers. Support is enabled through two new NuGet packages &lt;a href=&quot;https://www.nuget.org/packages/Apache.Arrow.Flight/&quot;&gt;Apache.Arrow.Flight&lt;/a&gt; (client) and &lt;a href=&quot;https://www.nuget.org/packages/Apache.Arrow.Flight.AspNetCore/&quot;&gt;Apache.Arrow.Flight.AspNetCore&lt;/a&gt; (server).&lt;/p&gt;

&lt;p&gt;Also fixed an issue where ArrowStreamWriter wasn’t writing schema metadata to Arrow streams.&lt;/p&gt;

&lt;h2 id=&quot;julia-notes&quot;&gt;Julia notes&lt;/h2&gt;

&lt;p&gt;This is the first release to officially include
&lt;a href=&quot;https://github.com/apache/arrow/tree/master/julia/Arrow&quot;&gt;an implementation&lt;/a&gt;
for the Julia language. The pure Julia implementation includes support
for &lt;a href=&quot;https://arrow.apache.org/docs/status.html&quot;&gt;wide coverage of the format specification&lt;/a&gt;.
Additional details can be found in the
&lt;a href=&quot;https://julialang.org/blog/2021/01/arrow/&quot;&gt;julialang.org blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;python-notes&quot;&gt;Python notes&lt;/h2&gt;

&lt;p&gt;Support for Python 3.9 was added (ARROW-10224), and support for Python 3.5
was removed (ARROW-5679).&lt;/p&gt;

&lt;p&gt;Support for building manylinux1 packages has been removed (ARROW-11212).
PyArrow continues to be available as manylinux2010 and manylinux2014 wheels.&lt;/p&gt;

&lt;p&gt;The minimal required version for NumPy is now 1.16.6. Note that when upgrading
NumPy to 1.20, you also need to upgrade pyarrow to 3.0.0 to ensure compatibility,
as this pyarrow release fixed a compatibility issue with NumPy 1.20 (ARROW-10833).&lt;/p&gt;

&lt;p&gt;Compute functions are now automatically exported from C++ to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pyarrow.compute&lt;/code&gt;
module, and they have docstrings matching their C++ definition.&lt;/p&gt;

&lt;p&gt;An &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;iter_batches()&lt;/code&gt; method is now available for reading a Parquet file iteratively
(ARROW-7800).&lt;/p&gt;

&lt;p&gt;Alternate memory pools (such as mimalloc, jemalloc or the C malloc-based memory
pool) are now available from Python (ARROW-11049).&lt;/p&gt;

&lt;p&gt;Fixed a potential deadlock when importing pandas from several threads (ARROW-10519).&lt;/p&gt;

&lt;p&gt;See the C++ notes above for additional details.&lt;/p&gt;

&lt;h2 id=&quot;r-notes&quot;&gt;R notes&lt;/h2&gt;

&lt;p&gt;This release contains new features for the Flight RPC wrapper, better
support for saving R metadata (including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sf&lt;/code&gt; spatial data) to Feather and
Parquet files, several significant improvements to speed and memory management,
and many other enhancements.&lt;/p&gt;

&lt;p&gt;For more on what’s in the 3.0.0 R package, see the &lt;a href=&quot;/docs/r/news/&quot;&gt;R changelog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;ruby-and-c-glib-notes&quot;&gt;Ruby and C GLib notes&lt;/h2&gt;

&lt;h3 id=&quot;ruby&quot;&gt;Ruby&lt;/h3&gt;

&lt;p&gt;In Ruby binding, 256-bit decimal support and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Arrow::FixedBinaryArrayBuilder&lt;/code&gt; are added likewise C GLib below.&lt;/p&gt;

&lt;h3 id=&quot;c-glib&quot;&gt;C GLib&lt;/h3&gt;

&lt;p&gt;In the version 3.0.0 of C GLib consists of many new features.&lt;/p&gt;

&lt;p&gt;A chunked array, a record batch, and a table support &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sort_indices&lt;/code&gt; function as well as an array.
These functions including array’s support to specify sorting option.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;garrow_array_sort_to_indices&lt;/code&gt; has been renamed to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;garrow_array_sort_indices&lt;/code&gt; and the previous name has been deprecated.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GArrowField&lt;/code&gt; supports functions to handle metadata.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GArrowSchema&lt;/code&gt; supports &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;garrow_schema_has_metadata()&lt;/code&gt; function.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GArrowArrayBuilder&lt;/code&gt; supports to add single null, multiple nulls, single empty value, and multiple empty values.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GArrowFixedSizedBinaryArrayBuilder&lt;/code&gt; is newly supported.&lt;/p&gt;

&lt;p&gt;256-bit decimal and extension types are newly supported.
Filesystem module supports Mock, HDFS, S3 file systems.
Dataset module supports CSV, IPC, and Parquet file formats.&lt;/p&gt;

&lt;h2 id=&quot;rust-notes&quot;&gt;Rust notes&lt;/h2&gt;
&lt;h3 id=&quot;core-arrow-crate&quot;&gt;Core Arrow Crate&lt;/h3&gt;

&lt;p&gt;The development of the arrow crate was focused on four main aspects:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Make the crate usable in stable Rust&lt;/li&gt;
  &lt;li&gt;Bug fixing and removal of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsafe&lt;/code&gt; code&lt;/li&gt;
  &lt;li&gt;Extend functionality to keep up with the specification&lt;/li&gt;
  &lt;li&gt;Increase performance of existing kernels&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;stable-rust&quot;&gt;Stable Rust&lt;/h4&gt;

&lt;p&gt;Possibly the biggest news for this release is that all project crates, including arrow, parquet, and datafusion now
build with stable Rust by default. Nightly / unstable Rust is still required when enabling the SIMD feature.&lt;/p&gt;

&lt;h4 id=&quot;parquet-arrow-writer&quot;&gt;Parquet Arrow writer&lt;/h4&gt;

&lt;p&gt;The Parquet Writer for Arrow arrays is now available, allowing the Rust programs to easily read and write Parquet
files and making it easier to integrate with the overall Arrow ecosystem. The reader and writer include both basic
and nested type support (List, Dictionary, Struct)&lt;/p&gt;

&lt;h4 id=&quot;first-class-arrow-flight-ipc-support&quot;&gt;First Class Arrow Flight IPC Support&lt;/h4&gt;

&lt;p&gt;This release the Arrow Flight IPC implementation in Rust became fully-featured enough to participate in the regular
cross-language integration tests, thus ensuring Rust applications written using Arrow can interoperate with the rest
of the ecosystem&lt;/p&gt;

&lt;h4 id=&quot;performance&quot;&gt;Performance&lt;/h4&gt;

&lt;p&gt;There have been numerous performance improvements in this release across the board. This includes both kernel
operations, such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;take&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;filter&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cast&lt;/code&gt;, as well as more fundamental parts such as bitwise comparison
and reading and writing to CSV.&lt;/p&gt;

&lt;h4 id=&quot;increased-data-type-support&quot;&gt;Increased Data Type Support&lt;/h4&gt;

&lt;p&gt;New DataTypes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Decimal data type for fixed-width decimal values&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Improved operation support for nested structures Dictionary, and Lists (filter, take, etc)&lt;/p&gt;

&lt;h4 id=&quot;other-improvements&quot;&gt;Other improvements:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Added support for Date and time on FFI&lt;/li&gt;
  &lt;li&gt;Added support for Binary type on FFI&lt;/li&gt;
  &lt;li&gt;Added support for i64 sized arrays to “take” kernel&lt;/li&gt;
  &lt;li&gt;Support for the i128 Decimal Type&lt;/li&gt;
  &lt;li&gt;Added support to cast string to date&lt;/li&gt;
  &lt;li&gt;Added API to create arrays out of existing arrays (e.g. for join, merge-sort, concatenate)&lt;/li&gt;
  &lt;li&gt;The simd feature is now also available on aarch64&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;api-changes&quot;&gt;API Changes&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;BooleanArray is no longer a PrimitiveArray&lt;/li&gt;
  &lt;li&gt;ArrowNativeType no longer includes bool since arrows boolean type is represented using bitpacking&lt;/li&gt;
  &lt;li&gt;Several Buffer methods are now infallible instead of returning a Result&lt;/li&gt;
  &lt;li&gt;DataType::List now contains a Field to track metadata about the contained elements&lt;/li&gt;
  &lt;li&gt;PrimitiveArray::raw_values, values_slice and values methods got replaced by a values method returning a slice&lt;/li&gt;
  &lt;li&gt;Buffer::data and raw_data were renamed to as_slice and as_ptr&lt;/li&gt;
  &lt;li&gt;MutableBuffer::data_mut and freeze were renamed to as_slice_mut and into to be more consistent with the stdlib naming conventions&lt;/li&gt;
  &lt;li&gt;The generic type parameter for BufferBuilder was changed from ArrowPrimitiveType to ArrowNativeType&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;datafusion&quot;&gt;DataFusion&lt;/h3&gt;

&lt;h4 id=&quot;sql&quot;&gt;SQL&lt;/h4&gt;

&lt;p&gt;In this release, we clarified that DataFusion will standardize on the PostgreSQL SQL dialect.&lt;/p&gt;

&lt;p&gt;New SQL support:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;JOIN, LEFT JOIN, RIGHT JOIN&lt;/li&gt;
  &lt;li&gt;COUNT DISTINCT&lt;/li&gt;
  &lt;li&gt;CASE WHEN&lt;/li&gt;
  &lt;li&gt;USING&lt;/li&gt;
  &lt;li&gt;BETWEEN&lt;/li&gt;
  &lt;li&gt;IS IN&lt;/li&gt;
  &lt;li&gt;Nested SELECT statements&lt;/li&gt;
  &lt;li&gt;Nested expressions in aggregations&lt;/li&gt;
  &lt;li&gt;LOWER(), UPPER(), TRIM()&lt;/li&gt;
  &lt;li&gt;NULLIF()&lt;/li&gt;
  &lt;li&gt;SHA224(), SHA256(), SHA384(), SHA512()&lt;/li&gt;
  &lt;li&gt;DATE_TRUNC()&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;performance-1&quot;&gt;Performance&lt;/h4&gt;

&lt;p&gt;There have been numerous performance improvements in this release:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Optimizations for JOINs such as using vectorized hashing.&lt;/li&gt;
  &lt;li&gt;We started with adding statistics and cost-based optimizations. We choose the smaller side of a join as the build
side if possible.&lt;/li&gt;
  &lt;li&gt;Improved parallelism when reading partitioned Parquet data sources&lt;/li&gt;
  &lt;li&gt;Concurrent writes of CSV and Parquet partitions to file&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;parquet-crate&quot;&gt;Parquet Crate&lt;/h3&gt;

&lt;p&gt;The Parquet has the following improvements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Nested reading&lt;/li&gt;
  &lt;li&gt;Support to write booleans&lt;/li&gt;
  &lt;li&gt;Add support to write temporal types&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;roadmap-for-400&quot;&gt;Roadmap for 4.0.0&lt;/h3&gt;

&lt;p&gt;We have also started building up a shared community roadmap for 4.0: &lt;a href=&quot;https://docs.google.com/document/d/1qspsOM_dknOxJKdGvKbC1aoVoO0M3i6x1CIo58mmN2Y/edit#heading=h.kstb571j5g5j&quot;&gt;Apache Arrow: Crowd Sourced Rust Roadmap for
Arrow 4.0, January 2021&lt;/a&gt;.&lt;/p&gt;</content><author><name>pmc</name></author><category term="release" /><summary type="html">The Apache Arrow team is pleased to announce the 3.0.0 release. This covers over 3 months of development work and includes 666 resolved issues from 106 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Columnar Format Notes The Decimal256 data type, which was already supported by the Arrow columnar format specification, is now implemented in C++ and Java (ARROW-9747). Arrow Flight RPC notes Authentication in C++/Java/Python has been overhauled, allowing more flexible authentication methods and use of standard headers. Support for cookies has also been added. The C++/Java implementations are now more permissive when parsing messages in order to interoperate better with other Flight implementations. A basic Flight implementation for C#/.NET has been added. See the implementation status matrix for details. C++ notes The default memory pool can now be changed at runtime using the environment variable ARROW_DEFAULT_MEMORY_POOL (ARROW-11009). The environment variable is inspected at process startup. This is useful when trying to diagnose memory consumption issues with Arrow. STL-like iterators are now provided over concrete arrays. Those are useful for non-performance critical tasks, for example testing (ARROW-10776). It is now possible to concatenate dictionary arrays with unequal dictionaries. The dictionaries are unified when concatenating, for supported data types (ARROW-5336). Threads in a thread pool are now spawned lazily as needed for enqueued tasks, up to the configured capacity. They used to be spawned upfront on creation of the thread pool (ARROW-10038). Compute layer Comprehensive documentation for compute functions is now available: https://arrow.apache.org/docs/cpp/compute.html Compute functions for string processing have been added for: splitting on whitespace (ASCII and Unicode flavors) and splitting on a pattern (ARROW-9991); trimming characters (ARROW-9128). Behavior of the index_in and is_in compute functions with nulls has been changed for consistency (ARROW-10663). Multiple-column sort kernels are now available for tables and record batches (ARROW-8199, ARROW-10796, ARROW-10790). Performance of table filtering has been vastly improved (ARROW-10569). Scalar arguments are now accepted for more compute functions. Compute functions quantile (ARROW-10831) and is_nan (ARROW-11043) have been added for numeric data. Aggregation functions any (ARROW-1846) and all (ARROW-10301) have been added for boolean data. Dataset The Expression hierarchy has simplified to a wrapper around literals, field references, or calls to named functions. This enables usage of any compute function while filtering with no boilerplate. Parquet statistics are lazily parsed in ParquetDatasetFactory and ParquetFileFragment for shorter construction time. CSV Conversion of string columns is now faster thanks to faster UTF-8 validation of small strings (ARROW-10313). Conversion of floating-point columns is now faster thanks to optimized string-to-double conversion routines (ARROW-10328). Parsing of ISO8601 timestamps is now more liberal: trailing zeros can be omitted in the fractional part (ARROW-10337). Fixed a bug where null detection could give the wrong results on some platforms (ARROW-11067). Added type inference for Date32 columns for values in the form YYYY-MM-DD (ARROW-11247). Feather Fixed reading of compressed Feather files written with Arrow 0.17 (ARROW-11163). Filesystem layer S3 recursive tree walks now benefit from a parallel implementation, where reads of multiple child directories are now issued concurrently (ARROW-10788). Improved empty directory detection to be mindful of differences between Amazon and Minio S3 implementations (ARROW-10942). Flight RPC IPv6 host addresses are now supported (ARROW-10475). IPC It is now possible to emit dictionary deltas where possible using the IPC stream writer. This is governed by a new variable in the IpcWriteOptions class (ARROW-6883). It is now possible to read wider tables, which used to fail due to reaching a limit during Flatbuffers verification (ARROW-10056). Parquet Fixed reading of LZ4-compressed Parquet columns emitted by the Java Parquet implementation (ARROW-11301). Fixed a bug where writing multiple batches of nullable nested strings to Parquet would not write any data in batches after the first one (ARROW-10493) The Decimal256 data type can be read from and written to Parquet (ARROW-10607). LargeString and LargeBinary data can now be written to Parquet (ARROW-10426). C# notes The .NET package added initial support for Arrow Flight clients and servers. Support is enabled through two new NuGet packages Apache.Arrow.Flight (client) and Apache.Arrow.Flight.AspNetCore (server). Also fixed an issue where ArrowStreamWriter wasn’t writing schema metadata to Arrow streams. Julia notes This is the first release to officially include an implementation for the Julia language. The pure Julia implementation includes support for wide coverage of the format specification. Additional details can be found in the julialang.org blog post. Python notes Support for Python 3.9 was added (ARROW-10224), and support for Python 3.5 was removed (ARROW-5679). Support for building manylinux1 packages has been removed (ARROW-11212). PyArrow continues to be available as manylinux2010 and manylinux2014 wheels. The minimal required version for NumPy is now 1.16.6. Note that when upgrading NumPy to 1.20, you also need to upgrade pyarrow to 3.0.0 to ensure compatibility, as this pyarrow release fixed a compatibility issue with NumPy 1.20 (ARROW-10833). Compute functions are now automatically exported from C++ to the pyarrow.compute module, and they have docstrings matching their C++ definition. An iter_batches() method is now available for reading a Parquet file iteratively (ARROW-7800). Alternate memory pools (such as mimalloc, jemalloc or the C malloc-based memory pool) are now available from Python (ARROW-11049). Fixed a potential deadlock when importing pandas from several threads (ARROW-10519). See the C++ notes above for additional details. R notes This release contains new features for the Flight RPC wrapper, better support for saving R metadata (including sf spatial data) to Feather and Parquet files, several significant improvements to speed and memory management, and many other enhancements. For more on what’s in the 3.0.0 R package, see the R changelog. Ruby and C GLib notes Ruby In Ruby binding, 256-bit decimal support and Arrow::FixedBinaryArrayBuilder are added likewise C GLib below. C GLib In the version 3.0.0 of C GLib consists of many new features. A chunked array, a record batch, and a table support sort_indices function as well as an array. These functions including array’s support to specify sorting option. garrow_array_sort_to_indices has been renamed to garrow_array_sort_indices and the previous name has been deprecated. GArrowField supports functions to handle metadata. GArrowSchema supports garrow_schema_has_metadata() function. GArrowArrayBuilder supports to add single null, multiple nulls, single empty value, and multiple empty values. GArrowFixedSizedBinaryArrayBuilder is newly supported. 256-bit decimal and extension types are newly supported. Filesystem module supports Mock, HDFS, S3 file systems. Dataset module supports CSV, IPC, and Parquet file formats. Rust notes Core Arrow Crate The development of the arrow crate was focused on four main aspects: Make the crate usable in stable Rust Bug fixing and removal of unsafe code Extend functionality to keep up with the specification Increase performance of existing kernels Stable Rust Possibly the biggest news for this release is that all project crates, including arrow, parquet, and datafusion now build with stable Rust by default. Nightly / unstable Rust is still required when enabling the SIMD feature. Parquet Arrow writer The Parquet Writer for Arrow arrays is now available, allowing the Rust programs to easily read and write Parquet files and making it easier to integrate with the overall Arrow ecosystem. The reader and writer include both basic and nested type support (List, Dictionary, Struct) First Class Arrow Flight IPC Support This release the Arrow Flight IPC implementation in Rust became fully-featured enough to participate in the regular cross-language integration tests, thus ensuring Rust applications written using Arrow can interoperate with the rest of the ecosystem Performance There have been numerous performance improvements in this release across the board. This includes both kernel operations, such as take, filter, and cast, as well as more fundamental parts such as bitwise comparison and reading and writing to CSV. Increased Data Type Support New DataTypes: Decimal data type for fixed-width decimal values Improved operation support for nested structures Dictionary, and Lists (filter, take, etc) Other improvements: Added support for Date and time on FFI Added support for Binary type on FFI Added support for i64 sized arrays to “take” kernel Support for the i128 Decimal Type Added support to cast string to date Added API to create arrays out of existing arrays (e.g. for join, merge-sort, concatenate) The simd feature is now also available on aarch64 API Changes BooleanArray is no longer a PrimitiveArray ArrowNativeType no longer includes bool since arrows boolean type is represented using bitpacking Several Buffer methods are now infallible instead of returning a Result DataType::List now contains a Field to track metadata about the contained elements PrimitiveArray::raw_values, values_slice and values methods got replaced by a values method returning a slice Buffer::data and raw_data were renamed to as_slice and as_ptr MutableBuffer::data_mut and freeze were renamed to as_slice_mut and into to be more consistent with the stdlib naming conventions The generic type parameter for BufferBuilder was changed from ArrowPrimitiveType to ArrowNativeType DataFusion SQL In this release, we clarified that DataFusion will standardize on the PostgreSQL SQL dialect. New SQL support: JOIN, LEFT JOIN, RIGHT JOIN COUNT DISTINCT CASE WHEN USING BETWEEN IS IN Nested SELECT statements Nested expressions in aggregations LOWER(), UPPER(), TRIM() NULLIF() SHA224(), SHA256(), SHA384(), SHA512() DATE_TRUNC() Performance There have been numerous performance improvements in this release: Optimizations for JOINs such as using vectorized hashing. We started with adding statistics and cost-based optimizations. We choose the smaller side of a join as the build side if possible. Improved parallelism when reading partitioned Parquet data sources Concurrent writes of CSV and Parquet partitions to file Parquet Crate The Parquet has the following improvements: Nested reading Support to write booleans Add support to write temporal types Roadmap for 4.0.0 We have also started building up a shared community roadmap for 4.0: Apache Arrow: Crowd Sourced Rust Roadmap for Arrow 4.0, January 2021.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 2.0.0 Rust Highlights</title><link href="https://arrow.apache.org/blog/2020/10/27/rust-2.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 2.0.0 Rust Highlights" /><published>2020-10-27T02:00:00-04:00</published><updated>2020-10-27T02:00:00-04:00</updated><id>https://arrow.apache.org/blog/2020/10/27/rust-2.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2020/10/27/rust-2.0.0-release/">&lt;!--

--&gt;

&lt;p&gt;Apache Arrow 2.0.0 is a significant release for the Apache Arrow project in general (&lt;a href=&quot;https://arrow.apache.org/blog/2020/10/22/2.0.0-release/&quot;&gt;release notes&lt;/a&gt;), and the Rust subproject
in particular, with almost 200 issues resolved by 15 contributors. In this blog post, we will go through the main changes 
affecting core Arrow, Parquet support, and DataFusion query engine. The full list of resolved issues can be found 
&lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-10295?jql=project%20%3D%20ARROW%20AND%20status%20not%20in%20%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)%20and%20fixVersion%20%3D%202.0.0%20AND%20text%20~%20rust%20ORDER%20BY%20created%20DESC&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While the Java and C/C++ (used by Python and R) Arrow implementations likely remain the most feature-rich, with the 
2.0.0 release, the Rust implementation is closing the feature gap quickly. Here are some of the highlights for this 
release.&lt;/p&gt;

&lt;h1 id=&quot;core-arrow-crate&quot;&gt;Core Arrow Crate&lt;/h1&gt;

&lt;h2 id=&quot;iterator-trait&quot;&gt;Iterator Trait&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Primitive arrays (e.g., array of integers) can now be converted to and initialized from an iterator. This exposes a 
very popular API - iterators - to arrow arrays. Work for other types will continue throughout 3.0.0.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;improved-variable-sized-arrays&quot;&gt;Improved Variable-sized Arrays&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Variable sized arrays (e.g., array of strings) have been internally refactored to more easily support their larger (64-bit 
size offset) version. This allowed us to generalize some of the kernels to both (32 and 64) versions and 
perform type checks when building them.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;kernels&quot;&gt;Kernels&lt;/h2&gt;

&lt;p&gt;There have been numerous improvements in the Arrow compute kernels, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;New kernels have been added for string operations, including substring, min, max, concat, and length.&lt;/li&gt;
  &lt;li&gt;Aggregate sum is now implemented for SIMD with a 5x improvement over the non-SIMD operation&lt;/li&gt;
  &lt;li&gt;Many kernels have been improved to support dictionary-encoded arrays&lt;/li&gt;
  &lt;li&gt;Some kernels were optimized for arrays without nulls, making them significantly faster in that case.&lt;/li&gt;
  &lt;li&gt;Many kernels were optimized in the number of memory copies that are needed to apply them and also on their 
implementation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;other-improvements&quot;&gt;Other Improvements&lt;/h2&gt;

&lt;p&gt;The Array trait now has &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_buffer_memory_size&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_array_memory_size&lt;/code&gt; methods for determining the amount of 
memory allocated for the array.&lt;/p&gt;

&lt;h1 id=&quot;parquet&quot;&gt;Parquet&lt;/h1&gt;

&lt;p&gt;A significant effort is underway to create a Parquet writer for Arrow data. This work has not been released as part of 
2.0.0, and is planned for the 3.0.0 release. The development of this writer is being carried out on the 
&lt;a href=&quot;https://github.com/apache/arrow/tree/rust-parquet-arrow-writer&quot;&gt;rust-parquet-arrow-writer&lt;/a&gt; branch, and the branch is regularly synchronized with the main branch.
As part of the writer, the necessary improvements and features are being added to the reader.&lt;/p&gt;

&lt;p&gt;The main focus areas are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Supporting nested Arrow types, such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;List&amp;lt;Struct&amp;lt;[Dictionary, String]&amp;gt;&amp;gt;&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Ensuring correct round-trip between the reader and writer by encoding Arrow schemas in the Parquet metadata&lt;/li&gt;
  &lt;li&gt;Improve null value writing for Parquet&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;parquet_derive&lt;/code&gt; crate has been created, which allows users to derive Parquet records for simple structs. Refer to 
the &lt;a href=&quot;https://github.com/apache/arrow-rs/tree/master/parquet_derive&quot;&gt;parquet_derive crate&lt;/a&gt; for usage examples.&lt;/p&gt;

&lt;h1 id=&quot;datafusion&quot;&gt;DataFusion&lt;/h1&gt;

&lt;p&gt;DataFusion is an in-memory query engine with DataFrame and SQL APIs, built on top of base Arrow support.&lt;/p&gt;

&lt;h2 id=&quot;dataframe-api&quot;&gt;DataFrame API&lt;/h2&gt;

&lt;p&gt;DataFusion now has a richer &lt;a href=&quot;https://docs.rs/datafusion/2.0.0/datafusion/dataframe/trait.DataFrame.html&quot;&gt;DataFrame API&lt;/a&gt; with improved documentation showing example usage, 
supporting the following operations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;select_columns&lt;/li&gt;
  &lt;li&gt;select&lt;/li&gt;
  &lt;li&gt;filter&lt;/li&gt;
  &lt;li&gt;aggregate&lt;/li&gt;
  &lt;li&gt;limit&lt;/li&gt;
  &lt;li&gt;sort&lt;/li&gt;
  &lt;li&gt;collect&lt;/li&gt;
  &lt;li&gt;explain&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;performance--scalability&quot;&gt;Performance &amp;amp; Scalability&lt;/h2&gt;

&lt;p&gt;DataFusion query execution now uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;async&lt;/code&gt;/&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;await&lt;/code&gt; with the tokio threaded runtime rather than launching dedicated 
threads, making queries scale much better across available cores.&lt;/p&gt;

&lt;p&gt;The hash aggregate physical operator has been largely re-written, resulting in significant performance improvements.&lt;/p&gt;

&lt;h2 id=&quot;expressions-and-compute&quot;&gt;Expressions and Compute&lt;/h2&gt;

&lt;h3 id=&quot;improved-scalar-functions&quot;&gt;Improved Scalar Functions&lt;/h3&gt;

&lt;p&gt;DataFusion has many new functions, both in the SQL and the DataFrame API:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Length of an string&lt;/li&gt;
  &lt;li&gt;COUNT(DISTINCT column)&lt;/li&gt;
  &lt;li&gt;to_timestamp&lt;/li&gt;
  &lt;li&gt;IsNull and IsNotNull&lt;/li&gt;
  &lt;li&gt;Min/Max for strings (lexicographic order)&lt;/li&gt;
  &lt;li&gt;Array of columns&lt;/li&gt;
  &lt;li&gt;Concatenation of strings&lt;/li&gt;
  &lt;li&gt;Aliases of aggregate expressions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Many existing expressions were also significantly optimized (2-3x speedups) by avoiding memory copies and leveraging 
Arrow format’s invariants.&lt;/p&gt;

&lt;p&gt;Unary mathematical functions (such as sqrt) now support both 32 and 64-bit floats and return the corresponding type, 
thereby allowing faster operations when higher precision is not needed.&lt;/p&gt;

&lt;h3 id=&quot;improved-user-defined-functions-udfs&quot;&gt;Improved User-defined Functions (UDFs)&lt;/h3&gt;
&lt;p&gt;The API to use and register UDFs has been significantly improved, allowing users to register UDFs and call them both 
via SQL and the DataFrame API. UDFs now also have the same generality as DataFusion’s functions, including variadic 
and dynamically typed arguments.&lt;/p&gt;

&lt;h3 id=&quot;user-defined-aggregate-functions-udafs&quot;&gt;User-defined Aggregate Functions (UDAFs)&lt;/h3&gt;
&lt;p&gt;DataFusion now supports user-defined aggregate functions that can be used to perform operations than span multiple 
rows, batches, and partitions. UDAFs have the same generality as DataFusion’s functions and support both row updates 
and batch updates. You can check out &lt;a href=&quot;https://github.com/apache/arrow-datafusion/blob/master/datafusion-examples/examples/simple_udaf.rs&quot;&gt;this example&lt;/a&gt; to learn how to declare and use a UDAF.&lt;/p&gt;

&lt;h3 id=&quot;user-defined-constants&quot;&gt;User-defined Constants&lt;/h3&gt;
&lt;p&gt;DataFusion now supports registering constants (e.g. “@version”) that live for the duration of the execution context 
and can be accessed from SQL.&lt;/p&gt;

&lt;h2 id=&quot;query-planning--optimization&quot;&gt;Query Planning &amp;amp; Optimization&lt;/h2&gt;

&lt;h3 id=&quot;user-defined-logical-plans&quot;&gt;User-defined logical plans&lt;/h3&gt;

&lt;p&gt;The Logical Plan enum is now extensible through an Extension variant which accepts a UserDefinedLogicalPlan trait using 
dynamic dispatch. Consequently, DataFusion now supports user-defined logical nodes, thereby allowing complex nodes to 
be planned and executed. You can check this example to learn how to declare a new node.&lt;/p&gt;

&lt;h3 id=&quot;predicate-push-down&quot;&gt;Predicate push-down&lt;/h3&gt;

&lt;p&gt;DataFusion now has a Predicate push-down optimizer rule that pushes filter operations as close as possible to scans, 
thereby speeding up the physical execution of suboptimal queries created via the DataFrame API.&lt;/p&gt;

&lt;h3 id=&quot;sql&quot;&gt;SQL&lt;/h3&gt;

&lt;p&gt;DataFusion now uses a more recent release of the sqlparser crate, which has much more comprehensive support for SQL 
syntax and also supports multiple dialects (Postgres, MS SQL, and MySQL).&lt;/p&gt;

&lt;p&gt;It is now possible to see the query plan for a SQL statement using EXPLAIN syntax.&lt;/p&gt;

&lt;h1 id=&quot;benchmarks&quot;&gt;Benchmarks&lt;/h1&gt;

&lt;p&gt;The benchmark crate now contains a new benchmark based on TPC-H that can execute TPC-H query 1 against CSV, Parquet, 
and memory data sources. This is useful for running benchmarks against larger data sets.&lt;/p&gt;

&lt;h1 id=&quot;integration-testing--ipc&quot;&gt;Integration Testing / IPC&lt;/h1&gt;

&lt;p&gt;Arrow IPC is the format for serialization and interprocess communication. It is described in &lt;a href=&quot;https://arrow.apache.org/&quot;&gt;arrow.apache.org&lt;/a&gt; and is 
the format used for file and stream I/O between applications wishing to interchange Arrow data.&lt;/p&gt;

&lt;p&gt;The Arrow project released IPC version 5 of the Arrow IPC format in version 1.0.0. Before that, a message padding 
change was made in version 0.15.0 to change the default padding to 8 bytes, while remaining in IPC version 4. Arrow 
release 0.14.1 and earlier were the last releases to use the legacy 4 byte alignment.
As part of 2.0.0, the Rust implementation was updated to comply with the changes up to release 0.15.0 of Arrow. 
Work on supporting IPC version 5 is underway, and is expected to be completed in time for 3.0.0.&lt;/p&gt;

&lt;p&gt;As part of the conformance work, Rust is being added to the Arrow integration suite, which tests that supported 
language implementations &lt;a href=&quot;https://issues.apache.org/jira/browse/ARROW-3690&quot;&gt;(ARROW-3690)&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Comply with the Arrow IPC format&lt;/li&gt;
  &lt;li&gt;Can read and write each other’s generated data&lt;/li&gt;
  &lt;li&gt;IPC version 4 is being verified through the above work.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;roadmap-for-300-and-beyond&quot;&gt;Roadmap for 3.0.0 and Beyond&lt;/h1&gt;

&lt;p&gt;Here are some of the initiatives that contributors are currently working on for future releases:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Support stable Rust&lt;/li&gt;
  &lt;li&gt;Improved DictionaryArray support and performance&lt;/li&gt;
  &lt;li&gt;Implement inner equijoins&lt;/li&gt;
  &lt;li&gt;Support for various platforms like ARMv8&lt;/li&gt;
  &lt;li&gt;Supporting the C Data Interface from Rust to better support interoperability with other Arrow implementations&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;how-to-get-involved&quot;&gt;How to Get Involved&lt;/h1&gt;

&lt;p&gt;If you are interested in contributing to the Rust subproject in Apache Arrow, you can find a list of open issues 
suitable for beginners &lt;a href=&quot;https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20resolution%20%3D%20Unresolved%20AND%20component%20in%20(Rust%2C%20%22Rust%20-%20DataFusion%22)%20AND%20labels%20%3D%20beginner&quot;&gt;here&lt;/a&gt; and the full list &lt;a href=&quot;https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20resolution%20%3D%20Unresolved%20AND%20component%20in%20(Rust%2C%20%22Rust%20-%20DataFusion%22)%20ORDER%20BY%20updated%20DESC%2C%20created%20DESC%2C%20priority%20DESC&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Other ways to get involved include trying out Arrow on some of your data and filing bug reports, and helping to 
improve the documentation.&lt;/p&gt;</content><author><name>pmc</name></author><category term="release" /><summary type="html">Apache Arrow 2.0.0 is a significant release for the Apache Arrow project in general (release notes), and the Rust subproject in particular, with almost 200 issues resolved by 15 contributors. In this blog post, we will go through the main changes affecting core Arrow, Parquet support, and DataFusion query engine. The full list of resolved issues can be found here. While the Java and C/C++ (used by Python and R) Arrow implementations likely remain the most feature-rich, with the 2.0.0 release, the Rust implementation is closing the feature gap quickly. Here are some of the highlights for this release. Core Arrow Crate Iterator Trait Primitive arrays (e.g., array of integers) can now be converted to and initialized from an iterator. This exposes a very popular API - iterators - to arrow arrays. Work for other types will continue throughout 3.0.0. Improved Variable-sized Arrays Variable sized arrays (e.g., array of strings) have been internally refactored to more easily support their larger (64-bit size offset) version. This allowed us to generalize some of the kernels to both (32 and 64) versions and perform type checks when building them. Kernels There have been numerous improvements in the Arrow compute kernels, including: New kernels have been added for string operations, including substring, min, max, concat, and length. Aggregate sum is now implemented for SIMD with a 5x improvement over the non-SIMD operation Many kernels have been improved to support dictionary-encoded arrays Some kernels were optimized for arrays without nulls, making them significantly faster in that case. Many kernels were optimized in the number of memory copies that are needed to apply them and also on their implementation. Other Improvements The Array trait now has get_buffer_memory_size and get_array_memory_size methods for determining the amount of memory allocated for the array. Parquet A significant effort is underway to create a Parquet writer for Arrow data. This work has not been released as part of 2.0.0, and is planned for the 3.0.0 release. The development of this writer is being carried out on the rust-parquet-arrow-writer branch, and the branch is regularly synchronized with the main branch. As part of the writer, the necessary improvements and features are being added to the reader. The main focus areas are: Supporting nested Arrow types, such as List&amp;lt;Struct&amp;lt;[Dictionary, String]&amp;gt;&amp;gt; Ensuring correct round-trip between the reader and writer by encoding Arrow schemas in the Parquet metadata Improve null value writing for Parquet A new parquet_derive crate has been created, which allows users to derive Parquet records for simple structs. Refer to the parquet_derive crate for usage examples. DataFusion DataFusion is an in-memory query engine with DataFrame and SQL APIs, built on top of base Arrow support. DataFrame API DataFusion now has a richer DataFrame API with improved documentation showing example usage, supporting the following operations: select_columns select filter aggregate limit sort collect explain Performance &amp;amp; Scalability DataFusion query execution now uses async/await with the tokio threaded runtime rather than launching dedicated threads, making queries scale much better across available cores. The hash aggregate physical operator has been largely re-written, resulting in significant performance improvements. Expressions and Compute Improved Scalar Functions DataFusion has many new functions, both in the SQL and the DataFrame API: Length of an string COUNT(DISTINCT column) to_timestamp IsNull and IsNotNull Min/Max for strings (lexicographic order) Array of columns Concatenation of strings Aliases of aggregate expressions Many existing expressions were also significantly optimized (2-3x speedups) by avoiding memory copies and leveraging Arrow format’s invariants. Unary mathematical functions (such as sqrt) now support both 32 and 64-bit floats and return the corresponding type, thereby allowing faster operations when higher precision is not needed. Improved User-defined Functions (UDFs) The API to use and register UDFs has been significantly improved, allowing users to register UDFs and call them both via SQL and the DataFrame API. UDFs now also have the same generality as DataFusion’s functions, including variadic and dynamically typed arguments. User-defined Aggregate Functions (UDAFs) DataFusion now supports user-defined aggregate functions that can be used to perform operations than span multiple rows, batches, and partitions. UDAFs have the same generality as DataFusion’s functions and support both row updates and batch updates. You can check out this example to learn how to declare and use a UDAF. User-defined Constants DataFusion now supports registering constants (e.g. “@version”) that live for the duration of the execution context and can be accessed from SQL. Query Planning &amp;amp; Optimization User-defined logical plans The Logical Plan enum is now extensible through an Extension variant which accepts a UserDefinedLogicalPlan trait using dynamic dispatch. Consequently, DataFusion now supports user-defined logical nodes, thereby allowing complex nodes to be planned and executed. You can check this example to learn how to declare a new node. Predicate push-down DataFusion now has a Predicate push-down optimizer rule that pushes filter operations as close as possible to scans, thereby speeding up the physical execution of suboptimal queries created via the DataFrame API. SQL DataFusion now uses a more recent release of the sqlparser crate, which has much more comprehensive support for SQL syntax and also supports multiple dialects (Postgres, MS SQL, and MySQL). It is now possible to see the query plan for a SQL statement using EXPLAIN syntax. Benchmarks The benchmark crate now contains a new benchmark based on TPC-H that can execute TPC-H query 1 against CSV, Parquet, and memory data sources. This is useful for running benchmarks against larger data sets. Integration Testing / IPC Arrow IPC is the format for serialization and interprocess communication. It is described in arrow.apache.org and is the format used for file and stream I/O between applications wishing to interchange Arrow data. The Arrow project released IPC version 5 of the Arrow IPC format in version 1.0.0. Before that, a message padding change was made in version 0.15.0 to change the default padding to 8 bytes, while remaining in IPC version 4. Arrow release 0.14.1 and earlier were the last releases to use the legacy 4 byte alignment. As part of 2.0.0, the Rust implementation was updated to comply with the changes up to release 0.15.0 of Arrow. Work on supporting IPC version 5 is underway, and is expected to be completed in time for 3.0.0. As part of the conformance work, Rust is being added to the Arrow integration suite, which tests that supported language implementations (ARROW-3690): Comply with the Arrow IPC format Can read and write each other’s generated data IPC version 4 is being verified through the above work. Roadmap for 3.0.0 and Beyond Here are some of the initiatives that contributors are currently working on for future releases: Support stable Rust Improved DictionaryArray support and performance Implement inner equijoins Support for various platforms like ARMv8 Supporting the C Data Interface from Rust to better support interoperability with other Arrow implementations How to Get Involved If you are interested in contributing to the Rust subproject in Apache Arrow, you can find a list of open issues suitable for beginners here and the full list here. Other ways to get involved include trying out Arrow on some of your data and filing bug reports, and helping to improve the documentation.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>