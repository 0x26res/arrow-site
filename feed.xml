<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://arrow.apache.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arrow.apache.org/" rel="alternate" type="text/html" /><updated>2023-02-14T11:02:59-05:00</updated><id>https://arrow.apache.org/feed.xml</id><title type="html">Apache Arrow</title><subtitle>Apache Arrow is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, C++, C#, Go, Java, JavaScript, MATLAB, Python, R, Ruby, and Rust.</subtitle><entry><title type="html">February 2023 Rust Apache Arrow Highlights</title><link href="https://arrow.apache.org/blog/2023/02/13/rust-32.0.0/" rel="alternate" type="text/html" title="February 2023 Rust Apache Arrow Highlights" /><published>2023-02-13T19:00:00-05:00</published><updated>2023-02-13T19:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/02/13/rust-32.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/02/13/rust-32.0.0/"><![CDATA[<!--

-->

<h1 id="introduction">Introduction</h1>

<p>With the recent release of <a href="https://crates.io/crates/arrow/32.0.0">32.0.0</a> of the Rust implementation of <a href="https://arrow.apache.org/">Apache Arrow</a>, it seemed timely to highlight some of the community works since the <a href="https://arrow.apache.org/blog/2022/06/16/rust-16.0.0/">last update</a>.</p>

<p>The most recent list of detailed changes can always be found in the <a href="https://github.com/apache/arrow-rs/blob/master/CHANGELOG.md">CHANGELOG</a>, with the full historical list available <a href="https://github.com/apache/arrow-rs/blob/master/CHANGELOG-old.md">here</a>.</p>

<h1 id="arrow">Arrow</h1>

<p><a href="https://crates.io/crates/arrow">arrow</a> and <a href="https://crates.io/crates/arrow-flight">arrow-flight</a> are native Rust implementations of Apache Arrow. Apache Arrow defines a language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware like CPUs and GPUs. The Arrow memory format also supports zero-copy reads for lightning-fast data access without serialization overhead.</p>

<p>The <a href="https://www.rust-lang.org/">Rust language</a> offers best in class performance,  memory safety, and the developer productivity of a modern programming language. These features make Rust an excellent choice for building modern high performance analytical systems. When combined, Rust and the Apache Arrow Ecosystem are a compelling toolkit for building the next generation of systems.</p>

<p>The <a href="https://github.com/apache/arrow-rs">repository</a> recently passed 1400 stars on github, and the community has been focused on performance and feature completeness.</p>

<p><strong>Major Highlights</strong></p>

<ul>
  <li><strong>New CSV and JSON Readers</strong>: The CSV and JSON readers have been revamped. Their performance has more than doubled, and they now support push-driven parsing facilitating async streaming decode from object storage.</li>
  <li><strong>Faster Build Times and Reduced Codegen</strong>: The <code class="language-plaintext highlighter-rouge">arrow</code> crate has been split into multiple smaller crates, and large kernels have been moved behind optional feature flags. These changes allow downstream projects to choose a smaller dependency footprint and build times, if desired.</li>
  <li><strong>Support for Copy-On-Write</strong>: Arrow arrays now support copy-on-write, via the <a href="https://docs.rs/arrow/32.0.0/arrow/array/struct.ArrayData.html#method.into_builder"><code class="language-plaintext highlighter-rouge">into_builder</code></a> methods</li>
  <li><strong>Comparable Row Format</strong>: <a href="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1/">Much faster multi-column Sorting and Grouping</a> is now possible with the the new spillable, comparable <a href="https://docs.rs/arrow-row/32.0.0/arrow_row/index.html">row-format</a></li>
  <li><strong>FlightSQL Support</strong>: <a href="https://arrow.apache.org/docs/format/FlightSql.html">FlightSQL</a> <a href="https://docs.rs/arrow-flight/32.0.0/arrow_flight/sql/index.html">support</a> has been expanded</li>
  <li><strong>Mid-Level Flight Client</strong>: A new <a href="https://docs.rs/arrow-flight/32.0.0/arrow_flight/client/struct.FlightClient.html">FlightClient</a> is available that handles lower level protocol details, and easier to use <a href="https://docs.rs/arrow-flight/32.0.0/arrow_flight/encode/struct.FlightDataEncoderBuilder.html">encoding</a> and <a href="https://docs.rs/arrow-flight/32.0.0/arrow_flight/decode/struct.FlightDataDecoder.html">decoding</a> APIs.</li>
  <li><strong>IPC File Compression</strong>: Arrow IPC file <a href="https://docs.rs/arrow-ipc/32.0.0/arrow_ipc/gen/Message/struct.CompressionType.html">compression</a> with ZSTD and LZ4 is now fully supported.</li>
  <li><strong>Full Decimal Support</strong>: 256-bit decimals and negative scales can be created and manipulated using many kernels, such as arithmetic.</li>
  <li><strong>Improved Dictionary Support</strong>: Dictionaries are now transparently supported in most kernels.</li>
  <li><strong>Improved Temporal Support</strong>: Timestamps with Timezones and other temporal types are supported in many more kernels.</li>
  <li><strong>Improved Generics</strong>: Improved generics allow writing code generic over all arrays, or all arrays with the same layout</li>
  <li><strong>Downcast Macros</strong>: Various <a href="https://docs.rs/arrow/32.0.0/arrow/macro.downcast_primitive_array.html">helper</a> <a href="https://docs.rs/arrow/32.0.0/arrow/macro.downcast_dictionary_array.html">macros</a> are now available to simplify dynamic dispatch to statically typed implementations.</li>
</ul>

<h1 id="parquet">Parquet</h1>

<p><a href="https://parquet.apache.org/">Apache Parquet</a> is an open source, column-oriented data file format designed for efficient data storage and retrieval. It provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk. The Apache Parquet implementation in Rust is one of the <a href="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/">fastest and most sophisticated</a> open source implementations available.</p>

<p><strong>Major Highlights</strong></p>

<ul>
  <li><strong>Arbitrarily Nested Schema</strong>: Arbitrarily nested schemas can be read to and written from arrow, as described in the series of <a href="https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/">blog posts</a> on the topic.</li>
  <li><strong>Predicate Pushdown</strong>: The <a href="https://docs.rs/parquet/32.0.0/parquet/arrow/index.html">arrow reader</a> now supports advanced predicate pushdown, including late materialization, as described <a href="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/">here</a>. See <a href="https://docs.rs/parquet/32.0.0/parquet/arrow/arrow_reader/struct.RowSelection.html">RowSelection</a> and <a href="https://docs.rs/parquet/32.0.0/parquet/arrow/arrow_reader/trait.ArrowPredicate.html">ArrowPredicate</a>.</li>
  <li><strong>Bloom Filter Support</strong>: Support for both <a href="https://docs.rs/parquet/32.0.0/parquet/bloom_filter/index.html">reading and writing bloom filters</a> has been added.</li>
  <li><strong>CLI Tools</strong>: additional <a href="https://github.com/apache/arrow-rs/tree/master/parquet/src/bin">CLI tools</a> have been added to introspect and manipulate parquet data.</li>
</ul>

<h1 id="object-store">Object Store</h1>

<p>Modern analytic workloads increasingly make use of blob storage facilities, such as S3, to store large volumes of queryable data. A native Rust object storage implementation that works well with the Rust Ecosystem in general, and the Arrow IO abstractions, is an important building block for many applications.
The <a href="https://docs.rs/object_store/latest/object_store/">object_store</a> crate was <a href="https://www.influxdata.com/blog/rust-object-store-donation/">donated to the Apache Arrow project in July 2022</a> to fill this need, and while it follows a separate release schedule than the <code class="language-plaintext highlighter-rouge">arrow</code> and <code class="language-plaintext highlighter-rouge">parquet</code> crates, it forms an integral part of the overarching Arrow IO story.</p>

<p>Recent improvements include the following:</p>

<p><strong>Major Highlights</strong></p>

<ul>
  <li><strong>Streaming Upload</strong>: Multipart uploads are now supported.</li>
  <li><strong>Minimised dependency footprint</strong>: Upstream SDKs are no longer used, improving consistency and reducing dependencies.</li>
  <li><strong>HTTP / WebDAV Support</strong>: Applications can read from arbitrary HTTP servers, with mutation and listing supported on WebDAV-compatible endpoints.</li>
  <li><strong>Configurable Networking</strong>: Socks proxies, and advanced HTTP client configuration are now supported.</li>
  <li><strong>Serializable Configuration</strong>: Configuration information can now be easily serialized and deserialized.</li>
  <li><strong>Additional Authentication</strong>: Additional authentication options are now available for the various cloud providers.</li>
</ul>

<h1 id="contributors">Contributors:</h1>

<p>While some open source software can be created mostly by a single contributor, we believe the greatest software with the largest impact and reach is built around its community. Thus, Arrow is proud to part of the <a href="https://www.apache.org/">Apache Software Foundation</a> and our releases both past and present are a result of our amazing community’s effort.</p>

<p>We would like to thank everyone who has contributed to the arrow-rs repository since the <code class="language-plaintext highlighter-rouge">16.0.0</code> release. Keep up the great work, and we look forward to continued improvements:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">% git shortlog -sn 16.0.0..32.0.0
   347  Raphael Taylor-Davies
   166  Liang-Chi Hsieh
    94  Andrew Lamb
    36  Remzi Yang
    30  Kun Liu
    21  Yang Jiang
    20  askoa
    17  dependabot[bot]
    15  Vrishabh
    12  Dan Harris
    12  Wei-Ting Kuo
    11  Daniël Heres
    11  Jörn Horstmann
     9  Brent Gardner
     9  Ian Alexander Joiner
     9  Jiayu Liu
     9  Martin Grigorov
     8  Palladium
     7  Jeffrey
     7  Marco Neumann
     6  Robert Pack
     6  Will Jones
     4  Andy Grove
     4  comphead
     3  Adrián Gallego Castellanos
     3  Markus Westerlind
     3  Quentin
     2  Alex Qyoun-ae
     2  Dmitry Patsura
     2  Frank
     2  Jiacai Liu
     2  Marc Garcia
     2  Marko Grujic
     2  Max Burke
     2  Your friendly neighborhood geek
     2  sachin agarwal
     1  Aarash Heydari
     1  Adam Gutglick
     1  Andrey Frolov
     1  Anthony Poncet
     1  Artjoms Iskovs
     1  Ben Kimock
     1  Brian Phillips
     1  Carol (Nichols || Goulding)
     1  Christian Salvati
     1  Dalton Modlin
     1  Daniel Martinez Maqueda
     1  Daniel Poelzleithner
     1  Davis Silverman
     1  Dhruv Vats
     1  Fabio Silva
     1  GeauxEric
     1  George Andronchik
     1  Ismail-Maj
     1  Ismaël Mejía
     1  JanKaul
     1  JasonLi
     1  Javier Goday
     1  Jayjeet Chakraborty
     1  Jean-Charles Campagne
     1  Jie Han
     1  John Hughes
     1  Jon Mease
     1  Kevin Lim
     1  Kohei Suzuki
     1  Konstantin Fastov
     1  Marius S
     1  Masato Kato
     1  Matthijs Brobbel
     1  Michael Edwards
     1  Pier-Olivier Thibault
     1  Remco Verhoef
     1  Rutvik Patel
     1  Sean Smith
     1  Sid
     1  Stanislav Lukeš
     1  Steve Vaughan
     1  Stuart Carnie
     1  Sumit
     1  Trent Feda
     1  Valeriy V. Vorotyntsev
     1  Wenjun L
     1  X
     1  aksharau
     1  bmmeijers
     1  chunshao.rcs
     1  jakevin
     1  kastolars
     1  nvartolomei
     1  xudong.w
     1  哇呜哇呜呀咦耶
     1  尹吉峰
</span></code></pre></div></div>

<h1 id="join-the-community">Join the community</h1>

<p>If you are interested in contributing to the Rust subproject in Apache Arrow, we encourage you to try out Arrow on some of your data, help
improve the documentation, or submit a PR. You can find a list of open issues
suitable for beginners <a href="https://github.com/apache/arrow-rs/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a>
and the full list <a href="https://github.com/apache/arrow-rs/issues">here</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[Introduction With the recent release of 32.0.0 of the Rust implementation of Apache Arrow, it seemed timely to highlight some of the community works since the last update. The most recent list of detailed changes can always be found in the CHANGELOG, with the full historical list available here. Arrow arrow and arrow-flight are native Rust implementations of Apache Arrow. Apache Arrow defines a language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware like CPUs and GPUs. The Arrow memory format also supports zero-copy reads for lightning-fast data access without serialization overhead. The Rust language offers best in class performance, memory safety, and the developer productivity of a modern programming language. These features make Rust an excellent choice for building modern high performance analytical systems. When combined, Rust and the Apache Arrow Ecosystem are a compelling toolkit for building the next generation of systems. The repository recently passed 1400 stars on github, and the community has been focused on performance and feature completeness. Major Highlights New CSV and JSON Readers: The CSV and JSON readers have been revamped. Their performance has more than doubled, and they now support push-driven parsing facilitating async streaming decode from object storage. Faster Build Times and Reduced Codegen: The arrow crate has been split into multiple smaller crates, and large kernels have been moved behind optional feature flags. These changes allow downstream projects to choose a smaller dependency footprint and build times, if desired. Support for Copy-On-Write: Arrow arrays now support copy-on-write, via the into_builder methods Comparable Row Format: Much faster multi-column Sorting and Grouping is now possible with the the new spillable, comparable row-format FlightSQL Support: FlightSQL support has been expanded Mid-Level Flight Client: A new FlightClient is available that handles lower level protocol details, and easier to use encoding and decoding APIs. IPC File Compression: Arrow IPC file compression with ZSTD and LZ4 is now fully supported. Full Decimal Support: 256-bit decimals and negative scales can be created and manipulated using many kernels, such as arithmetic. Improved Dictionary Support: Dictionaries are now transparently supported in most kernels. Improved Temporal Support: Timestamps with Timezones and other temporal types are supported in many more kernels. Improved Generics: Improved generics allow writing code generic over all arrays, or all arrays with the same layout Downcast Macros: Various helper macros are now available to simplify dynamic dispatch to statically typed implementations. Parquet Apache Parquet is an open source, column-oriented data file format designed for efficient data storage and retrieval. It provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk. The Apache Parquet implementation in Rust is one of the fastest and most sophisticated open source implementations available. Major Highlights Arbitrarily Nested Schema: Arbitrarily nested schemas can be read to and written from arrow, as described in the series of blog posts on the topic. Predicate Pushdown: The arrow reader now supports advanced predicate pushdown, including late materialization, as described here. See RowSelection and ArrowPredicate. Bloom Filter Support: Support for both reading and writing bloom filters has been added. CLI Tools: additional CLI tools have been added to introspect and manipulate parquet data. Object Store Modern analytic workloads increasingly make use of blob storage facilities, such as S3, to store large volumes of queryable data. A native Rust object storage implementation that works well with the Rust Ecosystem in general, and the Arrow IO abstractions, is an important building block for many applications. The object_store crate was donated to the Apache Arrow project in July 2022 to fill this need, and while it follows a separate release schedule than the arrow and parquet crates, it forms an integral part of the overarching Arrow IO story. Recent improvements include the following: Major Highlights Streaming Upload: Multipart uploads are now supported. Minimised dependency footprint: Upstream SDKs are no longer used, improving consistency and reducing dependencies. HTTP / WebDAV Support: Applications can read from arbitrary HTTP servers, with mutation and listing supported on WebDAV-compatible endpoints. Configurable Networking: Socks proxies, and advanced HTTP client configuration are now supported. Serializable Configuration: Configuration information can now be easily serialized and deserialized. Additional Authentication: Additional authentication options are now available for the various cloud providers. Contributors: While some open source software can be created mostly by a single contributor, we believe the greatest software with the largest impact and reach is built around its community. Thus, Arrow is proud to part of the Apache Software Foundation and our releases both past and present are a result of our amazing community’s effort. We would like to thank everyone who has contributed to the arrow-rs repository since the 16.0.0 release. Keep up the great work, and we look forward to continued improvements: % git shortlog -sn 16.0.0..32.0.0 347 Raphael Taylor-Davies 166 Liang-Chi Hsieh 94 Andrew Lamb 36 Remzi Yang 30 Kun Liu 21 Yang Jiang 20 askoa 17 dependabot[bot] 15 Vrishabh 12 Dan Harris 12 Wei-Ting Kuo 11 Daniël Heres 11 Jörn Horstmann 9 Brent Gardner 9 Ian Alexander Joiner 9 Jiayu Liu 9 Martin Grigorov 8 Palladium 7 Jeffrey 7 Marco Neumann 6 Robert Pack 6 Will Jones 4 Andy Grove 4 comphead 3 Adrián Gallego Castellanos 3 Markus Westerlind 3 Quentin 2 Alex Qyoun-ae 2 Dmitry Patsura 2 Frank 2 Jiacai Liu 2 Marc Garcia 2 Marko Grujic 2 Max Burke 2 Your friendly neighborhood geek 2 sachin agarwal 1 Aarash Heydari 1 Adam Gutglick 1 Andrey Frolov 1 Anthony Poncet 1 Artjoms Iskovs 1 Ben Kimock 1 Brian Phillips 1 Carol (Nichols || Goulding) 1 Christian Salvati 1 Dalton Modlin 1 Daniel Martinez Maqueda 1 Daniel Poelzleithner 1 Davis Silverman 1 Dhruv Vats 1 Fabio Silva 1 GeauxEric 1 George Andronchik 1 Ismail-Maj 1 Ismaël Mejía 1 JanKaul 1 JasonLi 1 Javier Goday 1 Jayjeet Chakraborty 1 Jean-Charles Campagne 1 Jie Han 1 John Hughes 1 Jon Mease 1 Kevin Lim 1 Kohei Suzuki 1 Konstantin Fastov 1 Marius S 1 Masato Kato 1 Matthijs Brobbel 1 Michael Edwards 1 Pier-Olivier Thibault 1 Remco Verhoef 1 Rutvik Patel 1 Sean Smith 1 Sid 1 Stanislav Lukeš 1 Steve Vaughan 1 Stuart Carnie 1 Sumit 1 Trent Feda 1 Valeriy V. Vorotyntsev 1 Wenjun L 1 X 1 aksharau 1 bmmeijers 1 chunshao.rcs 1 jakevin 1 kastolars 1 nvartolomei 1 xudong.w 1 哇呜哇呜呀咦耶 1 尹吉峰 Join the community If you are interested in contributing to the Rust subproject in Apache Arrow, we encourage you to try out Arrow on some of your data, help improve the documentation, or submit a PR. You can find a list of open issues suitable for beginners here and the full list here.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 11.0.0 Release</title><link href="https://arrow.apache.org/blog/2023/01/25/11.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 11.0.0 Release" /><published>2023-01-25T00:00:00-05:00</published><updated>2023-01-25T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/01/25/11.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/01/25/11.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 11.0.0 release. This covers
over 3 months of development work and includes <a href="https://github.com/apache/arrow/milestone/1?closed=1"><strong>423 resolved issues</strong></a>
from <a href="/release/11.0.0.html#contributors"><strong>95 distinct contributors</strong></a>. See the <a href="https://arrow.apache.org/install/">Install Page</a>
to learn how to get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/11.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 10.0.0 release, Ben Baumgold, Will Jones, Eric Patrick Hanson,
Curtis Vogt, Yang Jiang, Jarrett Revels, Raúl Cumplido, Jacob Wujciak,
Jie Wen and Brent Gardner have been invited to be committers.
Kun Liu have joined the Project Management Committee (PMC).</p>

<p>As per our newly started tradition of rotating the PMC chair once a year
Andrew Lamb was elected as the new PMC chair and VP.</p>

<p>Thanks for your contributions and participation in the project!</p>

<h2 id="columnar-format-notes">Columnar Format Notes</h2>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>

<p>In the C++/Python Flight clients, DoAction now properly streams the results, instead of blocking until the call finishes. Applications that did not consume the iterator before should fully consume the result. (<a href="https://github.com/apache/arrow/issues/15069">#15069</a>)</p>

<h2 id="c-notes">C++ notes</h2>
<ul>
  <li>It is now possible to specify alignment when making allocations with a MemoryPool <a href="https://github.com/apache/arrow/issues/33056">GH-33056</a></li>
  <li>It is now possible to run an ExecPlan without using any CPU threads</li>
  <li>Added kernel for slicing list values <a href="https://github.com/apache/arrow/issues/33168">GH-33168</a></li>
  <li>Added kernel for slicing binary arrays <a href="https://github.com/apache/arrow/issues/20357">GH-20357</a></li>
  <li>When comparing list arrays for equality the list field name is now ignored <a href="https://github.com/apache/arrow/issues/30519">GH-30519</a></li>
  <li>Add support for partitioning on columns that contain special characters <a href="https://github.com/apache/arrow/issues/33448">GH-33448</a></li>
  <li>Added a streaming reader for JSON <a href="https://github.com/apache/arrow/issues/33140">GH-33140</a></li>
  <li>Added support for incremental writes to the ORC writer <a href="https://github.com/apache/arrow/issues/33047">GH-33047</a></li>
  <li>Added support for casting decimal to string and writing decimal to CSV <a href="https://github.com/apache/arrow/issues/33002">GH-33002</a></li>
  <li>Fixed an assert in the scanner that would occur when batch_readahead was set to 0 <a href="https://github.com/apache/arrow/issues/15264">GH-15264</a></li>
  <li>Fixed bug where arrays with a null data buffer would not be accepted when imported via the C data API <a href="https://github.com/apache/arrow/issues/14875">GH-14875</a></li>
  <li>Fixed bug where arrays with a zero-case union data type would not be accepted when imported via the C data API <a href="https://github.com/apache/arrow/issues/14855">GH-14855</a></li>
  <li>Fixed bug where case_when could return incorrect values <a href="https://github.com/apache/arrow/issues/33382">GH-33382</a></li>
  <li>Fixed bug where RecordBatch::Equals was ignoring field names <a href="https://github.com/apache/arrow/issues/33285">GH-33285</a>
    <h2 id="c-notes-1">C# notes</h2>
  </li>
</ul>

<p>No major changes to C#.</p>

<h2 id="go-notes">Go notes</h2>
<ul>
  <li>Go’s benchmarks will now get added to <a href="https://conbench.ursa.dev">Conbench</a> alongside the benchmarks for other implementations <a href="https://github.com/apache/arrow/issues/32983">GH-32983</a></li>
  <li>Exposed FlightService_ServiceDesc and RegisterFlightServiceServer to allow easily incorporating a flight service into an existing gRPC server <a href="https://github.com/apache/arrow/issues/15174">GH-15174</a></li>
</ul>

<h3 id="arrow">Arrow</h3>
<ul>
  <li>Function <code class="language-plaintext highlighter-rouge">ApproxEquals</code> was implemented for scalar values <a href="https://github.com/apache/arrow/issues/29581">GH-29581</a></li>
  <li><code class="language-plaintext highlighter-rouge">UnmarshalJSON</code> for the <code class="language-plaintext highlighter-rouge">RecordBuilder</code> now properly handles extra unknown fields with complex/nested values <a href="https://github.com/apache/arrow/issues/31840">GH-31840</a></li>
  <li>Decimal128 and Decimal256 type support has been added to the CSV reader <a href="https://github.com/apache/arrow/issues/33111">GH-33111</a></li>
  <li>Fixed bug in <code class="language-plaintext highlighter-rouge">array.UnionBuilder</code> where <code class="language-plaintext highlighter-rouge">Len</code> method always returned 0 <a href="https://github.com/apache/arrow/issues/14775">GH-14775</a></li>
  <li>Fixed bug for handling slices of Map arrays when marshalling to JSON and for IPC <a href="https://github.com/apache/arrow/issues/14780">GH-14780</a></li>
  <li>Fixed memory leak when compressing IPC message body buffers <a href="https://github.com/apache/arrow/issues/14883">GH-14883</a></li>
  <li>Added the ability to easily append scalar values to array builders <a href="https://github.com/apache/arrow/issues/15005">GH-15005</a></li>
</ul>

<h4 id="compute">Compute</h4>
<ul>
  <li>Scalar binary (add/subtract/multiply/divide/etc.) and unary arithmetic (abs/neg/sqrt/sign/etc.) has been implemented for the compute package <a href="https://github.com/apache/arrow/issues/33086">GH-33086</a> this includes easy functions like <code class="language-plaintext highlighter-rouge">compute.Add</code> and <code class="language-plaintext highlighter-rouge">compute.Divide</code> etc.</li>
  <li>Scalar boolean functions like AND/OR/XOR/etc. have been implemented for compute <a href="https://github.com/apache/arrow/issues/33279">GH-33279</a></li>
  <li>Scalar comparison function kernels have been implemented for compute (equal/greater/greater_equal/less/less_equal) <a href="https://github.com/apache/arrow/issues/33308">GH-33308</a></li>
  <li>Scalar compute functions are compatible with dictionary encoded arrays by casting them to their value types <a href="https://github.com/apache/arrow/issues/33502">GH-33502</a></li>
</ul>

<h3 id="parquet">Parquet</h3>
<ul>
  <li>Panic when decoding a delta_bit_packed encoded column has been fixed <a href="https://github.com/apache/arrow/issues/33483">GH-33483</a></li>
  <li>Fixed memory leak from Allocator in <code class="language-plaintext highlighter-rouge">pqarrow.WriteArrowToColumn</code> <a href="https://github.com/apache/arrow/issues/14865">GH-14865</a></li>
  <li>Fixed <code class="language-plaintext highlighter-rouge">writer.WriteBatch</code> to properly handle writing encrypted parquet columns and no longer silently fail, but instead propagate an error <a href="https://github.com/apache/arrow/issues/14940">GH-14940</a></li>
</ul>

<h2 id="java-notes">Java notes</h2>
<ul>
  <li>Implement support for writing compressed files (<a href="https://github.com/apache/arrow/pull/15223">#15223</a>)</li>
  <li>Improve performance by short-circuiting null checks when comparing non null field types (<a href="https://github.com/apache/arrow/pull/15106">#15106</a>)</li>
  <li>Several enhancements to dictionary encoding (<a href="https://github.com/apache/arrow/pull/14891">#14891</a>, (<a href="https://github.com/apache/arrow/pull/14902">#14902</a>, (<a href="https://github.com/apache/arrow/pull/14874">#14874</a>)</li>
  <li>Extend Table to support additional vector types (<a href="https://github.com/apache/arrow/pull/14573">#14573</a>)</li>
  <li>Enhance and simplify handling of allocation management by integrating C Data into allocator hierarchy (<a href="https://github.com/apache/arrow/pull/14506">#14506</a>)</li>
  <li>Make ComplexCopier agnostic of specific implementation of MapWriter (<a href="https://github.com/apache/arrow/pull/14557">#14557</a>)</li>
  <li>Distribute Apple M1 compatible JNI libraries via mavencentral (<a href="https://github.com/apache/arrow/pull/14472">#14472</a>)</li>
  <li>Extend Table copy functionality, and support returning copies of individual vectors (<a href="https://github.com/apache/arrow/pull/14389">#14389</a>)</li>
</ul>

<h2 id="javascript-notes">JavaScript notes</h2>

<ul>
  <li>Bugfixes and dependency updates.</li>
  <li>Arrow now requires BigInt support. <a href="https://github.com/apache/arrow/pull/33682">GH-33681</a></li>
</ul>

<h2 id="python-notes">Python notes</h2>

<p>Compatibility notes:</p>

<ul>
  <li>PyArrow now requires pandas &gt;= 1.0 (<a href="https://issues.apache.org/jira/browse/ARROW-18173">ARROW-18173</a>)</li>
  <li>The <code class="language-plaintext highlighter-rouge">pyarrow.parquet.ParquetDataset()</code> class now by default uses the new Dataset API
under the hood (<code class="language-plaintext highlighter-rouge">use_legacy_dataset=False</code>). You can still pass
<code class="language-plaintext highlighter-rouge">use_legacy_dataset=True</code> to get the legacy implementation, but this option will be
removed in a next release
(<a href="https://issues.apache.org/jira/browse/ARROW-16728">ARROW-16728</a>).</li>
</ul>

<p>New features:</p>

<ul>
  <li>Added support for the <a href="https://data-apis.org/dataframe-protocol/latest/purpose_and_scope.html">DataFrame Interchange Protocol</a>
for <code class="language-plaintext highlighter-rouge">pyarrow.Table</code> (<a href="https://github.com/apache/arrow/issues/33346">GH-33346</a>).</li>
  <li>New kernels: <code class="language-plaintext highlighter-rouge">list_slice()</code> to slice each list element of a ListArray
returning a new ListArray (<a href="https://issues.apache.org/jira/browse/ARROW-17960">ARROW-17960</a>).</li>
  <li>A new <code class="language-plaintext highlighter-rouge">filter()</code> method on the Dataset class as additional API to filter a Dataset
before consuming it (<a href="https://issues.apache.org/jira/browse/ARROW-16616">ARROW-16616</a>).</li>
  <li>New <code class="language-plaintext highlighter-rouge">sort()</code> method for (Chunked)Array and <code class="language-plaintext highlighter-rouge">sort_by()</code> method for RecordBatch,
providing a convenience on top of the <code class="language-plaintext highlighter-rouge">sort_indices</code> kernel
(<a href="https://github.com/apache/arrow/issues/14778">GH-14778</a>), and a new
<code class="language-plaintext highlighter-rouge">Dataset.sort_by()</code> method (<a href="https://github.com/apache/arrow/issues/14975">GH-14975</a>).</li>
</ul>

<p>Other improvements:</p>

<ul>
  <li>Support for custom metadata of record batches in the IPC read and write APIs
(<a href="https://issues.apache.org/jira/browse/ARROW-16430">ARROW-16430</a>).</li>
  <li>Support URIs and the <code class="language-plaintext highlighter-rouge">filesystem</code> parameter in <code class="language-plaintext highlighter-rouge">pyarrow.parquet.ParquetFile</code>
(<a href="https://issues.apache.org/jira/browse/ARROW-18272">ARROW-18272</a>) and
<code class="language-plaintext highlighter-rouge">pyarrow.parquet.write_metadata</code>
(<a href="https://issues.apache.org/jira/browse/ARROW-18225">ARROW-18225</a>).</li>
  <li>When writing a dataset to IPC using <code class="language-plaintext highlighter-rouge">pyarrow.dataset.write_dataset()</code>, you can now
specify IPC specific options, such as compression
(<a href="https://issues.apache.org/jira/browse/ARROW-17991">ARROW-17991</a>)</li>
  <li>The <code class="language-plaintext highlighter-rouge">pyarrow.array()</code> function now allows to construct a MapArray from a sequence of
dicts (in addition to a sequence of tuples)
(<a href="https://issues.apache.org/jira/browse/ARROW-17832">ARROW-17832</a>).</li>
  <li>The <code class="language-plaintext highlighter-rouge">struct_field()</code> kernel now also accepts field names in addition to integer
indices (<a href="https://issues.apache.org/jira/browse/ARROW-17989">ARROW-17989</a>).</li>
  <li>Casting to string is now supported for duration (<a href="https://issues.apache.org/jira/browse/ARROW-15822">ARROW-15822</a>)
and decimal (<a href="https://issues.apache.org/jira/browse/ARROW-17458">ARROW-17458</a>) types,
which also means those can now be written to CSV.</li>
  <li>When writing to CSV, you can now specify the quoting style
(<a href="https://github.com/apache/arrow/issues/14755">GH-14755</a>).</li>
  <li>The <code class="language-plaintext highlighter-rouge">pyarrow.ipc.read_schema()</code> function now accepts a Message object
(<a href="https://issues.apache.org/jira/browse/ARROW-18423">ARROW-18423</a>).</li>
  <li>The Time32Scalar, Time64Scalar, Date32Scalar and Date64Scalar classes got a <code class="language-plaintext highlighter-rouge">.value</code>
attribute to access the underlying integer value, similar to the other date-time
related scalars (<a href="https://issues.apache.org/jira/browse/ARROW-18264">ARROW-18264</a>)</li>
  <li>Duration type is now supported in the hash kernels like <code class="language-plaintext highlighter-rouge">dictionary_encode</code>
(<a href="https://github.com/apache/arrow/issues/15226">GH-15226</a>).</li>
  <li>Fix silent overflow when converting <code class="language-plaintext highlighter-rouge">datetime.timedelta</code> to duration type
(<a href="https://issues.apache.org/jira/browse/ARROW-15026">ARROW-15026</a>).</li>
</ul>

<p>Relevant bug fixes:</p>

<ul>
  <li>Numpy conversion for ListArray is improved taking into account sliced offset, avoiding
increased memory usage (<a href="https://github.com/apache/arrow/issues/20512">GH-20512</a></li>
  <li>Fix writing files with multi-byte characters in file name
(<a href="https://issues.apache.org/jira/browse/ARROW-18123">ARROW-18123</a>).</li>
</ul>

<h2 id="r-notes">R notes</h2>
<ul>
  <li>map_batches() is lazy by default; it now returns a RecordBatchReader instead of a list of RecordBatch objects unless lazy = FALSE. <a href="https://github.com/apache/arrow/issues/14521">GH-14521</a></li>
  <li>A substantial reorganisation, rewrite of and addition to, many of the vignettes and README. <a href="https://github.com/apache/arrow/issues/14514">GH-14514</a></li>
</ul>

<p>For more on what’s in the 11.0.0 R package, see the <a href="/docs/r/news/">R changelog</a>.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<h3 id="ruby">Ruby</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Arrow::Table#save</code> now always returns self instead of the result of its <code class="language-plaintext highlighter-rouge">raw_records</code><a href="https://github.com/apache/arrow/issues/15289">GH-15289</a></li>
  <li>Improve the GC-related crash prevention system by guarding the shared objects from GC <a href="https://issues.apache.org/jira/browse/ARROW-18161">ARROW-18161</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">Arrow::HalfFloat</code> and <code class="language-plaintext highlighter-rouge">raw_records</code> support in <code class="language-plaintext highlighter-rouge">Arrow::HalfFloatArray</code> <a href="https://issues.apache.org/jira/browse/ARROW-18086">ARROW-18086</a></li>
  <li>Support omitting join keys in <code class="language-plaintext highlighter-rouge">Table#join</code> <a href="https://github.com/apache/arrow/issues/15084">GH-15084</a></li>
  <li>Add support for <code class="language-plaintext highlighter-rouge">Arrow::Table.load(uri, schema:)</code> <a href="https://issues.apache.org/jira/browse/ARROW-15206">ARROW-15206</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">Arrow::ColumnContainable#column_names</code> (e.g. <code class="language-plaintext highlighter-rouge">Arrow::Table#column_names</code>) <a href="https://github.com/apache/arrow/issues/15085">GH-15085</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">to_arrow_chunked_array</code> methods to support converting to <code class="language-plaintext highlighter-rouge">Arrow::ChunkedArray</code> <a href="https://issues.apache.org/jira/browse/ARROW-18405">ARROW-18405</a></li>
</ul>

<h3 id="c-glib">C GLib</h3>

<ul>
  <li>Add <code class="language-plaintext highlighter-rouge">garrow_chunked_array_new_empty()</code> <a href="https://github.com/apache/arrow/issues/33671">GH-33671</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">GArrowProjectNodeOptions</code> <a href="https://github.com/apache/arrow/issues/33670">GH-33670</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">GADatasetHivePartitioning</code> <a href="https://github.com/apache/arrow/issues/15257">GH-15257</a></li>
  <li>The signature of <code class="language-plaintext highlighter-rouge">garrow_execute_plain_wait()</code> was changed to take the <code class="language-plaintext highlighter-rouge">error</code> argument and to return the finished status <a href="https://github.com/apache/arrow/issues/15254">GH-15254</a></li>
  <li>Add support for half float <a href="https://github.com/apache/arrow/issues/15168">GH-15168</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">GADatasetFinishOptions</code> <a href="https://github.com/apache/arrow/issues/15146">GH-15146</a></li>
</ul>

<h2 id="rust-notes">Rust notes</h2>

<p>The Rust projects have moved to separate repositories outside the
main Arrow monorepo. For notes on the latest release of the Rust
implementation, see the latest <a href="https://github.com/apache/arrow-rs/tags">Arrow Rust changelog</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 11.0.0 release. This covers over 3 months of development work and includes 423 resolved issues from 95 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 10.0.0 release, Ben Baumgold, Will Jones, Eric Patrick Hanson, Curtis Vogt, Yang Jiang, Jarrett Revels, Raúl Cumplido, Jacob Wujciak, Jie Wen and Brent Gardner have been invited to be committers. Kun Liu have joined the Project Management Committee (PMC). As per our newly started tradition of rotating the PMC chair once a year Andrew Lamb was elected as the new PMC chair and VP. Thanks for your contributions and participation in the project! Columnar Format Notes Arrow Flight RPC notes In the C++/Python Flight clients, DoAction now properly streams the results, instead of blocking until the call finishes. Applications that did not consume the iterator before should fully consume the result. (#15069) C++ notes It is now possible to specify alignment when making allocations with a MemoryPool GH-33056 It is now possible to run an ExecPlan without using any CPU threads Added kernel for slicing list values GH-33168 Added kernel for slicing binary arrays GH-20357 When comparing list arrays for equality the list field name is now ignored GH-30519 Add support for partitioning on columns that contain special characters GH-33448 Added a streaming reader for JSON GH-33140 Added support for incremental writes to the ORC writer GH-33047 Added support for casting decimal to string and writing decimal to CSV GH-33002 Fixed an assert in the scanner that would occur when batch_readahead was set to 0 GH-15264 Fixed bug where arrays with a null data buffer would not be accepted when imported via the C data API GH-14875 Fixed bug where arrays with a zero-case union data type would not be accepted when imported via the C data API GH-14855 Fixed bug where case_when could return incorrect values GH-33382 Fixed bug where RecordBatch::Equals was ignoring field names GH-33285 C# notes No major changes to C#. Go notes Go’s benchmarks will now get added to Conbench alongside the benchmarks for other implementations GH-32983 Exposed FlightService_ServiceDesc and RegisterFlightServiceServer to allow easily incorporating a flight service into an existing gRPC server GH-15174 Arrow Function ApproxEquals was implemented for scalar values GH-29581 UnmarshalJSON for the RecordBuilder now properly handles extra unknown fields with complex/nested values GH-31840 Decimal128 and Decimal256 type support has been added to the CSV reader GH-33111 Fixed bug in array.UnionBuilder where Len method always returned 0 GH-14775 Fixed bug for handling slices of Map arrays when marshalling to JSON and for IPC GH-14780 Fixed memory leak when compressing IPC message body buffers GH-14883 Added the ability to easily append scalar values to array builders GH-15005 Compute Scalar binary (add/subtract/multiply/divide/etc.) and unary arithmetic (abs/neg/sqrt/sign/etc.) has been implemented for the compute package GH-33086 this includes easy functions like compute.Add and compute.Divide etc. Scalar boolean functions like AND/OR/XOR/etc. have been implemented for compute GH-33279 Scalar comparison function kernels have been implemented for compute (equal/greater/greater_equal/less/less_equal) GH-33308 Scalar compute functions are compatible with dictionary encoded arrays by casting them to their value types GH-33502 Parquet Panic when decoding a delta_bit_packed encoded column has been fixed GH-33483 Fixed memory leak from Allocator in pqarrow.WriteArrowToColumn GH-14865 Fixed writer.WriteBatch to properly handle writing encrypted parquet columns and no longer silently fail, but instead propagate an error GH-14940 Java notes Implement support for writing compressed files (#15223) Improve performance by short-circuiting null checks when comparing non null field types (#15106) Several enhancements to dictionary encoding (#14891, (#14902, (#14874) Extend Table to support additional vector types (#14573) Enhance and simplify handling of allocation management by integrating C Data into allocator hierarchy (#14506) Make ComplexCopier agnostic of specific implementation of MapWriter (#14557) Distribute Apple M1 compatible JNI libraries via mavencentral (#14472) Extend Table copy functionality, and support returning copies of individual vectors (#14389) JavaScript notes Bugfixes and dependency updates. Arrow now requires BigInt support. GH-33681 Python notes Compatibility notes: PyArrow now requires pandas &gt;= 1.0 (ARROW-18173) The pyarrow.parquet.ParquetDataset() class now by default uses the new Dataset API under the hood (use_legacy_dataset=False). You can still pass use_legacy_dataset=True to get the legacy implementation, but this option will be removed in a next release (ARROW-16728). New features: Added support for the DataFrame Interchange Protocol for pyarrow.Table (GH-33346). New kernels: list_slice() to slice each list element of a ListArray returning a new ListArray (ARROW-17960). A new filter() method on the Dataset class as additional API to filter a Dataset before consuming it (ARROW-16616). New sort() method for (Chunked)Array and sort_by() method for RecordBatch, providing a convenience on top of the sort_indices kernel (GH-14778), and a new Dataset.sort_by() method (GH-14975). Other improvements: Support for custom metadata of record batches in the IPC read and write APIs (ARROW-16430). Support URIs and the filesystem parameter in pyarrow.parquet.ParquetFile (ARROW-18272) and pyarrow.parquet.write_metadata (ARROW-18225). When writing a dataset to IPC using pyarrow.dataset.write_dataset(), you can now specify IPC specific options, such as compression (ARROW-17991) The pyarrow.array() function now allows to construct a MapArray from a sequence of dicts (in addition to a sequence of tuples) (ARROW-17832). The struct_field() kernel now also accepts field names in addition to integer indices (ARROW-17989). Casting to string is now supported for duration (ARROW-15822) and decimal (ARROW-17458) types, which also means those can now be written to CSV. When writing to CSV, you can now specify the quoting style (GH-14755). The pyarrow.ipc.read_schema() function now accepts a Message object (ARROW-18423). The Time32Scalar, Time64Scalar, Date32Scalar and Date64Scalar classes got a .value attribute to access the underlying integer value, similar to the other date-time related scalars (ARROW-18264) Duration type is now supported in the hash kernels like dictionary_encode (GH-15226). Fix silent overflow when converting datetime.timedelta to duration type (ARROW-15026). Relevant bug fixes: Numpy conversion for ListArray is improved taking into account sliced offset, avoiding increased memory usage (GH-20512 Fix writing files with multi-byte characters in file name (ARROW-18123). R notes map_batches() is lazy by default; it now returns a RecordBatchReader instead of a list of RecordBatch objects unless lazy = FALSE. GH-14521 A substantial reorganisation, rewrite of and addition to, many of the vignettes and README. GH-14514 For more on what’s in the 11.0.0 R package, see the R changelog. Ruby and C GLib notes Ruby Arrow::Table#save now always returns self instead of the result of its raw_recordsGH-15289 Improve the GC-related crash prevention system by guarding the shared objects from GC ARROW-18161 Add Arrow::HalfFloat and raw_records support in Arrow::HalfFloatArray ARROW-18086 Support omitting join keys in Table#join GH-15084 Add support for Arrow::Table.load(uri, schema:) ARROW-15206 Add Arrow::ColumnContainable#column_names (e.g. Arrow::Table#column_names) GH-15085 Add to_arrow_chunked_array methods to support converting to Arrow::ChunkedArray ARROW-18405 C GLib Add garrow_chunked_array_new_empty() GH-33671 Add GArrowProjectNodeOptions GH-33670 Add GADatasetHivePartitioning GH-15257 The signature of garrow_execute_plain_wait() was changed to take the error argument and to return the finished status GH-15254 Add support for half float GH-15168 Add GADatasetFinishOptions GH-15146 Rust notes The Rust projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Rust implementation, see the latest Arrow Rust changelog.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow DataFusion 16.0.0 Project Update</title><link href="https://arrow.apache.org/blog/2023/01/19/datafusion-16.0.0/" rel="alternate" type="text/html" title="Apache Arrow DataFusion 16.0.0 Project Update" /><published>2023-01-19T00:00:00-05:00</published><updated>2023-01-19T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/01/19/datafusion-16.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/01/19/datafusion-16.0.0/"><![CDATA[<!--

-->

<h1 id="introduction">Introduction</h1>

<p><a href="https://arrow.apache.org/datafusion/">DataFusion</a> is an extensible
query execution framework, written in <a href="https://www.rust-lang.org/">Rust</a>,
that uses <a href="https://arrow.apache.org">Apache Arrow</a> as its
in-memory format. It is targeted primarily at developers creating data
intensive analytics, and offers mature
<a href="https://arrow.apache.org/datafusion/user-guide/sql/index.html">SQL support</a>,
a DataFrame API, and many extension points.</p>

<p>Systems based on DataFusion perform very well in benchmarks,
especially considering they operate directly on parquet files rather
than first loading into a specialized format.  Some recent highlights
include <a href="https://benchmark.clickhouse.com/">clickbench</a> and the
<a href="https://www.cloudfuse.io/dashboards/standalone-engines">Cloudfuse.io standalone query
engines</a> page.</p>

<p>DataFusion is also part of a longer term trend, articulated clearly by
<a href="http://www.cs.cmu.edu/~pavlo/">Andy Pavlo</a> in his <a href="https://ottertune.com/blog/2022-databases-retrospective/">2022 Databases
Retrospective</a>.
Database frameworks are proliferating and it is likely that all OLAP
DBMSs and other data heavy applications, such as machine learning,
will <strong>require</strong> a vectorized, highly performant query engine in the next
5 years to remain relevant.  The only practical way to make such
technology so widely available without many millions of dollars of
investment is though open source engine such as DataFusion or
<a href="https://github.com/facebookincubator/velox">Velox</a>.</p>

<p>The rest of this post describes the improvements made to DataFusion
over the last three months and some hints of where we are heading.</p>

<h2 id="community-growth">Community Growth</h2>

<p>We again saw significant growth in the DataFusion community since <a href="https://arrow.apache.org/blog/2022/10/25/datafusion-13.0.0/">our last update</a>. There are some interesting metrics on <a href="https://ossrank.com/p/1573-apache-arrow-datafusion">OSSRank</a>.</p>

<p>The DataFusion 16.0.0 release consists of 543 PRs from 73 distinct contributors, not including all the work that goes into dependencies such as <a href="https://crates.io/crates/arrow">arrow</a>, <a href="https://crates.io/crates/parquet">parquet</a>, and <a href="https://crates.io/crates/object_store">object_store</a>, that much of the same community helps support. Thank you all for your help</p>

<!--
$ git log --pretty=oneline 13.0.0..16.0.0 . | wc -l
     543

$ git shortlog -sn 13.0.0..16.0.0 . | wc -l
      73
-->
<p>Several <a href="https://github.com/apache/arrow-datafusion#known-uses">new systems based on DataFusion</a> were recently added:</p>

<ul>
  <li><a href="https://github.com/GreptimeTeam/greptimedb">Greptime DB</a></li>
  <li><a href="https://synnada.ai/">Synnada</a></li>
  <li><a href="https://github.com/PRQL/prql-query">PRQL</a></li>
  <li><a href="https://github.com/parseablehq/parseable">Parseable</a></li>
  <li><a href="https://github.com/splitgraph/seafowl">SeaFowl</a></li>
</ul>

<h2 id="performance-">Performance 🚀</h2>

<p>Performance and efficiency are core values for
DataFusion. While there is still a gap between DataFusion and the best of
breed, tightly integrated systems such as <a href="https://duckdb.org">DuckDB</a>
and <a href="https://www.pola.rs/">Polars</a>, DataFusion is
closing the gap quickly. Performance highlights from the last three
months:</p>

<ul>
  <li>Up to 30% Faster Sorting and Merging using the new <a href="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1/">Row Format</a></li>
  <li><a href="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/">Advanced predicate pushdown</a>, directly on parquet, directly from object storage, enabling sub millisecond filtering. <!-- Andrew nots: we should really get this turned on by default --></li>
  <li><code class="language-plaintext highlighter-rouge">70%</code> faster <code class="language-plaintext highlighter-rouge">IN</code> expressions evaluation (<a href="https://github.com/apache/arrow-datafusion/issues/4057">#4057</a>)</li>
  <li>Sort and partition aware optimizations (<a href="https://github.com/apache/arrow-datafusion/issues/3969">#3969</a> and  <a href="https://github.com/apache/arrow-datafusion/issues/4691">#4691</a>)</li>
  <li>Filter selectivity analysis (<a href="https://github.com/apache/arrow-datafusion/issues/3868">#3868</a>)</li>
</ul>

<h2 id="runtime-resource-limits">Runtime Resource Limits</h2>

<p>Previously, DataFusion could potentially use unbounded amounts of memory for certain queries that included Sorts, Grouping or Joins.</p>

<p>In version 16.0.0, it is possible to limit DataFusion’s memory usage for Sorting and Grouping. We are looking for help adding similar limiting for Joins as well as expanding our algorithms to optionally spill to secondary storage. See <a href="https://github.com/apache/arrow-datafusion/issues/3941">#3941</a> for more detail.</p>

<h2 id="sql-window-functions">SQL Window Functions</h2>

<p><a href="https://en.wikipedia.org/wiki/Window_function_(SQL)">SQL Window Functions</a> are useful for a variety of analysis and DataFusion’s implementation support expanded significantly:</p>

<ul>
  <li>Custom window frames such as <code class="language-plaintext highlighter-rouge">... OVER (ORDER BY ... RANGE BETWEEN 0.2 PRECEDING AND 0.2 FOLLOWING)</code></li>
  <li>Unbounded window frames such as <code class="language-plaintext highlighter-rouge">... OVER (ORDER BY ... RANGE UNBOUNDED ROWS PRECEDING)</code></li>
  <li>Support for the <code class="language-plaintext highlighter-rouge">NTILE</code> window function (<a href="https://github.com/apache/arrow-datafusion/issues/4676">#4676</a>)</li>
  <li>Support for <code class="language-plaintext highlighter-rouge">GROUPS</code> mode (<a href="https://github.com/apache/arrow-datafusion/issues/4155">#4155</a>)</li>
</ul>

<h1 id="improved-joins">Improved Joins</h1>

<p>Joins are often the most complicated operations to handle well in
analytics systems and DataFusion 16.0.0 offers significant improvements
such as</p>

<ul>
  <li>Cost based optimizer (CBO) automatically reorders join evaluations, selects algorithms (Merge / Hash), and pick build side based on available statistics and join type (<code class="language-plaintext highlighter-rouge">INNER</code>, <code class="language-plaintext highlighter-rouge">LEFT</code>, etc) (<a href="https://github.com/apache/arrow-datafusion/issues/4219">#4219</a>)</li>
  <li>Fast non <code class="language-plaintext highlighter-rouge">column=column</code> equijoins such as <code class="language-plaintext highlighter-rouge">JOIN ON a.x + 5 = b.y</code></li>
  <li>Better performance on non-equijoins (<a href="https://github.com/apache/arrow-datafusion/issues/4562">#4562</a>) <!-- TODO is this a good thing to mention as any time this is usd the query is going to go slow or the data size is small --></li>
</ul>

<h1 id="streaming-execution">Streaming Execution</h1>

<p>One emerging use case for Datafusion is as a foundation for
streaming-first data platforms. An important prerequisite
is support for incremental execution for queries that can be computed
incrementally.</p>

<p>With this release, DataFusion now supports the following streaming features:</p>

<ul>
  <li>Data ingestion from infinite files such as FIFOs (<a href="https://github.com/apache/arrow-datafusion/issues/4694">#4694</a>),</li>
  <li>Detection of pipeline-breaking queries in streaming use cases (<a href="https://github.com/apache/arrow-datafusion/issues/4694">#4694</a>),</li>
  <li>Automatic input swapping for joins so probe side is a data stream (<a href="https://github.com/apache/arrow-datafusion/issues/4694">#4694</a>),</li>
  <li>Intelligent elision of pipeline-breaking sort operations whenever possible (<a href="https://github.com/apache/arrow-datafusion/issues/4691">#4691</a>),</li>
  <li>Incremental execution for more types of queries; e.g. queries involving finite window frames (<a href="https://github.com/apache/arrow-datafusion/issues/4777">#4777</a>).</li>
</ul>

<p>These are a major steps forward, and we plan even more improvements over the next few releases.</p>

<h1 id="better-support-for-distributed-catalogs">Better Support for Distributed Catalogs</h1>

<p>16.0.0 has been enhanced support for asynchronous catalogs (<a href="https://github.com/apache/arrow-datafusion/issues/4607">#4607</a>)
to better support distributed metadata stores such as
<a href="https://delta.io/">Delta.io</a> and <a href="https://iceberg.apache.org/">Apache
Iceberg</a> which require asynchronous I/O
during planning to access remote catalogs. Previously, DataFusion
required synchronous access to all relevant catalog information.</p>

<h1 id="additional-sql-support">Additional SQL Support</h1>
<p>SQL support continues to improve, including some of these highlights:</p>

<ul>
  <li>Add TPC-DS query planning regression tests <a href="https://github.com/apache/arrow-datafusion/issues/4719">#4719</a></li>
  <li>Support for <code class="language-plaintext highlighter-rouge">PREPARE</code> statement <a href="https://github.com/apache/arrow-datafusion/issues/4490">#4490</a></li>
  <li>Automatic coercions ast between Date and Timestamp <a href="https://github.com/apache/arrow-datafusion/issues/4726">#4726</a></li>
  <li>Support type coercion for timestamp and utf8 <a href="https://github.com/apache/arrow-datafusion/issues/4312">#4312</a></li>
  <li>Full support for time32 and time64 literal values (<code class="language-plaintext highlighter-rouge">ScalarValue</code>) <a href="https://github.com/apache/arrow-datafusion/issues/4156">#4156</a></li>
  <li>New functions, incuding <code class="language-plaintext highlighter-rouge">uuid()</code>  <a href="https://github.com/apache/arrow-datafusion/issues/4041">#4041</a>, <code class="language-plaintext highlighter-rouge">current_time</code>  <a href="https://github.com/apache/arrow-datafusion/issues/4054">#4054</a>, <code class="language-plaintext highlighter-rouge">current_date</code> <a href="https://github.com/apache/arrow-datafusion/issues/4022">#4022</a></li>
  <li>Compressed CSV/JSON support <a href="https://github.com/apache/arrow-datafusion/issues/3642">#3642</a></li>
</ul>

<p>The community has also invested in new <a href="https://github.com/apache/arrow-datafusion/blob/master/datafusion/core/tests/sqllogictests/README.md">sqllogic based</a> tests to keep improving DataFusion’s quality with less effort.</p>

<h1 id="plan-serialization-and-substrait">Plan Serialization and Substrait</h1>

<p>DataFusion now supports serialization of physical plans, with a custom protocol buffers format. In addition, we are adding initial support for <a href="https://substrait.io/">Substrait</a>, a Cross-Language Serialization for Relational Algebra</p>

<h1 id="how-to-get-involved">How to Get Involved</h1>

<p>Kudos to everyone in the community who contributed ideas, discussions, bug reports, documentation and code. It is exciting to be building something so cool together!</p>

<p>If you are interested in contributing to DataFusion, we would love to
have you join us. You can try out DataFusion on some of your own
data and projects and let us know how it goes or contribute a PR with
documentation, tests or code. A list of open issues suitable for
beginners is
<a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a>.</p>

<p>Check out our <a href="https://arrow.apache.org/datafusion/community/communication.html">Communication Doc</a> on more
ways to engage with the community.</p>

<h2 id="appendix-contributor-shoutout">Appendix: Contributor Shoutout</h2>

<p>Here is a list of people who have contributed PRs to this project over the last three releases, derived from <code class="language-plaintext highlighter-rouge">git shortlog -sn 13.0.0..16.0.0 .</code> Thank you all!</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   113	Andrew Lamb
    58	jakevin
    46	Raphael Taylor-Davies
    30	Andy Grove
    19	Batuhan Taskaya
    19	Remzi Yang
    17	ygf11
    16	Burak
    16	Jeffrey
    16	Marco Neumann
    14	Kun Liu
    12	Yang Jiang
    10	mingmwang
     9	Daniël Heres
     9	Mustafa akur
     9	comphead
     9	mvanschellebeeck
     9	xudong.w
     7	dependabot[bot]
     7	yahoNanJing
     6	Brent Gardner
     5	AssHero
     4	Jiayu Liu
     4	Wei-Ting Kuo
     4	askoa
     3	André Calado Coroado
     3	Jie Han
     3	Jon Mease
     3	Metehan Yıldırım
     3	Nga Tran
     3	Ruihang Xia
     3	baishen
     2	Berkay Şahin
     2	Dan Harris
     2	Dongyan Zhou
     2	Eduard Karacharov
     2	Kikkon
     2	Liang-Chi Hsieh
     2	Marko Milenković
     2	Martin Grigorov
     2	Roman Nozdrin
     2	Tim Van Wassenhove
     2	r.4ntix
     2	unconsolable
     2	unvalley
     1	Ajaya Agrawal
     1	Alexander Spies
     1	ArkashaJavelin
     1	Artjoms Iskovs
     1	BoredPerson
     1	Christian Salvati
     1	Creampanda
     1	Data Psycho
     1	Francis Du
     1	Francis Le Roy
     1	LFC
     1	Marko Grujic
     1	Matt Willian
     1	Matthijs Brobbel
     1	Max Burke
     1	Mehmet Ozan Kabak
     1	Rito Takeuchi
     1	Roman Zeyde
     1	Vrishabh
     1	Zhang Li
     1	ZuoTiJia
     1	byteink
     1	cfraz89
     1	nbr
     1	xxchan
     1	yujie.zhang
     1	zembunia
     1	哇呜哇呜呀咦耶
</code></pre></div></div>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[Introduction DataFusion is an extensible query execution framework, written in Rust, that uses Apache Arrow as its in-memory format. It is targeted primarily at developers creating data intensive analytics, and offers mature SQL support, a DataFrame API, and many extension points. Systems based on DataFusion perform very well in benchmarks, especially considering they operate directly on parquet files rather than first loading into a specialized format. Some recent highlights include clickbench and the Cloudfuse.io standalone query engines page. DataFusion is also part of a longer term trend, articulated clearly by Andy Pavlo in his 2022 Databases Retrospective. Database frameworks are proliferating and it is likely that all OLAP DBMSs and other data heavy applications, such as machine learning, will require a vectorized, highly performant query engine in the next 5 years to remain relevant. The only practical way to make such technology so widely available without many millions of dollars of investment is though open source engine such as DataFusion or Velox. The rest of this post describes the improvements made to DataFusion over the last three months and some hints of where we are heading. Community Growth We again saw significant growth in the DataFusion community since our last update. There are some interesting metrics on OSSRank. The DataFusion 16.0.0 release consists of 543 PRs from 73 distinct contributors, not including all the work that goes into dependencies such as arrow, parquet, and object_store, that much of the same community helps support. Thank you all for your help Several new systems based on DataFusion were recently added: Greptime DB Synnada PRQL Parseable SeaFowl Performance 🚀 Performance and efficiency are core values for DataFusion. While there is still a gap between DataFusion and the best of breed, tightly integrated systems such as DuckDB and Polars, DataFusion is closing the gap quickly. Performance highlights from the last three months: Up to 30% Faster Sorting and Merging using the new Row Format Advanced predicate pushdown, directly on parquet, directly from object storage, enabling sub millisecond filtering. 70% faster IN expressions evaluation (#4057) Sort and partition aware optimizations (#3969 and #4691) Filter selectivity analysis (#3868) Runtime Resource Limits Previously, DataFusion could potentially use unbounded amounts of memory for certain queries that included Sorts, Grouping or Joins. In version 16.0.0, it is possible to limit DataFusion’s memory usage for Sorting and Grouping. We are looking for help adding similar limiting for Joins as well as expanding our algorithms to optionally spill to secondary storage. See #3941 for more detail. SQL Window Functions SQL Window Functions are useful for a variety of analysis and DataFusion’s implementation support expanded significantly: Custom window frames such as ... OVER (ORDER BY ... RANGE BETWEEN 0.2 PRECEDING AND 0.2 FOLLOWING) Unbounded window frames such as ... OVER (ORDER BY ... RANGE UNBOUNDED ROWS PRECEDING) Support for the NTILE window function (#4676) Support for GROUPS mode (#4155) Improved Joins Joins are often the most complicated operations to handle well in analytics systems and DataFusion 16.0.0 offers significant improvements such as Cost based optimizer (CBO) automatically reorders join evaluations, selects algorithms (Merge / Hash), and pick build side based on available statistics and join type (INNER, LEFT, etc) (#4219) Fast non column=column equijoins such as JOIN ON a.x + 5 = b.y Better performance on non-equijoins (#4562) Streaming Execution One emerging use case for Datafusion is as a foundation for streaming-first data platforms. An important prerequisite is support for incremental execution for queries that can be computed incrementally. With this release, DataFusion now supports the following streaming features: Data ingestion from infinite files such as FIFOs (#4694), Detection of pipeline-breaking queries in streaming use cases (#4694), Automatic input swapping for joins so probe side is a data stream (#4694), Intelligent elision of pipeline-breaking sort operations whenever possible (#4691), Incremental execution for more types of queries; e.g. queries involving finite window frames (#4777). These are a major steps forward, and we plan even more improvements over the next few releases. Better Support for Distributed Catalogs 16.0.0 has been enhanced support for asynchronous catalogs (#4607) to better support distributed metadata stores such as Delta.io and Apache Iceberg which require asynchronous I/O during planning to access remote catalogs. Previously, DataFusion required synchronous access to all relevant catalog information. Additional SQL Support SQL support continues to improve, including some of these highlights: Add TPC-DS query planning regression tests #4719 Support for PREPARE statement #4490 Automatic coercions ast between Date and Timestamp #4726 Support type coercion for timestamp and utf8 #4312 Full support for time32 and time64 literal values (ScalarValue) #4156 New functions, incuding uuid() #4041, current_time #4054, current_date #4022 Compressed CSV/JSON support #3642 The community has also invested in new sqllogic based tests to keep improving DataFusion’s quality with less effort. Plan Serialization and Substrait DataFusion now supports serialization of physical plans, with a custom protocol buffers format. In addition, we are adding initial support for Substrait, a Cross-Language Serialization for Relational Algebra How to Get Involved Kudos to everyone in the community who contributed ideas, discussions, bug reports, documentation and code. It is exciting to be building something so cool together! If you are interested in contributing to DataFusion, we would love to have you join us. You can try out DataFusion on some of your own data and projects and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable for beginners is here. Check out our Communication Doc on more ways to engage with the community. Appendix: Contributor Shoutout Here is a list of people who have contributed PRs to this project over the last three releases, derived from git shortlog -sn 13.0.0..16.0.0 . Thank you all! 113 Andrew Lamb 58 jakevin 46 Raphael Taylor-Davies 30 Andy Grove 19 Batuhan Taskaya 19 Remzi Yang 17 ygf11 16 Burak 16 Jeffrey 16 Marco Neumann 14 Kun Liu 12 Yang Jiang 10 mingmwang 9 Daniël Heres 9 Mustafa akur 9 comphead 9 mvanschellebeeck 9 xudong.w 7 dependabot[bot] 7 yahoNanJing 6 Brent Gardner 5 AssHero 4 Jiayu Liu 4 Wei-Ting Kuo 4 askoa 3 André Calado Coroado 3 Jie Han 3 Jon Mease 3 Metehan Yıldırım 3 Nga Tran 3 Ruihang Xia 3 baishen 2 Berkay Şahin 2 Dan Harris 2 Dongyan Zhou 2 Eduard Karacharov 2 Kikkon 2 Liang-Chi Hsieh 2 Marko Milenković 2 Martin Grigorov 2 Roman Nozdrin 2 Tim Van Wassenhove 2 r.4ntix 2 unconsolable 2 unvalley 1 Ajaya Agrawal 1 Alexander Spies 1 ArkashaJavelin 1 Artjoms Iskovs 1 BoredPerson 1 Christian Salvati 1 Creampanda 1 Data Psycho 1 Francis Du 1 Francis Le Roy 1 LFC 1 Marko Grujic 1 Matt Willian 1 Matthijs Brobbel 1 Max Burke 1 Mehmet Ozan Kabak 1 Rito Takeuchi 1 Roman Zeyde 1 Vrishabh 1 Zhang Li 1 ZuoTiJia 1 byteink 1 cfraz89 1 nbr 1 xxchan 1 yujie.zhang 1 zembunia 1 哇呜哇呜呀咦耶]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 0.1.0 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/01/12/adbc-0.1.0-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.1.0 (Libraries) Release" /><published>2023-01-12T00:00:00-05:00</published><updated>2023-01-12T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/01/12/adbc-0.1.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/01/12/adbc-0.1.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.1.0 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/1"><strong>63
resolved issues</strong></a> from <a href="#contributors"><strong>7 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version
0.1.0.  The <strong>API specification</strong> is versioned separately and is
at version 1.0.0.  For more, see the <a href="/blog/2023/01/05/introducing-arrow-adbc/">introduction to ADBC</a>.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.1.0/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>This initial release includes the following:</p>

<ul>
  <li>Driver manager libraries for C/C++, Go, Java, Python, and Ruby.</li>
  <li>ADBC drivers for SQLite and PostgreSQL, available in C/C++, Go, Python, and Ruby.</li>
  <li>ADBC drivers for Arrow FLight SQL and JDBC, available in Java.</li>
</ul>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn fe96ea9186194af06f4a229b6e5a57815f05f6bd..apache-arrow-adbc-0.1.0
   169	David Li
    12	Sutou Kouhei
     5	Matt Topol
     2	Dewey Dunnington
     1	Ash
     1	Judah Rand
     1	Raúl Cumplido
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>Future development will expand the capabilities of the existing
drivers, as well as adding drivers for more targets.  Drivers for
Flight SQL in C/C++ and Go are being developed.  See the <a href="https://github.com/apache/arrow-adbc/milestone/2">0.2.0
milestone</a> for details.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.1.0 release of the Apache Arrow ADBC libraries. This covers includes 63 resolved issues from 7 distinct contributors. This is a release of the libraries, which are at version 0.1.0. The API specification is versioned separately and is at version 1.0.0. For more, see the introduction to ADBC. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights This initial release includes the following: Driver manager libraries for C/C++, Go, Java, Python, and Ruby. ADBC drivers for SQLite and PostgreSQL, available in C/C++, Go, Python, and Ruby. ADBC drivers for Arrow FLight SQL and JDBC, available in Java. Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn fe96ea9186194af06f4a229b6e5a57815f05f6bd..apache-arrow-adbc-0.1.0 169 David Li 12 Sutou Kouhei 5 Matt Topol 2 Dewey Dunnington 1 Ash 1 Judah Rand 1 Raúl Cumplido Roadmap Future development will expand the capabilities of the existing drivers, as well as adding drivers for more targets. Drivers for Flight SQL in C/C++ and Go are being developed. See the 0.2.0 milestone for details. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Introducing ADBC: Database Access for Apache Arrow</title><link href="https://arrow.apache.org/blog/2023/01/05/introducing-arrow-adbc/" rel="alternate" type="text/html" title="Introducing ADBC: Database Access for Apache Arrow" /><published>2023-01-05T00:00:00-05:00</published><updated>2023-01-05T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/01/05/introducing-arrow-adbc</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/01/05/introducing-arrow-adbc/"><![CDATA[<!--

-->

<p>The Arrow community would like to introduce version 1.0.0 of the <a href="https://github.com/apache/arrow-adbc">Arrow Database Connectivity (ADBC)</a> specification.
ADBC is a columnar, minimal-overhead alternative to JDBC/ODBC for analytical applications.
Or in other words: <strong>ADBC is a single API for getting Arrow data in and out of different databases</strong>.</p>

<h2 id="motivation">Motivation</h2>

<p>Applications often use API standards like <a href="https://docs.oracle.com/javase/tutorial/jdbc/overview/index.html">JDBC</a> and <a href="https://learn.microsoft.com/en-us/sql/odbc/reference/what-is-odbc?view=sql-server-ver16">ODBC</a> to work with databases.
That way, they can code to the same API regardless of the underlying database, saving on development time.
Roughly speaking, when an application executes a query with these APIs:</p>

<figure style="text-align: center;">
  <img src="/img/ADBCFlow1.svg" width="90%" class="img-responsive" alt="A diagram showing the query execution flow." />
  <figcaption>The query execution flow.</figcaption>
</figure>

<ol>
  <li>The application submits a SQL query via the JDBC/ODBC API.</li>
  <li>The query is passed on to the driver.</li>
  <li>The driver translates the query to a database-specific protocol and sends it to the database.</li>
  <li>The database executes the query and returns the result set in a database-specific format.</li>
  <li>The driver translates the result into the format required by the JDBC/ODBC API.</li>
  <li>The application iterates over the result rows using the JDBC/ODBC API.</li>
</ol>

<p>When columnar data comes into play, however, problems arise.
JDBC is a row-oriented API, and while ODBC can support columnar data, the type system and data representation is not a perfect match with Arrow.
So generally, columnar data must be converted to rows in step 5, spending resources without performing “useful” work.</p>

<p>This mismatch is problematic for columnar database systems, such as ClickHouse, Dremio, DuckDB, and Google BigQuery.
On the client side, tools such as Apache Spark and pandas would be better off getting columnar data directly, skipping that conversion.
Otherwise, they’re leaving performance on the table.
At the same time, that conversion isn’t always avoidable.
Row-oriented database systems like PostgreSQL aren’t going away, and these clients will still want to consume data from them.</p>

<p>Developers have a few options:</p>

<ul>
  <li><em>Just use JDBC/ODBC</em>.
These standards are here to stay, and it makes sense for databases to support them for applications that want them.
But when both the database and the application are columnar, that means converting data into rows for JDBC/ODBC, only for the client to convert them right back into columns!
Performance suffers, and developers have to spend time implementing the conversions.</li>
  <li><em>Use JDBC/ODBC-to-Arrow conversion libraries</em>.
Libraries like <a href="https://turbodbc.readthedocs.io/en/latest/">Turbodbc</a> and <a href="https://arrow.apache.org/docs/java/jdbc.html">arrow-jdbc</a> handle row-to-columnar conversions for clients.
But this doesn’t fundamentally solve the problem.
Unnecessary data conversions are still required.</li>
  <li><em>Use vendor-specific protocols</em>.
For some databases, applications can use a database-specific protocol or SDK to directly get Arrow data.
For example, applications could use Dremio via <a href="/blog/2022/02/16/introducing-arrow-flight-sql/">Arrow Flight SQL</a>.
But client applications that want to support multiple database vendors would need to integrate with each of them.
(Look at all the <a href="https://trino.io/docs/current/connector.html">connectors</a> that Trino implements.)
And databases like PostgreSQL don’t offer an option supporting Arrow in the first place.</li>
</ul>

<p>As is, clients must choose between either tedious integration work or leaving performance on the table. We can make this better.</p>

<h2 id="introducing-adbc">Introducing ADBC</h2>

<p>ADBC is an Arrow-based, vendor-neutral API for interacting with databases.
Applications that use ADBC simply receive Arrow data.
They don’t have to do any conversions themselves, and they don’t have to integrate each database’s specific SDK.</p>

<p>Just like JDBC/ODBC, underneath the ADBC API are drivers that translate the API for specific databases.</p>

<ul>
  <li>A driver for an Arrow-native database just passes Arrow data through without conversion.</li>
  <li>A driver for a non-Arrow-native database must convert the data to Arrow.
This saves the application from doing that, and the driver can optimize the conversion for its database.</li>
</ul>

<figure style="text-align: center;">
  <img src="/img/ADBCFlow2.svg" alt="A diagram showing the query execution flow with ADBC." width="90%" class="img-responsive" />
  <figcaption>The query execution flow with two different ADBC drivers.</figcaption>
</figure>

<ol>
  <li>The application submits a SQL query via the ADBC API.</li>
  <li>The query is passed on to the ADBC driver.</li>
  <li>The driver translates the query to a database-specific protocol and sends the query to the database.</li>
  <li>The database executes the query and returns the result set in a database-specific format, which is ideally Arrow data.</li>
  <li>If needed: the driver translates the result into Arrow data.</li>
  <li>The application iterates over batches of Arrow data.</li>
</ol>

<p>The application only deals with one API, and only works with Arrow data.</p>

<p>ADBC API and driver implementations are under development. For example, in Python, the ADBC packages offer a familiar <a href="https://www.python.org/dev/peps/pep-0249/">DBAPI 2.0 (PEP 249)</a>-style interface, with extensions to get Arrow data.
We can get Arrow data out of PostgreSQL easily:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">adbc_driver_postgresql.dbapi</span>

<span class="n">uri</span> <span class="o">=</span> <span class="s">"postgresql://localhost:5432/postgres?user=postgres&amp;password=password"</span>
<span class="k">with</span> <span class="n">adbc_driver_postgresql</span><span class="p">.</span><span class="n">dbapi</span><span class="p">.</span><span class="n">connect</span><span class="p">(</span><span class="n">uri</span><span class="p">)</span> <span class="k">as</span> <span class="n">conn</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">conn</span><span class="p">.</span><span class="n">cursor</span><span class="p">()</span> <span class="k">as</span> <span class="n">cur</span><span class="p">:</span>
        <span class="n">cur</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="s">"SELECT * FROM customer"</span><span class="p">)</span>
        <span class="n">table</span> <span class="o">=</span> <span class="n">cur</span><span class="p">.</span><span class="n">fetch_arrow_table</span><span class="p">()</span>
        <span class="c1"># Process the results
</span></code></pre></div></div>

<p>Or SQLite:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">adbc_driver_sqlite.dbapi</span>

<span class="n">uri</span> <span class="o">=</span> <span class="s">"file:mydb.sqlite"</span>
<span class="k">with</span> <span class="n">adbc_driver_sqlite</span><span class="p">.</span><span class="n">dbapi</span><span class="p">.</span><span class="n">connect</span><span class="p">(</span><span class="n">uri</span><span class="p">)</span> <span class="k">as</span> <span class="n">conn</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">conn</span><span class="p">.</span><span class="n">cursor</span><span class="p">()</span> <span class="k">as</span> <span class="n">cur</span><span class="p">:</span>
        <span class="n">cur</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="s">"SELECT * FROM customer"</span><span class="p">)</span>
        <span class="n">table</span> <span class="o">=</span> <span class="n">cur</span><span class="p">.</span><span class="n">fetch_arrow_table</span><span class="p">()</span>
        <span class="c1"># Process the results
</span></code></pre></div></div>

<p><em>Note: implementations are still under development. See the <a href="https://arrow.apache.org/adbc/">documentation</a> for up-to-date examples.</em></p>

<h2 id="what-about-flight-sql-jdbc-odbc-">What about {Flight SQL, JDBC, ODBC, …}?</h2>

<p>ADBC fills a specific niche that related projects do not address. It is both:</p>

<ul>
  <li><strong>Arrow-native</strong>: ADBC can pass through Arrow data with no overhead thanks to the <a href="/blog/2020/05/03/introducing-arrow-c-data-interface/">C Data Interface</a>.
JDBC is row-oriented, and ODBC has implementation caveats, as discussed, that make it hard to use with Arrow.</li>
  <li><strong>Vendor-agnostic</strong>: ADBC drivers can implement the API using any underlying protocol, while Flight SQL requires server-side support that may not be easy to add.</li>
</ul>

<table class="table table-hover" style="table-layout: fixed">
  <caption>Comparing database APIs and protocols</caption>
  <thead class="thead-dark">
    <tr>
      <th></th>
      <th class="align-top" style="width: 40%" scope="col">Vendor-neutral (database APIs)</th>
      <th class="align-top" style="width: 40%" scope="col">Vendor-specific (database protocols)</th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <th scope="row">Arrow-native</th>
      <td class="table-success"><strong>ADBC</strong></td>
      <td>Arrow Flight SQL<br />BigQuery Storage gRPC protocol</td>
    </tr>
    <tr>
      <th scope="row">Row-oriented</th>
      <td>JDBC<br />ODBC (typically row-oriented)</td>
      <td>PostgreSQL wire protocol<br />Tabular Data Stream (Microsoft SQL Server)</td>
    </tr>
  </tbody>
</table>

<p><strong>ADBC doesn’t intend to replace JDBC or ODBC in general</strong>.
But for applications that just want bulk columnar data access, ADBC lets them avoid data conversion overhead and tedious integration work.</p>

<p>Similarly, within the Arrow project, ADBC does not replace Flight SQL, but instead <em>complements</em> it.
ADBC is an <strong>API</strong> that lets <em>clients</em> work with different databases easily.
Meanwhile, Flight SQL is a <strong>wire protocol</strong> that <em>database servers</em> can implement to simultaneously support ADBC, <a href="/blog/2022/11/01/arrow-flight-sql-jdbc/">JDBC</a>, and ODBC users.</p>

<figure style="text-align: center;">
  <img src="/img/ADBC.svg" alt="ADBC abstracts over protocols and APIs like Flight SQL and JDBC for client applications. Flight SQL provides implementations of APIs like ADBC and JDBC for database servers." width="90%" class="img-responsive" />
</figure>

<h2 id="getting-involved">Getting Involved</h2>

<p>ADBC works as part of the Arrow ecosystem to “cover the bases” for database interaction:</p>

<ul>
  <li>Arrow offers a universal columnar data format,</li>
  <li>Arrow Flight SQL offers a universal wire protocol for database servers,</li>
  <li>and ADBC offers a universal API for database clients.</li>
</ul>

<p>To start using ADBC, see the <a href="https://arrow.apache.org/adbc/">documentation</a> for build instructions and a short tutorial.
(A formal release of the packages is still under way.)
If you’re interested in learning more or contributing, please reach out on the <a href="https://arrow.apache.org/community/">mailing list</a> or on <a href="https://github.com/apache/arrow-adbc/issues">GitHub Issues</a>.</p>

<p>ADBC was only possible with the help and involvement of several Arrow community members and projects.
In particular, we would like to thank members of the <a href="https://duckdb.org/">DuckDB project</a> and the <a href="https://www.r-dbi.org/">R DBI project</a>, who constructed prototypes based on early revisions of the standard and provided feedback on the design.
And ADBC builds on existing Arrow projects, including the <a href="/blog/2020/05/03/introducing-arrow-c-data-interface/">Arrow C Data Interface</a> and <a href="https://github.com/apache/arrow-nanoarrow">nanoarrow</a>.</p>

<p>Thanks to Fernanda Foertter for assistance with some of the diagrams.</p>]]></content><author><name>pmc</name></author><category term="application" /><summary type="html"><![CDATA[The Arrow community would like to introduce version 1.0.0 of the Arrow Database Connectivity (ADBC) specification. ADBC is a columnar, minimal-overhead alternative to JDBC/ODBC for analytical applications. Or in other words: ADBC is a single API for getting Arrow data in and out of different databases. Motivation Applications often use API standards like JDBC and ODBC to work with databases. That way, they can code to the same API regardless of the underlying database, saving on development time. Roughly speaking, when an application executes a query with these APIs: The query execution flow. The application submits a SQL query via the JDBC/ODBC API. The query is passed on to the driver. The driver translates the query to a database-specific protocol and sends it to the database. The database executes the query and returns the result set in a database-specific format. The driver translates the result into the format required by the JDBC/ODBC API. The application iterates over the result rows using the JDBC/ODBC API. When columnar data comes into play, however, problems arise. JDBC is a row-oriented API, and while ODBC can support columnar data, the type system and data representation is not a perfect match with Arrow. So generally, columnar data must be converted to rows in step 5, spending resources without performing “useful” work. This mismatch is problematic for columnar database systems, such as ClickHouse, Dremio, DuckDB, and Google BigQuery. On the client side, tools such as Apache Spark and pandas would be better off getting columnar data directly, skipping that conversion. Otherwise, they’re leaving performance on the table. At the same time, that conversion isn’t always avoidable. Row-oriented database systems like PostgreSQL aren’t going away, and these clients will still want to consume data from them. Developers have a few options: Just use JDBC/ODBC. These standards are here to stay, and it makes sense for databases to support them for applications that want them. But when both the database and the application are columnar, that means converting data into rows for JDBC/ODBC, only for the client to convert them right back into columns! Performance suffers, and developers have to spend time implementing the conversions. Use JDBC/ODBC-to-Arrow conversion libraries. Libraries like Turbodbc and arrow-jdbc handle row-to-columnar conversions for clients. But this doesn’t fundamentally solve the problem. Unnecessary data conversions are still required. Use vendor-specific protocols. For some databases, applications can use a database-specific protocol or SDK to directly get Arrow data. For example, applications could use Dremio via Arrow Flight SQL. But client applications that want to support multiple database vendors would need to integrate with each of them. (Look at all the connectors that Trino implements.) And databases like PostgreSQL don’t offer an option supporting Arrow in the first place. As is, clients must choose between either tedious integration work or leaving performance on the table. We can make this better. Introducing ADBC ADBC is an Arrow-based, vendor-neutral API for interacting with databases. Applications that use ADBC simply receive Arrow data. They don’t have to do any conversions themselves, and they don’t have to integrate each database’s specific SDK. Just like JDBC/ODBC, underneath the ADBC API are drivers that translate the API for specific databases. A driver for an Arrow-native database just passes Arrow data through without conversion. A driver for a non-Arrow-native database must convert the data to Arrow. This saves the application from doing that, and the driver can optimize the conversion for its database. The query execution flow with two different ADBC drivers. The application submits a SQL query via the ADBC API. The query is passed on to the ADBC driver. The driver translates the query to a database-specific protocol and sends the query to the database. The database executes the query and returns the result set in a database-specific format, which is ideally Arrow data. If needed: the driver translates the result into Arrow data. The application iterates over batches of Arrow data. The application only deals with one API, and only works with Arrow data. ADBC API and driver implementations are under development. For example, in Python, the ADBC packages offer a familiar DBAPI 2.0 (PEP 249)-style interface, with extensions to get Arrow data. We can get Arrow data out of PostgreSQL easily: import adbc_driver_postgresql.dbapi uri = "postgresql://localhost:5432/postgres?user=postgres&amp;password=password" with adbc_driver_postgresql.dbapi.connect(uri) as conn: with conn.cursor() as cur: cur.execute("SELECT * FROM customer") table = cur.fetch_arrow_table() # Process the results Or SQLite: import adbc_driver_sqlite.dbapi uri = "file:mydb.sqlite" with adbc_driver_sqlite.dbapi.connect(uri) as conn: with conn.cursor() as cur: cur.execute("SELECT * FROM customer") table = cur.fetch_arrow_table() # Process the results Note: implementations are still under development. See the documentation for up-to-date examples. What about {Flight SQL, JDBC, ODBC, …}? ADBC fills a specific niche that related projects do not address. It is both: Arrow-native: ADBC can pass through Arrow data with no overhead thanks to the C Data Interface. JDBC is row-oriented, and ODBC has implementation caveats, as discussed, that make it hard to use with Arrow. Vendor-agnostic: ADBC drivers can implement the API using any underlying protocol, while Flight SQL requires server-side support that may not be easy to add. Comparing database APIs and protocols Vendor-neutral (database APIs) Vendor-specific (database protocols) Arrow-native ADBC Arrow Flight SQLBigQuery Storage gRPC protocol Row-oriented JDBCODBC (typically row-oriented) PostgreSQL wire protocolTabular Data Stream (Microsoft SQL Server) ADBC doesn’t intend to replace JDBC or ODBC in general. But for applications that just want bulk columnar data access, ADBC lets them avoid data conversion overhead and tedious integration work. Similarly, within the Arrow project, ADBC does not replace Flight SQL, but instead complements it. ADBC is an API that lets clients work with different databases easily. Meanwhile, Flight SQL is a wire protocol that database servers can implement to simultaneously support ADBC, JDBC, and ODBC users. Getting Involved ADBC works as part of the Arrow ecosystem to “cover the bases” for database interaction: Arrow offers a universal columnar data format, Arrow Flight SQL offers a universal wire protocol for database servers, and ADBC offers a universal API for database clients. To start using ADBC, see the documentation for build instructions and a short tutorial. (A formal release of the packages is still under way.) If you’re interested in learning more or contributing, please reach out on the mailing list or on GitHub Issues. ADBC was only possible with the help and involvement of several Arrow community members and projects. In particular, we would like to thank members of the DuckDB project and the R DBI project, who constructed prototypes based on early revisions of the standard and provided feedback on the design. And ADBC builds on existing Arrow projects, including the Arrow C Data Interface and nanoarrow. Thanks to Fernanda Foertter for assistance with some of the diagrams.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Querying Parquet with Millisecond Latency</title><link href="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/" rel="alternate" type="text/html" title="Querying Parquet with Millisecond Latency" /><published>2022-12-26T00:00:00-05:00</published><updated>2022-12-26T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/"><![CDATA[<!--

-->

<h1 id="querying-parquet-with-millisecond-latency">Querying Parquet with Millisecond Latency</h1>
<p><em>Note: this article was originally published on the <a href="https://www.influxdata.com/blog/querying-parquet-millisecond-latency">InfluxData Blog</a>.</em></p>

<p>We believe that querying data in <a href="https://parquet.apache.org/">Apache Parquet</a> files directly can achieve similar or better storage efficiency and query performance than most specialized file formats. While it requires significant engineering effort, the benefits of Parquet’s open format and broad ecosystem support make it the obvious choice for a wide class of data systems.</p>

<p>In this article we explain several advanced techniques needed to query data stored in the Parquet format quickly that we implemented in the <a href="https://docs.rs/parquet/27.0.0/parquet/">Apache Arrow Rust Parquet reader</a>. Together these techniques make the Rust implementation one of, if not the fastest implementation for querying Parquet files — be it on local disk or remote object storage. It is able to query GBs of Parquet in a <a href="https://github.com/tustvold/access-log-bench">matter of milliseconds</a>.</p>

<p>We would like to acknowledge and thank <a href="https://www.influxdata.com/">InfluxData</a> for their support of this work. InfluxData has a deep and continuing commitment to Open source software, and it sponsored much of our time for writing this blog post as well as many contributions as part of building the <a href="https://www.influxdata.com/blog/influxdb-engine/">InfluxDB IOx Storage Engine</a>.</p>

<h1 id="background">Background</h1>

<p><a href="https://parquet.apache.org/">Apache Parquet</a> is an increasingly popular open format for storing <a href="https://www.influxdata.com/glossary/olap/">analytic datasets</a>, and has become the de-facto standard for cost-effective, DBMS-agnostic data storage. Initially created for the Hadoop ecosystem, Parquet’s reach now expands broadly across the data analytics ecosystem due to its compelling combination of:</p>

<ul>
  <li>High compression ratios</li>
  <li>Amenability to commodity blob-storage such as S3</li>
  <li>Broad ecosystem and tooling support</li>
  <li>Portability across many different platforms and tools</li>
  <li>Support for <a href="https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/">arbitrarily structured data</a></li>
</ul>

<p>Increasingly other systems, such as <a href="https://duckdb.org/2021/06/25/querying-parquet.html">DuckDB</a> and <a href="https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html#c-spectrum-overview">Redshift</a> allow querying data stored in Parquet directly, but support is still often a secondary consideration compared to their native (custom) file formats. Such formats include the DuckDB <code class="language-plaintext highlighter-rouge">.duckdb</code> file format, the Apache IOT <a href="https://github.com/apache/iotdb/blob/master/tsfile/README.md">TsFile</a>, the <a href="https://www.vldb.org/pvldb/vol8/p1816-teller.pdf">Gorilla format</a>, and others.</p>

<p>For the first time, access to the same sophisticated query techniques, previously only available in closed source commercial implementations, are now available as open source. The required engineering capacity comes from large, well-run open source projects with global contributor communities, such as <a href="https://arrow.apache.org/">Apache Arrow</a> and <a href="https://impala.apache.org/">Apache Impala</a>.</p>

<h1 id="parquet-file-format">Parquet file format</h1>

<p>Before diving into the details of efficiently reading from <a href="https://www.influxdata.com/glossary/apache-parquet/">Parquet</a>, it is important to understand the file layout. The file format is carefully designed to quickly locate the desired information, skip irrelevant portions, and decode what remains efficiently.</p>

<ul>
  <li>The data in a Parquet file is broken into horizontal slices called <code class="language-plaintext highlighter-rouge">RowGroup</code>s</li>
  <li>Each <code class="language-plaintext highlighter-rouge">RowGroup</code> contains a single <code class="language-plaintext highlighter-rouge">ColumnChunk</code> for each column in the schema</li>
</ul>

<p>For example, the following diagram illustrates a Parquet file with three columns “A”, “B” and “C” stored in two <code class="language-plaintext highlighter-rouge">RowGroup</code>s for a total of 6 <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┓          ┃
┃┃┌ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ┐┌ ─ ─ ─ ─ ─ ─  ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃│             │             ││              ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃│             │             ││              ┃ RowGroup ┃
┃┃             │                            │ ┃     1    ┃
┃┃│             │             ││              ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃└ ─ ─ ─ ─ ─ ─ └ ─ ─ ─ ─ ─ ─ ┘└ ─ ─ ─ ─ ─ ─  ┃          ┃
┃┃ColumnChunk 1  ColumnChunk 2 ColumnChunk 3  ┃          ┃
┃┃ (Column "A")   (Column "B")  (Column "C")  ┃          ┃
┃┗━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┛          ┃
┃┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┓          ┃
┃┃┌ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ┐┌ ─ ─ ─ ─ ─ ─  ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃│             │             ││              ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃│             │             ││              ┃ RowGroup ┃
┃┃             │                            │ ┃     2    ┃
┃┃│             │             ││              ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃└ ─ ─ ─ ─ ─ ─ └ ─ ─ ─ ─ ─ ─ ┘└ ─ ─ ─ ─ ─ ─  ┃          ┃
┃┃ColumnChunk 4  ColumnChunk 5 ColumnChunk 6  ┃          ┃
┃┃ (Column "A")   (Column "B")  (Column "C")  ┃          ┃
┃┗━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┛          ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
</code></pre></div></div>

<p>The logical values for a <code class="language-plaintext highlighter-rouge">ColumnChunk</code> are written using one of the many <a href="https://parquet.apache.org/docs/file-format/data-pages/encodings/">available encodings</a> into one or more Data Pages appended sequentially in the file. At the end of a Parquet file is a footer, which contains important metadata, such as:</p>

<ul>
  <li>The file’s schema information such as column names and types</li>
  <li>The locations of the <code class="language-plaintext highlighter-rouge">RowGroup</code> and <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s in the file</li>
</ul>

<p>The footer may also contain other specialized data structures:</p>

<ul>
  <li>Optional statistics for each <code class="language-plaintext highlighter-rouge">ColumnChunk</code> including min/max values and null counts</li>
  <li>Optional pointers to <a href="https://github.com/apache/parquet-format/blob/54e53e5d7794d383529dd30746378f19a12afd58/src/main/thrift/parquet.thrift#L926-L932">OffsetIndexes</a> containing the location of each individual Page</li>
  <li>Optional pointers to <a href="https://github.com/apache/parquet-format/blob/54e53e5d7794d383529dd30746378f19a12afd58/src/main/thrift/parquet.thrift#L938">ColumnIndex</a> containing row counts and summary statistics for each Page</li>
  <li>Optional pointers to <a href="https://github.com/apache/parquet-format/blob/54e53e5d7794d383529dd30746378f19a12afd58/src/main/thrift/parquet.thrift#L621-L630">BloomFilterData</a>, which can quickly check if a value is present in a <code class="language-plaintext highlighter-rouge">ColumnChunk</code></li>
</ul>

<p>For example, the logical structure of 2 Row Groups and 6 <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s in the previous diagram might be stored in a Parquet file as shown in the following diagram (not to scale). The pages for the <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s come first, followed by the footer. The data, the effectiveness of the encoding scheme, and the settings of the Parquet encoder determine the number of and size of the pages needed for each <code class="language-plaintext highlighter-rouge">ColumnChunk</code>. In this case, <code class="language-plaintext highlighter-rouge">ColumnChunk</code> 1 required 2 pages while <code class="language-plaintext highlighter-rouge">ColumnChunk</code> 6 required only 1 page. In addition to other information, the footer contains the locations of each Data Page and the types of the columns.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 1 ("A")             ◀─┃─ ─ ─│
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃     │
┃ Data Page for ColumnChunk 1 ("A")               ┃
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃     │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 2 ("B")               ┃     │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃     │
┃ Data Page for ColumnChunk 3 ("C")               ┃
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃     │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 3 ("C")               ┃     │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃     │
┃ Data Page for ColumnChunk 3 ("C")               ┃
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃     │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 4 ("A")             ◀─┃─ ─ ─│─ ┐
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃     │  │
┃ Data Page for ColumnChunk 5 ("B")               ┃
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃     │  │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 5 ("B")               ┃     │  │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃     │  │
┃ Data Page for ColumnChunk 5 ("B")               ┃
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃     │  │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 6 ("C")               ┃     │  │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃     │  │
┃┃Footer                                        ┃ ┃
┃┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃     │  │
┃┃ ┃File Metadata                             ┃ ┃ ┃
┃┃ ┃ Schema, etc                              ┃ ┃ ┃     │  │
┃┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓     ┃ ┃ ┃
┃┃ ┃ ┃Row Group 1 Metadata              ┃     ┃ ┃ ┃     │  │
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓             ┃     ┃ ┃ ┃
┃┃ ┃ ┃┃Column "A" Metadata┃ Location of ┃     ┃ ┃ ┃     │  │
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ first Data  ┣ ─ ─ ╋ ╋ ╋ ─ ─
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ Page, row   ┃     ┃ ┃ ┃        │
┃┃ ┃ ┃┃Column "B" Metadata┃ counts,     ┃     ┃ ┃ ┃
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ sizes,      ┃     ┃ ┃ ┃        │
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ min/max     ┃     ┃ ┃ ┃
┃┃ ┃ ┃┃Column "C" Metadata┃ values, etc ┃     ┃ ┃ ┃        │
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛             ┃     ┃ ┃ ┃
┃┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛     ┃ ┃ ┃        │
┃┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓     ┃ ┃ ┃
┃┃ ┃ ┃Row Group 2 Metadata              ┃     ┃ ┃ ┃        │
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ Location of ┃     ┃ ┃ ┃
┃┃ ┃ ┃┃Column "A" Metadata┃ first Data  ┃     ┃ ┃ ┃        │
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ Page, row   ┣ ─ ─ ╋ ╋ ╋ ─ ─ ─ ─
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ counts,     ┃     ┃ ┃ ┃
┃┃ ┃ ┃┃Column "B" Metadata┃ sizes,      ┃     ┃ ┃ ┃
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ min/max     ┃     ┃ ┃ ┃
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ values, etc ┃     ┃ ┃ ┃
┃┃ ┃ ┃┃Column "C" Metadata┃             ┃     ┃ ┃ ┃
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛             ┃     ┃ ┃ ┃
┃┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛     ┃ ┃ ┃
┃┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃
┃┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
</code></pre></div></div>

<p>There are many important criteria to consider when creating Parquet files such as how to optimally order/cluster data and structure it into <code class="language-plaintext highlighter-rouge">RowGroup</code>s and Data Pages. Such “physical design” considerations are complex, worthy of their own series of articles, and not addressed in this blog post. Instead, we focus on how to use the available structure to make queries very fast.</p>

<h1 id="optimizing-queries">Optimizing queries</h1>

<p>In any query processing system, the following techniques generally improve performance:</p>

<ol>
  <li>Reduce the data that must be transferred from secondary storage for processing (reduce I/O)</li>
  <li>Reduce the computational load for decoding the data (reduce CPU)</li>
  <li>Interleave/pipeline the reading and decoding of the data (improve parallelism)</li>
</ol>

<p>The same principles apply to querying Parquet files, as we describe below:</p>

<h1 id="decode-optimization">Decode optimization</h1>

<p>Parquet achieves impressive compression ratios by using <a href="https://parquet.apache.org/docs/file-format/data-pages/encodings/">sophisticated encoding techniques</a> such as run length compression, dictionary encoding, delta encoding, and others. Consequently, the CPU-bound task of decoding can dominate query latency. Parquet readers can use a number of techniques to improve the latency and throughput of this task, as we have done in the Rust implementation.</p>

<h2 id="vectorized-decode">Vectorized decode</h2>

<p>Most analytic systems decode multiple values at a time to a columnar memory format, such as Apache Arrow, rather than processing data row-by-row. This is often called vectorized or columnar processing, and is beneficial because it:</p>

<ul>
  <li>Amortizes dispatch overheads to switch on the type of column being decoded</li>
  <li>Improves cache locality by reading consecutive values from a <code class="language-plaintext highlighter-rouge">ColumnChunk</code></li>
  <li>Often allows multiple values to be decoded in a single instruction.</li>
  <li>Avoid many small heap allocations with a single large allocation, yielding significant savings for variable length types such as strings and byte arrays</li>
</ul>

<p>Thus, Rust Parquet Reader implements specialized decoders for reading Parquet directly into a <a href="https://www.influxdata.com/glossary/column-database/">columnar</a> memory format (Arrow Arrays).</p>

<h2 id="streaming-decode">Streaming decode</h2>

<p>There is no relationship between which rows are stored in which Pages across <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s. For example, the logical values for the 10,000th row may be in the first page of column A and in the third page of column B.</p>

<p>The simplest approach to vectorized decoding, and the one often initially implemented in Parquet decoders, is to decode an entire <code class="language-plaintext highlighter-rouge">RowGroup</code> (or <code class="language-plaintext highlighter-rouge">ColumnChunk</code>) at a time.</p>

<p>However, given Parquet’s high compression ratios, a single <code class="language-plaintext highlighter-rouge">RowGroup</code> may well contain millions of rows. Decoding so many rows at once is non-optimal because it:</p>

<ul>
  <li><strong>Requires large amounts of intermediate RAM</strong>: typical in-memory formats optimized for processing, such as Apache Arrow, require much more than their Parquet-encoded form.</li>
  <li><strong>Increases query latency</strong>: Subsequent processing steps (like filtering or aggregation) can only begin once the entire <code class="language-plaintext highlighter-rouge">RowGroup</code> (or <code class="language-plaintext highlighter-rouge">ColumnChunk</code>) is decoded.</li>
</ul>

<p>As such, the best Parquet readers support “streaming” data out in by producing configurable sized batches of rows on demand. The batch size must be large enough to amortize decode overhead, but small enough for efficient memory usage and to allow downstream processing to begin concurrently while the subsequent batch is decoded.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃
┃ Data Page for ColumnChunk 1 │◀┃─                   ┌── ─── ─── ─── ─── ┐
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃ │   ┏━━━━━━━┓        ┌ ─ ┐ ┌ ─ ┐ ┌ ─ ┐ │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃     ┃       ┃      │                   │
┃ Data Page for ColumnChunk 1 │ ┃ │   ┃       ┃   ─ ▶│ │   │ │   │ │   │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃  ─ ─┃       ┃─ ┤   │  ─ ─   ─ ─   ─ ─  │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃ │   ┃       ┃           A    B     C   │
┃ Data Page for ColumnChunk 2 │◀┃─    ┗━━━━━━━┛  │   └── ─── ─── ─── ─── ┘
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃ │    Parquet
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃      Decoder   │            ...
┃ Data Page for ColumnChunk 3 │ ┃ │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃                │   ┌── ─── ─── ─── ─── ┐
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃ │                    ┌ ─ ┐ ┌ ─ ┐ ┌ ─ ┐ │
┃ Data Page for ColumnChunk 3 │◀┃─               │   │                   │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃                 ─ ▶│ │   │ │   │ │   │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃                    │  ─ ─   ─ ─   ─ ─  │
┃ Data Page for ColumnChunk 3 │ ┃                         A    B     C   │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃                    └── ─── ─── ─── ─── ┘
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

      Parquet file                                    Smaller in memory
                                                         batches for
                                                         processing
</code></pre></div></div>

<p>While streaming is not a complicated feature to explain, the stateful nature of decoding, especially across multiple columns and <a href="https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/">arbitrarily nested data</a>, where the relationship between rows and values is not fixed, requires <a href="https://github.com/apache/arrow-rs/blob/b7af85cb8dfe6887bb3fd43d1d76f659473b6927/parquet/src/arrow/record_reader/mod.rs">complex intermediate buffering</a> and significant engineering effort to handle correctly.</p>

<h2 id="dictionary-preservation">Dictionary preservation</h2>

<p>Dictionary Encoding, also called <a href="https://pandas.pydata.org/docs/user_guide/categorical.html">categorical</a> encoding, is a technique where each value in a column is not stored directly, but instead, an index in a separate list called a “Dictionary” is stored. This technique achieves many of the benefits of <a href="https://en.wikipedia.org/wiki/Third_normal_form#:~:text=Third%20normal%20form%20(3NF)%20is,in%201971%20by%20Edgar%20F.">third normal form</a> for columns that have repeated values (low <a href="https://www.influxdata.com/glossary/cardinality/">cardinality</a>) and is especially effective for columns of strings such as “City”.</p>

<p>The first page in a <code class="language-plaintext highlighter-rouge">ColumnChunk</code> can optionally be a dictionary page, containing a list of values of the column’s type. Subsequent pages within this <code class="language-plaintext highlighter-rouge">ColumnChunk</code> can then encode an index into this dictionary, instead of encoding the values directly.</p>

<p>Given the effectiveness of this encoding, if a Parquet decoder simply decodes dictionary data into the native type, it will inefficiently replicate the same value over and over again, which is especially disastrous for string data. To handle dictionary-encoded data efficiently, the encoding must be preserved during decode. Conveniently, many columnar formats, such as the Arrow <a href="https://docs.rs/arrow/27.0.0/arrow/array/struct.DictionaryArray.html">DictionaryArray</a>, support such compatible encodings.</p>

<p>Preserving dictionary encoding drastically improves performance when reading to an Arrow array, in some cases in excess of <a href="https://github.com/apache/arrow-rs/pull/1180">60x</a>, as well as using significantly less memory.</p>

<p>The major complicating factor for preserving dictionaries is that the dictionaries are stored per <code class="language-plaintext highlighter-rouge">ColumnChunk</code>, and therefore the dictionary changes between <code class="language-plaintext highlighter-rouge">RowGroup</code>s. The reader must automatically recompute a dictionary for batches that span multiple <code class="language-plaintext highlighter-rouge">RowGroup</code>s, while also optimizing for the case that batch sizes divide evenly into the number of rows per <code class="language-plaintext highlighter-rouge">RowGroup</code>. Additionally a column may be only <a href="https://github.com/apache/parquet-format/blob/111dbdcf8eff2e9f8e0d4e958cecbc7e00028aca/README.md?plain=1#L194-L199">partly dictionary encoded</a>, further complicating implementation. More information on this technique and its complications can be found in the <a href="https://arrow.apache.org/blog/2019/09/05/faster-strings-cpp-parquet/">blog post</a> on applying this technique to the C++ Parquet reader.</p>

<h1 id="projection-pushdown">Projection pushdown</h1>

<p>The most basic Parquet optimization, and the one most commonly described for Parquet files, is <em>projection pushdown</em>, which reduces both I/Oand CPU requirements. Projection in this context means “selecting some but not all of the columns.” Given how Parquet organizes data, it is straightforward to read and decode only the <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s required for the referenced columns.</p>

<p>For example, consider a SQL query of the form</p>

<pre><code class="language-SQL">SELECT B from table where A &gt; 35
</code></pre>

<p>This query only needs data for columns A and B (and not C) and the projection can be “pushed down” to the Parquet reader.</p>

<p>Specifically, using the information in the footer, the Parquet reader can entirely skip fetching (I/O) and decoding (CPU) the Data Pages that store data for column C (<code class="language-plaintext highlighter-rouge">ColumnChunk</code> 3 and <code class="language-plaintext highlighter-rouge">ColumnChunk</code> 6 in our example).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                             ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
                             ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ┌─────▶ Data Page for ColumnChunk 1 ("A") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ├─────▶ Data Page for ColumnChunk 1 ("A") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ├─────▶ Data Page for ColumnChunk 2 ("B") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       │     ┃ Data Page for ColumnChunk 3 ("C") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
   A query that        │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
  accesses only        │     ┃ Data Page for ColumnChunk 3 ("C") ┃
 columns A and B       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
can read only the      │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
 relevant pages,  ─────┤     ┃ Data Page for ColumnChunk 3 ("C") ┃
skipping any Data      │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
Page for column C      │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ├─────▶ Data Page for ColumnChunk 4 ("A") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ├─────▶ Data Page for ColumnChunk 5 ("B") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ├─────▶ Data Page for ColumnChunk 5 ("B") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       └─────▶ Data Page for ColumnChunk 5 ("B") ┃
                             ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                             ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                             ┃ Data Page for ColumnChunk 6 ("C") ┃
                             ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                             ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
</code></pre></div></div>

<h1 id="predicate-pushdown">Predicate pushdown</h1>

<p>Similar to projection pushdown, <strong>predicate</strong> pushdown also avoids fetching and decoding data from Parquet files, but does so using filter expressions. This technique typically requires closer integration with a query engine such as <a href="https://arrow.apache.org/datafusion/">DataFusion</a>, to determine valid predicates and evaluate them during the scan. Unfortunately without careful API design, the Parquet decoder and query engine can end up tightly coupled, preventing reuse (e.g. there are different Impala and Spark implementations in <a href="https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/cdh_ig_predicate_pushdown_parquet.html#concept_pgs_plb_mgb">Cloudera Parquet Predicate Pushdown docs</a>). The Rust Parquet reader uses the <a href="https://docs.rs/parquet/27.0.0/parquet/arrow/arrow_reader/struct.RowSelector.html">RowSelection</a> API to avoid this coupling.</p>

<h2 id="rowgroup-pruning"><code class="language-plaintext highlighter-rouge">RowGroup</code> pruning</h2>

<p>The simplest form of predicate pushdown, supported by many Parquet based query engines, uses the statistics stored in the footer to skip entire <code class="language-plaintext highlighter-rouge">RowGroup</code>s. We call this operation <code class="language-plaintext highlighter-rouge">RowGroup</code> <em>pruning</em>, and it is analogous to <a href="https://docs.oracle.com/database/121/VLDBG/GUID-E677C85E-C5E3-4927-B3DF-684007A7B05D.htm#VLDBG00401">partition pruning</a> in many classical data warehouse systems.</p>

<p>For the example query above, if the maximum value for A in a particular <code class="language-plaintext highlighter-rouge">RowGroup</code> is less than 35, the decoder can skip fetching and decoding any <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s from that <strong>entire</strong> <code class="language-plaintext highlighter-rouge">RowGroup</code>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃Row Group 1 Metadata                      ┃
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃
┃ ┃Column "A" Metadata    Min:0 Max:15   ┃◀╋ ┐
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃       Using the min
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ │     and max values
┃ ┃Column "B" Metadata                   ┃ ┃       from the
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ │     metadata,
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃       RowGroup 1  can
┃ ┃Column "C" Metadata                   ┃ ┃ ├ ─ ─ be entirely
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃       skipped
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │     (pruned) when
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓       searching for
┃Row Group 2 Metadata                      ┃ │     rows with A &gt;
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃       35,
┃ ┃Column "A" Metadata   Min:10 Max:50   ┃◀╋ ┘
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃
┃ ┃Column "B" Metadata                   ┃ ┃
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃
┃ ┃Column "C" Metadata                   ┃ ┃
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
</code></pre></div></div>

<p>Note that pruning on minimum and maximum values is effective for many data layouts and column types, but not all. Specifically, it is not as effective for columns with many distinct pseudo-random values (e.g. identifiers or uuids). Thankfully for this use case, Parquet also supports per <code class="language-plaintext highlighter-rouge">ColumnChunk</code> <a href="https://github.com/apache/parquet-format/blob/master/BloomFilter.md">Bloom Filters</a>. We are actively working on <a href="https://github.com/apache/arrow-rs/issues/3023">adding bloom filter</a> support in Apache Rust’s implementation.</p>

<h2 id="page-pruning">Page pruning</h2>

<p>A more sophisticated form of predicate pushdown uses the optional <a href="https://github.com/apache/parquet-format/blob/master/PageIndex.md">page index</a> in the footer metadata to rule out entire Data Pages. The decoder decodes only the corresponding rows from other columns, often skipping entire pages.</p>

<p>The fact that pages in different <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s often contain different numbers of rows, due to various reasons, complicates this optimization. While the page index may identify the needed pages from one column, pruning a page from one column doesn’t immediately rule out entire pages in other columns.</p>

<p>Page pruning proceeds as follows:</p>

<ul>
  <li>Uses the predicates in combination with the page index to identify pages to skip</li>
  <li>Uses the offset index to determine what row ranges correspond to non-skipped pages</li>
  <li>Computes the intersection of ranges across non-skipped pages, and decodes only those rows</li>
</ul>

<p>This last point is highly non-trivial to implement, especially for nested lists where <a href="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/">a single row may correspond to multiple values</a>. Fortunately, the Rust Parquet reader hides this complexity internally, and can decode arbitrary <a href="https://docs.rs/parquet/27.0.0/parquet/arrow/arrow_reader/struct.RowSelection.html">RowSelections</a>.</p>

<p>For example, to scan Columns A and B, stored in 5 Data Pages as shown in the figure below:</p>

<p>If the predicate is <code class="language-plaintext highlighter-rouge">A &gt; 35</code>,</p>

<ul>
  <li>Page 1 is pruned using the page index (max value is <code class="language-plaintext highlighter-rouge">20</code>), leaving a RowSelection of  [200-&gt;onwards],</li>
  <li>Parquet reader skips Page 3 entirely (as its last row index is <code class="language-plaintext highlighter-rouge">99</code>)</li>
  <li>(Only) the relevant rows are read by reading pages 2, 4, and 5.</li>
</ul>

<p>If the predicate is instead <code class="language-plaintext highlighter-rouge">A &gt; 35 AND B = "F"</code> the page index is even more effective</p>

<ul>
  <li>Using <code class="language-plaintext highlighter-rouge">A &gt; 35</code>, yields a RowSelection of <code class="language-plaintext highlighter-rouge">[200-&gt;onwards]</code> as before</li>
  <li>Using <code class="language-plaintext highlighter-rouge">B = "F"</code>, on the remaining Page 4 and Page 5 of B, yields a RowSelection of <code class="language-plaintext highlighter-rouge">[100-244]</code></li>
  <li>Intersecting the two RowSelections leaves a combined RowSelection <code class="language-plaintext highlighter-rouge">[200-244]</code></li>
  <li>Parquet reader only decodes those 50 rows from Page 2 and Page 4.</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━
   ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   ┃
┃     ┌──────────────┐  │     ┌──────────────┐  │  ┃
┃  │  │              │     │  │              │     ┃
┃     │              │  │     │     Page     │  │
   │  │              │     │  │      3       │     ┃
┃     │              │  │     │   min: "A"   │  │  ┃
┃  │  │              │     │  │   max: "C"   │     ┃
┃     │     Page     │  │     │ first_row: 0 │  │
   │  │      1       │     │  │              │     ┃
┃     │   min: 10    │  │     └──────────────┘  │  ┃
┃  │  │   max: 20    │     │  ┌──────────────┐     ┃
┃     │ first_row: 0 │  │     │              │  │
   │  │              │     │  │     Page     │     ┃
┃     │              │  │     │      4       │  │  ┃
┃  │  │              │     │  │   min: "D"   │     ┃
┃     │              │  │     │   max: "G"   │  │
   │  │              │     │  │first_row: 100│     ┃
┃     └──────────────┘  │     │              │  │  ┃
┃  │  ┌──────────────┐     │  │              │     ┃
┃     │              │  │     └──────────────┘  │
   │  │     Page     │     │  ┌──────────────┐     ┃
┃     │      2       │  │     │              │  │  ┃
┃  │  │   min: 30    │     │  │     Page     │     ┃
┃     │   max: 40    │  │     │      5       │  │
   │  │first_row: 200│     │  │   min: "H"   │     ┃
┃     │              │  │     │   max: "Z"   │  │  ┃
┃  │  │              │     │  │first_row: 250│     ┃
┃     └──────────────┘  │     │              │  │
   │                       │  └──────────────┘     ┃
┃   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃       ColumnChunk            ColumnChunk         ┃
┃            A                      B
 ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━┛
</code></pre></div></div>

<p>Support for reading and writing these indexes from Arrow C++, and by extension pyarrow/pandas, is tracked in <a href="https://issues.apache.org/jira/browse/PARQUET-1404">PARQUET-1404</a>.</p>

<h2 id="late-materialization">Late materialization</h2>

<p>The two previous forms of predicate pushdown only operated on metadata stored for <code class="language-plaintext highlighter-rouge">RowGroup</code>s, <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s, and Data Pages prior to decoding values. However, the same techniques also extend to values of one or more columns <em>after</em> decoding them but prior to decoding other columns, which is often called “late materialization”.</p>

<p>This technique is especially effective when:</p>

<ul>
  <li>The predicate is very selective, i.e. filters out large numbers of rows</li>
  <li>Each row is large, either due to wide rows (e.g. JSON blobs) or many columns</li>
  <li>The selected data is clustered together</li>
  <li>The columns required by the predicate are relatively inexpensive to decode, e.g. PrimitiveArray / DictionaryArray</li>
</ul>

<p>There is additional discussion about the benefits of this technique in <a href="https://issues.apache.org/jira/browse/SPARK-36527">SPARK-36527</a> and<a href="https://docs.cloudera.com/cdw-runtime/cloud/impala-reference/topics/impala-lazy-materialization.html"> Impala</a>.</p>

<p>For example, given the predicate <code class="language-plaintext highlighter-rouge">A &gt; 35 AND B = "F"</code> from above where the engine uses the page index to determine only 50 rows within RowSelection of [100-244] could match, using late materialization, the Parquet decoder:</p>

<ul>
  <li>Decodes the 50 values of Column A</li>
  <li>Evaluates  <code class="language-plaintext highlighter-rouge">A &gt; 35 </code> on those 50 values</li>
  <li>In this case, only 5 rows pass, resulting in the RowSelection:
    <ul>
      <li>RowSelection[205-206]</li>
      <li>RowSelection[238-240]</li>
    </ul>
  </li>
  <li>Only decodes the 5 rows for Column B for those selections</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  Row Index
             ┌────────────────────┐            ┌────────────────────┐
       200   │         30         │            │        "F"         │
             └────────────────────┘            └────────────────────┘
                      ...                               ...
             ┌────────────────────┐            ┌────────────────────┐
       205   │         37         │─ ─ ─ ─ ─ ─▶│        "F"         │
             ├────────────────────┤            ├────────────────────┤
       206   │         36         │─ ─ ─ ─ ─ ─▶│        "G"         │
             └────────────────────┘            └────────────────────┘
                      ...                               ...
             ┌────────────────────┐            ┌────────────────────┐
       238   │         36         │─ ─ ─ ─ ─ ─▶│        "F"         │
             ├────────────────────┤            ├────────────────────┤
       239   │         36         │─ ─ ─ ─ ─ ─▶│        "G"         │
             ├────────────────────┤            ├────────────────────┤
       240   │         40         │─ ─ ─ ─ ─ ─▶│        "G"         │
             └────────────────────┘            └────────────────────┘
                      ...                               ...
             ┌────────────────────┐            ┌────────────────────┐
      244    │         26         │            │        "D"         │
             └────────────────────┘            └────────────────────┘


                   Column A                          Column B
                    Values                            Values
</code></pre></div></div>

<p>In certain cases, such as our example where B stores single character values, the cost of late materialization machinery can outweigh the savings in decoding. However, the savings can be substantial when some of the conditions listed above are fulfilled. The query engine must decide which predicates to push down and in which order to apply them for optimal results.</p>

<p>While it is outside the scope of this document, the same technique can be applied for multiple predicates as well as predicates on multiple columns. See the <a href="https://docs.rs/parquet/latest/parquet/arrow/arrow_reader/struct.RowFilter.html">RowFilter</a> interface in the Parquet crate for more information, and the <a href="https://github.com/apache/arrow-datafusion/blob/58b43f5c0b629be49a3efa0e37052ec51d9ba3fe/datafusion/core/src/physical_plan/file_format/parquet/row_filter.rs#L40-L70">row_filter</a> implementation in DataFusion.</p>

<h1 id="io-pushdown">I/O pushdown</h1>

<p>While Parquet was designed for efficient access on the <a href="https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html">HDFS distributed file system</a>, it works very well with commodity blob storage systems such as AWS S3 as they have very similar characteristics:</p>

<ul>
  <li><strong>Relatively slow “random access” reads</strong>: it is much more efficient to read large (MBs) sections of data in each request than issue many requests for smaller portions</li>
  <li><strong>Significant latency before retrieving the first byte</strong></li>
  <li><strong>High per-request cost:</strong> Often billed per request, regardless of number of bytes read, which incentivizes fewer requests that each read a large contiguous section of data.</li>
</ul>

<p>To read optimally from such systems, a Parquet reader must:</p>

<ol>
  <li>Minimize the number of I/O requests, while also applying the various pushdown techniques to avoid fetching large amounts of unused data.</li>
  <li>Integrate with the appropriate task scheduling mechanism to interleave I/O and processing on the data that is fetched to avoid pipeline bottlenecks.</li>
</ol>

<p>As these are substantial engineering and integration challenges, many Parquet readers still require the files to be fetched in their entirety to local storage.</p>

<p>Fetching the entire files in order to process them is not ideal for several reasons:</p>

<ol>
  <li><strong>High Latency</strong>: Decoding cannot begin until the entire file is fetched (Parquet metadata is at the end of the file, so the decoder must see the end prior to decoding the rest)</li>
  <li><strong>Wasted work</strong>: Fetching the entire file fetches all necessary data, but also potentially lots of unnecessary data that will be skipped after reading the footer. This increases the cost unnecessarily.</li>
  <li><strong>Requires costly “locally attached” storage (or memory)</strong>: Many cloud environments do not offer computing resources with locally attached storage – they either rely on expensive network block storage such as AWS EBS or else restrict local storage to certain classes of VMs.</li>
</ol>

<p>Avoiding the need to buffer the entire file requires a sophisticated Parquet decoder, integrated with the I/O subsystem, that can initially fetch and decode the metadata followed by ranged fetches for the relevant data blocks, interleaved with the decoding of Parquet data. This optimization requires careful engineering to fetch large enough blocks of data from the object store that the per request overhead doesn’t dominate gains from reducing the bytes transferred. <a href="https://issues.apache.org/jira/browse/SPARK-36529">SPARK-36529</a> describes the challenges of sequential processing in more detail.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                       ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
                                                                │
                       │
               Step 1: Fetch                                    │
 Parquet       Parquet metadata
 file on ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━▼━━━━━━━┓
 Remote  ┃      ▒▒▒▒▒▒▒▒▒▒          ▒▒▒▒▒▒▒▒▒▒               ░░░░░░░░░░ ┃
 Object  ┃      ▒▒▒data▒▒▒          ▒▒▒data▒▒▒               ░metadata░ ┃
  Store  ┃      ▒▒▒▒▒▒▒▒▒▒          ▒▒▒▒▒▒▒▒▒▒               ░░░░░░░░░░ ┃
         ┗━━━━━━━━━━━▲━━━━━━━━━━━━━━━━━━━━━▲━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
                     │                     └ ─ ─ ─
                                                  │
                     │                   Step 2: Fetch only
                      ─ ─ ─ ─ ─ ─ ─ ─ ─ relevant data blocks
</code></pre></div></div>

<p>Not included in this diagram picture are details like coalescing requests and ensuring minimum request sizes needed for an actual implementation.</p>

<p>The Rust Parquet crate provides an async Parquet reader, to efficiently read from any <a href="https://docs.rs/parquet/latest/parquet/arrow/async_reader/trait.AsyncFileReader.html">AsyncFileReader</a> that:</p>

<ul>
  <li>Efficiently reads from any storage medium that supports range requests</li>
  <li>Integrates with Rust’s futures ecosystem to avoid blocking threads waiting on network I/O <a href="https://www.influxdata.com/blog/using-rustlangs-async-tokio-runtime-for-cpu-bound-tasks/">and easily can interleave CPU and network </a></li>
  <li>Requests multiple ranges simultaneously, to allow the implementation to coalesce adjacent ranges, fetch ranges in parallel, etc.</li>
  <li>Uses the pushdown techniques described previously to eliminate fetching data where possible</li>
  <li>Integrates easily with the Apache Arrow <a href="https://docs.rs/object_store/latest/object_store/">object_store</a> crate which you can read more about <a href="https://www.influxdata.com/blog/rust-object-store-donation/">here</a></li>
</ul>

<p>To give a sense of what is possible, the following picture shows a timeline of fetching the footer metadata from remote files, using that metadata to determine what Data Pages to read, and then fetching data and decoding simultaneously. This process often must be done for more than one file at a time in order to match network latency, bandwidth, and available CPU.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                           begin
          metadata        read of   end read
            read  ─ ─ ─ ┐   data    of data          │
 begin    complete         block     block
read of                 │   │        │               │
metadata  ─ ─ ─ ┐                                       At any time, there are
             │          │   │        │               │     multiple network
             │  ▼       ▼   ▼        ▼                  requests outstanding to
  file 1     │ ░░░░░░░░░░   ▒▒▒read▒▒▒   ▒▒▒read▒▒▒  │    hide the individual
             │ ░░░read░░░   ▒▒▒data▒▒▒   ▒▒▒data▒▒▒        request latency
             │ ░metadata░                         ▓▓decode▓▓
             │ ░░░░░░░░░░                         ▓▓▓data▓▓▓
             │                                       │
             │
             │ ░░░░░░░░░░  ▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒read▒▒▒▒│▒▒▒▒▒▒▒▒▒▒▒▒▒▒
   file 2    │ ░░░read░░░  ▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒data▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒
             │ ░metadata░                            │              ▓▓▓▓▓decode▓▓▓▓▓▓
             │ ░░░░░░░░░░                                           ▓▓▓▓▓▓data▓▓▓▓▓▓▓
             │                                       │
             │
             │                                     ░░│░░░░░░░  ▒▒▒read▒▒▒  ▒▒▒▒read▒▒▒▒▒
   file 3    │                                     ░░░read░░░  ▒▒▒data▒▒▒  ▒▒▒▒data▒▒▒▒▒      ...
             │                                     ░m│tadata░            ▓▓decode▓▓
             │                                     ░░░░░░░░░░            ▓▓▓data▓▓▓
             └───────────────────────────────────────┼──────────────────────────────▶Time


                                                     │
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>We hope you enjoyed reading about the Parquet file format, and the various techniques used to quickly query parquet files.</p>

<p>We believe that the reason most open source implementations of Parquet do not have the breadth of features described in this post is that it takes a monumental effort, that was previously only possible at well-financed commercial enterprises which kept their implementations closed source.</p>

<p>However, with the growth and quality of the Apache Arrow community, both Rust practitioners and the wider Arrow community, our ability to collaborate and build a cutting-edge open source implementation is exhilarating and immensely satisfying. The technology described in this blog is the result of the contributions of many engineers spread across companies, hobbyists, and the world in several repositories, notably <a href="https://github.com/apache/arrow-datafusion">Apache Arrow DataFusion</a>, <a href="https://github.com/apache/arrow-rs">Apache Arrow</a> and <a href="https://github.com/apache/arrow-ballista">Apache Arrow Ballista.</a></p>

<p>If you are interested in joining the DataFusion Community, please <a href="https://arrow.apache.org/datafusion/contributor-guide/communication.html">get in touch</a>.</p>]]></content><author><name>tustvold and alamb</name></author><category term="parquet" /><summary type="html"><![CDATA[Querying Parquet with Millisecond Latency Note: this article was originally published on the InfluxData Blog. We believe that querying data in Apache Parquet files directly can achieve similar or better storage efficiency and query performance than most specialized file formats. While it requires significant engineering effort, the benefits of Parquet’s open format and broad ecosystem support make it the obvious choice for a wide class of data systems. In this article we explain several advanced techniques needed to query data stored in the Parquet format quickly that we implemented in the Apache Arrow Rust Parquet reader. Together these techniques make the Rust implementation one of, if not the fastest implementation for querying Parquet files — be it on local disk or remote object storage. It is able to query GBs of Parquet in a matter of milliseconds. We would like to acknowledge and thank InfluxData for their support of this work. InfluxData has a deep and continuing commitment to Open source software, and it sponsored much of our time for writing this blog post as well as many contributions as part of building the InfluxDB IOx Storage Engine. Background Apache Parquet is an increasingly popular open format for storing analytic datasets, and has become the de-facto standard for cost-effective, DBMS-agnostic data storage. Initially created for the Hadoop ecosystem, Parquet’s reach now expands broadly across the data analytics ecosystem due to its compelling combination of: High compression ratios Amenability to commodity blob-storage such as S3 Broad ecosystem and tooling support Portability across many different platforms and tools Support for arbitrarily structured data Increasingly other systems, such as DuckDB and Redshift allow querying data stored in Parquet directly, but support is still often a secondary consideration compared to their native (custom) file formats. Such formats include the DuckDB .duckdb file format, the Apache IOT TsFile, the Gorilla format, and others. For the first time, access to the same sophisticated query techniques, previously only available in closed source commercial implementations, are now available as open source. The required engineering capacity comes from large, well-run open source projects with global contributor communities, such as Apache Arrow and Apache Impala. Parquet file format Before diving into the details of efficiently reading from Parquet, it is important to understand the file layout. The file format is carefully designed to quickly locate the desired information, skip irrelevant portions, and decode what remains efficiently. The data in a Parquet file is broken into horizontal slices called RowGroups Each RowGroup contains a single ColumnChunk for each column in the schema For example, the following diagram illustrates a Parquet file with three columns “A”, “B” and “C” stored in two RowGroups for a total of 6 ColumnChunks. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┓ ┃ ┃┃┌ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ┐┌ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ RowGroup ┃ ┃┃ │ │ ┃ 1 ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃└ ─ ─ ─ ─ ─ ─ └ ─ ─ ─ ─ ─ ─ ┘└ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ColumnChunk 1 ColumnChunk 2 ColumnChunk 3 ┃ ┃ ┃┃ (Column "A") (Column "B") (Column "C") ┃ ┃ ┃┗━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┛ ┃ ┃┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┓ ┃ ┃┃┌ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ┐┌ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ RowGroup ┃ ┃┃ │ │ ┃ 2 ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃└ ─ ─ ─ ─ ─ ─ └ ─ ─ ─ ─ ─ ─ ┘└ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ColumnChunk 4 ColumnChunk 5 ColumnChunk 6 ┃ ┃ ┃┃ (Column "A") (Column "B") (Column "C") ┃ ┃ ┃┗━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┛ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ The logical values for a ColumnChunk are written using one of the many available encodings into one or more Data Pages appended sequentially in the file. At the end of a Parquet file is a footer, which contains important metadata, such as: The file’s schema information such as column names and types The locations of the RowGroup and ColumnChunks in the file The footer may also contain other specialized data structures: Optional statistics for each ColumnChunk including min/max values and null counts Optional pointers to OffsetIndexes containing the location of each individual Page Optional pointers to ColumnIndex containing row counts and summary statistics for each Page Optional pointers to BloomFilterData, which can quickly check if a value is present in a ColumnChunk For example, the logical structure of 2 Row Groups and 6 ColumnChunks in the previous diagram might be stored in a Parquet file as shown in the following diagram (not to scale). The pages for the ColumnChunks come first, followed by the footer. The data, the effectiveness of the encoding scheme, and the settings of the Parquet encoder determine the number of and size of the pages needed for each ColumnChunk. In this case, ColumnChunk 1 required 2 pages while ColumnChunk 6 required only 1 page. In addition to other information, the footer contains the locations of each Data Page and the types of the columns. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 1 ("A") ◀─┃─ ─ ─│ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ ┃ Data Page for ColumnChunk 1 ("A") ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 2 ("B") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ ┃ Data Page for ColumnChunk 3 ("C") ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 3 ("C") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ ┃ Data Page for ColumnChunk 3 ("C") ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 4 ("A") ◀─┃─ ─ ─│─ ┐ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ │ ┃ Data Page for ColumnChunk 5 ("B") ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 5 ("B") ┃ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ │ ┃ Data Page for ColumnChunk 5 ("B") ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 6 ("C") ┃ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ │ │ ┃┃Footer ┃ ┃ ┃┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ │ │ ┃┃ ┃File Metadata ┃ ┃ ┃ ┃┃ ┃ Schema, etc ┃ ┃ ┃ │ │ ┃┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃ ┃┃ ┃ ┃Row Group 1 Metadata ┃ ┃ ┃ ┃ │ │ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column "A" Metadata┃ Location of ┃ ┃ ┃ ┃ │ │ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ first Data ┣ ─ ─ ╋ ╋ ╋ ─ ─ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ Page, row ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┃Column "B" Metadata┃ counts, ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ sizes, ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ min/max ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column "C" Metadata┃ values, etc ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ ┃ ┃┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ │ ┃┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃ ┃┃ ┃ ┃Row Group 2 Metadata ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ Location of ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column "A" Metadata┃ first Data ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ Page, row ┣ ─ ─ ╋ ╋ ╋ ─ ─ ─ ─ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ counts, ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column "B" Metadata┃ sizes, ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ min/max ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ values, etc ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column "C" Metadata┃ ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ ┃ ┃┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ ┃┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ There are many important criteria to consider when creating Parquet files such as how to optimally order/cluster data and structure it into RowGroups and Data Pages. Such “physical design” considerations are complex, worthy of their own series of articles, and not addressed in this blog post. Instead, we focus on how to use the available structure to make queries very fast. Optimizing queries In any query processing system, the following techniques generally improve performance: Reduce the data that must be transferred from secondary storage for processing (reduce I/O) Reduce the computational load for decoding the data (reduce CPU) Interleave/pipeline the reading and decoding of the data (improve parallelism) The same principles apply to querying Parquet files, as we describe below: Decode optimization Parquet achieves impressive compression ratios by using sophisticated encoding techniques such as run length compression, dictionary encoding, delta encoding, and others. Consequently, the CPU-bound task of decoding can dominate query latency. Parquet readers can use a number of techniques to improve the latency and throughput of this task, as we have done in the Rust implementation. Vectorized decode Most analytic systems decode multiple values at a time to a columnar memory format, such as Apache Arrow, rather than processing data row-by-row. This is often called vectorized or columnar processing, and is beneficial because it: Amortizes dispatch overheads to switch on the type of column being decoded Improves cache locality by reading consecutive values from a ColumnChunk Often allows multiple values to be decoded in a single instruction. Avoid many small heap allocations with a single large allocation, yielding significant savings for variable length types such as strings and byte arrays Thus, Rust Parquet Reader implements specialized decoders for reading Parquet directly into a columnar memory format (Arrow Arrays). Streaming decode There is no relationship between which rows are stored in which Pages across ColumnChunks. For example, the logical values for the 10,000th row may be in the first page of column A and in the third page of column B. The simplest approach to vectorized decoding, and the one often initially implemented in Parquet decoders, is to decode an entire RowGroup (or ColumnChunk) at a time. However, given Parquet’s high compression ratios, a single RowGroup may well contain millions of rows. Decoding so many rows at once is non-optimal because it: Requires large amounts of intermediate RAM: typical in-memory formats optimized for processing, such as Apache Arrow, require much more than their Parquet-encoded form. Increases query latency: Subsequent processing steps (like filtering or aggregation) can only begin once the entire RowGroup (or ColumnChunk) is decoded. As such, the best Parquet readers support “streaming” data out in by producing configurable sized batches of rows on demand. The batch size must be large enough to amortize decode overhead, but small enough for efficient memory usage and to allow downstream processing to begin concurrently while the subsequent batch is decoded. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ┃ Data Page for ColumnChunk 1 │◀┃─ ┌── ─── ─── ─── ─── ┐ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┏━━━━━━━┓ ┌ ─ ┐ ┌ ─ ┐ ┌ ─ ┐ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃ │ │ ┃ Data Page for ColumnChunk 1 │ ┃ │ ┃ ┃ ─ ▶│ │ │ │ │ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ─ ─┃ ┃─ ┤ │ ─ ─ ─ ─ ─ ─ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┃ ┃ A B C │ ┃ Data Page for ColumnChunk 2 │◀┃─ ┗━━━━━━━┛ │ └── ─── ─── ─── ─── ┘ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ Parquet ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ Decoder │ ... ┃ Data Page for ColumnChunk 3 │ ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┌── ─── ─── ─── ─── ┐ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┌ ─ ┐ ┌ ─ ┐ ┌ ─ ┐ │ ┃ Data Page for ColumnChunk 3 │◀┃─ │ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ─ ▶│ │ │ │ │ │ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ─ ─ ─ ─ ─ ─ │ ┃ Data Page for ColumnChunk 3 │ ┃ A B C │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ └── ─── ─── ─── ─── ┘ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ Parquet file Smaller in memory batches for processing While streaming is not a complicated feature to explain, the stateful nature of decoding, especially across multiple columns and arbitrarily nested data, where the relationship between rows and values is not fixed, requires complex intermediate buffering and significant engineering effort to handle correctly. Dictionary preservation Dictionary Encoding, also called categorical encoding, is a technique where each value in a column is not stored directly, but instead, an index in a separate list called a “Dictionary” is stored. This technique achieves many of the benefits of third normal form for columns that have repeated values (low cardinality) and is especially effective for columns of strings such as “City”. The first page in a ColumnChunk can optionally be a dictionary page, containing a list of values of the column’s type. Subsequent pages within this ColumnChunk can then encode an index into this dictionary, instead of encoding the values directly. Given the effectiveness of this encoding, if a Parquet decoder simply decodes dictionary data into the native type, it will inefficiently replicate the same value over and over again, which is especially disastrous for string data. To handle dictionary-encoded data efficiently, the encoding must be preserved during decode. Conveniently, many columnar formats, such as the Arrow DictionaryArray, support such compatible encodings. Preserving dictionary encoding drastically improves performance when reading to an Arrow array, in some cases in excess of 60x, as well as using significantly less memory. The major complicating factor for preserving dictionaries is that the dictionaries are stored per ColumnChunk, and therefore the dictionary changes between RowGroups. The reader must automatically recompute a dictionary for batches that span multiple RowGroups, while also optimizing for the case that batch sizes divide evenly into the number of rows per RowGroup. Additionally a column may be only partly dictionary encoded, further complicating implementation. More information on this technique and its complications can be found in the blog post on applying this technique to the C++ Parquet reader. Projection pushdown The most basic Parquet optimization, and the one most commonly described for Parquet files, is projection pushdown, which reduces both I/Oand CPU requirements. Projection in this context means “selecting some but not all of the columns.” Given how Parquet organizes data, it is straightforward to read and decode only the ColumnChunks required for the referenced columns. For example, consider a SQL query of the form SELECT B from table where A &gt; 35 This query only needs data for columns A and B (and not C) and the projection can be “pushed down” to the Parquet reader. Specifically, using the information in the footer, the Parquet reader can entirely skip fetching (I/O) and decoding (CPU) the Data Pages that store data for column C (ColumnChunk 3 and ColumnChunk 6 in our example). ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ┌─────▶ Data Page for ColumnChunk 1 ("A") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 1 ("A") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 2 ("B") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ │ ┃ Data Page for ColumnChunk 3 ("C") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ A query that │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ accesses only │ ┃ Data Page for ColumnChunk 3 ("C") ┃ columns A and B │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ can read only the │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ relevant pages, ─────┤ ┃ Data Page for ColumnChunk 3 ("C") ┃ skipping any Data │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ Page for column C │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 4 ("A") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 5 ("B") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 5 ("B") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ └─────▶ Data Page for ColumnChunk 5 ("B") ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ┃ Data Page for ColumnChunk 6 ("C") ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ Predicate pushdown Similar to projection pushdown, predicate pushdown also avoids fetching and decoding data from Parquet files, but does so using filter expressions. This technique typically requires closer integration with a query engine such as DataFusion, to determine valid predicates and evaluate them during the scan. Unfortunately without careful API design, the Parquet decoder and query engine can end up tightly coupled, preventing reuse (e.g. there are different Impala and Spark implementations in Cloudera Parquet Predicate Pushdown docs). The Rust Parquet reader uses the RowSelection API to avoid this coupling. RowGroup pruning The simplest form of predicate pushdown, supported by many Parquet based query engines, uses the statistics stored in the footer to skip entire RowGroups. We call this operation RowGroup pruning, and it is analogous to partition pruning in many classical data warehouse systems. For the example query above, if the maximum value for A in a particular RowGroup is less than 35, the decoder can skip fetching and decoding any ColumnChunks from that entire RowGroup. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃Row Group 1 Metadata ┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃Column "A" Metadata Min:0 Max:15 ┃◀╋ ┐ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ Using the min ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ │ and max values ┃ ┃Column "B" Metadata ┃ ┃ from the ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ │ metadata, ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ RowGroup 1 can ┃ ┃Column "C" Metadata ┃ ┃ ├ ─ ─ be entirely ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ skipped ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │ (pruned) when ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ searching for ┃Row Group 2 Metadata ┃ │ rows with A &gt; ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ 35, ┃ ┃Column "A" Metadata Min:10 Max:50 ┃◀╋ ┘ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃Column "B" Metadata ┃ ┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃Column "C" Metadata ┃ ┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ Note that pruning on minimum and maximum values is effective for many data layouts and column types, but not all. Specifically, it is not as effective for columns with many distinct pseudo-random values (e.g. identifiers or uuids). Thankfully for this use case, Parquet also supports per ColumnChunk Bloom Filters. We are actively working on adding bloom filter support in Apache Rust’s implementation. Page pruning A more sophisticated form of predicate pushdown uses the optional page index in the footer metadata to rule out entire Data Pages. The decoder decodes only the corresponding rows from other columns, often skipping entire pages. The fact that pages in different ColumnChunks often contain different numbers of rows, due to various reasons, complicates this optimization. While the page index may identify the needed pages from one column, pruning a page from one column doesn’t immediately rule out entire pages in other columns. Page pruning proceeds as follows: Uses the predicates in combination with the page index to identify pages to skip Uses the offset index to determine what row ranges correspond to non-skipped pages Computes the intersection of ranges across non-skipped pages, and decodes only those rows This last point is highly non-trivial to implement, especially for nested lists where a single row may correspond to multiple values. Fortunately, the Rust Parquet reader hides this complexity internally, and can decode arbitrary RowSelections. For example, to scan Columns A and B, stored in 5 Data Pages as shown in the figure below: If the predicate is A &gt; 35, Page 1 is pruned using the page index (max value is 20), leaving a RowSelection of [200-&gt;onwards], Parquet reader skips Page 3 entirely (as its last row index is 99) (Only) the relevant rows are read by reading pages 2, 4, and 5. If the predicate is instead A &gt; 35 AND B = "F" the page index is even more effective Using A &gt; 35, yields a RowSelection of [200-&gt;onwards] as before Using B = "F", on the remaining Page 4 and Page 5 of B, yields a RowSelection of [100-244] Intersecting the two RowSelections leaves a combined RowSelection [200-244] Parquet reader only decodes those 50 rows from Page 2 and Page 4. ┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ┃ ┌──────────────┐ │ ┌──────────────┐ │ ┃ ┃ │ │ │ │ │ │ ┃ ┃ │ │ │ │ Page │ │ │ │ │ │ │ 3 │ ┃ ┃ │ │ │ │ min: "A" │ │ ┃ ┃ │ │ │ │ │ max: "C" │ ┃ ┃ │ Page │ │ │ first_row: 0 │ │ │ │ 1 │ │ │ │ ┃ ┃ │ min: 10 │ │ └──────────────┘ │ ┃ ┃ │ │ max: 20 │ │ ┌──────────────┐ ┃ ┃ │ first_row: 0 │ │ │ │ │ │ │ │ │ │ Page │ ┃ ┃ │ │ │ │ 4 │ │ ┃ ┃ │ │ │ │ │ min: "D" │ ┃ ┃ │ │ │ │ max: "G" │ │ │ │ │ │ │first_row: 100│ ┃ ┃ └──────────────┘ │ │ │ │ ┃ ┃ │ ┌──────────────┐ │ │ │ ┃ ┃ │ │ │ └──────────────┘ │ │ │ Page │ │ ┌──────────────┐ ┃ ┃ │ 2 │ │ │ │ │ ┃ ┃ │ │ min: 30 │ │ │ Page │ ┃ ┃ │ max: 40 │ │ │ 5 │ │ │ │first_row: 200│ │ │ min: "H" │ ┃ ┃ │ │ │ │ max: "Z" │ │ ┃ ┃ │ │ │ │ │first_row: 250│ ┃ ┃ └──────────────┘ │ │ │ │ │ │ └──────────────┘ ┃ ┃ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃ ColumnChunk ColumnChunk ┃ ┃ A B ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━┛ Support for reading and writing these indexes from Arrow C++, and by extension pyarrow/pandas, is tracked in PARQUET-1404. Late materialization The two previous forms of predicate pushdown only operated on metadata stored for RowGroups, ColumnChunks, and Data Pages prior to decoding values. However, the same techniques also extend to values of one or more columns after decoding them but prior to decoding other columns, which is often called “late materialization”. This technique is especially effective when: The predicate is very selective, i.e. filters out large numbers of rows Each row is large, either due to wide rows (e.g. JSON blobs) or many columns The selected data is clustered together The columns required by the predicate are relatively inexpensive to decode, e.g. PrimitiveArray / DictionaryArray There is additional discussion about the benefits of this technique in SPARK-36527 and Impala. For example, given the predicate A &gt; 35 AND B = "F" from above where the engine uses the page index to determine only 50 rows within RowSelection of [100-244] could match, using late materialization, the Parquet decoder: Decodes the 50 values of Column A Evaluates A &gt; 35 on those 50 values In this case, only 5 rows pass, resulting in the RowSelection: RowSelection[205-206] RowSelection[238-240] Only decodes the 5 rows for Column B for those selections Row Index ┌────────────────────┐ ┌────────────────────┐ 200 │ 30 │ │ "F" │ └────────────────────┘ └────────────────────┘ ... ... ┌────────────────────┐ ┌────────────────────┐ 205 │ 37 │─ ─ ─ ─ ─ ─▶│ "F" │ ├────────────────────┤ ├────────────────────┤ 206 │ 36 │─ ─ ─ ─ ─ ─▶│ "G" │ └────────────────────┘ └────────────────────┘ ... ... ┌────────────────────┐ ┌────────────────────┐ 238 │ 36 │─ ─ ─ ─ ─ ─▶│ "F" │ ├────────────────────┤ ├────────────────────┤ 239 │ 36 │─ ─ ─ ─ ─ ─▶│ "G" │ ├────────────────────┤ ├────────────────────┤ 240 │ 40 │─ ─ ─ ─ ─ ─▶│ "G" │ └────────────────────┘ └────────────────────┘ ... ... ┌────────────────────┐ ┌────────────────────┐ 244 │ 26 │ │ "D" │ └────────────────────┘ └────────────────────┘ Column A Column B Values Values In certain cases, such as our example where B stores single character values, the cost of late materialization machinery can outweigh the savings in decoding. However, the savings can be substantial when some of the conditions listed above are fulfilled. The query engine must decide which predicates to push down and in which order to apply them for optimal results. While it is outside the scope of this document, the same technique can be applied for multiple predicates as well as predicates on multiple columns. See the RowFilter interface in the Parquet crate for more information, and the row_filter implementation in DataFusion. I/O pushdown While Parquet was designed for efficient access on the HDFS distributed file system, it works very well with commodity blob storage systems such as AWS S3 as they have very similar characteristics: Relatively slow “random access” reads: it is much more efficient to read large (MBs) sections of data in each request than issue many requests for smaller portions Significant latency before retrieving the first byte High per-request cost: Often billed per request, regardless of number of bytes read, which incentivizes fewer requests that each read a large contiguous section of data. To read optimally from such systems, a Parquet reader must: Minimize the number of I/O requests, while also applying the various pushdown techniques to avoid fetching large amounts of unused data. Integrate with the appropriate task scheduling mechanism to interleave I/O and processing on the data that is fetched to avoid pipeline bottlenecks. As these are substantial engineering and integration challenges, many Parquet readers still require the files to be fetched in their entirety to local storage. Fetching the entire files in order to process them is not ideal for several reasons: High Latency: Decoding cannot begin until the entire file is fetched (Parquet metadata is at the end of the file, so the decoder must see the end prior to decoding the rest) Wasted work: Fetching the entire file fetches all necessary data, but also potentially lots of unnecessary data that will be skipped after reading the footer. This increases the cost unnecessarily. Requires costly “locally attached” storage (or memory): Many cloud environments do not offer computing resources with locally attached storage – they either rely on expensive network block storage such as AWS EBS or else restrict local storage to certain classes of VMs. Avoiding the need to buffer the entire file requires a sophisticated Parquet decoder, integrated with the I/O subsystem, that can initially fetch and decode the metadata followed by ranged fetches for the relevant data blocks, interleaved with the decoding of Parquet data. This optimization requires careful engineering to fetch large enough blocks of data from the object store that the per request overhead doesn’t dominate gains from reducing the bytes transferred. SPARK-36529 describes the challenges of sequential processing in more detail. ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ │ Step 1: Fetch │ Parquet Parquet metadata file on ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━▼━━━━━━━┓ Remote ┃ ▒▒▒▒▒▒▒▒▒▒ ▒▒▒▒▒▒▒▒▒▒ ░░░░░░░░░░ ┃ Object ┃ ▒▒▒data▒▒▒ ▒▒▒data▒▒▒ ░metadata░ ┃ Store ┃ ▒▒▒▒▒▒▒▒▒▒ ▒▒▒▒▒▒▒▒▒▒ ░░░░░░░░░░ ┃ ┗━━━━━━━━━━━▲━━━━━━━━━━━━━━━━━━━━━▲━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │ └ ─ ─ ─ │ │ Step 2: Fetch only ─ ─ ─ ─ ─ ─ ─ ─ ─ relevant data blocks Not included in this diagram picture are details like coalescing requests and ensuring minimum request sizes needed for an actual implementation. The Rust Parquet crate provides an async Parquet reader, to efficiently read from any AsyncFileReader that: Efficiently reads from any storage medium that supports range requests Integrates with Rust’s futures ecosystem to avoid blocking threads waiting on network I/O and easily can interleave CPU and network Requests multiple ranges simultaneously, to allow the implementation to coalesce adjacent ranges, fetch ranges in parallel, etc. Uses the pushdown techniques described previously to eliminate fetching data where possible Integrates easily with the Apache Arrow object_store crate which you can read more about here To give a sense of what is possible, the following picture shows a timeline of fetching the footer metadata from remote files, using that metadata to determine what Data Pages to read, and then fetching data and decoding simultaneously. This process often must be done for more than one file at a time in order to match network latency, bandwidth, and available CPU. begin metadata read of end read read ─ ─ ─ ┐ data of data │ begin complete block block read of │ │ │ │ metadata ─ ─ ─ ┐ At any time, there are │ │ │ │ │ multiple network │ ▼ ▼ ▼ ▼ requests outstanding to file 1 │ ░░░░░░░░░░ ▒▒▒read▒▒▒ ▒▒▒read▒▒▒ │ hide the individual │ ░░░read░░░ ▒▒▒data▒▒▒ ▒▒▒data▒▒▒ request latency │ ░metadata░ ▓▓decode▓▓ │ ░░░░░░░░░░ ▓▓▓data▓▓▓ │ │ │ │ ░░░░░░░░░░ ▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒read▒▒▒▒│▒▒▒▒▒▒▒▒▒▒▒▒▒▒ file 2 │ ░░░read░░░ ▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒data▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ │ ░metadata░ │ ▓▓▓▓▓decode▓▓▓▓▓▓ │ ░░░░░░░░░░ ▓▓▓▓▓▓data▓▓▓▓▓▓▓ │ │ │ │ ░░│░░░░░░░ ▒▒▒read▒▒▒ ▒▒▒▒read▒▒▒▒▒ file 3 │ ░░░read░░░ ▒▒▒data▒▒▒ ▒▒▒▒data▒▒▒▒▒ ... │ ░m│tadata░ ▓▓decode▓▓ │ ░░░░░░░░░░ ▓▓▓data▓▓▓ └───────────────────────────────────────┼──────────────────────────────▶Time │ Conclusion We hope you enjoyed reading about the Parquet file format, and the various techniques used to quickly query parquet files. We believe that the reason most open source implementations of Parquet do not have the breadth of features described in this post is that it takes a monumental effort, that was previously only possible at well-financed commercial enterprises which kept their implementations closed source. However, with the growth and quality of the Apache Arrow community, both Rust practitioners and the wider Arrow community, our ability to collaborate and build a cutting-edge open source implementation is exhilarating and immensely satisfying. The technology described in this blog is the result of the contributions of many engineers spread across companies, hobbyists, and the world in several repositories, notably Apache Arrow DataFusion, Apache Arrow and Apache Arrow Ballista. If you are interested in joining the DataFusion Community, please get in touch.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 10.0.1 Release</title><link href="https://arrow.apache.org/blog/2022/11/22/10.0.1-release/" rel="alternate" type="text/html" title="Apache Arrow 10.0.1 Release" /><published>2022-11-22T00:00:00-05:00</published><updated>2022-11-22T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2022/11/22/10.0.1-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/11/22/10.0.1-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 10.0.1 release.
This is mostly a bugfix release that includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%2010.0.1"><strong>30 resolved issues</strong></a>
from <a href="/release/10.0.1.html#contributors"><strong>15 distinct contributors</strong></a>. See the Install Page to learn how to
get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/10.0.1.html#changelog">complete changelog</a>.</p>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>
<p>The Arrow Flight SQL JDBC driver now properly handles <code class="language-plaintext highlighter-rouge">executeUpdate</code> (<a href="https://issues.apache.org/jira/browse/ARROW-18294">ARROW-18294</a>), and will no longer try to handle URIs that it does not recognize (<a href="https://issues.apache.org/jira/browse/ARROW-18296">ARROW-18296</a>).</p>

<h2 id="c-notes">C++ notes</h2>
<ul>
  <li>Add support for ARMv6 (<a href="https://issues.apache.org/jira/browse/ARROW-18255">ARROW-18255</a>)
And some other minor fixes.</li>
</ul>

<h2 id="go-notes">Go notes</h2>
<ul>
  <li>Added option to support dictionary deltas with IPC (<a href="https://issues.apache.org/jira/browse/ARROW-18326">ARROW-18326</a>)</li>
  <li>Fix dictionary replacement during IPC stream (<a href="https://issues.apache.org/jira/browse/ARROW-18317">ARROW-18317</a>)</li>
  <li>Fix StructBuilder premature release fields (<a href="https://issues.apache.org/jira/browse/ARROW-18274">ARROW-18274</a>)</li>
</ul>

<h2 id="python-notes">Python notes</h2>

<p>New features and improvements:</p>

<ul>
  <li>Added support and wheels for Python 3.11
(<a href="https://issues.apache.org/jira/browse/ARROW-17487">ARROW-17487</a>).</li>
  <li>Updated OpenSSL bundled on wheels to 3.0.7 due to CVE-2022-3786
(<a href="https://issues.apache.org/jira/browse/ARROW-18302">ARROW-18302</a>).</li>
</ul>

<h2 id="r-notes">R notes</h2>
<ul>
  <li>Fix for failing test after lubridate 1.9 release (<a href="https://issues.apache.org/jira/browse/ARROW-18285">ARROW-18285</a>)</li>
  <li>Add deprecation cycle for pull() change (<a href="https://issues.apache.org/jira/browse/ARROW-18132">ARROW-18132</a>)</li>
  <li>Fix to correctly handle .data pronoun in group_by() (<a href="https://issues.apache.org/jira/browse/ARROW-18131">ARROW-18131</a>)</li>
  <li>Fix for dev purrr (<a href="https://issues.apache.org/jira/browse/ARROW-18305">ARROW-18305</a>)</li>
</ul>

<h2 id="other-modules-and-languages">Other modules and languages</h2>

<p>No general changes were made to the other libraries or languages.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 10.0.1 release. This is mostly a bugfix release that includes 30 resolved issues from 15 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Arrow Flight RPC notes The Arrow Flight SQL JDBC driver now properly handles executeUpdate (ARROW-18294), and will no longer try to handle URIs that it does not recognize (ARROW-18296). C++ notes Add support for ARMv6 (ARROW-18255) And some other minor fixes. Go notes Added option to support dictionary deltas with IPC (ARROW-18326) Fix dictionary replacement during IPC stream (ARROW-18317) Fix StructBuilder premature release fields (ARROW-18274) Python notes New features and improvements: Added support and wheels for Python 3.11 (ARROW-17487). Updated OpenSSL bundled on wheels to 3.0.7 due to CVE-2022-3786 (ARROW-18302). R notes Fix for failing test after lubridate 1.9 release (ARROW-18285) Add deprecation cycle for pull() change (ARROW-18132) Fix to correctly handle .data pronoun in group_by() (ARROW-18131) Fix for dev purrr (ARROW-18305) Other modules and languages No general changes were made to the other libraries or languages.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Fast and Memory Efficient Multi-Column Sorts in Apache Arrow Rust, Part 1</title><link href="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1/" rel="alternate" type="text/html" title="Fast and Memory Efficient Multi-Column Sorts in Apache Arrow Rust, Part 1" /><published>2022-11-07T00:00:00-05:00</published><updated>2022-11-07T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1/"><![CDATA[<!--

-->

<h2 id="introduction">Introduction</h2>

<p>Sorting is one of the most fundamental operations in modern databases and other analytic systems, underpinning important operators such as aggregates, joins, window functions, merge, and more. By some estimates, more than half of the execution time in data processing systems is spent sorting. Optimizing sorts is therefore vital to improving query performance and overall system efficiency.</p>

<p>Sorting is also one of the most well studied topics in computer science. The classic survey paper for databases is <a href="https://dl.acm.org/doi/10.1145/1132960.1132964">Implementing Sorting in Database Systems</a> by Goetz Graefe which provides a thorough academic treatment and is still very applicable today. However, it may not be obvious how to apply the wisdom and advanced techniques described in that paper to modern systems. In addition, the excellent <a href="https://duckdb.org/2021/08/27/external-sorting.html">DuckDB blog on sorting</a> highlights many sorting techniques, and mentions a comparable row format, but it does not explain how to efficiently sort variable length strings or dictionary encoded data.</p>

<p>In this series we explain in detail the new <a href="https://docs.rs/arrow/27.0.0/arrow/row/index.html">row format</a> in the <a href="https://github.com/apache/arrow-rs">Rust implementation</a> of <a href="https://arrow.apache.org/">Apache Arrow</a>, and how we used to make sorting more than <a href="https://github.com/apache/arrow-rs/pull/2929">3x</a> faster than an alternate comparator based approach. The benefits are especially pronounced for strings, dictionary encoded data, and sorts with large numbers of columns.</p>

<h2 id="multicolumn--lexicographical-sort-problem">Multicolumn / Lexicographical Sort Problem</h2>

<p>Most languages have native, optimized operations to sort a single column (array) of data, which are specialized based on the type of data being sorted. The reason that sorting is typically more challenging in analytic systems is that:</p>

<ol>
  <li>They must support multiple columns of data</li>
  <li>The column types are not knowable at compile time, and thus the compiler can not typically generate optimized code</li>
</ol>

<p>Multicolumn sorting is also referred to as lexicographical sorting in some libraries.</p>

<p>For example, given sales data for various customers and their state of residence, a user might want to find the lowest 10 orders for each state.</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Customer | State | Orders
—--------+-------+-------
12345    |  MA   |  10.12
532432   |  MA   |  8.44
12345    |  CA   |  3.25
56232    |  WA   |  6.00
23442    |  WA   |  132.50
7844     |  CA   |  9.33
852353   |  MA   |  1.30
</code></pre></div></div>

<p>One way to do so is to order the data first by <code class="language-plaintext highlighter-rouge">State</code> and then by <code class="language-plaintext highlighter-rouge">Orders</code>:</p>
<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Customer | State | Orders
—--------+-------+-------
12345    |  CA   |  3.25
7844     |  CA   |  9.33
852353   |  MA   |  1.30
532432   |  MA   |  8.44
12345    |  MA   |  10.12
56232    |  WA   |  6.00
23442    |  WA   |  132.50
</code></pre></div></div>

<p>(Note: While there are specialized ways for computing this particular query other than fully sorting the entire input (e.g. “TopK”), they typically need the same multi-column comparison operation described below. Thus while we will use the simplified example in this series, it applies much more broadly)</p>

<h2 id="basic-implementation">Basic Implementation</h2>

<p>Let us take the example of a basic sort kernel which takes a set of columns as input, and returns a list of indices identifying a sorted order.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="n">lexsort_to_indices</span><span class="p">([</span>
    <span class="p">[</span><span class="s">"MA"</span><span class="p">,</span> <span class="s">"MA"</span><span class="p">,</span> <span class="s">"CA"</span><span class="p">,</span> <span class="s">"WA"</span><span class="p">,</span> <span class="s">"WA"</span><span class="p">,</span> <span class="s">"CA"</span><span class="p">,</span> <span class="s">"MA"</span><span class="p">]</span>
  <span class="p">])</span>

<span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>

<span class="o">&gt;</span> <span class="n">lexsort_to_indices</span><span class="p">([</span>
    <span class="p">[</span><span class="s">"MA"</span><span class="p">,</span> <span class="s">"MA"</span><span class="p">,</span> <span class="s">"CA"</span><span class="p">,</span> <span class="s">"WA"</span><span class="p">,</span> <span class="s">"WA"</span><span class="p">,</span>   <span class="s">"CA"</span><span class="p">,</span> <span class="s">"MA"</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">10.10</span><span class="p">,</span> <span class="mf">8.44</span><span class="p">,</span> <span class="mf">3.25</span><span class="p">,</span> <span class="mf">6.00</span><span class="p">,</span> <span class="mf">132.50</span><span class="p">,</span> <span class="mf">9.33</span><span class="p">,</span> <span class="mf">1.30</span><span class="p">]</span>
  <span class="p">])</span>

<span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
</code></pre></div></div>

<p>This function returns a list of indices instead of sorting the columns directly because it:</p>
<ol>
  <li>Avoids expensive copying data during the sorting process</li>
  <li>Allows deferring copying of values until the latest possible moment</li>
  <li>Can be used to reorder additional columns that weren’t part of the sort key</li>
</ol>

<p>A straightforward implementation of lexsort_to_indices uses a comparator function,</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   row
  index
        ┌─────┐   ┌─────┐   ┌─────┐     compare(left_index, right_index)
      0 │     │   │     │   │     │
       ┌├─────┤─ ─├─────┤─ ─├─────┤┐                   │             │
        │     │   │     │   │     │ ◀──────────────────┘             │
       └├─────┤─ ─├─────┤─ ─├─────┤┘                                 │
        │     │   │     │   │     │Comparator function compares one  │
        ├─────┤   ├─────┤   ├─────┤ multi-column row with another.   │
        │     │   │     │   │     │                                  │
        ├─────┤   ├─────┤   ├─────┤ The data types of the columns    │
        │     │   │     │   │     │  and the sort options are not    │
        └─────┘   └─────┘   └─────┘  known at compile time, only     │
                    ...                        runtime               │
                                                                     │
       ┌┌─────┐─ ─┌─────┐─ ─┌─────┐┐                                 │
        │     │   │     │   │     │ ◀────────────────────────────────┘
       └├─────┤─ ─├─────┤─ ─├─────┤┘
        │     │   │     │   │     │
        ├─────┤   ├─────┤   ├─────┤
    N-1 │     │   │     │   │     │
        └─────┘   └─────┘   └─────┘
        Customer    State    Orders
         UInt64      Utf8     F64
</code></pre></div></div>

<p>The comparator function compares each row a column at a time, based on the column types</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                         ┌────────────────────────────────┐
                         │                                │
                         ▼                                │
                     ┌ ─ ─ ─ ┐ ┌ ─ ─ ─ ┐                  │
                                                          │
            ┌─────┐  │┌─────┐│ │┌─────┐│                  │
left_index  │     │   │     │   │     │                   │
            └─────┘  │└─────┘│ │└─────┘│   Step 1: Compare State
                                                    (UInt64)
                     │       │ │       │

                     │       │ │       │
            ┌─────┐   ┌─────┐   ┌─────┐
 right_index│     │  ││     ││ ││     ││
            └─────┘   └─────┘   └─────┘    Step 2: If State values equal
                     │       │ │       │   compare Orders (F64)
            Customer   State     Orders                     │
             UInt64  │  Utf8 │ │  F64  │                    │
                      ─ ─ ─ ─   ─ ─ ─ ─                     │
                                    ▲                       │
                                    │                       │
                                    └───────────────────────┘
</code></pre></div></div>

<p>Pseudocode for this operation might look something like</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Takes a list of columns and returns the lexicographically
# sorted order as a list of indices
</span><span class="k">def</span> <span class="nf">lexsort_to_indices</span><span class="p">(</span><span class="n">columns</span><span class="p">):</span>
  <span class="n">comparator</span> <span class="o">=</span> <span class="n">build_comparator</span><span class="p">(</span><span class="n">columns</span><span class="p">)</span>

  <span class="c1"># Construct a list of integers from 0 to the number of rows
</span>  <span class="c1"># and sort it according to the comparator
</span>  <span class="p">[</span><span class="mf">0.</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">num_rows</span><span class="p">()].</span><span class="n">sort_by</span><span class="p">(</span><span class="n">comparator</span><span class="p">)</span>

<span class="c1"># Build a function that given indexes (left_idx, right_idx)
# returns the comparison of the sort keys at the left
# and right indices respectively
</span><span class="k">def</span> <span class="nf">build_comparator</span><span class="p">(</span><span class="n">columns</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">comparator</span><span class="p">(</span><span class="n">left_idx</span><span class="p">,</span> <span class="n">right_idx</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">:</span>
      <span class="c1"># call a compare function which performs
</span>      <span class="c1"># dynamic dispatch on type of left and right columns
</span>      <span class="n">ordering</span> <span class="o">=</span> <span class="n">compare</span><span class="p">(</span><span class="n">column</span><span class="p">,</span> <span class="n">left_idx</span><span class="p">,</span><span class="n">right_idx</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">ordering</span> <span class="o">!=</span> <span class="n">Equal</span> <span class="p">{</span>
        <span class="k">return</span> <span class="n">ordering</span>
      <span class="p">}</span>
    <span class="c1"># All values equal
</span>    <span class="n">Equal</span>
  <span class="c1"># Return comparator function
</span>  <span class="n">comparator</span>

  <span class="c1"># compares the values in a single column at left_idx and right_idx
</span>  <span class="k">def</span> <span class="nf">compare</span><span class="p">(</span><span class="n">column</span><span class="p">,</span> <span class="n">left_idx</span><span class="p">,</span> <span class="n">right_idx</span><span class="p">):</span>
    <span class="c1"># Choose comparison based on type of column ("dynamic dispatch")
</span>    <span class="k">if</span> <span class="n">column</span><span class="p">.</span><span class="nb">type</span> <span class="o">==</span> <span class="n">Int</span><span class="p">:</span>
     <span class="nb">cmp</span><span class="p">(</span><span class="n">column</span><span class="p">[</span><span class="n">left_idx</span><span class="p">].</span><span class="n">as_int</span><span class="p">(),</span> <span class="n">column</span><span class="p">[</span><span class="n">right_idx</span><span class="p">].</span><span class="n">as_int</span><span class="p">())</span>
    <span class="k">elif</span> <span class="n">column</span><span class="p">.</span><span class="nb">type</span> <span class="o">==</span> <span class="n">Float</span><span class="p">:</span>
     <span class="nb">cmp</span><span class="p">(</span><span class="n">column</span><span class="p">[</span><span class="n">left_idx</span><span class="p">].</span><span class="n">as_float</span><span class="p">(),</span> <span class="n">column</span><span class="p">[</span><span class="n">right_idx</span><span class="p">].</span><span class="n">as_float</span><span class="p">())</span>
    <span class="p">...</span>
</code></pre></div></div>

<p>Greater detail is beyond the scope of this post, but in general the more predictable the behavior of a block of code, the better its performance will be. In the case of this pseudocode,  there is clear room for improvement:</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">comparator</code> performs a large number of unpredictable conditional branches, where the path execution takes depends on the data values</li>
  <li><code class="language-plaintext highlighter-rouge">comparator</code> and <code class="language-plaintext highlighter-rouge">compare</code> use dynamic dispatch, which not only adds further conditional branches, but also function call overhead</li>
  <li><code class="language-plaintext highlighter-rouge">comparator</code> performs a large number of reads of memory at unpredictable locations</li>
</ol>

<p>You can find the complete implementation of multi-column comparator construction in arrow-rs in <a href="https://github.com/apache/arrow-rs/blob/f629a2ebe08033e7b78585d82e98c50a4439e7a2/arrow/src/compute/kernels/sort.rs#L905-L1036">sort.rs</a> and <a href="https://github.com/apache/arrow-rs/blob/f629a2e/arrow/src/array/ord.rs#L178-L313">ord.rs</a>.</p>

<h1 id="normalized-keys--byte-array-comparisons">Normalized Keys / Byte Array Comparisons</h1>

<p>Now imagine we had a way to represent each logical row of data as a sequence of bytes, and that byte-wise comparison of that sequence yielded the same result as comparing the actual column values using the code above. Such a representation would require no switching on column types, and the kernel would become</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">lexsort_to_indices</span><span class="p">(</span><span class="n">columns</span><span class="p">):</span>
  <span class="n">rows</span> <span class="o">=</span> <span class="n">convert_to_rows</span><span class="p">(</span><span class="n">columns</span><span class="p">)</span>
  <span class="p">[</span><span class="mf">0.</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">num_rows</span><span class="p">()].</span><span class="n">sort_by</span><span class="p">(</span><span class="k">lambda</span> <span class="n">l</span><span class="p">,</span> <span class="n">r</span><span class="p">:</span> <span class="nb">cmp</span><span class="p">(</span><span class="n">rows</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">rows</span><span class="p">[</span><span class="n">r</span><span class="p">]))</span>
</code></pre></div></div>

<p>While this approach does require converting to/from the byte array representation, it has some major advantages:</p>

<ul>
  <li>Rows can be compared by comparing bytes in memory, which modern computer hardware excels at with the extremely well optimized <a href="https://www.man7.org/linux/man-pages/man3/memcmp.3.html">memcmp</a></li>
  <li>Memory accesses are largely predictable</li>
  <li>There is no dynamic dispatch overhead</li>
  <li>Extends straightforwardly to more sophisticated sorting strategies such as
    <ul>
      <li>Distribution-based sorting techniques such as radix sort</li>
      <li>Parallel merge sort</li>
      <li>External sort</li>
      <li>…</li>
    </ul>
  </li>
</ul>

<p>You can find more information on how to leverage such representation in the “Binary String Comparison” section of the <a href="https://duckdb.org/2021/08/27/external-sorting.html">DuckDB blog post</a> on the topic as well as <a href="https://dl.acm.org/doi/10.1145/1132960.1132964">Graefe’s paper</a>. However, we found it wasn’t immediately obvious how to apply this technique to variable length string or dictionary encoded data, which we will explain in the next post in this series.</p>

<h2 id="next-up-row-format">Next up: Row Format</h2>

<p>This post has introduced the concept and challenges of multi column sorting, and shown why a comparable byte array representation, such as the <a href="https://docs.rs/arrow/27.0.0/arrow/row/index.html">row format</a> introduced to the <a href="https://github.com/apache/arrow-rs">Rust implementation</a> of <a href="https://arrow.apache.org/">Apache Arrow</a>, is such a compelling primitive.</p>

<p>In <a href="/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-2/">the next post</a> we explain how this encoding works, but if you just want to use it, check out the <a href="https://docs.rs/arrow/27.0.0/arrow/row/index.html">docs</a> for getting started, and report any issues on our <a href="https://github.com/apache/arrow-rs/issues">bugtracker</a>. As always, the <a href="https://github.com/apache/arrow-rs#arrow-rust-community">Arrow community</a> very much looks forward to seeing what you build with it!</p>]]></content><author><name>tustvold and alamb</name></author><category term="arrow" /><summary type="html"><![CDATA[Introduction Sorting is one of the most fundamental operations in modern databases and other analytic systems, underpinning important operators such as aggregates, joins, window functions, merge, and more. By some estimates, more than half of the execution time in data processing systems is spent sorting. Optimizing sorts is therefore vital to improving query performance and overall system efficiency. Sorting is also one of the most well studied topics in computer science. The classic survey paper for databases is Implementing Sorting in Database Systems by Goetz Graefe which provides a thorough academic treatment and is still very applicable today. However, it may not be obvious how to apply the wisdom and advanced techniques described in that paper to modern systems. In addition, the excellent DuckDB blog on sorting highlights many sorting techniques, and mentions a comparable row format, but it does not explain how to efficiently sort variable length strings or dictionary encoded data. In this series we explain in detail the new row format in the Rust implementation of Apache Arrow, and how we used to make sorting more than 3x faster than an alternate comparator based approach. The benefits are especially pronounced for strings, dictionary encoded data, and sorts with large numbers of columns. Multicolumn / Lexicographical Sort Problem Most languages have native, optimized operations to sort a single column (array) of data, which are specialized based on the type of data being sorted. The reason that sorting is typically more challenging in analytic systems is that: They must support multiple columns of data The column types are not knowable at compile time, and thus the compiler can not typically generate optimized code Multicolumn sorting is also referred to as lexicographical sorting in some libraries. For example, given sales data for various customers and their state of residence, a user might want to find the lowest 10 orders for each state. Customer | State | Orders —--------+-------+------- 12345 | MA | 10.12 532432 | MA | 8.44 12345 | CA | 3.25 56232 | WA | 6.00 23442 | WA | 132.50 7844 | CA | 9.33 852353 | MA | 1.30 One way to do so is to order the data first by State and then by Orders: Customer | State | Orders —--------+-------+------- 12345 | CA | 3.25 7844 | CA | 9.33 852353 | MA | 1.30 532432 | MA | 8.44 12345 | MA | 10.12 56232 | WA | 6.00 23442 | WA | 132.50 (Note: While there are specialized ways for computing this particular query other than fully sorting the entire input (e.g. “TopK”), they typically need the same multi-column comparison operation described below. Thus while we will use the simplified example in this series, it applies much more broadly) Basic Implementation Let us take the example of a basic sort kernel which takes a set of columns as input, and returns a list of indices identifying a sorted order. &gt; lexsort_to_indices([ ["MA", "MA", "CA", "WA", "WA", "CA", "MA"] ]) [2, 5, 0, 1, 6, 3, 4] &gt; lexsort_to_indices([ ["MA", "MA", "CA", "WA", "WA", "CA", "MA"], [10.10, 8.44, 3.25, 6.00, 132.50, 9.33, 1.30] ]) [2, 5, 6, 1, 0, 3, 4] This function returns a list of indices instead of sorting the columns directly because it: Avoids expensive copying data during the sorting process Allows deferring copying of values until the latest possible moment Can be used to reorder additional columns that weren’t part of the sort key A straightforward implementation of lexsort_to_indices uses a comparator function, row index ┌─────┐ ┌─────┐ ┌─────┐ compare(left_index, right_index) 0 │ │ │ │ │ │ ┌├─────┤─ ─├─────┤─ ─├─────┤┐ │ │ │ │ │ │ │ │ ◀──────────────────┘ │ └├─────┤─ ─├─────┤─ ─├─────┤┘ │ │ │ │ │ │ │Comparator function compares one │ ├─────┤ ├─────┤ ├─────┤ multi-column row with another. │ │ │ │ │ │ │ │ ├─────┤ ├─────┤ ├─────┤ The data types of the columns │ │ │ │ │ │ │ and the sort options are not │ └─────┘ └─────┘ └─────┘ known at compile time, only │ ... runtime │ │ ┌┌─────┐─ ─┌─────┐─ ─┌─────┐┐ │ │ │ │ │ │ │ ◀────────────────────────────────┘ └├─────┤─ ─├─────┤─ ─├─────┤┘ │ │ │ │ │ │ ├─────┤ ├─────┤ ├─────┤ N-1 │ │ │ │ │ │ └─────┘ └─────┘ └─────┘ Customer State Orders UInt64 Utf8 F64 The comparator function compares each row a column at a time, based on the column types ┌────────────────────────────────┐ │ │ ▼ │ ┌ ─ ─ ─ ┐ ┌ ─ ─ ─ ┐ │ │ ┌─────┐ │┌─────┐│ │┌─────┐│ │ left_index │ │ │ │ │ │ │ └─────┘ │└─────┘│ │└─────┘│ Step 1: Compare State (UInt64) │ │ │ │ │ │ │ │ ┌─────┐ ┌─────┐ ┌─────┐ right_index│ │ ││ ││ ││ ││ └─────┘ └─────┘ └─────┘ Step 2: If State values equal │ │ │ │ compare Orders (F64) Customer State Orders │ UInt64 │ Utf8 │ │ F64 │ │ ─ ─ ─ ─ ─ ─ ─ ─ │ ▲ │ │ │ └───────────────────────┘ Pseudocode for this operation might look something like # Takes a list of columns and returns the lexicographically # sorted order as a list of indices def lexsort_to_indices(columns): comparator = build_comparator(columns) # Construct a list of integers from 0 to the number of rows # and sort it according to the comparator [0..columns.num_rows()].sort_by(comparator) # Build a function that given indexes (left_idx, right_idx) # returns the comparison of the sort keys at the left # and right indices respectively def build_comparator(columns): def comparator(left_idx, right_idx): for column in columns: # call a compare function which performs # dynamic dispatch on type of left and right columns ordering = compare(column, left_idx,right_idx) if ordering != Equal { return ordering } # All values equal Equal # Return comparator function comparator # compares the values in a single column at left_idx and right_idx def compare(column, left_idx, right_idx): # Choose comparison based on type of column ("dynamic dispatch") if column.type == Int: cmp(column[left_idx].as_int(), column[right_idx].as_int()) elif column.type == Float: cmp(column[left_idx].as_float(), column[right_idx].as_float()) ... Greater detail is beyond the scope of this post, but in general the more predictable the behavior of a block of code, the better its performance will be. In the case of this pseudocode, there is clear room for improvement: comparator performs a large number of unpredictable conditional branches, where the path execution takes depends on the data values comparator and compare use dynamic dispatch, which not only adds further conditional branches, but also function call overhead comparator performs a large number of reads of memory at unpredictable locations You can find the complete implementation of multi-column comparator construction in arrow-rs in sort.rs and ord.rs. Normalized Keys / Byte Array Comparisons Now imagine we had a way to represent each logical row of data as a sequence of bytes, and that byte-wise comparison of that sequence yielded the same result as comparing the actual column values using the code above. Such a representation would require no switching on column types, and the kernel would become def lexsort_to_indices(columns): rows = convert_to_rows(columns) [0..columns.num_rows()].sort_by(lambda l, r: cmp(rows[l], rows[r])) While this approach does require converting to/from the byte array representation, it has some major advantages: Rows can be compared by comparing bytes in memory, which modern computer hardware excels at with the extremely well optimized memcmp Memory accesses are largely predictable There is no dynamic dispatch overhead Extends straightforwardly to more sophisticated sorting strategies such as Distribution-based sorting techniques such as radix sort Parallel merge sort External sort … You can find more information on how to leverage such representation in the “Binary String Comparison” section of the DuckDB blog post on the topic as well as Graefe’s paper. However, we found it wasn’t immediately obvious how to apply this technique to variable length string or dictionary encoded data, which we will explain in the next post in this series. Next up: Row Format This post has introduced the concept and challenges of multi column sorting, and shown why a comparable byte array representation, such as the row format introduced to the Rust implementation of Apache Arrow, is such a compelling primitive. In the next post we explain how this encoding works, but if you just want to use it, check out the docs for getting started, and report any issues on our bugtracker. As always, the Arrow community very much looks forward to seeing what you build with it!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Fast and Memory Efficient Multi-Column Sorts in Apache Arrow Rust, Part 2</title><link href="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-2/" rel="alternate" type="text/html" title="Fast and Memory Efficient Multi-Column Sorts in Apache Arrow Rust, Part 2" /><published>2022-11-07T00:00:00-05:00</published><updated>2022-11-07T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-2</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-2/"><![CDATA[<!--

-->

<h2 id="introduction">Introduction</h2>

<p>In <a href="/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1/">Part 1</a> of this post, we described the problem of Multi-Column Sorting and the challenges of implementing it efficiently. This second post explains how the new <a href="https://docs.rs/arrow/27.0.0/arrow/row/index.html">row format</a> in the <a href="https://github.com/apache/arrow-rs">Rust implementation</a> of <a href="https://arrow.apache.org/">Apache Arrow</a> works and is constructed.</p>

<h2 id="row-format">Row Format</h2>

<p>The row format is a variable length byte sequence created by concatenating the encoded form of each column. The encoding for each column depends on its datatype (and sort options).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   ┌─────┐   ┌─────┐   ┌─────┐
   │     │   │     │   │     │
   ├─────┤ ┌ ┼─────┼ ─ ┼─────┼ ┐              ┏━━━━━━━━━━━━━┓
   │     │   │     │   │     │  ─────────────▶┃             ┃
   ├─────┤ └ ┼─────┼ ─ ┼─────┼ ┘              ┗━━━━━━━━━━━━━┛
   │     │   │     │   │     │
   └─────┘   └─────┘   └─────┘
               ...
   ┌─────┐ ┌ ┬─────┬ ─ ┬─────┬ ┐              ┏━━━━━━━━┓
   │     │   │     │   │     │  ─────────────▶┃        ┃
   └─────┘ └ ┴─────┴ ─ ┴─────┴ ┘              ┗━━━━━━━━┛
   Customer    State    Orders
    UInt64      Utf8     F64

          Input Arrays                          Row Format
           (Columns)
</code></pre></div></div>

<p>The encoding is carefully designed in such a way that escaping is unnecessary: it is never ambiguous as to whether a byte is part of a sentinel (e.g. null) or a value.</p>

<h3 id="unsigned-integers">Unsigned Integers</h3>

<p>To encode a non-null unsigned integer, the byte <code class="language-plaintext highlighter-rouge">0x01</code> is written, followed by the integer’s bytes starting with the most significant, i.e. big endian. A null is encoded as a <code class="language-plaintext highlighter-rouge">0x00</code> byte, followed by the encoded bytes of the integer’s zero value</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>              ┌──┬──┬──┬──┐      ┌──┬──┬──┬──┬──┐
   3          │03│00│00│00│      │01│00│00│00│03│
              └──┴──┴──┴──┘      └──┴──┴──┴──┴──┘
              ┌──┬──┬──┬──┐      ┌──┬──┬──┬──┬──┐
  258         │02│01│00│00│      │01│00│00│01│02│
              └──┴──┴──┴──┘      └──┴──┴──┴──┴──┘
              ┌──┬──┬──┬──┐      ┌──┬──┬──┬──┬──┐
 23423        │7F│5B│00│00│      │01│00│00│5B│7F│
              └──┴──┴──┴──┘      └──┴──┴──┴──┴──┘
              ┌──┬──┬──┬──┐      ┌──┬──┬──┬──┬──┐
 NULL         │??│??│??│??│      │00│00│00│00│00│
              └──┴──┴──┴──┘      └──┴──┴──┴──┴──┘

             32-bit (4 bytes)        Row Format
 Value        Little Endian
</code></pre></div></div>

<h3 id="signed-integers">Signed Integers</h3>

<p>In Rust and most modern computer architectures, signed integers are encoded using <a href="https://en.wikipedia.org/wiki/Two%27s_complement">two’s complement</a>, where a number is negated by flipping all the bits, and adding 1. Therefore, flipping the top-most bit and treating the result as an unsigned integer preserves the order. This unsigned integer can then be encoded using the same encoding for unsigned integers described in the previous section. For example</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>       ┌──┬──┬──┬──┐       ┌──┬──┬──┬──┐       ┌──┬──┬──┬──┬──┐
    5  │05│00│00│00│       │05│00│00│80│       │01│80│00│00│05│
       └──┴──┴──┴──┘       └──┴──┴──┴──┘       └──┴──┴──┴──┴──┘
       ┌──┬──┬──┬──┐       ┌──┬──┬──┬──┐       ┌──┬──┬──┬──┬──┐
   -5  │FB│FF│FF│FF│       │FB│FF│FF│7F│       │01│7F│FF│FF│FB│
       └──┴──┴──┴──┘       └──┴──┴──┴──┘       └──┴──┴──┴──┴──┘

 Value  32-bit (4 bytes)    High bit flipped      Row Format
         Little Endian
</code></pre></div></div>

<h3 id="floating-point">Floating Point</h3>

<p>Floating point values can be ordered according to the <a href="https://en.wikipedia.org/wiki/IEEE_754#Total-ordering_predicate">IEEE 754 totalOrder predicate</a> (implemented in Rust by <a href="https://doc.rust-lang.org/std/primitive.f32.html#method.total_cmp">f32::total_cmp</a>). This ordering interprets the bytes of the floating point value as the correspondingly sized, signed, little-endian integer, flipping all the bits except the sign bit in the case of negatives.</p>

<p>Floating point values are therefore encoded to row format by converting them to the appropriate sized signed integer representation, and then using the same encoding for signed integers described in the previous section.</p>

<h3 id="byte-arrays-including-strings">Byte Arrays (Including Strings)</h3>

<p>Unlike primitive types above, byte arrays are variable length. For short strings, such as <code class="language-plaintext highlighter-rouge">state</code> in our example above, it is possible to pad all values to the length of the longest one with some fixed value such as <code class="language-plaintext highlighter-rouge">0x00</code> and produce a fixed length row. This is the approach described in the DuckDB blog for encoding <code class="language-plaintext highlighter-rouge">c_birth_country</code>.</p>

<p>However, often values in string columns differ substantially in length or the maximum length is not known at the start of execution, making it inadvisable and/or impractical to pad the strings to a fixed length. The Rust Arrow row format therefore uses a variable length encoding.</p>

<p>We need an encoding that unambiguously terminates the end of the byte array. This not only permits recovering the original value from the row format, but ensures that bytes of a longer byte array are not compared against bytes from a different column when compared against a row containing a shorter byte array.</p>

<p>A null byte array is encoded as a single <code class="language-plaintext highlighter-rouge">0x00</code> byte. Similarly, an empty byte array is encoded as a single <code class="language-plaintext highlighter-rouge">0x01</code> byte.</p>

<p>To encode a non-null, non-empty array, first a single <code class="language-plaintext highlighter-rouge">0x02</code> byte  is written. Then the array is written in 32-byte blocks, with each complete block followed by a <code class="language-plaintext highlighter-rouge">0xFF</code> byte as a continuation token. The final block is padded to 32-bytes with <code class="language-plaintext highlighter-rouge">0x00</code>, and is then followed by the unpadded length of this final block as a single byte in place of a continuation token</p>

<p>Note the following example encodings use a block size of 4 bytes, as opposed to 32 bytes for brevity</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                      ┌───┬───┬───┬───┬───┬───┐
 "MEEP"               │02 │'M'│'E'│'E'│'P'│04 │
                      └───┴───┴───┴───┴───┴───┘

                      ┌───┐
 ""                   │01 |
                      └───┘

 NULL                 ┌───┐
                      │00 │
                      └───┘

"Defenestration"      ┌───┬───┬───┬───┬───┬───┐
                      │02 │'D'│'e'│'f'│'e'│FF │
                      └───┼───┼───┼───┼───┼───┤
                          │'n'│'e'│'s'│'t'│FF │
                          ├───┼───┼───┼───┼───┤
                          │'r'│'a'│'t'│'i'│FF │
                          ├───┼───┼───┼───┼───┤
                          │'o'│'n'│00 │00 │02 │
                          └───┴───┴───┴───┴───┘
</code></pre></div></div>

<p>This approach is loosely inspired by <a href="https://en.wikipedia.org/wiki/Consistent_Overhead_Byte_Stuffing">COBS encoding</a>, and chosen over more traditional <a href="https://en.wikipedia.org/wiki/High-Level_Data_Link_Control#Asynchronous_framing">byte stuffing</a> as it is more amenable to vectorization, in particular hardware with AVX-256 can copy a 32-byte block in a single instruction.</p>

<h3 id="dictionary-arrays">Dictionary Arrays</h3>
<p>Dictionary Encoded Data (called <a href="https://pandas.pydata.org/docs/user_guide/categorical.html">categorical</a> in pandas) is increasingly important because they can store and process low cardinality data very efficiently.</p>

<p>A simple approach to encoding dictionary arrays would be to encode the logical values directly using the encodings for primitive values described previously. However, this would lose the benefits of dictionary encoding to reduce memory and CPU consumption.</p>

<p>To further complicate matters, the <a href="https://arrow.apache.org/docs/format/Columnar.html#dictionary-encoded-layout">Arrow implementation of Dictionary encoding</a> is quite general, and we can make no assumptions about the contents of the dictionaries. In particular, we cannot assume that the dictionary values are sorted, nor that the same dictionary is used for all arrays within a column</p>

<p>The following example shows how a string column might be encoded in two arrays using two different dictionaries. The dictionary keys <code class="language-plaintext highlighter-rouge">0</code>, <code class="language-plaintext highlighter-rouge">1</code>, and <code class="language-plaintext highlighter-rouge">2</code> in the first batch correspond to different values than the same keys in the second dictionary.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
  ┌───────────┐ ┌─────┐    │
│ │"Fabulous" │ │  0  │
  ├───────────┤ ├─────┤    │
│ │   "Bar"   │ │  2  │
  ├───────────┤ ├─────┤    │       ┌───────────┐
│ │  "Soup"   │ │  2  │            │"Fabulous" │
  └───────────┘ ├─────┤    │       ├───────────┤
│               │  0  │            │  "Soup"   │
                ├─────┤    │       ├───────────┤
│               │  1  │            │  "Soup"   │
                └─────┘    │       ├───────────┤
│                                  │"Fabulous" │
                 Values    │       ├───────────┤
│ Dictionary   (indexes in         │   "Bar"   │
               dictionary) │       ├───────────┤
│                                  │   "ZZ"    │
 ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘       ├───────────┤
┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─        │   "Bar"   │
                           │       ├───────────┤
│ ┌───────────┐ ┌─────┐            │   "ZZ"    │
  │"Fabulous" │ │  1  │    │       ├───────────┤
│ ├───────────┤ ├─────┤            │"Fabulous" │
  │   "ZZ"    │ │  2  │    │       └───────────┘
│ ├───────────┤ ├─────┤
  │   "Bar"   │ │  1  │    │
│ └───────────┘ ├─────┤
                │  0  │    │      Logical column
│               └─────┘               values
                Values     │
│  Dictionary (indexes in
              dictionary)  │
 ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘
</code></pre></div></div>

<p>The key observation which allows us to efficiently create a row format for this kind of data is that given a byte array, a new byte array can always be created which comes before or after it in the sort order by adding an additional byte.</p>

<p>Therefore we can incrementally build an order-preserving mapping from dictionary values to variable length byte arrays, without needing to know all possible dictionary values beforehand, instead introducing mappings for new dictionary values as we encounter them.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌──────────┐                 ┌─────┐
│  "Bar"   │ ───────────────▶│ 01  │
└──────────┘                 └─────┘
┌──────────┐                 ┌─────┬─────┐
│"Fabulous"│ ───────────────▶│ 01  │ 02  │
└──────────┘                 └─────┴─────┘
┌──────────┐                 ┌─────┐
│  "Soup"  │ ───────────────▶│ 05  │
└──────────┘                 └─────┘
┌──────────┐                 ┌─────┐
│   "ZZ"   │ ───────────────▶│ 07  │
└──────────┘                 └─────┘

    Example Order Preserving Mapping
</code></pre></div></div>

<p>The details of the data structure used to generate this mapping are beyond the scope of this blog post, but may be the topic of a future post. You can find <a href="https://github.com/apache/arrow-rs/blob/07024f6a16b870fda81cba5779b8817b20386ebf/arrow/src/row/interner.rs">the code here</a>.</p>

<p>The data structure also ensures that no values contain <code class="language-plaintext highlighter-rouge">0x00</code> and therefore we can encode the arrays directly using <code class="language-plaintext highlighter-rouge">0x00</code> as an end-delimiter.</p>

<p>A null value is encoded as a single <code class="language-plaintext highlighter-rouge">0x00</code> byte, and a non-null value encoded as a single <code class="language-plaintext highlighter-rouge">0x01</code> byte, followed by the <code class="language-plaintext highlighter-rouge">0x00</code> terminated byte array determined by the order preserving mapping.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                          ┌─────┬─────┬─────┬─────┐
   "Fabulous"             │ 01  │ 03  │ 05  │ 00  │
                          └─────┴─────┴─────┴─────┘

                          ┌─────┬─────┬─────┐
   "ZZ"                   │ 01  │ 07  │ 00  │
                          └─────┴─────┴─────┘

                          ┌─────┐
    NULL                  │ 00  │
                          └─────┘

     Input                  Row Format
</code></pre></div></div>

<h3 id="sort-options">Sort Options</h3>

<p>One detail we have so far ignored over is how to support ascending and descending sorts (e.g. <code class="language-plaintext highlighter-rouge">ASC</code> or <code class="language-plaintext highlighter-rouge">DESC</code> in SQL). The Arrow Rust row format supports these options by simply inverting the bytes of the encoded representation, except the initial byte used for nullability encoding, on a per column basis.</p>

<p>Similarly, supporting SQL compatible sorting also requires a format that can specify the order of <code class="language-plaintext highlighter-rouge">NULL</code>s (before or after all non <code class="language-plaintext highlighter-rouge">NULL</code> values). The row format supports this option by optionally encoding nulls as <code class="language-plaintext highlighter-rouge">0xFF</code> instead of <code class="language-plaintext highlighter-rouge">0x00</code> on a per column basis.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Hopefully these two articles have given you a flavor of what is possible with a comparable row format and how it works. Feel free to check out the <a href="https://docs.rs/arrow/27.0.0/arrow/row/index.html">docs</a> for instructions on getting started, and report any issues on our <a href="https://github.com/apache/arrow-rs/issues">bugtracker</a>.</p>

<p>Using this format for lexicographic sorting is more than <a href="https://github.com/apache/arrow-rs/pull/2929">3x</a> faster than the comparator based approach, with the benefits especially pronounced for strings, dictionaries and sorts with large numbers of columns.</p>

<p>We have also already used it to more than <a href="https://github.com/apache/arrow-datafusion/pull/3386">double</a> the performance of sort preserving merge in the <a href="https://arrow.apache.org/datafusion/">DataFusion project</a>, and expect similar or greater performance uplift as we apply it to sort, grouping, join, and window function operators as well.</p>

<p>As always, the <a href="https://github.com/apache/arrow-rs#arrow-rust-community">Arrow community</a> very much looks forward to seeing what you build with it!</p>]]></content><author><name>tustvold and alamb</name></author><category term="arrow" /><summary type="html"><![CDATA[Introduction In Part 1 of this post, we described the problem of Multi-Column Sorting and the challenges of implementing it efficiently. This second post explains how the new row format in the Rust implementation of Apache Arrow works and is constructed. Row Format The row format is a variable length byte sequence created by concatenating the encoded form of each column. The encoding for each column depends on its datatype (and sort options). ┌─────┐ ┌─────┐ ┌─────┐ │ │ │ │ │ │ ├─────┤ ┌ ┼─────┼ ─ ┼─────┼ ┐ ┏━━━━━━━━━━━━━┓ │ │ │ │ │ │ ─────────────▶┃ ┃ ├─────┤ └ ┼─────┼ ─ ┼─────┼ ┘ ┗━━━━━━━━━━━━━┛ │ │ │ │ │ │ └─────┘ └─────┘ └─────┘ ... ┌─────┐ ┌ ┬─────┬ ─ ┬─────┬ ┐ ┏━━━━━━━━┓ │ │ │ │ │ │ ─────────────▶┃ ┃ └─────┘ └ ┴─────┴ ─ ┴─────┴ ┘ ┗━━━━━━━━┛ Customer State Orders UInt64 Utf8 F64 Input Arrays Row Format (Columns) The encoding is carefully designed in such a way that escaping is unnecessary: it is never ambiguous as to whether a byte is part of a sentinel (e.g. null) or a value. Unsigned Integers To encode a non-null unsigned integer, the byte 0x01 is written, followed by the integer’s bytes starting with the most significant, i.e. big endian. A null is encoded as a 0x00 byte, followed by the encoded bytes of the integer’s zero value ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┬──┐ 3 │03│00│00│00│ │01│00│00│00│03│ └──┴──┴──┴──┘ └──┴──┴──┴──┴──┘ ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┬──┐ 258 │02│01│00│00│ │01│00│00│01│02│ └──┴──┴──┴──┘ └──┴──┴──┴──┴──┘ ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┬──┐ 23423 │7F│5B│00│00│ │01│00│00│5B│7F│ └──┴──┴──┴──┘ └──┴──┴──┴──┴──┘ ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┬──┐ NULL │??│??│??│??│ │00│00│00│00│00│ └──┴──┴──┴──┘ └──┴──┴──┴──┴──┘ 32-bit (4 bytes) Row Format Value Little Endian Signed Integers In Rust and most modern computer architectures, signed integers are encoded using two’s complement, where a number is negated by flipping all the bits, and adding 1. Therefore, flipping the top-most bit and treating the result as an unsigned integer preserves the order. This unsigned integer can then be encoded using the same encoding for unsigned integers described in the previous section. For example ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┬──┐ 5 │05│00│00│00│ │05│00│00│80│ │01│80│00│00│05│ └──┴──┴──┴──┘ └──┴──┴──┴──┘ └──┴──┴──┴──┴──┘ ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┬──┐ -5 │FB│FF│FF│FF│ │FB│FF│FF│7F│ │01│7F│FF│FF│FB│ └──┴──┴──┴──┘ └──┴──┴──┴──┘ └──┴──┴──┴──┴──┘ Value 32-bit (4 bytes) High bit flipped Row Format Little Endian Floating Point Floating point values can be ordered according to the IEEE 754 totalOrder predicate (implemented in Rust by f32::total_cmp). This ordering interprets the bytes of the floating point value as the correspondingly sized, signed, little-endian integer, flipping all the bits except the sign bit in the case of negatives. Floating point values are therefore encoded to row format by converting them to the appropriate sized signed integer representation, and then using the same encoding for signed integers described in the previous section. Byte Arrays (Including Strings) Unlike primitive types above, byte arrays are variable length. For short strings, such as state in our example above, it is possible to pad all values to the length of the longest one with some fixed value such as 0x00 and produce a fixed length row. This is the approach described in the DuckDB blog for encoding c_birth_country. However, often values in string columns differ substantially in length or the maximum length is not known at the start of execution, making it inadvisable and/or impractical to pad the strings to a fixed length. The Rust Arrow row format therefore uses a variable length encoding. We need an encoding that unambiguously terminates the end of the byte array. This not only permits recovering the original value from the row format, but ensures that bytes of a longer byte array are not compared against bytes from a different column when compared against a row containing a shorter byte array. A null byte array is encoded as a single 0x00 byte. Similarly, an empty byte array is encoded as a single 0x01 byte. To encode a non-null, non-empty array, first a single 0x02 byte is written. Then the array is written in 32-byte blocks, with each complete block followed by a 0xFF byte as a continuation token. The final block is padded to 32-bytes with 0x00, and is then followed by the unpadded length of this final block as a single byte in place of a continuation token Note the following example encodings use a block size of 4 bytes, as opposed to 32 bytes for brevity ┌───┬───┬───┬───┬───┬───┐ "MEEP" │02 │'M'│'E'│'E'│'P'│04 │ └───┴───┴───┴───┴───┴───┘ ┌───┐ "" │01 | └───┘ NULL ┌───┐ │00 │ └───┘ "Defenestration" ┌───┬───┬───┬───┬───┬───┐ │02 │'D'│'e'│'f'│'e'│FF │ └───┼───┼───┼───┼───┼───┤ │'n'│'e'│'s'│'t'│FF │ ├───┼───┼───┼───┼───┤ │'r'│'a'│'t'│'i'│FF │ ├───┼───┼───┼───┼───┤ │'o'│'n'│00 │00 │02 │ └───┴───┴───┴───┴───┘ This approach is loosely inspired by COBS encoding, and chosen over more traditional byte stuffing as it is more amenable to vectorization, in particular hardware with AVX-256 can copy a 32-byte block in a single instruction. Dictionary Arrays Dictionary Encoded Data (called categorical in pandas) is increasingly important because they can store and process low cardinality data very efficiently. A simple approach to encoding dictionary arrays would be to encode the logical values directly using the encodings for primitive values described previously. However, this would lose the benefits of dictionary encoding to reduce memory and CPU consumption. To further complicate matters, the Arrow implementation of Dictionary encoding is quite general, and we can make no assumptions about the contents of the dictionaries. In particular, we cannot assume that the dictionary values are sorted, nor that the same dictionary is used for all arrays within a column The following example shows how a string column might be encoded in two arrays using two different dictionaries. The dictionary keys 0, 1, and 2 in the first batch correspond to different values than the same keys in the second dictionary. ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌───────────┐ ┌─────┐ │ │ │"Fabulous" │ │ 0 │ ├───────────┤ ├─────┤ │ │ │ "Bar" │ │ 2 │ ├───────────┤ ├─────┤ │ ┌───────────┐ │ │ "Soup" │ │ 2 │ │"Fabulous" │ └───────────┘ ├─────┤ │ ├───────────┤ │ │ 0 │ │ "Soup" │ ├─────┤ │ ├───────────┤ │ │ 1 │ │ "Soup" │ └─────┘ │ ├───────────┤ │ │"Fabulous" │ Values │ ├───────────┤ │ Dictionary (indexes in │ "Bar" │ dictionary) │ ├───────────┤ │ │ "ZZ" │ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ├───────────┤ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ "Bar" │ │ ├───────────┤ │ ┌───────────┐ ┌─────┐ │ "ZZ" │ │"Fabulous" │ │ 1 │ │ ├───────────┤ │ ├───────────┤ ├─────┤ │"Fabulous" │ │ "ZZ" │ │ 2 │ │ └───────────┘ │ ├───────────┤ ├─────┤ │ "Bar" │ │ 1 │ │ │ └───────────┘ ├─────┤ │ 0 │ │ Logical column │ └─────┘ values Values │ │ Dictionary (indexes in dictionary) │ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ The key observation which allows us to efficiently create a row format for this kind of data is that given a byte array, a new byte array can always be created which comes before or after it in the sort order by adding an additional byte. Therefore we can incrementally build an order-preserving mapping from dictionary values to variable length byte arrays, without needing to know all possible dictionary values beforehand, instead introducing mappings for new dictionary values as we encounter them. ┌──────────┐ ┌─────┐ │ "Bar" │ ───────────────▶│ 01 │ └──────────┘ └─────┘ ┌──────────┐ ┌─────┬─────┐ │"Fabulous"│ ───────────────▶│ 01 │ 02 │ └──────────┘ └─────┴─────┘ ┌──────────┐ ┌─────┐ │ "Soup" │ ───────────────▶│ 05 │ └──────────┘ └─────┘ ┌──────────┐ ┌─────┐ │ "ZZ" │ ───────────────▶│ 07 │ └──────────┘ └─────┘ Example Order Preserving Mapping The details of the data structure used to generate this mapping are beyond the scope of this blog post, but may be the topic of a future post. You can find the code here. The data structure also ensures that no values contain 0x00 and therefore we can encode the arrays directly using 0x00 as an end-delimiter. A null value is encoded as a single 0x00 byte, and a non-null value encoded as a single 0x01 byte, followed by the 0x00 terminated byte array determined by the order preserving mapping. ┌─────┬─────┬─────┬─────┐ "Fabulous" │ 01 │ 03 │ 05 │ 00 │ └─────┴─────┴─────┴─────┘ ┌─────┬─────┬─────┐ "ZZ" │ 01 │ 07 │ 00 │ └─────┴─────┴─────┘ ┌─────┐ NULL │ 00 │ └─────┘ Input Row Format Sort Options One detail we have so far ignored over is how to support ascending and descending sorts (e.g. ASC or DESC in SQL). The Arrow Rust row format supports these options by simply inverting the bytes of the encoded representation, except the initial byte used for nullability encoding, on a per column basis. Similarly, supporting SQL compatible sorting also requires a format that can specify the order of NULLs (before or after all non NULL values). The row format supports this option by optionally encoding nulls as 0xFF instead of 0x00 on a per column basis. Conclusion Hopefully these two articles have given you a flavor of what is possible with a comparable row format and how it works. Feel free to check out the docs for instructions on getting started, and report any issues on our bugtracker. Using this format for lexicographic sorting is more than 3x faster than the comparator based approach, with the benefits especially pronounced for strings, dictionaries and sorts with large numbers of columns. We have also already used it to more than double the performance of sort preserving merge in the DataFusion project, and expect similar or greater performance uplift as we apply it to sort, grouping, join, and window function operators as well. As always, the Arrow community very much looks forward to seeing what you build with it!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Expanding Arrow’s Reach with a JDBC Driver for Arrow Flight SQL</title><link href="https://arrow.apache.org/blog/2022/11/01/arrow-flight-sql-jdbc/" rel="alternate" type="text/html" title="Expanding Arrow’s Reach with a JDBC Driver for Arrow Flight SQL" /><published>2022-11-01T00:00:00-04:00</published><updated>2022-11-01T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/11/01/arrow-flight-sql-jdbc</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/11/01/arrow-flight-sql-jdbc/"><![CDATA[<!--

-->

<p>We’re excited to announce that as of version 10.0.0, the Arrow project
now includes a <a href="https://docs.oracle.com/javase/tutorial/jdbc/overview/index.html">JDBC</a> driver <a href="https://github.com/apache/arrow/tree/master/java/flight/flight-sql-jdbc-driver">implementation</a> based on
<a href="/docs/format/FlightSql.html">Arrow Flight SQL</a>.  This is courtesy of a software grant
from <a href="https://www.dremio.com/resources/guides/apache-arrow/">Dremio</a>, a data lakehouse platform. Contributors
from Dremio developed and open-sourced this driver implementation, in
addition to designing and contributing Flight SQL itself.</p>

<p>Flight SQL is a protocol for client-server database interactions.  It
defines how a client should talk to a server and execute queries,
fetch result sets, and so on.  Note that despite the name, Flight SQL
is <em>not</em> a SQL dialect, or even specific to SQL itself.  Underneath,
it builds on <a href="/docs/format/Flight.html">Arrow Flight RPC</a>, a framework for efficient
transfer of Arrow data across the network.  While Flight RPC is
flexible and can be used in any type of application, from the
beginning, it was designed with an eye towards the kinds of use cases
that Flight SQL supports.</p>

<p>With this new JDBC driver, applications can talk to any database
server implementing the Flight SQL protocol using familiar JDBC APIs.
Underneath, the driver sends queries to the server via Flight SQL and
adapts the Arrow result set to the JDBC interface, so that the
database can support JDBC users without implementing additional APIs
or its own JDBC driver.</p>

<h2 id="why-use-jdbc-with-flight-sql">Why use JDBC with Flight SQL?</h2>

<p>JDBC offers a row-oriented API, which is opposite of Arrow’s columnar
structure.  However, it is a popular and time-tested choice for many
applications.  For example, many business intelligence (BI) tools take
advantage of JDBC to interoperate generically with multiple databases.
An Arrow-native database may still wish to be accessible to all of
this existing software, without having to implement multiple client
drivers itself.  Additionally, columnar data transfer alone can be a
<a href="https://ir.cwi.nl/pub/26415">significant speedup</a> for analytical use cases.</p>

<p>This JDBC driver implementation demonstrates the generality of Arrow
and Flight SQL, and increases the reach of Arrow-based applications.
Additionally, an <a href="https://docs.dremio.com/software/drivers/arrow-flight-sql-odbc-driver/">ODBC driver implementation</a> based on
Flight SQL is also available courtesy of Dremio, though it is not yet
part of the Arrow project due to dependency licensing issues.</p>

<p>Now, a database can support the vast body of existing code that uses
JDBC or ODBC, as well as Arrow-native applications, just by
implementing a single wire protocol: Flight SQL.  Some projects
instead do things like reimplementing the Postgres wire protocol to
benefit from its existing drivers.  But for Arrow-native databases,
this gives up the benefits of columnar data.  On the other hand,
Flight SQL is:</p>

<ol>
  <li>Columnar and Arrow-native, using Arrow for result sets to avoid
unnecessary data copies and transformations;</li>
  <li>Designed for implementation by multiple databases, with high-level
C++ and Java libraries and a Protobuf protocol definition; and</li>
  <li>Usable both through APIs like JDBC and ODBC thanks to this software
grant, as well as directly (or via <a href="htttps://github.com/apache/arrow-adbc">ADBC</a>) for applications
that want columnar data.</li>
</ol>

<h2 id="getting-involved">Getting Involved</h2>

<p>The JDBC driver was merged for the <a href="/blog/2022/10/31/10.0.0-release/">Arrow 10.0.0 release</a>, and
the <a href="https://github.com/apache/arrow/tree/master/java/flight/flight-sql-jdbc-driver">source code</a> can be found in the Arrow repository.
Official builds of the driver are <a href="https://search.maven.org/search?q=a:flight-sql-jdbc-driver">available on Maven Central</a>.
Dremio is already making use of the driver, and we’re looking forward
to seeing what else gets built on top.  Of course, there are still
improvements to be made.  If you’re interested in contributing, or
have feedback or questions, please reach out on the <a href="/community/">mailing list</a>
or <a href="htttps://github.com/apache/arrow">GitHub</a>.</p>

<p>To learn more about when to use the Flight SQL JDBC driver vs the
Flight SQL native client libraries, see this section of Dremio’s
presentation, <a href="https://www.youtube.com/watch?v=6q8AMrQV3vE&amp;t=1343s">“Apache Arrow Flight SQL: a universal standard for high
performance data transfers from databases”</a>
(starting at 22:23).  For more about how Dremio uses Apache Arrow, see
their <a href="https://www.dremio.com/resources/guides/apache-arrow/">guide</a>.</p>]]></content><author><name>pmc</name></author><category term="application" /><summary type="html"><![CDATA[We’re excited to announce that as of version 10.0.0, the Arrow project now includes a JDBC driver implementation based on Arrow Flight SQL. This is courtesy of a software grant from Dremio, a data lakehouse platform. Contributors from Dremio developed and open-sourced this driver implementation, in addition to designing and contributing Flight SQL itself. Flight SQL is a protocol for client-server database interactions. It defines how a client should talk to a server and execute queries, fetch result sets, and so on. Note that despite the name, Flight SQL is not a SQL dialect, or even specific to SQL itself. Underneath, it builds on Arrow Flight RPC, a framework for efficient transfer of Arrow data across the network. While Flight RPC is flexible and can be used in any type of application, from the beginning, it was designed with an eye towards the kinds of use cases that Flight SQL supports. With this new JDBC driver, applications can talk to any database server implementing the Flight SQL protocol using familiar JDBC APIs. Underneath, the driver sends queries to the server via Flight SQL and adapts the Arrow result set to the JDBC interface, so that the database can support JDBC users without implementing additional APIs or its own JDBC driver. Why use JDBC with Flight SQL? JDBC offers a row-oriented API, which is opposite of Arrow’s columnar structure. However, it is a popular and time-tested choice for many applications. For example, many business intelligence (BI) tools take advantage of JDBC to interoperate generically with multiple databases. An Arrow-native database may still wish to be accessible to all of this existing software, without having to implement multiple client drivers itself. Additionally, columnar data transfer alone can be a significant speedup for analytical use cases. This JDBC driver implementation demonstrates the generality of Arrow and Flight SQL, and increases the reach of Arrow-based applications. Additionally, an ODBC driver implementation based on Flight SQL is also available courtesy of Dremio, though it is not yet part of the Arrow project due to dependency licensing issues. Now, a database can support the vast body of existing code that uses JDBC or ODBC, as well as Arrow-native applications, just by implementing a single wire protocol: Flight SQL. Some projects instead do things like reimplementing the Postgres wire protocol to benefit from its existing drivers. But for Arrow-native databases, this gives up the benefits of columnar data. On the other hand, Flight SQL is: Columnar and Arrow-native, using Arrow for result sets to avoid unnecessary data copies and transformations; Designed for implementation by multiple databases, with high-level C++ and Java libraries and a Protobuf protocol definition; and Usable both through APIs like JDBC and ODBC thanks to this software grant, as well as directly (or via ADBC) for applications that want columnar data. Getting Involved The JDBC driver was merged for the Arrow 10.0.0 release, and the source code can be found in the Arrow repository. Official builds of the driver are available on Maven Central. Dremio is already making use of the driver, and we’re looking forward to seeing what else gets built on top. Of course, there are still improvements to be made. If you’re interested in contributing, or have feedback or questions, please reach out on the mailing list or GitHub. To learn more about when to use the Flight SQL JDBC driver vs the Flight SQL native client libraries, see this section of Dremio’s presentation, “Apache Arrow Flight SQL: a universal standard for high performance data transfers from databases” (starting at 22:23). For more about how Dremio uses Apache Arrow, see their guide.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>