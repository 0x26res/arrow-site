<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://arrow.apache.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arrow.apache.org/" rel="alternate" type="text/html" /><updated>2023-04-11T07:09:06-04:00</updated><id>https://arrow.apache.org/feed.xml</id><title type="html">Apache Arrow</title><subtitle>Apache Arrow is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, C++, C#, Go, Java, JavaScript, MATLAB, Python, R, Ruby, and Rust.</subtitle><entry><title type="html">Our journey at F5 with Apache Arrow (part 1)</title><link href="https://arrow.apache.org/blog/2023/04/11/our-journey-at-f5-with-apache-arrow-part-1/" rel="alternate" type="text/html" title="Our journey at F5 with Apache Arrow (part 1)" /><published>2023-04-11T00:00:00-04:00</published><updated>2023-04-11T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/04/11/our-journey-at-f5-with-apache-arrow-part-1</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/04/11/our-journey-at-f5-with-apache-arrow-part-1/"><![CDATA[<!--

-->

<p>Apache Arrow is a technology widely adopted in big data, analytics, and machine learning applications. In this article, we share <a href="https://www.f5.com/">F5</a>’s experience with Arrow, specifically its application to telemetry, and the challenges we encountered while optimizing the OpenTelemetry protocol to significantly reduce bandwidth costs. The promising results we achieved inspired us to share our insights. This article specifically focuses on transforming relatively complex data structure from various formats into an efficient Arrow representation that optimizes both compression ratio, transport, and data processing. We also explore the trade-offs between different mapping and normalization strategies, as well as the nuances of streaming and batch communication using Arrow and Arrow Flight. Our benchmarks thus far have shown promising results, with compression ratio improvements ranging from 1.5x to 5x, depending on the data type (metrics, logs, traces), distribution, and compression algorithm. The approaches presented for addressing these challenges may be applicable to other Arrow domains as well. This article serves as the first installment in a two-part series.</p>

<h2 id="what-is-apache-arrow">What is Apache Arrow</h2>

<p><a href="https://arrow.apache.org/docs/index.html">Apache Arrow</a> is an open-source project offering a standardized, language-agnostic in-memory format for representing structured and semi-structured data. This enables data sharing and zero-copy data access between systems, eliminating the need for serialization and deserialization when exchanging datasets between varying CPU architectures and programming languages. Furthermore, Arrow libraries feature an extensive set of high-performance, parallel, and vectorized kernel functions designed for efficiently processing massive amounts of columnar data. These features make Arrow an appealing technology for big data processing, data transport, analytics, and machine learning applications. The growing number of <a href="https://arrow.apache.org/powered_by/">products and open-source projects</a> that have adopted Apache Arrow at their core or offer Arrow support reflects the widespread recognition and appreciation of its benefits (refer to this <a href="https://www.dremio.com/blog/apache-arrows-rapid-growth-over-the-years/">article</a> for an in-depth overview of the Arrow ecosystem and adoption). Over 11,000 GitHub users support this project, and 840+ are contributors who make this project an undeniable success.</p>

<p>Very often people ask about the differences between Arrow and <a href="https://parquet.apache.org/">Apache Parquet</a> or other columnar file formats. Arrow is designed and optimized for in-memory processing, while Parquet is tailored for disk-based storage. In reality, these technologies are complementary, with bridges existing between them to simplify interoperability. In both cases, data is represented in columns to optimize access, data locality and compressibility. However, the tradeoffs differ slightly. Arrow prioritizes data processing speed over the optimal data encoding. Complex encodings that don’t benefit from SIMD instruction sets are generally not natively supported by Arrow, unlike formats such as Parquet. Storing data in Parquet format and processing and transporting it in Arrow format has become a prevalent model within the big data community.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/row-vs-columnar.svg" width="100%" class="img-responsive" alt="Memory representations: row vs columnar data." />
  <figcaption>Fig 1: Memory representations: row vs columnar data.</figcaption>
</figure>

<p>Figure 1 illustrates the differences in memory representation between row-oriented and column-oriented approaches. The column-oriented approach groups data from the same column in a continuous memory area, which facilitates parallel processing (SIMD) and enhances compression performance.</p>

<h2 id="why-are-we-interested-in-apache-arrow">Why are we interested in Apache Arrow</h2>

<p>At <a href="https://www.f5.com/">F5</a>, we’ve adopted <a href="https://opentelemetry.io/">OpenTelemetry</a> (OTel) as the standard for all telemetry across our products, such as BIGIP and NGINX. These products may generate large volumes of metrics and logs for various reasons, from performance evaluation to forensic purposes. The data produced by these systems is typically centralized and processed in dedicated systems. Transporting and processing this data accounts for a significant portion of the cost associated with telemetry pipelines. In this context, we became interested in Apache Arrow. Instead of reinventing yet another telemetry solution, we decided to invest in the OpenTelemetry project, working on improvements to the protocol to significantly increase its efficiency with high telemetry data volumes. We collaborated with <a href="https://github.com/jmacd">Joshua MacDonald</a> from <a href="https://lightstep.com/">Lightstep</a> to integrate these optimizations into an <a href="https://github.com/open-telemetry/experimental-arrow-collector">experimental OTel collector</a> and are currently in discussions with the OTel technical committee to finalize a code <a href="https://github.com/open-telemetry/community/issues/1332">donation</a>.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/performance.svg" width="100%" class="img-responsive" alt="Performance improvement in the OpenTelemetry Arrow experimental project." />
  <figcaption>Fig 2: Performance improvement in the OpenTelemetry Arrow experimental project.</figcaption>
</figure>

<p>This project has been divided into two phases. The first phase, which is nearing completion, aims to enhance the protocol’s compression ratio. The second phase, planned for the future, focuses on improving end-to-end performance by incorporating Apache Arrow throughout all levels, eliminating the need for conversion between old and new protocols. The results so far are promising, with our benchmarks showing compression ratio improvements ranging from x1.5 to x5, depending on the data type (metrics, logs, traces), distribution, and compression algorithm. For the second phase, our estimates suggest that data processing acceleration could range from x2 to x12, again depending on the data’s nature and distribution. For more information, we encourage you to review the <a href="https://github.com/lquerel/oteps/blob/main/text/0156-columnar-encoding.md">specifications</a> and the <a href="https://github.com/f5/otel-arrow-adapter">reference implementation</a>.</p>

<p>Arrow relies on a schema to define the structure of data batches that it processes and transports. The subsequent sections will discuss various techniques that can be employed to optimize the creation of these schemas.</p>

<h2 id="how-to-leverage-arrow-to-optimize-network-transport-cost">How to leverage Arrow to optimize network transport cost</h2>

<p>Apache Arrow is a complex project with a rapidly evolving ecosystem, which can sometimes be overwhelming for newcomers. Fortunately the Arrow community has published three introductory articles <a href="https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/">1</a>, <a href="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/">2</a>, and <a href="https://arrow.apache.org/blog/2022/10/17/arrow-parquet-encoding-part-3/">3</a> that we recommend for those interested in exploring this technology.</p>

<p>This article primarily focuses on transforming data from an XYZ format into an efficient Arrow representation that optimizes both compression ratio and data processing. There are numerous approaches to this transformation, and we will examine how these methods can impact compression ratio, CPU usage, and memory consumption during the conversion process, among other factors.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/schema-optim-process.svg" width="100%" class="img-responsive" alt="Fig 3: Optimization process for the definition of an Arrow schema." />
  <figcaption>Fig 3: Optimization process for the definition of an Arrow schema.</figcaption>
</figure>

<p>The complexity of your initial model significantly impacts the Arrow mapping choices you need to make. To begin, it’s essential to identify the properties you want to optimize for your specific context. Compression rate, conversion speed, memory consumption, speed and ease of use of the final model, compatibility, and extensibility are all factors that can influence your final mapping decisions. From there, you must explore multiple alternative schemas.</p>

<p>The choice of the Arrow type and data encoding for each individual field will affect the performance of your schema. There are various ways to represent hierarchical data or highly dynamic data models, and multiple options need to be evaluated in coordination with the configuration of the transport layer. This transport layer should also be carefully considered. Arrow supports compression mechanisms and dictionary deltas that may not be active by default.</p>

<p>After several iterations of this process, you should arrive at an optimized schema that meets the goals you initially set. It’s crucial to compare the performance of your different approaches using real data, as the distribution of data in each individual field may influence whether you use dictionary encoding or not. We will now examine these choices in greater detail throughout the remainder of this article.</p>

<h2 id="arrow-data-type-selection">Arrow data type selection</h2>

<p>The principles of selecting an Arrow data type are quite similar to those used when defining a data model for databases. Arrow supports a wide range of data types. Some of these types are supported by all implementations, while others are only available for languages with the strongest Arrow community support (see this <a href="https://arrow.apache.org/docs/status.html">page</a> for a comparison matrix of the different implementations). For primitive types, it is generally preferable to choose the type that offers the most concise representation and is closest to the semantics of your initial field. For example, while it’s possible to represent a timestamp with an int64, it’s more advantageous to use the native Arrow Timestamp type. This choice isn’t due to a more efficient binary representation, but rather because it will be easier to process and manipulate in your pipeline. Query engines such as <a href="https://github.com/apache/arrow-datafusion">DataFusion</a> offer dedicated timestamp handling functions for columns of this type. The same choices can be made for primitive types such as date, time, duration, and interval. However, if your project requires maximum compatibility, it may be crucial in some cases to favor types with universal support instead of the most optimal type in terms of memory occupation.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/data-types.svg" width="100%" class="img-responsive" alt="Fig 4: Data types supported by Apache Arrow." />
  <figcaption>Fig 4: Data types supported by Apache Arrow.</figcaption>
</figure>

<p>When selecting the Arrow data type, it’s important to consider the size of the data before and after compression. It’s quite possible that the size after compression is the same for two different types, but the actual size in memory may be two, four, or even eight times larger (e.g., uint8 vs. uint64). This difference will impact your ability to process large batches of data and will also significantly influence the speed of processing these data in memory (e.g., cache optimization, SIMD instruction efficiency).</p>

<p>It’s also possible to extend these types using an <a href="https://arrow.apache.org/docs/format/Columnar.html#extension-types">extension type</a> mechanism that builds upon one of the currently supported primitive types while adding specific semantics. This extension mechanism can simplify the use of this data in your own project, while remaining transparent to intermediate systems that will interpret this data as a basic primitive type.</p>

<p>There are some variations in the encoding of primitive types, which we will explore next.</p>

<h2 id="data-encoding">Data encoding</h2>

<p>Another crucial aspect of optimizing your Arrow schema is analyzing the cardinality of your data. Fields that can have only a limited number of values will typically be more efficiently represented with a dictionary encoding.</p>

<p>The maximum cardinality of a field determines the data type characteristics of your dictionary. For instance, for a field representing the status code of an HTTP transaction, it’s preferable to use a dictionary with an index of type ‘uint8’ and a value of type ‘uint16’ (notation: ‘Dictionary&lt;uint8, uint16&gt;’). This consumes less memory because the main array will be of type ‘[]uint8’. Even if the range of possible values is greater than 255, as long as the number of distinct values does not exceed 255, the representation remains efficient. Similarly, the representation of a ‘user-agent’ will be more efficient with a dictionary of type ‘Dictionary&lt;uint16, string&gt;’ (see figure 5). In this case, the main array will be of type ‘uint16’, allowing a compact representation in memory and during transfers at the cost of an indirection during reverse conversion.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/dictionary-encoding.svg" width="90%" class="img-responsive" alt="Fig 5: Dictionary encoding." />
  <figcaption>Fig 5: Dictionary encoding.</figcaption>
</figure>

<p>Dictionary encoding is highly flexible in Apache Arrow, allowing the creation of encodings for any Arrow primitive type. The size of the indices can also be configured based on the context.</p>

<p>In general, it is advisable to use dictionaries in the following cases:</p>
<ul>
  <li>Representation of enumerations</li>
  <li>Representation of textual or binary fields with a high probability of having redundant values.</li>
  <li>Representation of fields with cardinalities known to be below 2^16 or 2^32.</li>
</ul>

<p>Sometimes, the cardinality of a field is not known a priori. For example, a proxy that transforms a data stream from a row-oriented format into a series of columnar-encoded batches (e.g., OpenTelemetry collector) may not be able to predict in advance whether a field will have a fixed number of distinct values. Two approaches are possible: 
1) a conservative approach using the largest data type (e.g., ‘int64’, ‘string’, etc., instead of dictionary), 
2) an adaptive approach that modifies the schema on the fly based on the observed cardinality of the field(s). In this second approach, without cardinality information, you can optimistically start by using a ‘Dictionary&lt;uint8, original-field-type&gt;’ dictionary, then detect a potential dictionary overflow during conversion, and change the schema to a ‘Dictionary&lt;uint16, original-field-type&gt;’ in case of an overflow. This technique of automatic management of dictionary overflows will be presented in greater detail in a future article.</p>

<p>Recent advancements in Apache Arrow include the implementation of <a href="https://arrow.apache.org/docs/format/Columnar.html#run-end-encoded-layout">run-end encoding</a>, a technique that efficiently represents data with sequences of repeated values. This encoding method is particularly beneficial for handling data sets containing long stretches of identical values, as it offers a more compact and optimized representation.</p>

<p>In conclusion, dictionary encoding not only occupies less space in memory and during transfers but also significantly improves the compression ratio and data processing speed. However, this type of representation requires indirection when extracting the initial values (although this isn’t always necessary, even during some data processing operations). Additionally, it is important to manage dictionary index overflow, especially when the encoded field doesn’t have a well-defined cardinality.</p>

<h2 id="hierarchical-data">Hierarchical data</h2>

<p>Basic hierarchical data structures translate relatively well into Arrow. However, as we will see, there are some complications to handle in more  general cases (see figure 6). While Arrow schemas do support nested structures, maps, and unions, some components of the Arrow ecosystem do not fully support them, which can make these Arrow data types unsuitable for certain scenarios. Additionally, unlike most languages and formats, such as Protobuf, Arrow doesn’t support the concept of a recursively defined schema. An Arrow schema is static in its definition, and the depth of its nested elements must be known in advance. There are multiple strategies to work around this limitation and we’ll explore these in the following sections.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/simple-vs-complex-data-model.svg" width="100%" class="img-responsive" alt="Fig 6: simple vs complex data model." />
  <figcaption>Fig 6: simple vs complex data model.</figcaption>
</figure>

<h3 id="natural-representation">Natural representation</h3>

<p>The most straightforward and intuitive approach to representing a simple hierarchical data model is to use Arrow’s list, map, and union data types. However, it’s important to note that some of these data types are not fully supported throughout the entire Arrow ecosystem. For example, the conversion of unions to Parquet is <a href="https://issues.apache.org/jira/browse/PARQUET-756">not directly supported</a> and requires a transformation step (see <a href="https://docs.google.com/document/d/11lG7Go2IgKOyW-RReBRW6r7HIdV1X7lu5WrDGlW5LbQ/edit#heading=h.nlplaj34c4ke">denormalization &amp; flattening representation</a> to decompose a sparse union into a nullable struct and type ids column). Similarly, lists and maps are <a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html">not yet supported</a> in DataFusion version 20 (nested structures are partially supported).</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/hierarchical-data-model.svg" width="80%" class="img-responsive" alt="Fig 7: initial data model." />
  <figcaption>Fig 7: initial data model.</figcaption>
</figure>

<p>The following example is a Go program snippet of an Arrow schema using these different data types to represent the model above.</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="s">"github.com/apache/arrow/go/v11/arrow"</span>


<span class="k">const</span> <span class="p">(</span>
  <span class="n">GaugeMetricCode</span> <span class="n">arrow</span><span class="o">.</span><span class="n">UnionTypeCode</span> <span class="o">=</span> <span class="m">0</span>
  <span class="n">SumMetricCode</span>   <span class="n">arrow</span><span class="o">.</span><span class="n">UnionTypeCode</span> <span class="o">=</span> <span class="m">1</span>
<span class="p">)</span>


<span class="k">var</span> <span class="p">(</span>
  <span class="c">// uint8Dictionary represent a Dictionary&lt;Uint8, String&gt;</span>
  <span class="n">uint8Dictionary</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">DictionaryType</span><span class="p">{</span>
     <span class="n">IndexType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint8</span><span class="p">,</span>
     <span class="n">ValueType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span>
  <span class="p">}</span>
  <span class="c">// uint16Dictionary represent a Dictionary&lt;Uint16, String&gt;</span>
  <span class="n">uint16Dictionary</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">DictionaryType</span><span class="p">{</span>
     <span class="n">IndexType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint16</span><span class="p">,</span>
     <span class="n">ValueType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span>
  <span class="p">}</span>


  <span class="n">Schema</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">.</span><span class="n">NewSchema</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
     <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"resource_metrics"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">ListOf</span><span class="p">(</span><span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"scope"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
           <span class="c">// Name and Version are declared as dictionaries (Dictionary&lt;Uint16, String&gt;)).</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"version"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
        <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">ListOf</span><span class="p">(</span><span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"unit"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint8Dictionary</span><span class="p">},</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"timestamp"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">TIMESTAMP</span><span class="p">},</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metric_type"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">UINT8</span><span class="p">},</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"data_point"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">ListOf</span><span class="p">(</span><span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
              <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metric"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">DenseUnionOf</span><span class="p">(</span>
                 <span class="p">[]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
                    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"gauge"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
                       <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"data_point"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FLOAT64</span><span class="p">},</span>
                    <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
                    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"sum"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
                       <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"data_point"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FLOAT64</span><span class="p">},</span>
                       <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"is_monotonic"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BOOL</span><span class="p">},</span>
                    <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
                 <span class="p">},</span>
                 <span class="p">[]</span><span class="n">arrow</span><span class="o">.</span><span class="n">UnionTypeCode</span><span class="p">{</span><span class="n">GaugeMetricCode</span><span class="p">,</span> <span class="n">SumMetricCode</span><span class="p">},</span>
              <span class="p">)},</span>
           <span class="p">}</span><span class="o">...</span><span class="p">))},</span>
        <span class="p">}</span><span class="o">...</span><span class="p">))},</span>
     <span class="p">}</span><span class="o">...</span><span class="p">))},</span>
  <span class="p">},</span> <span class="no">nil</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div>

<p>In this pattern, we use a union type to represent an inheritance relationship. There are two types of Arrow union that are optimized for different cases. The dense union type has a relatively succinct memory representation but doesn’t support vectorizable operations, making it less efficient during the processing phase. Conversely, a sparse union supports vectorization operations, but comes with a memory overhead directly proportional to the number of variants in the union. Dense and sparse unions have quite similar compression rates, with sometimes a slight advantage for sparse unions. In addition, sparse unions with a large number of variants should generally be avoided, as they can lead to excessive memory consumption. For more details on the memory representation of unions, you can consult this <a href="https://wesm.github.io/arrow-site-test/format/Layout.html#dense-union-type">page</a>.</p>

<p>In certain scenarios, it may be more idiomatic to represent the inheritance relationship using multiple schemas (i.e., one schema per subtype), thereby avoiding the use of the union type. However, applying this approach to the aforementioned model may not be optimal, as the data preceding the inheritance relationship (i.e., <code class="language-plaintext highlighter-rouge">ResourceMetrics</code>, <code class="language-plaintext highlighter-rouge">Scope</code>, and <code class="language-plaintext highlighter-rouge">Metrics</code>) could potentially be duplicated numerous times. If the relationships between <code class="language-plaintext highlighter-rouge">ResourceMetrics</code>, <code class="language-plaintext highlighter-rouge">Metrics</code>, and <code class="language-plaintext highlighter-rouge">DataPoint</code> were 0..1 (zero-to-one) relationships, then the multi-schema approach would likely be the simplest and most idiomatic solution.</p>

<h3 id="denormalization--flattening-representations">Denormalization &amp; Flattening representations</h3>

<p>If the <code class="language-plaintext highlighter-rouge">List</code> type is not supported in your telemetry pipeline, you can denormalize your data model. This process is often used in the database world to remove a join between two tables for optimization purposes. In the Arrow world, denormalization is employed to eliminate the <code class="language-plaintext highlighter-rouge">List</code> type by duplicating some data. Once transformed, the previous Arrow schema becomes.</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Schema</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">.</span><span class="n">NewSchema</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"resource_metrics"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
     <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"scope"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
        <span class="c">// Name and Version are declared as dictionaries (Dictionary&lt;Uint16, String&gt;)).</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"version"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
     <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
     <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"unit"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint8Dictionary</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"timestamp"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">TIMESTAMP</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metric_type"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">UINT8</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"data_point"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metric"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">DenseUnionOf</span><span class="p">(</span>
              <span class="p">[]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
                 <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"gauge"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
                    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"value"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FLOAT64</span><span class="p">},</span>
                 <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
                 <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"sum"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
                    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"value"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FLOAT64</span><span class="p">},</span>
                    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"is_monotonic"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BOOL</span><span class="p">},</span>
                 <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
              <span class="p">},</span>
              <span class="p">[]</span><span class="n">arrow</span><span class="o">.</span><span class="n">UnionTypeCode</span><span class="p">{</span><span class="n">GaugeMetricCode</span><span class="p">,</span> <span class="n">SumMetricCode</span><span class="p">},</span>
           <span class="p">)},</span>
        <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
     <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
  <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
<span class="p">},</span> <span class="no">nil</span><span class="p">)</span>
</code></pre></div></div>

<p>List types are eliminated at all levels. The initial semantics of the model are preserved by duplicating the data of the levels below each data point value. The memory representation will generally be much larger than the previous one, but a query engine that does not support the <code class="language-plaintext highlighter-rouge">List</code> type will still be able to process this data. Interestingly, once compressed, this way of representing data may not necessarily be larger than the previous approach. This is because the columnar representation compresses very well when there is redundancy in the data.</p>

<p>If the union type is not supported by some components of your pipeline, it is also possible to eliminate them by merging the union variants (the nested structure ‘metric’ is removed, see below).</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Schema</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">.</span><span class="n">NewSchema</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"resource_metrics"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
     <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"scope"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
        <span class="c">// Name and Version are declared as dictionaries (Dictionary&lt;Uint16, String&gt;)).</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"version"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
     <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
     <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"unit"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint8Dictionary</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"timestamp"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">TIMESTAMP</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metric_type"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">UINT8</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"data_point"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"value"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FLOAT64</span><span class="p">},</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"is_monotonic"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BOOL</span><span class="p">},</span>
        <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
     <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
  <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
<span class="p">},</span> <span class="no">nil</span><span class="p">)</span>
</code></pre></div></div>

<p>The final schema has evolved into a series of nested structures, where the fields of the union variants are merged into one structure. The trade-off of this approach is similar to that of sparse union - the more variants, the higher the memory occupation. Arrow supports the concept of bitmap validity to identify null values (1 bit per entry) for various data types, including those that do not have a unique null representation (e.g., primitive types). The use of bitmap validity makes the query part easier, and query engines such as DataFusion know how to use if efficiently, Columns with numerous nulls typically compress quite efficiently since the underlying arrays are generally initialized with 0’s. Upon compression, these extensive sequences of 0’s result in high compression efficiency, despite the memory overhead before compression in the case of sparse unions. Consequently, it is essential to select the appropriate trade-off based on your specific context.</p>

<p>In some extreme situations where nested structures are not supported, a flattening approach can be used to address this problem.</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Schema</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">.</span><span class="n">NewSchema</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"scope_name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"scope_version"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics_name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics_unit"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint8Dictionary</span><span class="p">},</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics_timestamp"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">TIMESTAMP</span><span class="p">},</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics_metric_type"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">UINT8</span><span class="p">},</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics_data_point_value"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FLOAT64</span><span class="p">},</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics_data_point_is_monotonic"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BOOL</span><span class="p">},</span>
<span class="p">},</span> <span class="no">nil</span><span class="p">)</span>
</code></pre></div></div>

<p>The terminal fields (leaves) are renamed by concatenating the names of the parent structures to provide proper scoping. This type of structure is supported by all components of the Arrow ecosystem. This approach can be useful if compatibility is a crucial criterion for your system. However, it shares the same drawbacks as other alternative denormalization models.</p>

<p>The Arrow ecosystem is evolving rapidly, so it is likely that support for List, Map, and Union data types in query engines will improve quickly. If kernel functions are sufficient or preferable for your application, it is usually possible to utilize these nested types.</p>

<h3 id="adaptivedynamic-representation">Adaptive/Dynamic representation</h3>

<p>Some data models can be more challenging to translate into an Arrow schema, such as the following Protobuf example. In this example, a collection of attributes is added to each data point. These attributes are defined using a recursive definition that most languages and formats, like Protobuf, support (see the ‘AnyValue’ definition below). Unfortunately, Arrow (like most classical database schemas) does not support such recursive definition within schemas.</p>

<div class="language-protobuf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">syntax</span> <span class="o">=</span> <span class="s">"proto3"</span><span class="p">;</span>


<span class="kd">message</span> <span class="nc">Metric</span> <span class="p">{</span>
 <span class="kd">message</span> <span class="nc">DataPoint</span> <span class="p">{</span>
   <span class="k">repeated</span> <span class="n">Attribute</span> <span class="na">attributes</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
   <span class="k">oneof</span> <span class="n">value</span> <span class="p">{</span>
     <span class="kt">int64</span> <span class="na">int_value</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
     <span class="kt">double</span> <span class="na">double_value</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
   <span class="p">}</span>
 <span class="p">}</span>


 <span class="kd">enum</span> <span class="n">MetricType</span> <span class="p">{</span>
   <span class="na">UNSPECIFIED</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
   <span class="na">GAUGE</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
   <span class="na">SUM</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
 <span class="p">}</span>


 <span class="kd">message</span> <span class="nc">Gauge</span> <span class="p">{</span>
   <span class="n">DataPoint</span> <span class="na">data_point</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
 <span class="p">}</span>


 <span class="kd">message</span> <span class="nc">Sum</span> <span class="p">{</span>
   <span class="n">DataPoint</span> <span class="na">data_point</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
   <span class="kt">bool</span> <span class="na">is_monotonic</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
 <span class="p">}</span>


 <span class="kt">string</span> <span class="na">name</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
 <span class="kt">int64</span> <span class="na">timestamp</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
 <span class="kt">string</span> <span class="na">unit</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
 <span class="n">MetricType</span> <span class="na">type</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span>
 <span class="k">oneof</span> <span class="n">metric</span> <span class="p">{</span>
   <span class="n">Gauge</span> <span class="na">gauge</span> <span class="o">=</span> <span class="mi">5</span><span class="p">;</span>
   <span class="n">Sum</span> <span class="na">sum</span> <span class="o">=</span> <span class="mi">6</span><span class="p">;</span>
 <span class="p">}</span>
<span class="p">}</span>


<span class="kd">message</span> <span class="nc">Attribute</span> <span class="p">{</span>
 <span class="kt">string</span> <span class="na">name</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
 <span class="n">AnyValue</span> <span class="na">value</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
<span class="p">}</span>


<span class="c1">// Recursive definition of AnyValue. AnyValue can be a primitive value, a list</span>
<span class="c1">// of AnyValues, or a list of key-value pairs where the key is a string and</span>
<span class="c1">// the value is an AnyValue.</span>
<span class="kd">message</span> <span class="nc">AnyValue</span> <span class="p">{</span>
 <span class="kd">message</span> <span class="nc">ArrayValue</span> <span class="p">{</span>
   <span class="k">repeated</span> <span class="n">AnyValue</span> <span class="na">values</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
 <span class="p">}</span>
 <span class="kd">message</span> <span class="nc">KeyValueList</span> <span class="p">{</span>
   <span class="kd">message</span> <span class="nc">KeyValue</span> <span class="p">{</span>
     <span class="kt">string</span> <span class="na">key</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
     <span class="n">AnyValue</span> <span class="na">value</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
   <span class="p">}</span>
   <span class="k">repeated</span> <span class="n">KeyValue</span> <span class="na">values</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
 <span class="p">}</span>


 <span class="k">oneof</span> <span class="n">value</span> <span class="p">{</span>
   <span class="kt">int64</span> <span class="na">int_value</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
   <span class="kt">double</span> <span class="na">double_value</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
   <span class="kt">string</span> <span class="na">string_value</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
   <span class="n">ArrayValue</span> <span class="na">list_value</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span>
   <span class="n">KeyValueList</span> <span class="na">kvlist_value</span> <span class="o">=</span> <span class="mi">5</span><span class="p">;</span>
 <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>If the definition of the attributes were non-recursive, it would have been possible to directly translate them into an Arrow Map type.</p>

<p>To address this kind of issue and further optimize Arrow schema definitions, you can employ an adaptive and iterative method that automatically constructs the Arrow schema based on the data being translated. With this approach, fields are automatically dictionary-encoded according to their cardinalities, unused fields are eliminated, and recursive structures are represented in a specific manner. Another solution involves using a multi-schema approach, in which attributes are depicted in a separate Arrow Record, and the inheritance relation is represented by a self-referential relationship. These strategies will be covered in more depth in a future article. For those eager to learn more, the first method is utilized in the reference implementation of the <a href="https://github.com/f5/otel-arrow-adapter">OTel Arrow Adapter</a>.</p>

<h2 id="data-transport">Data transport</h2>

<p>Unlike to Protobuf, an Arrow schema is generally not known a priori by the two parties participating in an exchange. Before being able to exchange data in Arrow format, the sender must first communicate the schema to the receiver, as well as the contents of the dictionaries used in the data. Only after this initialization phase has been completed can the sender transmit batches of data in Arrow format. This process, known as <a href="https://wesmckinney.com/blog/arrow-streaming-columnar/">Arrow IPC Stream</a>, plays an essential role transporting Arrow data between systems. Several approaches can be employed to communicate these Arrow IPC Streams. The simplest method is to use <a href="https://arrow.apache.org/docs/format/Flight.html">Arrow Flight</a>, which encapsulates Arrow IPC streams in a gRPC-based protocol. However, it is also possible to use your own implementation for specific contexts. Regardless of the solution you choose, it is crucial to understand that the underlying protocol must be stateful to take full advantage of the Arrow IPC stream approach. To achieve the best compression rates, it is vital to send schemas and dictionaries only once in order to amortize the cost and minimize data redundancy between batches. This necessitates a transport that supports stream-oriented communications, such as gRPC.</p>

<p>Using a stateless protocol is possible for large batches because the overhead of the schema will be negligible compared to the compression gains achieved using dictionary encoding and columnar representation. However, dictionaries will have to be communicated for each batch, making this approach generally less efficient than a stream-oriented approach.</p>

<p>Arrow IPC Stream also supports the concept of “delta dictionaries,” which allows for further optimization of batch transport. When a batch adds data to an existing dictionary (at the sender’s end), Arrow IPC enables sending the delta dictionary followed by the batch that references it. On the receiver side, this delta is used to update the existing dictionary, eliminating the need to retransmit the entire dictionary when changes occur. This optimization is only possible with a stateful protocol.</p>

<p>To fully leverage the column-oriented format of Apache Arrow, it is essential to consider sorting and compression. If your data model is simple (i.e., flat) and has one or more columns representing a natural order for your data (e.g., timestamp), it might be beneficial to sort your data to optimize the final compression ratio. Before implementing this optimization, it is recommended to perform tests on real data since the benefits may vary. In any case, using a compression algorithm when sending your batches is advantageous. Arrow IPC generally supports the ZSTD compression algorithm, which strikes an excellent balance between speed and compression efficiency, especially for column-oriented data.</p>

<p>Lastly, some implementations (e.g., Arrow Go) are not configured by default to support delta dictionaries and compression algorithms. Therefore, it is crucial to ensure that your code employs these options to maximize data transport efficiency.</p>

<h2 id="experiments">Experiments</h2>

<p>If your initial data is complex, it is advisable to conduct your own experiments to optimize the Arrow representation according to your data and goals (e.g., optimizing the compression ratio or enhancing the query-ability of your data in Arrow format). In our case, we developed an overlay for Apache Arrow that enables us to carry out these experiments with ease, without having to deal with the intrinsic complexity of Arrow APIs. However, this comes at the expense of a slower conversion phase compared to using Arrow APIs directly. While this library is not currently public, it may become available if there is sufficient interest.</p>

<p>We also employed a “black box optimization” approach, which automatically finds the best combination to meet the objectives we aimed to optimize (refer to “<a href="https://cloud.google.com/blog/products/ai-machine-learning/optimize-your-applications-using-google-vertex-ai-vizier">Optimize your applications using Google Vertex AI Vizier</a>” for a description of this approach).</p>

<h2 id="conclusion-and-next-steps">Conclusion and next steps</h2>

<p>Essentially, the key concept behind Apache Arrow is that it eliminates the need for serialization and deserialization, enabling zero-copy data sharing. Arrow achieves this by defining a language-agnostic, in-memory format that remains consistent across various implementations. Consequently, raw memory bytes can be transmitted directly over a network without requiring any serialization or deserialization, significantly enhancing data processing efficiency.</p>

<p>Converting a data model to Apache Arrow necessitates adaptation and optimization work, as we have begun to describe in this article. Many parameters must be considered, and it is recommended to perform a series of experiments to validate the various choices made during this process.</p>

<p>Handling highly dynamic data with Arrow can be challenging. Arrow requires the definition of a static schema, which can sometimes make representing this type of data complex or suboptimal, especially when the initial schema contains recursive definitions. This article has discussed several approaches to address this issue. The next article will be dedicated to a hybrid strategy that involves adapting the Arrow schema on-the-fly to optimize memory usage, compression ratio, and processing speed based on the data being represented. This approach is quite unique and deserves a separate article.</p>]]></content><author><name>Laurent Quérel</name></author><category term="application" /><summary type="html"><![CDATA[Apache Arrow is a technology widely adopted in big data, analytics, and machine learning applications. In this article, we share F5’s experience with Arrow, specifically its application to telemetry, and the challenges we encountered while optimizing the OpenTelemetry protocol to significantly reduce bandwidth costs. The promising results we achieved inspired us to share our insights. This article specifically focuses on transforming relatively complex data structure from various formats into an efficient Arrow representation that optimizes both compression ratio, transport, and data processing. We also explore the trade-offs between different mapping and normalization strategies, as well as the nuances of streaming and batch communication using Arrow and Arrow Flight. Our benchmarks thus far have shown promising results, with compression ratio improvements ranging from 1.5x to 5x, depending on the data type (metrics, logs, traces), distribution, and compression algorithm. The approaches presented for addressing these challenges may be applicable to other Arrow domains as well. This article serves as the first installment in a two-part series. What is Apache Arrow Apache Arrow is an open-source project offering a standardized, language-agnostic in-memory format for representing structured and semi-structured data. This enables data sharing and zero-copy data access between systems, eliminating the need for serialization and deserialization when exchanging datasets between varying CPU architectures and programming languages. Furthermore, Arrow libraries feature an extensive set of high-performance, parallel, and vectorized kernel functions designed for efficiently processing massive amounts of columnar data. These features make Arrow an appealing technology for big data processing, data transport, analytics, and machine learning applications. The growing number of products and open-source projects that have adopted Apache Arrow at their core or offer Arrow support reflects the widespread recognition and appreciation of its benefits (refer to this article for an in-depth overview of the Arrow ecosystem and adoption). Over 11,000 GitHub users support this project, and 840+ are contributors who make this project an undeniable success. Very often people ask about the differences between Arrow and Apache Parquet or other columnar file formats. Arrow is designed and optimized for in-memory processing, while Parquet is tailored for disk-based storage. In reality, these technologies are complementary, with bridges existing between them to simplify interoperability. In both cases, data is represented in columns to optimize access, data locality and compressibility. However, the tradeoffs differ slightly. Arrow prioritizes data processing speed over the optimal data encoding. Complex encodings that don’t benefit from SIMD instruction sets are generally not natively supported by Arrow, unlike formats such as Parquet. Storing data in Parquet format and processing and transporting it in Arrow format has become a prevalent model within the big data community. Fig 1: Memory representations: row vs columnar data. Figure 1 illustrates the differences in memory representation between row-oriented and column-oriented approaches. The column-oriented approach groups data from the same column in a continuous memory area, which facilitates parallel processing (SIMD) and enhances compression performance. Why are we interested in Apache Arrow At F5, we’ve adopted OpenTelemetry (OTel) as the standard for all telemetry across our products, such as BIGIP and NGINX. These products may generate large volumes of metrics and logs for various reasons, from performance evaluation to forensic purposes. The data produced by these systems is typically centralized and processed in dedicated systems. Transporting and processing this data accounts for a significant portion of the cost associated with telemetry pipelines. In this context, we became interested in Apache Arrow. Instead of reinventing yet another telemetry solution, we decided to invest in the OpenTelemetry project, working on improvements to the protocol to significantly increase its efficiency with high telemetry data volumes. We collaborated with Joshua MacDonald from Lightstep to integrate these optimizations into an experimental OTel collector and are currently in discussions with the OTel technical committee to finalize a code donation. Fig 2: Performance improvement in the OpenTelemetry Arrow experimental project. This project has been divided into two phases. The first phase, which is nearing completion, aims to enhance the protocol’s compression ratio. The second phase, planned for the future, focuses on improving end-to-end performance by incorporating Apache Arrow throughout all levels, eliminating the need for conversion between old and new protocols. The results so far are promising, with our benchmarks showing compression ratio improvements ranging from x1.5 to x5, depending on the data type (metrics, logs, traces), distribution, and compression algorithm. For the second phase, our estimates suggest that data processing acceleration could range from x2 to x12, again depending on the data’s nature and distribution. For more information, we encourage you to review the specifications and the reference implementation. Arrow relies on a schema to define the structure of data batches that it processes and transports. The subsequent sections will discuss various techniques that can be employed to optimize the creation of these schemas. How to leverage Arrow to optimize network transport cost Apache Arrow is a complex project with a rapidly evolving ecosystem, which can sometimes be overwhelming for newcomers. Fortunately the Arrow community has published three introductory articles 1, 2, and 3 that we recommend for those interested in exploring this technology. This article primarily focuses on transforming data from an XYZ format into an efficient Arrow representation that optimizes both compression ratio and data processing. There are numerous approaches to this transformation, and we will examine how these methods can impact compression ratio, CPU usage, and memory consumption during the conversion process, among other factors. Fig 3: Optimization process for the definition of an Arrow schema. The complexity of your initial model significantly impacts the Arrow mapping choices you need to make. To begin, it’s essential to identify the properties you want to optimize for your specific context. Compression rate, conversion speed, memory consumption, speed and ease of use of the final model, compatibility, and extensibility are all factors that can influence your final mapping decisions. From there, you must explore multiple alternative schemas. The choice of the Arrow type and data encoding for each individual field will affect the performance of your schema. There are various ways to represent hierarchical data or highly dynamic data models, and multiple options need to be evaluated in coordination with the configuration of the transport layer. This transport layer should also be carefully considered. Arrow supports compression mechanisms and dictionary deltas that may not be active by default. After several iterations of this process, you should arrive at an optimized schema that meets the goals you initially set. It’s crucial to compare the performance of your different approaches using real data, as the distribution of data in each individual field may influence whether you use dictionary encoding or not. We will now examine these choices in greater detail throughout the remainder of this article. Arrow data type selection The principles of selecting an Arrow data type are quite similar to those used when defining a data model for databases. Arrow supports a wide range of data types. Some of these types are supported by all implementations, while others are only available for languages with the strongest Arrow community support (see this page for a comparison matrix of the different implementations). For primitive types, it is generally preferable to choose the type that offers the most concise representation and is closest to the semantics of your initial field. For example, while it’s possible to represent a timestamp with an int64, it’s more advantageous to use the native Arrow Timestamp type. This choice isn’t due to a more efficient binary representation, but rather because it will be easier to process and manipulate in your pipeline. Query engines such as DataFusion offer dedicated timestamp handling functions for columns of this type. The same choices can be made for primitive types such as date, time, duration, and interval. However, if your project requires maximum compatibility, it may be crucial in some cases to favor types with universal support instead of the most optimal type in terms of memory occupation. Fig 4: Data types supported by Apache Arrow. When selecting the Arrow data type, it’s important to consider the size of the data before and after compression. It’s quite possible that the size after compression is the same for two different types, but the actual size in memory may be two, four, or even eight times larger (e.g., uint8 vs. uint64). This difference will impact your ability to process large batches of data and will also significantly influence the speed of processing these data in memory (e.g., cache optimization, SIMD instruction efficiency). It’s also possible to extend these types using an extension type mechanism that builds upon one of the currently supported primitive types while adding specific semantics. This extension mechanism can simplify the use of this data in your own project, while remaining transparent to intermediate systems that will interpret this data as a basic primitive type. There are some variations in the encoding of primitive types, which we will explore next. Data encoding Another crucial aspect of optimizing your Arrow schema is analyzing the cardinality of your data. Fields that can have only a limited number of values will typically be more efficiently represented with a dictionary encoding. The maximum cardinality of a field determines the data type characteristics of your dictionary. For instance, for a field representing the status code of an HTTP transaction, it’s preferable to use a dictionary with an index of type ‘uint8’ and a value of type ‘uint16’ (notation: ‘Dictionary&lt;uint8, uint16&gt;’). This consumes less memory because the main array will be of type ‘[]uint8’. Even if the range of possible values is greater than 255, as long as the number of distinct values does not exceed 255, the representation remains efficient. Similarly, the representation of a ‘user-agent’ will be more efficient with a dictionary of type ‘Dictionary&lt;uint16, string&gt;’ (see figure 5). In this case, the main array will be of type ‘uint16’, allowing a compact representation in memory and during transfers at the cost of an indirection during reverse conversion. Fig 5: Dictionary encoding. Dictionary encoding is highly flexible in Apache Arrow, allowing the creation of encodings for any Arrow primitive type. The size of the indices can also be configured based on the context. In general, it is advisable to use dictionaries in the following cases: Representation of enumerations Representation of textual or binary fields with a high probability of having redundant values. Representation of fields with cardinalities known to be below 2^16 or 2^32. Sometimes, the cardinality of a field is not known a priori. For example, a proxy that transforms a data stream from a row-oriented format into a series of columnar-encoded batches (e.g., OpenTelemetry collector) may not be able to predict in advance whether a field will have a fixed number of distinct values. Two approaches are possible: 1) a conservative approach using the largest data type (e.g., ‘int64’, ‘string’, etc., instead of dictionary), 2) an adaptive approach that modifies the schema on the fly based on the observed cardinality of the field(s). In this second approach, without cardinality information, you can optimistically start by using a ‘Dictionary&lt;uint8, original-field-type&gt;’ dictionary, then detect a potential dictionary overflow during conversion, and change the schema to a ‘Dictionary&lt;uint16, original-field-type&gt;’ in case of an overflow. This technique of automatic management of dictionary overflows will be presented in greater detail in a future article. Recent advancements in Apache Arrow include the implementation of run-end encoding, a technique that efficiently represents data with sequences of repeated values. This encoding method is particularly beneficial for handling data sets containing long stretches of identical values, as it offers a more compact and optimized representation. In conclusion, dictionary encoding not only occupies less space in memory and during transfers but also significantly improves the compression ratio and data processing speed. However, this type of representation requires indirection when extracting the initial values (although this isn’t always necessary, even during some data processing operations). Additionally, it is important to manage dictionary index overflow, especially when the encoded field doesn’t have a well-defined cardinality. Hierarchical data Basic hierarchical data structures translate relatively well into Arrow. However, as we will see, there are some complications to handle in more general cases (see figure 6). While Arrow schemas do support nested structures, maps, and unions, some components of the Arrow ecosystem do not fully support them, which can make these Arrow data types unsuitable for certain scenarios. Additionally, unlike most languages and formats, such as Protobuf, Arrow doesn’t support the concept of a recursively defined schema. An Arrow schema is static in its definition, and the depth of its nested elements must be known in advance. There are multiple strategies to work around this limitation and we’ll explore these in the following sections. Fig 6: simple vs complex data model. Natural representation The most straightforward and intuitive approach to representing a simple hierarchical data model is to use Arrow’s list, map, and union data types. However, it’s important to note that some of these data types are not fully supported throughout the entire Arrow ecosystem. For example, the conversion of unions to Parquet is not directly supported and requires a transformation step (see denormalization &amp; flattening representation to decompose a sparse union into a nullable struct and type ids column). Similarly, lists and maps are not yet supported in DataFusion version 20 (nested structures are partially supported). Fig 7: initial data model. The following example is a Go program snippet of an Arrow schema using these different data types to represent the model above. import "github.com/apache/arrow/go/v11/arrow" const ( GaugeMetricCode arrow.UnionTypeCode = 0 SumMetricCode arrow.UnionTypeCode = 1 ) var ( // uint8Dictionary represent a Dictionary&lt;Uint8, String&gt; uint8Dictionary = &amp;arrow.DictionaryType{ IndexType: arrow.PrimitiveTypes.Uint8, ValueType: arrow.BinaryTypes.String, } // uint16Dictionary represent a Dictionary&lt;Uint16, String&gt; uint16Dictionary = &amp;arrow.DictionaryType{ IndexType: arrow.PrimitiveTypes.Uint16, ValueType: arrow.BinaryTypes.String, } Schema = arrow.NewSchema([]arrow.Field{ {Name: "resource_metrics", Type: arrow.ListOf(arrow.StructOf([]arrow.Field{ {Name: "scope", Type: arrow.StructOf([]arrow.Field{ // Name and Version are declared as dictionaries (Dictionary&lt;Uint16, String&gt;)). {Name: "name", Type: uint16Dictionary}, {Name: "version", Type: uint16Dictionary}, }...)}, {Name: "metrics", Type: arrow.ListOf(arrow.StructOf([]arrow.Field{ {Name: "name", Type: uint16Dictionary}, {Name: "unit", Type: uint8Dictionary}, {Name: "timestamp", Type: arrow.TIMESTAMP}, {Name: "metric_type", Type: arrow.UINT8}, {Name: "data_point", Type: arrow.ListOf(arrow.StructOf([]arrow.Field{ {Name: "metric", Type: arrow.DenseUnionOf( []arrow.Field{ {Name: "gauge", Type: arrow.StructOf([]arrow.Field{ {Name: "data_point", Type: arrow.FLOAT64}, }...)}, {Name: "sum", Type: arrow.StructOf([]arrow.Field{ {Name: "data_point", Type: arrow.FLOAT64}, {Name: "is_monotonic", Type: arrow.BOOL}, }...)}, }, []arrow.UnionTypeCode{GaugeMetricCode, SumMetricCode}, )}, }...))}, }...))}, }...))}, }, nil) ) In this pattern, we use a union type to represent an inheritance relationship. There are two types of Arrow union that are optimized for different cases. The dense union type has a relatively succinct memory representation but doesn’t support vectorizable operations, making it less efficient during the processing phase. Conversely, a sparse union supports vectorization operations, but comes with a memory overhead directly proportional to the number of variants in the union. Dense and sparse unions have quite similar compression rates, with sometimes a slight advantage for sparse unions. In addition, sparse unions with a large number of variants should generally be avoided, as they can lead to excessive memory consumption. For more details on the memory representation of unions, you can consult this page. In certain scenarios, it may be more idiomatic to represent the inheritance relationship using multiple schemas (i.e., one schema per subtype), thereby avoiding the use of the union type. However, applying this approach to the aforementioned model may not be optimal, as the data preceding the inheritance relationship (i.e., ResourceMetrics, Scope, and Metrics) could potentially be duplicated numerous times. If the relationships between ResourceMetrics, Metrics, and DataPoint were 0..1 (zero-to-one) relationships, then the multi-schema approach would likely be the simplest and most idiomatic solution. Denormalization &amp; Flattening representations If the List type is not supported in your telemetry pipeline, you can denormalize your data model. This process is often used in the database world to remove a join between two tables for optimization purposes. In the Arrow world, denormalization is employed to eliminate the List type by duplicating some data. Once transformed, the previous Arrow schema becomes. Schema = arrow.NewSchema([]arrow.Field{ {Name: "resource_metrics", Type: arrow.StructOf([]arrow.Field{ {Name: "scope", Type: arrow.StructOf([]arrow.Field{ // Name and Version are declared as dictionaries (Dictionary&lt;Uint16, String&gt;)). {Name: "name", Type: uint16Dictionary}, {Name: "version", Type: uint16Dictionary}, }...)}, {Name: "metrics", Type: arrow.StructOf([]arrow.Field{ {Name: "name", Type: uint16Dictionary}, {Name: "unit", Type: uint8Dictionary}, {Name: "timestamp", Type: arrow.TIMESTAMP}, {Name: "metric_type", Type: arrow.UINT8}, {Name: "data_point", Type: arrow.StructOf([]arrow.Field{ {Name: "metric", Type: arrow.DenseUnionOf( []arrow.Field{ {Name: "gauge", Type: arrow.StructOf([]arrow.Field{ {Name: "value", Type: arrow.FLOAT64}, }...)}, {Name: "sum", Type: arrow.StructOf([]arrow.Field{ {Name: "value", Type: arrow.FLOAT64}, {Name: "is_monotonic", Type: arrow.BOOL}, }...)}, }, []arrow.UnionTypeCode{GaugeMetricCode, SumMetricCode}, )}, }...)}, }...)}, }...)}, }, nil) List types are eliminated at all levels. The initial semantics of the model are preserved by duplicating the data of the levels below each data point value. The memory representation will generally be much larger than the previous one, but a query engine that does not support the List type will still be able to process this data. Interestingly, once compressed, this way of representing data may not necessarily be larger than the previous approach. This is because the columnar representation compresses very well when there is redundancy in the data. If the union type is not supported by some components of your pipeline, it is also possible to eliminate them by merging the union variants (the nested structure ‘metric’ is removed, see below). Schema = arrow.NewSchema([]arrow.Field{ {Name: "resource_metrics", Type: arrow.StructOf([]arrow.Field{ {Name: "scope", Type: arrow.StructOf([]arrow.Field{ // Name and Version are declared as dictionaries (Dictionary&lt;Uint16, String&gt;)). {Name: "name", Type: uint16Dictionary}, {Name: "version", Type: uint16Dictionary}, }...)}, {Name: "metrics", Type: arrow.StructOf([]arrow.Field{ {Name: "name", Type: uint16Dictionary}, {Name: "unit", Type: uint8Dictionary}, {Name: "timestamp", Type: arrow.TIMESTAMP}, {Name: "metric_type", Type: arrow.UINT8}, {Name: "data_point", Type: arrow.StructOf([]arrow.Field{ {Name: "value", Type: arrow.FLOAT64}, {Name: "is_monotonic", Type: arrow.BOOL}, }...)}, }...)}, }...)}, }, nil) The final schema has evolved into a series of nested structures, where the fields of the union variants are merged into one structure. The trade-off of this approach is similar to that of sparse union - the more variants, the higher the memory occupation. Arrow supports the concept of bitmap validity to identify null values (1 bit per entry) for various data types, including those that do not have a unique null representation (e.g., primitive types). The use of bitmap validity makes the query part easier, and query engines such as DataFusion know how to use if efficiently, Columns with numerous nulls typically compress quite efficiently since the underlying arrays are generally initialized with 0’s. Upon compression, these extensive sequences of 0’s result in high compression efficiency, despite the memory overhead before compression in the case of sparse unions. Consequently, it is essential to select the appropriate trade-off based on your specific context. In some extreme situations where nested structures are not supported, a flattening approach can be used to address this problem. Schema = arrow.NewSchema([]arrow.Field{ {Name: "scope_name", Type: uint16Dictionary}, {Name: "scope_version", Type: uint16Dictionary}, {Name: "metrics_name", Type: uint16Dictionary}, {Name: "metrics_unit", Type: uint8Dictionary}, {Name: "metrics_timestamp", Type: arrow.TIMESTAMP}, {Name: "metrics_metric_type", Type: arrow.UINT8}, {Name: "metrics_data_point_value", Type: arrow.FLOAT64}, {Name: "metrics_data_point_is_monotonic", Type: arrow.BOOL}, }, nil) The terminal fields (leaves) are renamed by concatenating the names of the parent structures to provide proper scoping. This type of structure is supported by all components of the Arrow ecosystem. This approach can be useful if compatibility is a crucial criterion for your system. However, it shares the same drawbacks as other alternative denormalization models. The Arrow ecosystem is evolving rapidly, so it is likely that support for List, Map, and Union data types in query engines will improve quickly. If kernel functions are sufficient or preferable for your application, it is usually possible to utilize these nested types. Adaptive/Dynamic representation Some data models can be more challenging to translate into an Arrow schema, such as the following Protobuf example. In this example, a collection of attributes is added to each data point. These attributes are defined using a recursive definition that most languages and formats, like Protobuf, support (see the ‘AnyValue’ definition below). Unfortunately, Arrow (like most classical database schemas) does not support such recursive definition within schemas. syntax = "proto3"; message Metric { message DataPoint { repeated Attribute attributes = 1; oneof value { int64 int_value = 2; double double_value = 3; } } enum MetricType { UNSPECIFIED = 0; GAUGE = 1; SUM = 2; } message Gauge { DataPoint data_point = 1; } message Sum { DataPoint data_point = 1; bool is_monotonic = 2; } string name = 1; int64 timestamp = 2; string unit = 3; MetricType type = 4; oneof metric { Gauge gauge = 5; Sum sum = 6; } } message Attribute { string name = 1; AnyValue value = 2; } // Recursive definition of AnyValue. AnyValue can be a primitive value, a list // of AnyValues, or a list of key-value pairs where the key is a string and // the value is an AnyValue. message AnyValue { message ArrayValue { repeated AnyValue values = 1; } message KeyValueList { message KeyValue { string key = 1; AnyValue value = 2; } repeated KeyValue values = 1; } oneof value { int64 int_value = 1; double double_value = 2; string string_value = 3; ArrayValue list_value = 4; KeyValueList kvlist_value = 5; } } If the definition of the attributes were non-recursive, it would have been possible to directly translate them into an Arrow Map type. To address this kind of issue and further optimize Arrow schema definitions, you can employ an adaptive and iterative method that automatically constructs the Arrow schema based on the data being translated. With this approach, fields are automatically dictionary-encoded according to their cardinalities, unused fields are eliminated, and recursive structures are represented in a specific manner. Another solution involves using a multi-schema approach, in which attributes are depicted in a separate Arrow Record, and the inheritance relation is represented by a self-referential relationship. These strategies will be covered in more depth in a future article. For those eager to learn more, the first method is utilized in the reference implementation of the OTel Arrow Adapter. Data transport Unlike to Protobuf, an Arrow schema is generally not known a priori by the two parties participating in an exchange. Before being able to exchange data in Arrow format, the sender must first communicate the schema to the receiver, as well as the contents of the dictionaries used in the data. Only after this initialization phase has been completed can the sender transmit batches of data in Arrow format. This process, known as Arrow IPC Stream, plays an essential role transporting Arrow data between systems. Several approaches can be employed to communicate these Arrow IPC Streams. The simplest method is to use Arrow Flight, which encapsulates Arrow IPC streams in a gRPC-based protocol. However, it is also possible to use your own implementation for specific contexts. Regardless of the solution you choose, it is crucial to understand that the underlying protocol must be stateful to take full advantage of the Arrow IPC stream approach. To achieve the best compression rates, it is vital to send schemas and dictionaries only once in order to amortize the cost and minimize data redundancy between batches. This necessitates a transport that supports stream-oriented communications, such as gRPC. Using a stateless protocol is possible for large batches because the overhead of the schema will be negligible compared to the compression gains achieved using dictionary encoding and columnar representation. However, dictionaries will have to be communicated for each batch, making this approach generally less efficient than a stream-oriented approach. Arrow IPC Stream also supports the concept of “delta dictionaries,” which allows for further optimization of batch transport. When a batch adds data to an existing dictionary (at the sender’s end), Arrow IPC enables sending the delta dictionary followed by the batch that references it. On the receiver side, this delta is used to update the existing dictionary, eliminating the need to retransmit the entire dictionary when changes occur. This optimization is only possible with a stateful protocol. To fully leverage the column-oriented format of Apache Arrow, it is essential to consider sorting and compression. If your data model is simple (i.e., flat) and has one or more columns representing a natural order for your data (e.g., timestamp), it might be beneficial to sort your data to optimize the final compression ratio. Before implementing this optimization, it is recommended to perform tests on real data since the benefits may vary. In any case, using a compression algorithm when sending your batches is advantageous. Arrow IPC generally supports the ZSTD compression algorithm, which strikes an excellent balance between speed and compression efficiency, especially for column-oriented data. Lastly, some implementations (e.g., Arrow Go) are not configured by default to support delta dictionaries and compression algorithms. Therefore, it is crucial to ensure that your code employs these options to maximize data transport efficiency. Experiments If your initial data is complex, it is advisable to conduct your own experiments to optimize the Arrow representation according to your data and goals (e.g., optimizing the compression ratio or enhancing the query-ability of your data in Arrow format). In our case, we developed an overlay for Apache Arrow that enables us to carry out these experiments with ease, without having to deal with the intrinsic complexity of Arrow APIs. However, this comes at the expense of a slower conversion phase compared to using Arrow APIs directly. While this library is not currently public, it may become available if there is sufficient interest. We also employed a “black box optimization” approach, which automatically finds the best combination to meet the objectives we aimed to optimize (refer to “Optimize your applications using Google Vertex AI Vizier” for a description of this approach). Conclusion and next steps Essentially, the key concept behind Apache Arrow is that it eliminates the need for serialization and deserialization, enabling zero-copy data sharing. Arrow achieves this by defining a language-agnostic, in-memory format that remains consistent across various implementations. Consequently, raw memory bytes can be transmitted directly over a network without requiring any serialization or deserialization, significantly enhancing data processing efficiency. Converting a data model to Apache Arrow necessitates adaptation and optimization work, as we have begun to describe in this article. Many parameters must be considered, and it is recommended to perform a series of experiments to validate the various choices made during this process. Handling highly dynamic data with Arrow can be challenging. Arrow requires the definition of a static schema, which can sometimes make representing this type of data complex or suboptimal, especially when the initial schema contains recursive definitions. This article has discussed several approaches to address this issue. The next article will be dedicated to a hybrid strategy that involves adapting the Arrow schema on-the-fly to optimize memory usage, compression ratio, and processing speed based on the data being represented. This approach is quite unique and deserves a separate article.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 0.3.0 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/03/21/adbc-0.3.0-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.3.0 (Libraries) Release" /><published>2023-03-21T00:00:00-04:00</published><updated>2023-03-21T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/03/21/adbc-0.3.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/03/21/adbc-0.3.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.3.0 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/4"><strong>24
resolved issues</strong></a> from <a href="#contributors"><strong>7 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version
0.3.0.  The <strong>API specification</strong> is versioned separately and is
at version 1.0.0.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.3.0/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>R bindings for the driver manager have been added, as well as an R package that repackages the SQLite driver.</p>

<p>The ADBC Flight SQL driver now supports transactions and executing Substrait plans.  Also, it has had several bugs fixed, including setting timeouts and sending headers properly.</p>

<p>The Python ADBC packages now expose enums for driver-specific options.  Also, the DBAPI layer now implements <code class="language-plaintext highlighter-rouge">__del__</code> on objects to assist in cleaning up resources (though context managers are still recommended).</p>

<p>The ADBC JDBC driver now exposes more metadata about constraints via <code class="language-plaintext highlighter-rouge">getObjects</code>.</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.2.0..apache-arrow-adbc-0.3.0
    32	David Li
     5	Dewey Dunnington
     3	Matt Topol
     1	Dave Hirschfeld
     1	Jacob Marble
     1	Tornike Gurgenidze
     1	Will Jones
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>New extensions to the API specification have been proposed.  These
will be backwards-compatible and will become API specification 1.1.0.
For details, see the <a href="https://lists.apache.org/thread/247z3t06mf132nocngc1jkp3oqglz7jp">mailing list discussion</a> and the
<a href="https://github.com/apache/arrow-adbc/milestone/3">milestone</a> tracking the proposed features.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.3.0 release of the Apache Arrow ADBC libraries. This covers includes 24 resolved issues from 7 distinct contributors. This is a release of the libraries, which are at version 0.3.0. The API specification is versioned separately and is at version 1.0.0. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights R bindings for the driver manager have been added, as well as an R package that repackages the SQLite driver. The ADBC Flight SQL driver now supports transactions and executing Substrait plans. Also, it has had several bugs fixed, including setting timeouts and sending headers properly. The Python ADBC packages now expose enums for driver-specific options. Also, the DBAPI layer now implements __del__ on objects to assist in cleaning up resources (though context managers are still recommended). The ADBC JDBC driver now exposes more metadata about constraints via getObjects. Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.2.0..apache-arrow-adbc-0.3.0 32 David Li 5 Dewey Dunnington 3 Matt Topol 1 Dave Hirschfeld 1 Jacob Marble 1 Tornike Gurgenidze 1 Will Jones Roadmap New extensions to the API specification have been proposed. These will be backwards-compatible and will become API specification 1.1.0. For details, see the mailing list discussion and the milestone tracking the proposed features. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow nanoarrow 0.1 Release</title><link href="https://arrow.apache.org/blog/2023/03/07/nanoarrow-0.1.0-release/" rel="alternate" type="text/html" title="Apache Arrow nanoarrow 0.1 Release" /><published>2023-03-07T00:00:00-05:00</published><updated>2023-03-07T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/03/07/nanoarrow-0.1.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/03/07/nanoarrow-0.1.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.1.0 release of
Apache Arrow nanoarrow. This initial release covers 31 resolved issues from
6 contributors.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>This initial release includes the following:</p>

<ul>
  <li>A <a href="#using-nanoarrow-in-c">C library</a> bundled as two files (nanoarrow.c
and nanoarrow.h).</li>
  <li>An <a href="#using-nanoarrow-in-c-r-and-python">R package</a> providing bindings for users
of the R programming language.</li>
</ul>

<p>See the
<a href="https://github.com/apache/arrow-nanoarrow/blob/apache-arrow-nanoarrow-0.1.0/CHANGELOG.md">Changelog</a>
for a detailed list of contributions leading up to the initial release.</p>

<h2 id="why-nanoarrow">Why nanoarrow?</h2>

<p>The adoption of the
<a href="https://arrow.apache.org/docs/format/CDataInterface.html">Arrow C Data Interface</a>
and the <a href="https://arrow.apache.org/docs/format/CStreamInterface.html">Arrow C Stream Interface</a>
since their
<a href="https://arrow.apache.org/blog/2020/05/03/introducing-arrow-c-data-interface/">introduction</a>
have been impressive and enthusiastic: not only have Arrow language bindings
adopted the standard to pass data among themselves, a growing number of
high-profile libraries like
<a href="https://gdal.org/development/rfc/rfc86_column_oriented_api.html">GDAL</a> and
<a href="https://duckdb.org/2021/12/03/duck-arrow.html">DuckDB</a> use the standard to
improve performance and provide an ABI-stable interface to tabular input and output.</p>

<p>GDAL and DuckDB are fortunate to have hard-working and forward-thinking maintainers
that were motivated to provide support for the Arrow C Data and Stream interfaces
even though the code to do so required an intimate knowledge of both the interface
and the columnar specification on which it is based.</p>

<p>The vision of <a href="https://github.com/apache/arrow-nanoarrow">nanoarrow</a>
is that it should be trivial for a library or application to implement an Arrow-based
interface: if a library consumes or produces tabular data, Arrow should be the
first place developers look. Developers shouldn’t have to be familiar with the
details of the columnar specification—nor should they have to take on any
build-time dependencies—to get started.</p>

<p>The <a href="https://arrow.apache.org/docs/format/ADBC.html">Arrow Database Connectivity (ADBC)</a>
specification is a good example of such a project, and provided a strong
motivator for the development of nanoarrow: at the heart of ADBC is the
idea of a core “driver manager” and database-specific drivers that are distributed
as independent C/C++/Python/R/Java/Go projects. At least in R and Python,
embedding an existing Arrow implementation (e.g., Arrow C++) is challenging
in the context of multiple packages intended to be loaded into the same process.
As of this writing, ADBC includes nanoarrow-based SQLite and PostgreSQL drivers
and a nanoarrow-based validation suite for drivers.</p>

<h2 id="using-nanoarrow-in-c">Using nanoarrow in C</h2>

<p>The nanoarrow C library is distributed as
<a href="https://github.com/apache/arrow-nanoarrow/tree/main/dist">two files (nanoarrow.h and nanoarrow.c)</a>
that can be copied and vendored into an existing code base. This results in
a static library of about 50  KB and builds in less than a second. Some features
that nanoarrow provides are:</p>

<ul>
  <li><a href="https://apache.github.io/arrow-nanoarrow/dev/c.html#creating-schemas">Helpers to create types, schemas, and metadata</a></li>
  <li><a href="https://apache.github.io/arrow-nanoarrow/dev/c.html#owning-growable-buffers">Growable buffers</a>,
including the option for custom allocators/deallocators.</li>
  <li><a href="https://apache.github.io/arrow-nanoarrow/dev/c.html#bitmap-utilities">Bitmap (i.e., bitpacked boolean) utilities</a></li>
  <li>An <a href="https://apache.github.io/arrow-nanoarrow/dev/c.html#creating-arrays">API for building arrays from buffers</a></li>
  <li>An <a href="https://apache.github.io/arrow-nanoarrow/dev/c.html#creating-arrays">API for building arrays element-wise</a></li>
  <li>An <a href="https://apache.github.io/arrow-nanoarrow/dev/c.html#reading-arrays">API to extract elements element-wise</a>
from an existing array.</li>
</ul>

<p>For example, one can build an integer array element-wise:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">"nanoarrow.h"</span><span class="cp">
</span>
<span class="kt">int</span> <span class="nf">make_simple_array</span><span class="p">(</span><span class="k">struct</span> <span class="n">ArrowArray</span><span class="o">*</span> <span class="n">array_out</span><span class="p">,</span> <span class="k">struct</span> <span class="n">ArrowSchema</span><span class="o">*</span> <span class="n">schema_out</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">struct</span> <span class="n">ArrowError</span> <span class="n">error</span><span class="p">;</span>
  <span class="n">array_out</span><span class="o">-&gt;</span><span class="n">release</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
  <span class="n">schema_out</span><span class="o">-&gt;</span><span class="n">release</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>

  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowArrayInitFromType</span><span class="p">(</span><span class="n">array_out</span><span class="p">,</span> <span class="n">NANOARROW_TYPE_INT32</span><span class="p">));</span>

  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowArrayStartAppending</span><span class="p">(</span><span class="n">array_out</span><span class="p">));</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowArrayAppendInt</span><span class="p">(</span><span class="n">array_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">));</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowArrayAppendInt</span><span class="p">(</span><span class="n">array_out</span><span class="p">,</span> <span class="mi">2</span><span class="p">));</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowArrayAppendInt</span><span class="p">(</span><span class="n">array_out</span><span class="p">,</span> <span class="mi">3</span><span class="p">));</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowArrayFinishBuilding</span><span class="p">(</span><span class="n">array_out</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">error</span><span class="p">));</span>

  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowSchemaInitFromType</span><span class="p">(</span><span class="n">schema_out</span><span class="p">,</span> <span class="n">NANOARROW_TYPE_INT32</span><span class="p">));</span>

  <span class="k">return</span> <span class="n">NANOARROW_OK</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Similarly, one can extract elements from an array:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp">
#include</span> <span class="cpf">"nanoarrow.h"</span><span class="cp">
</span>
<span class="kt">int</span> <span class="nf">print_simple_array</span><span class="p">(</span><span class="k">struct</span> <span class="n">ArrowArray</span><span class="o">*</span> <span class="n">array</span><span class="p">,</span> <span class="k">struct</span> <span class="n">ArrowSchema</span><span class="o">*</span> <span class="n">schema</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">struct</span> <span class="n">ArrowError</span> <span class="n">error</span><span class="p">;</span>
  <span class="k">struct</span> <span class="n">ArrowArrayView</span> <span class="n">array_view</span><span class="p">;</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowArrayViewInitFromSchema</span><span class="p">(</span><span class="o">&amp;</span><span class="n">array_view</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">error</span><span class="p">));</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">array_view</span><span class="p">.</span><span class="n">storage_type</span> <span class="o">!=</span> <span class="n">NANOARROW_TYPE_INT32</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"Array has storage that is not int32</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="kt">int</span> <span class="n">result</span> <span class="o">=</span> <span class="n">ArrowArrayViewSetArray</span><span class="p">(</span><span class="o">&amp;</span><span class="n">array_view</span><span class="p">,</span> <span class="n">array</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">error</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">result</span> <span class="o">!=</span> <span class="n">NANOARROW_OK</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">ArrowArrayViewReset</span><span class="p">(</span><span class="o">&amp;</span><span class="n">array_view</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">result</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">int64_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">array</span><span class="o">-&gt;</span><span class="n">length</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"%d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">ArrowArrayViewGetIntUnsafe</span><span class="p">(</span><span class="o">&amp;</span><span class="n">array_view</span><span class="p">,</span> <span class="n">i</span><span class="p">));</span>
  <span class="p">}</span>

  <span class="n">ArrowArrayViewReset</span><span class="p">(</span><span class="o">&amp;</span><span class="n">array_view</span><span class="p">);</span>
  <span class="k">return</span> <span class="n">NANOARROW_OK</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<h2 id="using-nanoarrow-in-c-r-and-python">Using nanoarrow in C++, R, and Python</h2>

<p>Recognizing that many projects for which nanoarrow may be useful will have
access a higher-level runtime than C, there are experiments to provide
these users with a minimal set of useful tools.</p>

<p>For C++ projects, an experimental
<a href="https://apache.github.io/arrow-nanoarrow/dev/cpp.html">“nanoarrow.hpp”</a>
interface provides <code class="language-plaintext highlighter-rouge">unique_ptr</code>-like wrappers for nanoarrow C objects to
reduce the verbosity of using the nanoarrow API. For example, the previous
<code class="language-plaintext highlighter-rouge">print_simple_array()</code> implementation would collapse to:</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp">
#include</span> <span class="cpf">"nanoarrow.hpp"</span><span class="cp">
</span>
<span class="kt">int</span> <span class="nf">print_simple_array2</span><span class="p">(</span><span class="k">struct</span> <span class="nc">ArrowArray</span><span class="o">*</span> <span class="n">array</span><span class="p">,</span> <span class="k">struct</span> <span class="nc">ArrowSchema</span><span class="o">*</span> <span class="n">schema</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">struct</span> <span class="nc">ArrowError</span> <span class="n">error</span><span class="p">;</span>
  <span class="n">nanoarrow</span><span class="o">::</span><span class="n">UniqueArrayView</span> <span class="n">array_view</span><span class="p">;</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowArrayViewInitFromSchema</span><span class="p">(</span><span class="n">array_view</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span> <span class="n">schema</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">error</span><span class="p">));</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowArrayViewSetArray</span><span class="p">(</span><span class="n">array_view</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span> <span class="n">array</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">error</span><span class="p">));</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int64_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">array</span><span class="o">-&gt;</span><span class="n">length</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"%d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">ArrowArrayViewGetIntUnsafe</span><span class="p">(</span><span class="n">array_view</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span> <span class="n">i</span><span class="p">));</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">NANOARROW_OK</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>For R packages, experimental
<a href="https://apache.github.io/arrow-nanoarrow/dev/r/index.html">R bindings</a> provide
a limited set of conversions between R vectors and Arrow arrays such that
R bindings for a library with an Arrow-based interface do not need to provide
this behaviour themselves. Additional features include printing and validating
the content of the C structures at the heart of the C Data and C Stream
interfaces to facilitate the development of bindings to Arrow-based libraries.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># install.packages("remotes")</span><span class="w">
</span><span class="n">remotes</span><span class="o">::</span><span class="n">install_github</span><span class="p">(</span><span class="s2">"apache/arrow-nanoarrow/r"</span><span class="p">,</span><span class="w"> </span><span class="n">build</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">nanoarrow</span><span class="p">)</span><span class="w">

</span><span class="n">as_nanoarrow_array</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">5</span><span class="p">)</span><span class="w">
</span><span class="c1">#&gt; &lt;nanoarrow_array int32[5]&gt;</span><span class="w">
</span><span class="c1">#&gt;  $ length    : int 5</span><span class="w">
</span><span class="c1">#&gt;  $ null_count: int 0</span><span class="w">
</span><span class="c1">#&gt;  $ offset    : int 0</span><span class="w">
</span><span class="c1">#&gt;  $ buffers   :List of 2</span><span class="w">
</span><span class="c1">#&gt;   ..$ :&lt;nanoarrow_buffer_validity[0 b] at 0x0&gt;</span><span class="w">
</span><span class="c1">#&gt;   ..$ :&lt;nanoarrow_buffer_data_int32[20 b] at 0x135d13c28&gt;</span><span class="w">
</span><span class="c1">#&gt;  $ dictionary: NULL</span><span class="w">
</span><span class="c1">#&gt;  $ children  : list()</span><span class="w">
</span></code></pre></div></div>

<p>A <a href="https://github.com/apache/arrow-nanoarrow/tree/main/python">Python package skeleton</a>
exists in the nanoarrow repository and further functionality may be added once
the C library interface has stabilized.</p>

<h2 id="try-nanoarrow">Try nanoarrow</h2>

<p>For any interested in giving nanoarrow a try, the easiest way to get started is to clone the
<a href="https://github.com/apache/arrow-nanoarrow/tree/apache-arrow-nanoarrow-0.1.0">nanoarrow repository from GitHub</a>
and build/modify the
<a href="https://github.com/apache/arrow-nanoarrow/tree/apache-arrow-nanoarrow-0.1.0/examples/cmake-minimal">minimal CMake build example</a>.
For applied usage, one can refer to the
<a href="https://github.com/apache/arrow-adbc/tree/main/c/driver/sqlite">ADBC SQLite driver</a>
and the <a href="https://github.com/apache/arrow-adbc/tree/main/c/driver/postgresql">ADBC PostgreSQL driver</a>.</p>

<h2 id="contributors">Contributors</h2>

<p>This initial release consists of contributions from 6 contributors in addition
to the invaluable advice and support of the Apache Arrow developer mailing list.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog -sn 8339114637919b661c1c8fae6764ceed532c935e..apache-arrow-nanoarrow-0.1.0 | grep -v "GitHub Actions"
   100  Dewey Dunnington
     7  David Li
     2  Dirk Eddelbuettel
     1  Dane Pitkin
     1  Jonathan Keane
     1  Joris Van den Bossche
</code></pre></div></div>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.1.0 release of Apache Arrow nanoarrow. This initial release covers 31 resolved issues from 6 contributors. Release Highlights This initial release includes the following: A C library bundled as two files (nanoarrow.c and nanoarrow.h). An R package providing bindings for users of the R programming language. See the Changelog for a detailed list of contributions leading up to the initial release. Why nanoarrow? The adoption of the Arrow C Data Interface and the Arrow C Stream Interface since their introduction have been impressive and enthusiastic: not only have Arrow language bindings adopted the standard to pass data among themselves, a growing number of high-profile libraries like GDAL and DuckDB use the standard to improve performance and provide an ABI-stable interface to tabular input and output. GDAL and DuckDB are fortunate to have hard-working and forward-thinking maintainers that were motivated to provide support for the Arrow C Data and Stream interfaces even though the code to do so required an intimate knowledge of both the interface and the columnar specification on which it is based. The vision of nanoarrow is that it should be trivial for a library or application to implement an Arrow-based interface: if a library consumes or produces tabular data, Arrow should be the first place developers look. Developers shouldn’t have to be familiar with the details of the columnar specification—nor should they have to take on any build-time dependencies—to get started. The Arrow Database Connectivity (ADBC) specification is a good example of such a project, and provided a strong motivator for the development of nanoarrow: at the heart of ADBC is the idea of a core “driver manager” and database-specific drivers that are distributed as independent C/C++/Python/R/Java/Go projects. At least in R and Python, embedding an existing Arrow implementation (e.g., Arrow C++) is challenging in the context of multiple packages intended to be loaded into the same process. As of this writing, ADBC includes nanoarrow-based SQLite and PostgreSQL drivers and a nanoarrow-based validation suite for drivers. Using nanoarrow in C The nanoarrow C library is distributed as two files (nanoarrow.h and nanoarrow.c) that can be copied and vendored into an existing code base. This results in a static library of about 50 KB and builds in less than a second. Some features that nanoarrow provides are: Helpers to create types, schemas, and metadata Growable buffers, including the option for custom allocators/deallocators. Bitmap (i.e., bitpacked boolean) utilities An API for building arrays from buffers An API for building arrays element-wise An API to extract elements element-wise from an existing array. For example, one can build an integer array element-wise: #include "nanoarrow.h" int make_simple_array(struct ArrowArray* array_out, struct ArrowSchema* schema_out) { struct ArrowError error; array_out-&gt;release = NULL; schema_out-&gt;release = NULL; NANOARROW_RETURN_NOT_OK(ArrowArrayInitFromType(array_out, NANOARROW_TYPE_INT32)); NANOARROW_RETURN_NOT_OK(ArrowArrayStartAppending(array_out)); NANOARROW_RETURN_NOT_OK(ArrowArrayAppendInt(array_out, 1)); NANOARROW_RETURN_NOT_OK(ArrowArrayAppendInt(array_out, 2)); NANOARROW_RETURN_NOT_OK(ArrowArrayAppendInt(array_out, 3)); NANOARROW_RETURN_NOT_OK(ArrowArrayFinishBuilding(array_out, &amp;error)); NANOARROW_RETURN_NOT_OK(ArrowSchemaInitFromType(schema_out, NANOARROW_TYPE_INT32)); return NANOARROW_OK; } Similarly, one can extract elements from an array: #include &lt;stdio.h&gt; #include "nanoarrow.h" int print_simple_array(struct ArrowArray* array, struct ArrowSchema* schema) { struct ArrowError error; struct ArrowArrayView array_view; NANOARROW_RETURN_NOT_OK(ArrowArrayViewInitFromSchema(&amp;array_view, schema, &amp;error)); if (array_view.storage_type != NANOARROW_TYPE_INT32) { printf("Array has storage that is not int32\n"); } int result = ArrowArrayViewSetArray(&amp;array_view, array, &amp;error); if (result != NANOARROW_OK) { ArrowArrayViewReset(&amp;array_view); return result; } for (int64_t i = 0; i &lt; array-&gt;length; i++) { printf("%d\n", (int)ArrowArrayViewGetIntUnsafe(&amp;array_view, i)); } ArrowArrayViewReset(&amp;array_view); return NANOARROW_OK; } Using nanoarrow in C++, R, and Python Recognizing that many projects for which nanoarrow may be useful will have access a higher-level runtime than C, there are experiments to provide these users with a minimal set of useful tools. For C++ projects, an experimental “nanoarrow.hpp” interface provides unique_ptr-like wrappers for nanoarrow C objects to reduce the verbosity of using the nanoarrow API. For example, the previous print_simple_array() implementation would collapse to: #include &lt;stdio.h&gt; #include "nanoarrow.hpp" int print_simple_array2(struct ArrowArray* array, struct ArrowSchema* schema) { struct ArrowError error; nanoarrow::UniqueArrayView array_view; NANOARROW_RETURN_NOT_OK(ArrowArrayViewInitFromSchema(array_view.get(), schema, &amp;error)); NANOARROW_RETURN_NOT_OK(ArrowArrayViewSetArray(array_view.get(), array, &amp;error)); for (int64_t i = 0; i &lt; array-&gt;length; i++) { printf("%d\n", (int)ArrowArrayViewGetIntUnsafe(array_view.get(), i)); } return NANOARROW_OK; } For R packages, experimental R bindings provide a limited set of conversions between R vectors and Arrow arrays such that R bindings for a library with an Arrow-based interface do not need to provide this behaviour themselves. Additional features include printing and validating the content of the C structures at the heart of the C Data and C Stream interfaces to facilitate the development of bindings to Arrow-based libraries. # install.packages("remotes") remotes::install_github("apache/arrow-nanoarrow/r", build = FALSE) library(nanoarrow) as_nanoarrow_array(1:5) #&gt; &lt;nanoarrow_array int32[5]&gt; #&gt; $ length : int 5 #&gt; $ null_count: int 0 #&gt; $ offset : int 0 #&gt; $ buffers :List of 2 #&gt; ..$ :&lt;nanoarrow_buffer_validity[0 b] at 0x0&gt; #&gt; ..$ :&lt;nanoarrow_buffer_data_int32[20 b] at 0x135d13c28&gt; #&gt; $ dictionary: NULL #&gt; $ children : list() A Python package skeleton exists in the nanoarrow repository and further functionality may be added once the C library interface has stabilized. Try nanoarrow For any interested in giving nanoarrow a try, the easiest way to get started is to clone the nanoarrow repository from GitHub and build/modify the minimal CMake build example. For applied usage, one can refer to the ADBC SQLite driver and the ADBC PostgreSQL driver. Contributors This initial release consists of contributions from 6 contributors in addition to the invaluable advice and support of the Apache Arrow developer mailing list. $ git shortlog -sn 8339114637919b661c1c8fae6764ceed532c935e..apache-arrow-nanoarrow-0.1.0 | grep -v "GitHub Actions" 100 Dewey Dunnington 7 David Li 2 Dirk Eddelbuettel 1 Dane Pitkin 1 Jonathan Keane 1 Joris Van den Bossche]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 0.2.0 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/02/16/adbc-0.2.0-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.2.0 (Libraries) Release" /><published>2023-02-16T00:00:00-05:00</published><updated>2023-02-16T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/02/16/adbc-0.2.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/02/16/adbc-0.2.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.2.0 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/2"><strong>34
resolved issues</strong></a> from <a href="#contributors"><strong>7 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version
0.2.0.  The <strong>API specification</strong> is versioned separately and is
at version 1.0.0.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.2.0/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>An ADBC Flight SQL driver in Go has been added.  This driver has
bindings for C/C++ and Python as well.  This lets us distribute and
update the driver independently of PyArrow, without causing conflicts
in dependencies like Protobuf and gRPC.</p>

<p>The Go database/sql interface now returns standard library <code class="language-plaintext highlighter-rouge">time.Time</code>
values for Arrow time and date columns.</p>

<p>The PostgreSQL driver has support for more types.</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.1.0..apache-arrow-adbc-0.2.0
    56	David Li
     8	Sutou Kouhei
     7	Matt Topol
     4	Jacob Marble
     2	Benson Muite
     1	Dave Hirschfeld
     1	Jianfeng Mao
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>New extensions to the API specification have been proposed.  These
will be backwards-compatible and will become API specification 1.1.0.
For details, see the <a href="https://lists.apache.org/thread/247z3t06mf132nocngc1jkp3oqglz7jp">mailing list discussion</a> and the
<a href="https://github.com/apache/arrow-adbc/milestone/3">milestone</a> tracking the proposed features.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.2.0 release of the Apache Arrow ADBC libraries. This covers includes 34 resolved issues from 7 distinct contributors. This is a release of the libraries, which are at version 0.2.0. The API specification is versioned separately and is at version 1.0.0. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights An ADBC Flight SQL driver in Go has been added. This driver has bindings for C/C++ and Python as well. This lets us distribute and update the driver independently of PyArrow, without causing conflicts in dependencies like Protobuf and gRPC. The Go database/sql interface now returns standard library time.Time values for Arrow time and date columns. The PostgreSQL driver has support for more types. Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.1.0..apache-arrow-adbc-0.2.0 56 David Li 8 Sutou Kouhei 7 Matt Topol 4 Jacob Marble 2 Benson Muite 1 Dave Hirschfeld 1 Jianfeng Mao Roadmap New extensions to the API specification have been proposed. These will be backwards-compatible and will become API specification 1.1.0. For details, see the mailing list discussion and the milestone tracking the proposed features. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">February 2023 Rust Apache Arrow Highlights</title><link href="https://arrow.apache.org/blog/2023/02/13/rust-32.0.0/" rel="alternate" type="text/html" title="February 2023 Rust Apache Arrow Highlights" /><published>2023-02-13T19:00:00-05:00</published><updated>2023-02-13T19:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/02/13/rust-32.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/02/13/rust-32.0.0/"><![CDATA[<!--

-->

<h1 id="introduction">Introduction</h1>

<p>With the recent release of <a href="https://crates.io/crates/arrow/32.0.0">32.0.0</a> of the Rust implementation of <a href="https://arrow.apache.org/">Apache Arrow</a>, it seemed timely to highlight some of the community works since the <a href="https://arrow.apache.org/blog/2022/06/16/rust-16.0.0/">last update</a>.</p>

<p>The most recent list of detailed changes can always be found in the <a href="https://github.com/apache/arrow-rs/blob/master/CHANGELOG.md">CHANGELOG</a>, with the full historical list available <a href="https://github.com/apache/arrow-rs/blob/master/CHANGELOG-old.md">here</a>.</p>

<h1 id="arrow">Arrow</h1>

<p><a href="https://crates.io/crates/arrow">arrow</a> and <a href="https://crates.io/crates/arrow-flight">arrow-flight</a> are native Rust implementations of Apache Arrow. Apache Arrow defines a language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware like CPUs and GPUs. The Arrow memory format also supports zero-copy reads for lightning-fast data access without serialization overhead.</p>

<p>The <a href="https://www.rust-lang.org/">Rust language</a> offers best in class performance,  memory safety, and the developer productivity of a modern programming language. These features make Rust an excellent choice for building modern high performance analytical systems. When combined, Rust and the Apache Arrow Ecosystem are a compelling toolkit for building the next generation of systems.</p>

<p>The <a href="https://github.com/apache/arrow-rs">repository</a> recently passed 1400 stars on github, and the community has been focused on performance and feature completeness.</p>

<p><strong>Major Highlights</strong></p>

<ul>
  <li><strong>New CSV and JSON Readers</strong>: The CSV and JSON readers have been revamped. Their performance has more than doubled, and they now support push-driven parsing facilitating async streaming decode from object storage.</li>
  <li><strong>Faster Build Times and Reduced Codegen</strong>: The <code class="language-plaintext highlighter-rouge">arrow</code> crate has been split into multiple smaller crates, and large kernels have been moved behind optional feature flags. These changes allow downstream projects to choose a smaller dependency footprint and build times, if desired.</li>
  <li><strong>Support for Copy-On-Write</strong>: Arrow arrays now support copy-on-write, via the <a href="https://docs.rs/arrow/32.0.0/arrow/array/struct.ArrayData.html#method.into_builder"><code class="language-plaintext highlighter-rouge">into_builder</code></a> methods</li>
  <li><strong>Comparable Row Format</strong>: <a href="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1/">Much faster multi-column Sorting and Grouping</a> is now possible with the the new spillable, comparable <a href="https://docs.rs/arrow-row/32.0.0/arrow_row/index.html">row-format</a></li>
  <li><strong>FlightSQL Support</strong>: <a href="https://arrow.apache.org/docs/format/FlightSql.html">FlightSQL</a> <a href="https://docs.rs/arrow-flight/32.0.0/arrow_flight/sql/index.html">support</a> has been expanded</li>
  <li><strong>Mid-Level Flight Client</strong>: A new <a href="https://docs.rs/arrow-flight/32.0.0/arrow_flight/client/struct.FlightClient.html">FlightClient</a> is available that handles lower level protocol details, and easier to use <a href="https://docs.rs/arrow-flight/32.0.0/arrow_flight/encode/struct.FlightDataEncoderBuilder.html">encoding</a> and <a href="https://docs.rs/arrow-flight/32.0.0/arrow_flight/decode/struct.FlightDataDecoder.html">decoding</a> APIs.</li>
  <li><strong>IPC File Compression</strong>: Arrow IPC file <a href="https://docs.rs/arrow-ipc/32.0.0/arrow_ipc/gen/Message/struct.CompressionType.html">compression</a> with ZSTD and LZ4 is now fully supported.</li>
  <li><strong>Full Decimal Support</strong>: 256-bit decimals and negative scales can be created and manipulated using many kernels, such as arithmetic.</li>
  <li><strong>Improved Dictionary Support</strong>: Dictionaries are now transparently supported in most kernels.</li>
  <li><strong>Improved Temporal Support</strong>: Timestamps with Timezones and other temporal types are supported in many more kernels.</li>
  <li><strong>Improved Generics</strong>: Improved generics allow writing code generic over all arrays, or all arrays with the same layout</li>
  <li><strong>Downcast Macros</strong>: Various <a href="https://docs.rs/arrow/32.0.0/arrow/macro.downcast_primitive_array.html">helper</a> <a href="https://docs.rs/arrow/32.0.0/arrow/macro.downcast_dictionary_array.html">macros</a> are now available to simplify dynamic dispatch to statically typed implementations.</li>
</ul>

<h1 id="parquet">Parquet</h1>

<p><a href="https://parquet.apache.org/">Apache Parquet</a> is an open source, column-oriented data file format designed for efficient data storage and retrieval. It provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk. The Apache Parquet implementation in Rust is one of the <a href="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/">fastest and most sophisticated</a> open source implementations available.</p>

<p><strong>Major Highlights</strong></p>

<ul>
  <li><strong>Arbitrarily Nested Schema</strong>: Arbitrarily nested schemas can be read to and written from arrow, as described in the series of <a href="https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/">blog posts</a> on the topic.</li>
  <li><strong>Predicate Pushdown</strong>: The <a href="https://docs.rs/parquet/32.0.0/parquet/arrow/index.html">arrow reader</a> now supports advanced predicate pushdown, including late materialization, as described <a href="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/">here</a>. See <a href="https://docs.rs/parquet/32.0.0/parquet/arrow/arrow_reader/struct.RowSelection.html">RowSelection</a> and <a href="https://docs.rs/parquet/32.0.0/parquet/arrow/arrow_reader/trait.ArrowPredicate.html">ArrowPredicate</a>.</li>
  <li><strong>Bloom Filter Support</strong>: Support for both <a href="https://docs.rs/parquet/32.0.0/parquet/bloom_filter/index.html">reading and writing bloom filters</a> has been added.</li>
  <li><strong>CLI Tools</strong>: additional <a href="https://github.com/apache/arrow-rs/tree/master/parquet/src/bin">CLI tools</a> have been added to introspect and manipulate parquet data.</li>
</ul>

<h1 id="object-store">Object Store</h1>

<p>Modern analytic workloads increasingly make use of blob storage facilities, such as S3, to store large volumes of queryable data. A native Rust object storage implementation that works well with the Rust Ecosystem in general, and the Arrow IO abstractions, is an important building block for many applications.
The <a href="https://docs.rs/object_store/latest/object_store/">object_store</a> crate was <a href="https://www.influxdata.com/blog/rust-object-store-donation/">donated to the Apache Arrow project in July 2022</a> to fill this need, and while it follows a separate release schedule than the <code class="language-plaintext highlighter-rouge">arrow</code> and <code class="language-plaintext highlighter-rouge">parquet</code> crates, it forms an integral part of the overarching Arrow IO story.</p>

<p>Recent improvements include the following:</p>

<p><strong>Major Highlights</strong></p>

<ul>
  <li><strong>Streaming Upload</strong>: Multipart uploads are now supported.</li>
  <li><strong>Minimised dependency footprint</strong>: Upstream SDKs are no longer used, improving consistency and reducing dependencies.</li>
  <li><strong>HTTP / WebDAV Support</strong>: Applications can read from arbitrary HTTP servers, with mutation and listing supported on WebDAV-compatible endpoints.</li>
  <li><strong>Configurable Networking</strong>: Socks proxies, and advanced HTTP client configuration are now supported.</li>
  <li><strong>Serializable Configuration</strong>: Configuration information can now be easily serialized and deserialized.</li>
  <li><strong>Additional Authentication</strong>: Additional authentication options are now available for the various cloud providers.</li>
</ul>

<h1 id="contributors">Contributors:</h1>

<p>While some open source software can be created mostly by a single contributor, we believe the greatest software with the largest impact and reach is built around its community. Thus, Arrow is proud to part of the <a href="https://www.apache.org/">Apache Software Foundation</a> and our releases both past and present are a result of our amazing community’s effort.</p>

<p>We would like to thank everyone who has contributed to the arrow-rs repository since the <code class="language-plaintext highlighter-rouge">16.0.0</code> release. Keep up the great work, and we look forward to continued improvements:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">% git shortlog -sn 16.0.0..32.0.0
   347  Raphael Taylor-Davies
   166  Liang-Chi Hsieh
    94  Andrew Lamb
    36  Remzi Yang
    30  Kun Liu
    21  Yang Jiang
    20  askoa
    17  dependabot[bot]
    15  Vrishabh
    12  Dan Harris
    12  Wei-Ting Kuo
    11  Daniël Heres
    11  Jörn Horstmann
     9  Brent Gardner
     9  Ian Alexander Joiner
     9  Jiayu Liu
     9  Martin Grigorov
     8  Palladium
     7  Jeffrey
     7  Marco Neumann
     6  Robert Pack
     6  Will Jones
     4  Andy Grove
     4  comphead
     3  Adrián Gallego Castellanos
     3  Markus Westerlind
     3  Quentin
     2  Alex Qyoun-ae
     2  Dmitry Patsura
     2  Frank
     2  Jiacai Liu
     2  Marc Garcia
     2  Marko Grujic
     2  Max Burke
     2  Your friendly neighborhood geek
     2  sachin agarwal
     1  Aarash Heydari
     1  Adam Gutglick
     1  Andrey Frolov
     1  Anthony Poncet
     1  Artjoms Iskovs
     1  Ben Kimock
     1  Brian Phillips
     1  Carol (Nichols || Goulding)
     1  Christian Salvati
     1  Dalton Modlin
     1  Daniel Martinez Maqueda
     1  Daniel Poelzleithner
     1  Davis Silverman
     1  Dhruv Vats
     1  Fabio Silva
     1  GeauxEric
     1  George Andronchik
     1  Ismail-Maj
     1  Ismaël Mejía
     1  JanKaul
     1  JasonLi
     1  Javier Goday
     1  Jayjeet Chakraborty
     1  Jean-Charles Campagne
     1  Jie Han
     1  John Hughes
     1  Jon Mease
     1  Kevin Lim
     1  Kohei Suzuki
     1  Konstantin Fastov
     1  Marius S
     1  Masato Kato
     1  Matthijs Brobbel
     1  Michael Edwards
     1  Pier-Olivier Thibault
     1  Remco Verhoef
     1  Rutvik Patel
     1  Sean Smith
     1  Sid
     1  Stanislav Lukeš
     1  Steve Vaughan
     1  Stuart Carnie
     1  Sumit
     1  Trent Feda
     1  Valeriy V. Vorotyntsev
     1  Wenjun L
     1  X
     1  aksharau
     1  bmmeijers
     1  chunshao.rcs
     1  jakevin
     1  kastolars
     1  nvartolomei
     1  xudong.w
     1  哇呜哇呜呀咦耶
     1  尹吉峰
</span></code></pre></div></div>

<h1 id="join-the-community">Join the community</h1>

<p>If you are interested in contributing to the Rust subproject in Apache Arrow, we encourage you to try out Arrow on some of your data, help
improve the documentation, or submit a PR. You can find a list of open issues
suitable for beginners <a href="https://github.com/apache/arrow-rs/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a>
and the full list <a href="https://github.com/apache/arrow-rs/issues">here</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[Introduction With the recent release of 32.0.0 of the Rust implementation of Apache Arrow, it seemed timely to highlight some of the community works since the last update. The most recent list of detailed changes can always be found in the CHANGELOG, with the full historical list available here. Arrow arrow and arrow-flight are native Rust implementations of Apache Arrow. Apache Arrow defines a language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware like CPUs and GPUs. The Arrow memory format also supports zero-copy reads for lightning-fast data access without serialization overhead. The Rust language offers best in class performance, memory safety, and the developer productivity of a modern programming language. These features make Rust an excellent choice for building modern high performance analytical systems. When combined, Rust and the Apache Arrow Ecosystem are a compelling toolkit for building the next generation of systems. The repository recently passed 1400 stars on github, and the community has been focused on performance and feature completeness. Major Highlights New CSV and JSON Readers: The CSV and JSON readers have been revamped. Their performance has more than doubled, and they now support push-driven parsing facilitating async streaming decode from object storage. Faster Build Times and Reduced Codegen: The arrow crate has been split into multiple smaller crates, and large kernels have been moved behind optional feature flags. These changes allow downstream projects to choose a smaller dependency footprint and build times, if desired. Support for Copy-On-Write: Arrow arrays now support copy-on-write, via the into_builder methods Comparable Row Format: Much faster multi-column Sorting and Grouping is now possible with the the new spillable, comparable row-format FlightSQL Support: FlightSQL support has been expanded Mid-Level Flight Client: A new FlightClient is available that handles lower level protocol details, and easier to use encoding and decoding APIs. IPC File Compression: Arrow IPC file compression with ZSTD and LZ4 is now fully supported. Full Decimal Support: 256-bit decimals and negative scales can be created and manipulated using many kernels, such as arithmetic. Improved Dictionary Support: Dictionaries are now transparently supported in most kernels. Improved Temporal Support: Timestamps with Timezones and other temporal types are supported in many more kernels. Improved Generics: Improved generics allow writing code generic over all arrays, or all arrays with the same layout Downcast Macros: Various helper macros are now available to simplify dynamic dispatch to statically typed implementations. Parquet Apache Parquet is an open source, column-oriented data file format designed for efficient data storage and retrieval. It provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk. The Apache Parquet implementation in Rust is one of the fastest and most sophisticated open source implementations available. Major Highlights Arbitrarily Nested Schema: Arbitrarily nested schemas can be read to and written from arrow, as described in the series of blog posts on the topic. Predicate Pushdown: The arrow reader now supports advanced predicate pushdown, including late materialization, as described here. See RowSelection and ArrowPredicate. Bloom Filter Support: Support for both reading and writing bloom filters has been added. CLI Tools: additional CLI tools have been added to introspect and manipulate parquet data. Object Store Modern analytic workloads increasingly make use of blob storage facilities, such as S3, to store large volumes of queryable data. A native Rust object storage implementation that works well with the Rust Ecosystem in general, and the Arrow IO abstractions, is an important building block for many applications. The object_store crate was donated to the Apache Arrow project in July 2022 to fill this need, and while it follows a separate release schedule than the arrow and parquet crates, it forms an integral part of the overarching Arrow IO story. Recent improvements include the following: Major Highlights Streaming Upload: Multipart uploads are now supported. Minimised dependency footprint: Upstream SDKs are no longer used, improving consistency and reducing dependencies. HTTP / WebDAV Support: Applications can read from arbitrary HTTP servers, with mutation and listing supported on WebDAV-compatible endpoints. Configurable Networking: Socks proxies, and advanced HTTP client configuration are now supported. Serializable Configuration: Configuration information can now be easily serialized and deserialized. Additional Authentication: Additional authentication options are now available for the various cloud providers. Contributors: While some open source software can be created mostly by a single contributor, we believe the greatest software with the largest impact and reach is built around its community. Thus, Arrow is proud to part of the Apache Software Foundation and our releases both past and present are a result of our amazing community’s effort. We would like to thank everyone who has contributed to the arrow-rs repository since the 16.0.0 release. Keep up the great work, and we look forward to continued improvements: % git shortlog -sn 16.0.0..32.0.0 347 Raphael Taylor-Davies 166 Liang-Chi Hsieh 94 Andrew Lamb 36 Remzi Yang 30 Kun Liu 21 Yang Jiang 20 askoa 17 dependabot[bot] 15 Vrishabh 12 Dan Harris 12 Wei-Ting Kuo 11 Daniël Heres 11 Jörn Horstmann 9 Brent Gardner 9 Ian Alexander Joiner 9 Jiayu Liu 9 Martin Grigorov 8 Palladium 7 Jeffrey 7 Marco Neumann 6 Robert Pack 6 Will Jones 4 Andy Grove 4 comphead 3 Adrián Gallego Castellanos 3 Markus Westerlind 3 Quentin 2 Alex Qyoun-ae 2 Dmitry Patsura 2 Frank 2 Jiacai Liu 2 Marc Garcia 2 Marko Grujic 2 Max Burke 2 Your friendly neighborhood geek 2 sachin agarwal 1 Aarash Heydari 1 Adam Gutglick 1 Andrey Frolov 1 Anthony Poncet 1 Artjoms Iskovs 1 Ben Kimock 1 Brian Phillips 1 Carol (Nichols || Goulding) 1 Christian Salvati 1 Dalton Modlin 1 Daniel Martinez Maqueda 1 Daniel Poelzleithner 1 Davis Silverman 1 Dhruv Vats 1 Fabio Silva 1 GeauxEric 1 George Andronchik 1 Ismail-Maj 1 Ismaël Mejía 1 JanKaul 1 JasonLi 1 Javier Goday 1 Jayjeet Chakraborty 1 Jean-Charles Campagne 1 Jie Han 1 John Hughes 1 Jon Mease 1 Kevin Lim 1 Kohei Suzuki 1 Konstantin Fastov 1 Marius S 1 Masato Kato 1 Matthijs Brobbel 1 Michael Edwards 1 Pier-Olivier Thibault 1 Remco Verhoef 1 Rutvik Patel 1 Sean Smith 1 Sid 1 Stanislav Lukeš 1 Steve Vaughan 1 Stuart Carnie 1 Sumit 1 Trent Feda 1 Valeriy V. Vorotyntsev 1 Wenjun L 1 X 1 aksharau 1 bmmeijers 1 chunshao.rcs 1 jakevin 1 kastolars 1 nvartolomei 1 xudong.w 1 哇呜哇呜呀咦耶 1 尹吉峰 Join the community If you are interested in contributing to the Rust subproject in Apache Arrow, we encourage you to try out Arrow on some of your data, help improve the documentation, or submit a PR. You can find a list of open issues suitable for beginners here and the full list here.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 11.0.0 Release</title><link href="https://arrow.apache.org/blog/2023/01/25/11.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 11.0.0 Release" /><published>2023-01-25T00:00:00-05:00</published><updated>2023-01-25T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/01/25/11.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/01/25/11.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 11.0.0 release. This covers
over 3 months of development work and includes <a href="https://github.com/apache/arrow/milestone/1?closed=1"><strong>423 resolved issues</strong></a>
from <a href="/release/11.0.0.html#contributors"><strong>95 distinct contributors</strong></a>. See the <a href="https://arrow.apache.org/install/">Install Page</a>
to learn how to get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/11.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 10.0.0 release, Ben Baumgold, Will Jones, Eric Patrick Hanson,
Curtis Vogt, Yang Jiang, Jarrett Revels, Raúl Cumplido, Jacob Wujciak,
Jie Wen and Brent Gardner have been invited to be committers.
Kun Liu have joined the Project Management Committee (PMC).</p>

<p>As per our newly started tradition of rotating the PMC chair once a year
Andrew Lamb was elected as the new PMC chair and VP.</p>

<p>Thanks for your contributions and participation in the project!</p>

<h2 id="columnar-format-notes">Columnar Format Notes</h2>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>

<p>In the C++/Python Flight clients, DoAction now properly streams the results, instead of blocking until the call finishes. Applications that did not consume the iterator before should fully consume the result. (<a href="https://github.com/apache/arrow/issues/15069">#15069</a>)</p>

<h2 id="c-notes">C++ notes</h2>
<ul>
  <li>It is now possible to specify alignment when making allocations with a MemoryPool <a href="https://github.com/apache/arrow/issues/33056">GH-33056</a></li>
  <li>It is now possible to run an ExecPlan without using any CPU threads</li>
  <li>Added kernel for slicing list values <a href="https://github.com/apache/arrow/issues/33168">GH-33168</a></li>
  <li>Added kernel for slicing binary arrays <a href="https://github.com/apache/arrow/issues/20357">GH-20357</a></li>
  <li>When comparing list arrays for equality the list field name is now ignored <a href="https://github.com/apache/arrow/issues/30519">GH-30519</a></li>
  <li>Add support for partitioning on columns that contain special characters <a href="https://github.com/apache/arrow/issues/33448">GH-33448</a></li>
  <li>Added a streaming reader for JSON <a href="https://github.com/apache/arrow/issues/33140">GH-33140</a></li>
  <li>Added support for incremental writes to the ORC writer <a href="https://github.com/apache/arrow/issues/33047">GH-33047</a></li>
  <li>Added support for casting decimal to string and writing decimal to CSV <a href="https://github.com/apache/arrow/issues/33002">GH-33002</a></li>
  <li>Fixed an assert in the scanner that would occur when batch_readahead was set to 0 <a href="https://github.com/apache/arrow/issues/15264">GH-15264</a></li>
  <li>Fixed bug where arrays with a null data buffer would not be accepted when imported via the C data API <a href="https://github.com/apache/arrow/issues/14875">GH-14875</a></li>
  <li>Fixed bug where arrays with a zero-case union data type would not be accepted when imported via the C data API <a href="https://github.com/apache/arrow/issues/14855">GH-14855</a></li>
  <li>Fixed bug where case_when could return incorrect values <a href="https://github.com/apache/arrow/issues/33382">GH-33382</a></li>
  <li>Fixed bug where RecordBatch::Equals was ignoring field names <a href="https://github.com/apache/arrow/issues/33285">GH-33285</a>
    <h2 id="c-notes-1">C# notes</h2>
  </li>
</ul>

<p>No major changes to C#.</p>

<h2 id="go-notes">Go notes</h2>
<ul>
  <li>Go’s benchmarks will now get added to <a href="https://conbench.ursa.dev">Conbench</a> alongside the benchmarks for other implementations <a href="https://github.com/apache/arrow/issues/32983">GH-32983</a></li>
  <li>Exposed FlightService_ServiceDesc and RegisterFlightServiceServer to allow easily incorporating a flight service into an existing gRPC server <a href="https://github.com/apache/arrow/issues/15174">GH-15174</a></li>
</ul>

<h3 id="arrow">Arrow</h3>
<ul>
  <li>Function <code class="language-plaintext highlighter-rouge">ApproxEquals</code> was implemented for scalar values <a href="https://github.com/apache/arrow/issues/29581">GH-29581</a></li>
  <li><code class="language-plaintext highlighter-rouge">UnmarshalJSON</code> for the <code class="language-plaintext highlighter-rouge">RecordBuilder</code> now properly handles extra unknown fields with complex/nested values <a href="https://github.com/apache/arrow/issues/31840">GH-31840</a></li>
  <li>Decimal128 and Decimal256 type support has been added to the CSV reader <a href="https://github.com/apache/arrow/issues/33111">GH-33111</a></li>
  <li>Fixed bug in <code class="language-plaintext highlighter-rouge">array.UnionBuilder</code> where <code class="language-plaintext highlighter-rouge">Len</code> method always returned 0 <a href="https://github.com/apache/arrow/issues/14775">GH-14775</a></li>
  <li>Fixed bug for handling slices of Map arrays when marshalling to JSON and for IPC <a href="https://github.com/apache/arrow/issues/14780">GH-14780</a></li>
  <li>Fixed memory leak when compressing IPC message body buffers <a href="https://github.com/apache/arrow/issues/14883">GH-14883</a></li>
  <li>Added the ability to easily append scalar values to array builders <a href="https://github.com/apache/arrow/issues/15005">GH-15005</a></li>
</ul>

<h4 id="compute">Compute</h4>
<ul>
  <li>Scalar binary (add/subtract/multiply/divide/etc.) and unary arithmetic (abs/neg/sqrt/sign/etc.) has been implemented for the compute package <a href="https://github.com/apache/arrow/issues/33086">GH-33086</a> this includes easy functions like <code class="language-plaintext highlighter-rouge">compute.Add</code> and <code class="language-plaintext highlighter-rouge">compute.Divide</code> etc.</li>
  <li>Scalar boolean functions like AND/OR/XOR/etc. have been implemented for compute <a href="https://github.com/apache/arrow/issues/33279">GH-33279</a></li>
  <li>Scalar comparison function kernels have been implemented for compute (equal/greater/greater_equal/less/less_equal) <a href="https://github.com/apache/arrow/issues/33308">GH-33308</a></li>
  <li>Scalar compute functions are compatible with dictionary encoded arrays by casting them to their value types <a href="https://github.com/apache/arrow/issues/33502">GH-33502</a></li>
</ul>

<h3 id="parquet">Parquet</h3>
<ul>
  <li>Panic when decoding a delta_bit_packed encoded column has been fixed <a href="https://github.com/apache/arrow/issues/33483">GH-33483</a></li>
  <li>Fixed memory leak from Allocator in <code class="language-plaintext highlighter-rouge">pqarrow.WriteArrowToColumn</code> <a href="https://github.com/apache/arrow/issues/14865">GH-14865</a></li>
  <li>Fixed <code class="language-plaintext highlighter-rouge">writer.WriteBatch</code> to properly handle writing encrypted parquet columns and no longer silently fail, but instead propagate an error <a href="https://github.com/apache/arrow/issues/14940">GH-14940</a></li>
</ul>

<h2 id="java-notes">Java notes</h2>
<ul>
  <li>Implement support for writing compressed files (<a href="https://github.com/apache/arrow/pull/15223">#15223</a>)</li>
  <li>Improve performance by short-circuiting null checks when comparing non null field types (<a href="https://github.com/apache/arrow/pull/15106">#15106</a>)</li>
  <li>Several enhancements to dictionary encoding (<a href="https://github.com/apache/arrow/pull/14891">#14891</a>, (<a href="https://github.com/apache/arrow/pull/14902">#14902</a>, (<a href="https://github.com/apache/arrow/pull/14874">#14874</a>)</li>
  <li>Extend Table to support additional vector types (<a href="https://github.com/apache/arrow/pull/14573">#14573</a>)</li>
  <li>Enhance and simplify handling of allocation management by integrating C Data into allocator hierarchy (<a href="https://github.com/apache/arrow/pull/14506">#14506</a>)</li>
  <li>Make ComplexCopier agnostic of specific implementation of MapWriter (<a href="https://github.com/apache/arrow/pull/14557">#14557</a>)</li>
  <li>Distribute Apple M1 compatible JNI libraries via mavencentral (<a href="https://github.com/apache/arrow/pull/14472">#14472</a>)</li>
  <li>Extend Table copy functionality, and support returning copies of individual vectors (<a href="https://github.com/apache/arrow/pull/14389">#14389</a>)</li>
</ul>

<h2 id="javascript-notes">JavaScript notes</h2>

<ul>
  <li>Bugfixes and dependency updates.</li>
  <li>Arrow now requires BigInt support. <a href="https://github.com/apache/arrow/pull/33682">GH-33681</a></li>
</ul>

<h2 id="python-notes">Python notes</h2>

<p>Compatibility notes:</p>

<ul>
  <li>PyArrow now requires pandas &gt;= 1.0 (<a href="https://issues.apache.org/jira/browse/ARROW-18173">ARROW-18173</a>)</li>
  <li>The <code class="language-plaintext highlighter-rouge">pyarrow.parquet.ParquetDataset()</code> class now by default uses the new Dataset API
under the hood (<code class="language-plaintext highlighter-rouge">use_legacy_dataset=False</code>). You can still pass
<code class="language-plaintext highlighter-rouge">use_legacy_dataset=True</code> to get the legacy implementation, but this option will be
removed in a next release
(<a href="https://issues.apache.org/jira/browse/ARROW-16728">ARROW-16728</a>).</li>
</ul>

<p>New features:</p>

<ul>
  <li>Added support for the <a href="https://data-apis.org/dataframe-protocol/latest/purpose_and_scope.html">DataFrame Interchange Protocol</a>
for <code class="language-plaintext highlighter-rouge">pyarrow.Table</code> (<a href="https://github.com/apache/arrow/issues/33346">GH-33346</a>).</li>
  <li>New kernels: <code class="language-plaintext highlighter-rouge">list_slice()</code> to slice each list element of a ListArray
returning a new ListArray (<a href="https://issues.apache.org/jira/browse/ARROW-17960">ARROW-17960</a>).</li>
  <li>A new <code class="language-plaintext highlighter-rouge">filter()</code> method on the Dataset class as additional API to filter a Dataset
before consuming it (<a href="https://issues.apache.org/jira/browse/ARROW-16616">ARROW-16616</a>).</li>
  <li>New <code class="language-plaintext highlighter-rouge">sort()</code> method for (Chunked)Array and <code class="language-plaintext highlighter-rouge">sort_by()</code> method for RecordBatch,
providing a convenience on top of the <code class="language-plaintext highlighter-rouge">sort_indices</code> kernel
(<a href="https://github.com/apache/arrow/issues/14778">GH-14778</a>), and a new
<code class="language-plaintext highlighter-rouge">Dataset.sort_by()</code> method (<a href="https://github.com/apache/arrow/issues/14975">GH-14975</a>).</li>
</ul>

<p>Other improvements:</p>

<ul>
  <li>Support for custom metadata of record batches in the IPC read and write APIs
(<a href="https://issues.apache.org/jira/browse/ARROW-16430">ARROW-16430</a>).</li>
  <li>Support URIs and the <code class="language-plaintext highlighter-rouge">filesystem</code> parameter in <code class="language-plaintext highlighter-rouge">pyarrow.parquet.ParquetFile</code>
(<a href="https://issues.apache.org/jira/browse/ARROW-18272">ARROW-18272</a>) and
<code class="language-plaintext highlighter-rouge">pyarrow.parquet.write_metadata</code>
(<a href="https://issues.apache.org/jira/browse/ARROW-18225">ARROW-18225</a>).</li>
  <li>When writing a dataset to IPC using <code class="language-plaintext highlighter-rouge">pyarrow.dataset.write_dataset()</code>, you can now
specify IPC specific options, such as compression
(<a href="https://issues.apache.org/jira/browse/ARROW-17991">ARROW-17991</a>)</li>
  <li>The <code class="language-plaintext highlighter-rouge">pyarrow.array()</code> function now allows to construct a MapArray from a sequence of
dicts (in addition to a sequence of tuples)
(<a href="https://issues.apache.org/jira/browse/ARROW-17832">ARROW-17832</a>).</li>
  <li>The <code class="language-plaintext highlighter-rouge">struct_field()</code> kernel now also accepts field names in addition to integer
indices (<a href="https://issues.apache.org/jira/browse/ARROW-17989">ARROW-17989</a>).</li>
  <li>Casting to string is now supported for duration (<a href="https://issues.apache.org/jira/browse/ARROW-15822">ARROW-15822</a>)
and decimal (<a href="https://issues.apache.org/jira/browse/ARROW-17458">ARROW-17458</a>) types,
which also means those can now be written to CSV.</li>
  <li>When writing to CSV, you can now specify the quoting style
(<a href="https://github.com/apache/arrow/issues/14755">GH-14755</a>).</li>
  <li>The <code class="language-plaintext highlighter-rouge">pyarrow.ipc.read_schema()</code> function now accepts a Message object
(<a href="https://issues.apache.org/jira/browse/ARROW-18423">ARROW-18423</a>).</li>
  <li>The Time32Scalar, Time64Scalar, Date32Scalar and Date64Scalar classes got a <code class="language-plaintext highlighter-rouge">.value</code>
attribute to access the underlying integer value, similar to the other date-time
related scalars (<a href="https://issues.apache.org/jira/browse/ARROW-18264">ARROW-18264</a>)</li>
  <li>Duration type is now supported in the hash kernels like <code class="language-plaintext highlighter-rouge">dictionary_encode</code>
(<a href="https://github.com/apache/arrow/issues/15226">GH-15226</a>).</li>
  <li>Fix silent overflow when converting <code class="language-plaintext highlighter-rouge">datetime.timedelta</code> to duration type
(<a href="https://issues.apache.org/jira/browse/ARROW-15026">ARROW-15026</a>).</li>
</ul>

<p>Relevant bug fixes:</p>

<ul>
  <li>Numpy conversion for ListArray is improved taking into account sliced offset, avoiding
increased memory usage (<a href="https://github.com/apache/arrow/issues/20512">GH-20512</a></li>
  <li>Fix writing files with multi-byte characters in file name
(<a href="https://issues.apache.org/jira/browse/ARROW-18123">ARROW-18123</a>).</li>
</ul>

<h2 id="r-notes">R notes</h2>
<ul>
  <li>map_batches() is lazy by default; it now returns a RecordBatchReader instead of a list of RecordBatch objects unless lazy = FALSE. <a href="https://github.com/apache/arrow/issues/14521">GH-14521</a></li>
  <li>A substantial reorganisation, rewrite of and addition to, many of the vignettes and README. <a href="https://github.com/apache/arrow/issues/14514">GH-14514</a></li>
</ul>

<p>For more on what’s in the 11.0.0 R package, see the <a href="/docs/r/news/">R changelog</a>.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<h3 id="ruby">Ruby</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Arrow::Table#save</code> now always returns self instead of the result of its <code class="language-plaintext highlighter-rouge">raw_records</code><a href="https://github.com/apache/arrow/issues/15289">GH-15289</a></li>
  <li>Improve the GC-related crash prevention system by guarding the shared objects from GC <a href="https://issues.apache.org/jira/browse/ARROW-18161">ARROW-18161</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">Arrow::HalfFloat</code> and <code class="language-plaintext highlighter-rouge">raw_records</code> support in <code class="language-plaintext highlighter-rouge">Arrow::HalfFloatArray</code> <a href="https://issues.apache.org/jira/browse/ARROW-18086">ARROW-18086</a></li>
  <li>Support omitting join keys in <code class="language-plaintext highlighter-rouge">Table#join</code> <a href="https://github.com/apache/arrow/issues/15084">GH-15084</a></li>
  <li>Add support for <code class="language-plaintext highlighter-rouge">Arrow::Table.load(uri, schema:)</code> <a href="https://issues.apache.org/jira/browse/ARROW-15206">ARROW-15206</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">Arrow::ColumnContainable#column_names</code> (e.g. <code class="language-plaintext highlighter-rouge">Arrow::Table#column_names</code>) <a href="https://github.com/apache/arrow/issues/15085">GH-15085</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">to_arrow_chunked_array</code> methods to support converting to <code class="language-plaintext highlighter-rouge">Arrow::ChunkedArray</code> <a href="https://issues.apache.org/jira/browse/ARROW-18405">ARROW-18405</a></li>
</ul>

<h3 id="c-glib">C GLib</h3>

<ul>
  <li>Add <code class="language-plaintext highlighter-rouge">garrow_chunked_array_new_empty()</code> <a href="https://github.com/apache/arrow/issues/33671">GH-33671</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">GArrowProjectNodeOptions</code> <a href="https://github.com/apache/arrow/issues/33670">GH-33670</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">GADatasetHivePartitioning</code> <a href="https://github.com/apache/arrow/issues/15257">GH-15257</a></li>
  <li>The signature of <code class="language-plaintext highlighter-rouge">garrow_execute_plain_wait()</code> was changed to take the <code class="language-plaintext highlighter-rouge">error</code> argument and to return the finished status <a href="https://github.com/apache/arrow/issues/15254">GH-15254</a></li>
  <li>Add support for half float <a href="https://github.com/apache/arrow/issues/15168">GH-15168</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">GADatasetFinishOptions</code> <a href="https://github.com/apache/arrow/issues/15146">GH-15146</a></li>
</ul>

<h2 id="rust-notes">Rust notes</h2>

<p>The Rust projects have moved to separate repositories outside the
main Arrow monorepo. For notes on the latest release of the Rust
implementation, see the latest <a href="https://github.com/apache/arrow-rs/tags">Arrow Rust changelog</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 11.0.0 release. This covers over 3 months of development work and includes 423 resolved issues from 95 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 10.0.0 release, Ben Baumgold, Will Jones, Eric Patrick Hanson, Curtis Vogt, Yang Jiang, Jarrett Revels, Raúl Cumplido, Jacob Wujciak, Jie Wen and Brent Gardner have been invited to be committers. Kun Liu have joined the Project Management Committee (PMC). As per our newly started tradition of rotating the PMC chair once a year Andrew Lamb was elected as the new PMC chair and VP. Thanks for your contributions and participation in the project! Columnar Format Notes Arrow Flight RPC notes In the C++/Python Flight clients, DoAction now properly streams the results, instead of blocking until the call finishes. Applications that did not consume the iterator before should fully consume the result. (#15069) C++ notes It is now possible to specify alignment when making allocations with a MemoryPool GH-33056 It is now possible to run an ExecPlan without using any CPU threads Added kernel for slicing list values GH-33168 Added kernel for slicing binary arrays GH-20357 When comparing list arrays for equality the list field name is now ignored GH-30519 Add support for partitioning on columns that contain special characters GH-33448 Added a streaming reader for JSON GH-33140 Added support for incremental writes to the ORC writer GH-33047 Added support for casting decimal to string and writing decimal to CSV GH-33002 Fixed an assert in the scanner that would occur when batch_readahead was set to 0 GH-15264 Fixed bug where arrays with a null data buffer would not be accepted when imported via the C data API GH-14875 Fixed bug where arrays with a zero-case union data type would not be accepted when imported via the C data API GH-14855 Fixed bug where case_when could return incorrect values GH-33382 Fixed bug where RecordBatch::Equals was ignoring field names GH-33285 C# notes No major changes to C#. Go notes Go’s benchmarks will now get added to Conbench alongside the benchmarks for other implementations GH-32983 Exposed FlightService_ServiceDesc and RegisterFlightServiceServer to allow easily incorporating a flight service into an existing gRPC server GH-15174 Arrow Function ApproxEquals was implemented for scalar values GH-29581 UnmarshalJSON for the RecordBuilder now properly handles extra unknown fields with complex/nested values GH-31840 Decimal128 and Decimal256 type support has been added to the CSV reader GH-33111 Fixed bug in array.UnionBuilder where Len method always returned 0 GH-14775 Fixed bug for handling slices of Map arrays when marshalling to JSON and for IPC GH-14780 Fixed memory leak when compressing IPC message body buffers GH-14883 Added the ability to easily append scalar values to array builders GH-15005 Compute Scalar binary (add/subtract/multiply/divide/etc.) and unary arithmetic (abs/neg/sqrt/sign/etc.) has been implemented for the compute package GH-33086 this includes easy functions like compute.Add and compute.Divide etc. Scalar boolean functions like AND/OR/XOR/etc. have been implemented for compute GH-33279 Scalar comparison function kernels have been implemented for compute (equal/greater/greater_equal/less/less_equal) GH-33308 Scalar compute functions are compatible with dictionary encoded arrays by casting them to their value types GH-33502 Parquet Panic when decoding a delta_bit_packed encoded column has been fixed GH-33483 Fixed memory leak from Allocator in pqarrow.WriteArrowToColumn GH-14865 Fixed writer.WriteBatch to properly handle writing encrypted parquet columns and no longer silently fail, but instead propagate an error GH-14940 Java notes Implement support for writing compressed files (#15223) Improve performance by short-circuiting null checks when comparing non null field types (#15106) Several enhancements to dictionary encoding (#14891, (#14902, (#14874) Extend Table to support additional vector types (#14573) Enhance and simplify handling of allocation management by integrating C Data into allocator hierarchy (#14506) Make ComplexCopier agnostic of specific implementation of MapWriter (#14557) Distribute Apple M1 compatible JNI libraries via mavencentral (#14472) Extend Table copy functionality, and support returning copies of individual vectors (#14389) JavaScript notes Bugfixes and dependency updates. Arrow now requires BigInt support. GH-33681 Python notes Compatibility notes: PyArrow now requires pandas &gt;= 1.0 (ARROW-18173) The pyarrow.parquet.ParquetDataset() class now by default uses the new Dataset API under the hood (use_legacy_dataset=False). You can still pass use_legacy_dataset=True to get the legacy implementation, but this option will be removed in a next release (ARROW-16728). New features: Added support for the DataFrame Interchange Protocol for pyarrow.Table (GH-33346). New kernels: list_slice() to slice each list element of a ListArray returning a new ListArray (ARROW-17960). A new filter() method on the Dataset class as additional API to filter a Dataset before consuming it (ARROW-16616). New sort() method for (Chunked)Array and sort_by() method for RecordBatch, providing a convenience on top of the sort_indices kernel (GH-14778), and a new Dataset.sort_by() method (GH-14975). Other improvements: Support for custom metadata of record batches in the IPC read and write APIs (ARROW-16430). Support URIs and the filesystem parameter in pyarrow.parquet.ParquetFile (ARROW-18272) and pyarrow.parquet.write_metadata (ARROW-18225). When writing a dataset to IPC using pyarrow.dataset.write_dataset(), you can now specify IPC specific options, such as compression (ARROW-17991) The pyarrow.array() function now allows to construct a MapArray from a sequence of dicts (in addition to a sequence of tuples) (ARROW-17832). The struct_field() kernel now also accepts field names in addition to integer indices (ARROW-17989). Casting to string is now supported for duration (ARROW-15822) and decimal (ARROW-17458) types, which also means those can now be written to CSV. When writing to CSV, you can now specify the quoting style (GH-14755). The pyarrow.ipc.read_schema() function now accepts a Message object (ARROW-18423). The Time32Scalar, Time64Scalar, Date32Scalar and Date64Scalar classes got a .value attribute to access the underlying integer value, similar to the other date-time related scalars (ARROW-18264) Duration type is now supported in the hash kernels like dictionary_encode (GH-15226). Fix silent overflow when converting datetime.timedelta to duration type (ARROW-15026). Relevant bug fixes: Numpy conversion for ListArray is improved taking into account sliced offset, avoiding increased memory usage (GH-20512 Fix writing files with multi-byte characters in file name (ARROW-18123). R notes map_batches() is lazy by default; it now returns a RecordBatchReader instead of a list of RecordBatch objects unless lazy = FALSE. GH-14521 A substantial reorganisation, rewrite of and addition to, many of the vignettes and README. GH-14514 For more on what’s in the 11.0.0 R package, see the R changelog. Ruby and C GLib notes Ruby Arrow::Table#save now always returns self instead of the result of its raw_recordsGH-15289 Improve the GC-related crash prevention system by guarding the shared objects from GC ARROW-18161 Add Arrow::HalfFloat and raw_records support in Arrow::HalfFloatArray ARROW-18086 Support omitting join keys in Table#join GH-15084 Add support for Arrow::Table.load(uri, schema:) ARROW-15206 Add Arrow::ColumnContainable#column_names (e.g. Arrow::Table#column_names) GH-15085 Add to_arrow_chunked_array methods to support converting to Arrow::ChunkedArray ARROW-18405 C GLib Add garrow_chunked_array_new_empty() GH-33671 Add GArrowProjectNodeOptions GH-33670 Add GADatasetHivePartitioning GH-15257 The signature of garrow_execute_plain_wait() was changed to take the error argument and to return the finished status GH-15254 Add support for half float GH-15168 Add GADatasetFinishOptions GH-15146 Rust notes The Rust projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Rust implementation, see the latest Arrow Rust changelog.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow DataFusion 16.0.0 Project Update</title><link href="https://arrow.apache.org/blog/2023/01/19/datafusion-16.0.0/" rel="alternate" type="text/html" title="Apache Arrow DataFusion 16.0.0 Project Update" /><published>2023-01-19T00:00:00-05:00</published><updated>2023-01-19T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/01/19/datafusion-16.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/01/19/datafusion-16.0.0/"><![CDATA[<!--

-->

<h1 id="introduction">Introduction</h1>

<p><a href="https://arrow.apache.org/datafusion/">DataFusion</a> is an extensible
query execution framework, written in <a href="https://www.rust-lang.org/">Rust</a>,
that uses <a href="https://arrow.apache.org">Apache Arrow</a> as its
in-memory format. It is targeted primarily at developers creating data
intensive analytics, and offers mature
<a href="https://arrow.apache.org/datafusion/user-guide/sql/index.html">SQL support</a>,
a DataFrame API, and many extension points.</p>

<p>Systems based on DataFusion perform very well in benchmarks,
especially considering they operate directly on parquet files rather
than first loading into a specialized format.  Some recent highlights
include <a href="https://benchmark.clickhouse.com/">clickbench</a> and the
<a href="https://www.cloudfuse.io/dashboards/standalone-engines">Cloudfuse.io standalone query
engines</a> page.</p>

<p>DataFusion is also part of a longer term trend, articulated clearly by
<a href="http://www.cs.cmu.edu/~pavlo/">Andy Pavlo</a> in his <a href="https://ottertune.com/blog/2022-databases-retrospective/">2022 Databases
Retrospective</a>.
Database frameworks are proliferating and it is likely that all OLAP
DBMSs and other data heavy applications, such as machine learning,
will <strong>require</strong> a vectorized, highly performant query engine in the next
5 years to remain relevant.  The only practical way to make such
technology so widely available without many millions of dollars of
investment is though open source engine such as DataFusion or
<a href="https://github.com/facebookincubator/velox">Velox</a>.</p>

<p>The rest of this post describes the improvements made to DataFusion
over the last three months and some hints of where we are heading.</p>

<h2 id="community-growth">Community Growth</h2>

<p>We again saw significant growth in the DataFusion community since <a href="https://arrow.apache.org/blog/2022/10/25/datafusion-13.0.0/">our last update</a>. There are some interesting metrics on <a href="https://ossrank.com/p/1573-apache-arrow-datafusion">OSSRank</a>.</p>

<p>The DataFusion 16.0.0 release consists of 543 PRs from 73 distinct contributors, not including all the work that goes into dependencies such as <a href="https://crates.io/crates/arrow">arrow</a>, <a href="https://crates.io/crates/parquet">parquet</a>, and <a href="https://crates.io/crates/object_store">object_store</a>, that much of the same community helps support. Thank you all for your help</p>

<!--
$ git log --pretty=oneline 13.0.0..16.0.0 . | wc -l
     543

$ git shortlog -sn 13.0.0..16.0.0 . | wc -l
      73
-->
<p>Several <a href="https://github.com/apache/arrow-datafusion#known-uses">new systems based on DataFusion</a> were recently added:</p>

<ul>
  <li><a href="https://github.com/GreptimeTeam/greptimedb">Greptime DB</a></li>
  <li><a href="https://synnada.ai/">Synnada</a></li>
  <li><a href="https://github.com/PRQL/prql-query">PRQL</a></li>
  <li><a href="https://github.com/parseablehq/parseable">Parseable</a></li>
  <li><a href="https://github.com/splitgraph/seafowl">SeaFowl</a></li>
</ul>

<h2 id="performance-">Performance 🚀</h2>

<p>Performance and efficiency are core values for
DataFusion. While there is still a gap between DataFusion and the best of
breed, tightly integrated systems such as <a href="https://duckdb.org">DuckDB</a>
and <a href="https://www.pola.rs/">Polars</a>, DataFusion is
closing the gap quickly. Performance highlights from the last three
months:</p>

<ul>
  <li>Up to 30% Faster Sorting and Merging using the new <a href="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1/">Row Format</a></li>
  <li><a href="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/">Advanced predicate pushdown</a>, directly on parquet, directly from object storage, enabling sub millisecond filtering. <!-- Andrew nots: we should really get this turned on by default --></li>
  <li><code class="language-plaintext highlighter-rouge">70%</code> faster <code class="language-plaintext highlighter-rouge">IN</code> expressions evaluation (<a href="https://github.com/apache/arrow-datafusion/issues/4057">#4057</a>)</li>
  <li>Sort and partition aware optimizations (<a href="https://github.com/apache/arrow-datafusion/issues/3969">#3969</a> and  <a href="https://github.com/apache/arrow-datafusion/issues/4691">#4691</a>)</li>
  <li>Filter selectivity analysis (<a href="https://github.com/apache/arrow-datafusion/issues/3868">#3868</a>)</li>
</ul>

<h2 id="runtime-resource-limits">Runtime Resource Limits</h2>

<p>Previously, DataFusion could potentially use unbounded amounts of memory for certain queries that included Sorts, Grouping or Joins.</p>

<p>In version 16.0.0, it is possible to limit DataFusion’s memory usage for Sorting and Grouping. We are looking for help adding similar limiting for Joins as well as expanding our algorithms to optionally spill to secondary storage. See <a href="https://github.com/apache/arrow-datafusion/issues/3941">#3941</a> for more detail.</p>

<h2 id="sql-window-functions">SQL Window Functions</h2>

<p><a href="https://en.wikipedia.org/wiki/Window_function_(SQL)">SQL Window Functions</a> are useful for a variety of analysis and DataFusion’s implementation support expanded significantly:</p>

<ul>
  <li>Custom window frames such as <code class="language-plaintext highlighter-rouge">... OVER (ORDER BY ... RANGE BETWEEN 0.2 PRECEDING AND 0.2 FOLLOWING)</code></li>
  <li>Unbounded window frames such as <code class="language-plaintext highlighter-rouge">... OVER (ORDER BY ... RANGE UNBOUNDED ROWS PRECEDING)</code></li>
  <li>Support for the <code class="language-plaintext highlighter-rouge">NTILE</code> window function (<a href="https://github.com/apache/arrow-datafusion/issues/4676">#4676</a>)</li>
  <li>Support for <code class="language-plaintext highlighter-rouge">GROUPS</code> mode (<a href="https://github.com/apache/arrow-datafusion/issues/4155">#4155</a>)</li>
</ul>

<h1 id="improved-joins">Improved Joins</h1>

<p>Joins are often the most complicated operations to handle well in
analytics systems and DataFusion 16.0.0 offers significant improvements
such as</p>

<ul>
  <li>Cost based optimizer (CBO) automatically reorders join evaluations, selects algorithms (Merge / Hash), and pick build side based on available statistics and join type (<code class="language-plaintext highlighter-rouge">INNER</code>, <code class="language-plaintext highlighter-rouge">LEFT</code>, etc) (<a href="https://github.com/apache/arrow-datafusion/issues/4219">#4219</a>)</li>
  <li>Fast non <code class="language-plaintext highlighter-rouge">column=column</code> equijoins such as <code class="language-plaintext highlighter-rouge">JOIN ON a.x + 5 = b.y</code></li>
  <li>Better performance on non-equijoins (<a href="https://github.com/apache/arrow-datafusion/issues/4562">#4562</a>) <!-- TODO is this a good thing to mention as any time this is usd the query is going to go slow or the data size is small --></li>
</ul>

<h1 id="streaming-execution">Streaming Execution</h1>

<p>One emerging use case for Datafusion is as a foundation for
streaming-first data platforms. An important prerequisite
is support for incremental execution for queries that can be computed
incrementally.</p>

<p>With this release, DataFusion now supports the following streaming features:</p>

<ul>
  <li>Data ingestion from infinite files such as FIFOs (<a href="https://github.com/apache/arrow-datafusion/issues/4694">#4694</a>),</li>
  <li>Detection of pipeline-breaking queries in streaming use cases (<a href="https://github.com/apache/arrow-datafusion/issues/4694">#4694</a>),</li>
  <li>Automatic input swapping for joins so probe side is a data stream (<a href="https://github.com/apache/arrow-datafusion/issues/4694">#4694</a>),</li>
  <li>Intelligent elision of pipeline-breaking sort operations whenever possible (<a href="https://github.com/apache/arrow-datafusion/issues/4691">#4691</a>),</li>
  <li>Incremental execution for more types of queries; e.g. queries involving finite window frames (<a href="https://github.com/apache/arrow-datafusion/issues/4777">#4777</a>).</li>
</ul>

<p>These are a major steps forward, and we plan even more improvements over the next few releases.</p>

<h1 id="better-support-for-distributed-catalogs">Better Support for Distributed Catalogs</h1>

<p>16.0.0 has been enhanced support for asynchronous catalogs (<a href="https://github.com/apache/arrow-datafusion/issues/4607">#4607</a>)
to better support distributed metadata stores such as
<a href="https://delta.io/">Delta.io</a> and <a href="https://iceberg.apache.org/">Apache
Iceberg</a> which require asynchronous I/O
during planning to access remote catalogs. Previously, DataFusion
required synchronous access to all relevant catalog information.</p>

<h1 id="additional-sql-support">Additional SQL Support</h1>
<p>SQL support continues to improve, including some of these highlights:</p>

<ul>
  <li>Add TPC-DS query planning regression tests <a href="https://github.com/apache/arrow-datafusion/issues/4719">#4719</a></li>
  <li>Support for <code class="language-plaintext highlighter-rouge">PREPARE</code> statement <a href="https://github.com/apache/arrow-datafusion/issues/4490">#4490</a></li>
  <li>Automatic coercions ast between Date and Timestamp <a href="https://github.com/apache/arrow-datafusion/issues/4726">#4726</a></li>
  <li>Support type coercion for timestamp and utf8 <a href="https://github.com/apache/arrow-datafusion/issues/4312">#4312</a></li>
  <li>Full support for time32 and time64 literal values (<code class="language-plaintext highlighter-rouge">ScalarValue</code>) <a href="https://github.com/apache/arrow-datafusion/issues/4156">#4156</a></li>
  <li>New functions, incuding <code class="language-plaintext highlighter-rouge">uuid()</code>  <a href="https://github.com/apache/arrow-datafusion/issues/4041">#4041</a>, <code class="language-plaintext highlighter-rouge">current_time</code>  <a href="https://github.com/apache/arrow-datafusion/issues/4054">#4054</a>, <code class="language-plaintext highlighter-rouge">current_date</code> <a href="https://github.com/apache/arrow-datafusion/issues/4022">#4022</a></li>
  <li>Compressed CSV/JSON support <a href="https://github.com/apache/arrow-datafusion/issues/3642">#3642</a></li>
</ul>

<p>The community has also invested in new <a href="https://github.com/apache/arrow-datafusion/blob/master/datafusion/core/tests/sqllogictests/README.md">sqllogic based</a> tests to keep improving DataFusion’s quality with less effort.</p>

<h1 id="plan-serialization-and-substrait">Plan Serialization and Substrait</h1>

<p>DataFusion now supports serialization of physical plans, with a custom protocol buffers format. In addition, we are adding initial support for <a href="https://substrait.io/">Substrait</a>, a Cross-Language Serialization for Relational Algebra</p>

<h1 id="how-to-get-involved">How to Get Involved</h1>

<p>Kudos to everyone in the community who contributed ideas, discussions, bug reports, documentation and code. It is exciting to be building something so cool together!</p>

<p>If you are interested in contributing to DataFusion, we would love to
have you join us. You can try out DataFusion on some of your own
data and projects and let us know how it goes or contribute a PR with
documentation, tests or code. A list of open issues suitable for
beginners is
<a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a>.</p>

<p>Check out our <a href="https://arrow.apache.org/datafusion/community/communication.html">Communication Doc</a> on more
ways to engage with the community.</p>

<h2 id="appendix-contributor-shoutout">Appendix: Contributor Shoutout</h2>

<p>Here is a list of people who have contributed PRs to this project over the last three releases, derived from <code class="language-plaintext highlighter-rouge">git shortlog -sn 13.0.0..16.0.0 .</code> Thank you all!</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   113	Andrew Lamb
    58	jakevin
    46	Raphael Taylor-Davies
    30	Andy Grove
    19	Batuhan Taskaya
    19	Remzi Yang
    17	ygf11
    16	Burak
    16	Jeffrey
    16	Marco Neumann
    14	Kun Liu
    12	Yang Jiang
    10	mingmwang
     9	Daniël Heres
     9	Mustafa akur
     9	comphead
     9	mvanschellebeeck
     9	xudong.w
     7	dependabot[bot]
     7	yahoNanJing
     6	Brent Gardner
     5	AssHero
     4	Jiayu Liu
     4	Wei-Ting Kuo
     4	askoa
     3	André Calado Coroado
     3	Jie Han
     3	Jon Mease
     3	Metehan Yıldırım
     3	Nga Tran
     3	Ruihang Xia
     3	baishen
     2	Berkay Şahin
     2	Dan Harris
     2	Dongyan Zhou
     2	Eduard Karacharov
     2	Kikkon
     2	Liang-Chi Hsieh
     2	Marko Milenković
     2	Martin Grigorov
     2	Roman Nozdrin
     2	Tim Van Wassenhove
     2	r.4ntix
     2	unconsolable
     2	unvalley
     1	Ajaya Agrawal
     1	Alexander Spies
     1	ArkashaJavelin
     1	Artjoms Iskovs
     1	BoredPerson
     1	Christian Salvati
     1	Creampanda
     1	Data Psycho
     1	Francis Du
     1	Francis Le Roy
     1	LFC
     1	Marko Grujic
     1	Matt Willian
     1	Matthijs Brobbel
     1	Max Burke
     1	Mehmet Ozan Kabak
     1	Rito Takeuchi
     1	Roman Zeyde
     1	Vrishabh
     1	Zhang Li
     1	ZuoTiJia
     1	byteink
     1	cfraz89
     1	nbr
     1	xxchan
     1	yujie.zhang
     1	zembunia
     1	哇呜哇呜呀咦耶
</code></pre></div></div>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[Introduction DataFusion is an extensible query execution framework, written in Rust, that uses Apache Arrow as its in-memory format. It is targeted primarily at developers creating data intensive analytics, and offers mature SQL support, a DataFrame API, and many extension points. Systems based on DataFusion perform very well in benchmarks, especially considering they operate directly on parquet files rather than first loading into a specialized format. Some recent highlights include clickbench and the Cloudfuse.io standalone query engines page. DataFusion is also part of a longer term trend, articulated clearly by Andy Pavlo in his 2022 Databases Retrospective. Database frameworks are proliferating and it is likely that all OLAP DBMSs and other data heavy applications, such as machine learning, will require a vectorized, highly performant query engine in the next 5 years to remain relevant. The only practical way to make such technology so widely available without many millions of dollars of investment is though open source engine such as DataFusion or Velox. The rest of this post describes the improvements made to DataFusion over the last three months and some hints of where we are heading. Community Growth We again saw significant growth in the DataFusion community since our last update. There are some interesting metrics on OSSRank. The DataFusion 16.0.0 release consists of 543 PRs from 73 distinct contributors, not including all the work that goes into dependencies such as arrow, parquet, and object_store, that much of the same community helps support. Thank you all for your help Several new systems based on DataFusion were recently added: Greptime DB Synnada PRQL Parseable SeaFowl Performance 🚀 Performance and efficiency are core values for DataFusion. While there is still a gap between DataFusion and the best of breed, tightly integrated systems such as DuckDB and Polars, DataFusion is closing the gap quickly. Performance highlights from the last three months: Up to 30% Faster Sorting and Merging using the new Row Format Advanced predicate pushdown, directly on parquet, directly from object storage, enabling sub millisecond filtering. 70% faster IN expressions evaluation (#4057) Sort and partition aware optimizations (#3969 and #4691) Filter selectivity analysis (#3868) Runtime Resource Limits Previously, DataFusion could potentially use unbounded amounts of memory for certain queries that included Sorts, Grouping or Joins. In version 16.0.0, it is possible to limit DataFusion’s memory usage for Sorting and Grouping. We are looking for help adding similar limiting for Joins as well as expanding our algorithms to optionally spill to secondary storage. See #3941 for more detail. SQL Window Functions SQL Window Functions are useful for a variety of analysis and DataFusion’s implementation support expanded significantly: Custom window frames such as ... OVER (ORDER BY ... RANGE BETWEEN 0.2 PRECEDING AND 0.2 FOLLOWING) Unbounded window frames such as ... OVER (ORDER BY ... RANGE UNBOUNDED ROWS PRECEDING) Support for the NTILE window function (#4676) Support for GROUPS mode (#4155) Improved Joins Joins are often the most complicated operations to handle well in analytics systems and DataFusion 16.0.0 offers significant improvements such as Cost based optimizer (CBO) automatically reorders join evaluations, selects algorithms (Merge / Hash), and pick build side based on available statistics and join type (INNER, LEFT, etc) (#4219) Fast non column=column equijoins such as JOIN ON a.x + 5 = b.y Better performance on non-equijoins (#4562) Streaming Execution One emerging use case for Datafusion is as a foundation for streaming-first data platforms. An important prerequisite is support for incremental execution for queries that can be computed incrementally. With this release, DataFusion now supports the following streaming features: Data ingestion from infinite files such as FIFOs (#4694), Detection of pipeline-breaking queries in streaming use cases (#4694), Automatic input swapping for joins so probe side is a data stream (#4694), Intelligent elision of pipeline-breaking sort operations whenever possible (#4691), Incremental execution for more types of queries; e.g. queries involving finite window frames (#4777). These are a major steps forward, and we plan even more improvements over the next few releases. Better Support for Distributed Catalogs 16.0.0 has been enhanced support for asynchronous catalogs (#4607) to better support distributed metadata stores such as Delta.io and Apache Iceberg which require asynchronous I/O during planning to access remote catalogs. Previously, DataFusion required synchronous access to all relevant catalog information. Additional SQL Support SQL support continues to improve, including some of these highlights: Add TPC-DS query planning regression tests #4719 Support for PREPARE statement #4490 Automatic coercions ast between Date and Timestamp #4726 Support type coercion for timestamp and utf8 #4312 Full support for time32 and time64 literal values (ScalarValue) #4156 New functions, incuding uuid() #4041, current_time #4054, current_date #4022 Compressed CSV/JSON support #3642 The community has also invested in new sqllogic based tests to keep improving DataFusion’s quality with less effort. Plan Serialization and Substrait DataFusion now supports serialization of physical plans, with a custom protocol buffers format. In addition, we are adding initial support for Substrait, a Cross-Language Serialization for Relational Algebra How to Get Involved Kudos to everyone in the community who contributed ideas, discussions, bug reports, documentation and code. It is exciting to be building something so cool together! If you are interested in contributing to DataFusion, we would love to have you join us. You can try out DataFusion on some of your own data and projects and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable for beginners is here. Check out our Communication Doc on more ways to engage with the community. Appendix: Contributor Shoutout Here is a list of people who have contributed PRs to this project over the last three releases, derived from git shortlog -sn 13.0.0..16.0.0 . Thank you all! 113 Andrew Lamb 58 jakevin 46 Raphael Taylor-Davies 30 Andy Grove 19 Batuhan Taskaya 19 Remzi Yang 17 ygf11 16 Burak 16 Jeffrey 16 Marco Neumann 14 Kun Liu 12 Yang Jiang 10 mingmwang 9 Daniël Heres 9 Mustafa akur 9 comphead 9 mvanschellebeeck 9 xudong.w 7 dependabot[bot] 7 yahoNanJing 6 Brent Gardner 5 AssHero 4 Jiayu Liu 4 Wei-Ting Kuo 4 askoa 3 André Calado Coroado 3 Jie Han 3 Jon Mease 3 Metehan Yıldırım 3 Nga Tran 3 Ruihang Xia 3 baishen 2 Berkay Şahin 2 Dan Harris 2 Dongyan Zhou 2 Eduard Karacharov 2 Kikkon 2 Liang-Chi Hsieh 2 Marko Milenković 2 Martin Grigorov 2 Roman Nozdrin 2 Tim Van Wassenhove 2 r.4ntix 2 unconsolable 2 unvalley 1 Ajaya Agrawal 1 Alexander Spies 1 ArkashaJavelin 1 Artjoms Iskovs 1 BoredPerson 1 Christian Salvati 1 Creampanda 1 Data Psycho 1 Francis Du 1 Francis Le Roy 1 LFC 1 Marko Grujic 1 Matt Willian 1 Matthijs Brobbel 1 Max Burke 1 Mehmet Ozan Kabak 1 Rito Takeuchi 1 Roman Zeyde 1 Vrishabh 1 Zhang Li 1 ZuoTiJia 1 byteink 1 cfraz89 1 nbr 1 xxchan 1 yujie.zhang 1 zembunia 1 哇呜哇呜呀咦耶]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 0.1.0 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/01/12/adbc-0.1.0-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.1.0 (Libraries) Release" /><published>2023-01-12T00:00:00-05:00</published><updated>2023-01-12T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/01/12/adbc-0.1.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/01/12/adbc-0.1.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.1.0 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/1"><strong>63
resolved issues</strong></a> from <a href="#contributors"><strong>7 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version
0.1.0.  The <strong>API specification</strong> is versioned separately and is
at version 1.0.0.  For more, see the <a href="/blog/2023/01/05/introducing-arrow-adbc/">introduction to ADBC</a>.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.1.0/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>This initial release includes the following:</p>

<ul>
  <li>Driver manager libraries for C/C++, Go, Java, Python, and Ruby.</li>
  <li>ADBC drivers for SQLite and PostgreSQL, available in C/C++, Go, Python, and Ruby.</li>
  <li>ADBC drivers for Arrow FLight SQL and JDBC, available in Java.</li>
</ul>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn fe96ea9186194af06f4a229b6e5a57815f05f6bd..apache-arrow-adbc-0.1.0
   169	David Li
    12	Sutou Kouhei
     5	Matt Topol
     2	Dewey Dunnington
     1	Ash
     1	Judah Rand
     1	Raúl Cumplido
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>Future development will expand the capabilities of the existing
drivers, as well as adding drivers for more targets.  Drivers for
Flight SQL in C/C++ and Go are being developed.  See the <a href="https://github.com/apache/arrow-adbc/milestone/2">0.2.0
milestone</a> for details.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.1.0 release of the Apache Arrow ADBC libraries. This covers includes 63 resolved issues from 7 distinct contributors. This is a release of the libraries, which are at version 0.1.0. The API specification is versioned separately and is at version 1.0.0. For more, see the introduction to ADBC. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights This initial release includes the following: Driver manager libraries for C/C++, Go, Java, Python, and Ruby. ADBC drivers for SQLite and PostgreSQL, available in C/C++, Go, Python, and Ruby. ADBC drivers for Arrow FLight SQL and JDBC, available in Java. Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn fe96ea9186194af06f4a229b6e5a57815f05f6bd..apache-arrow-adbc-0.1.0 169 David Li 12 Sutou Kouhei 5 Matt Topol 2 Dewey Dunnington 1 Ash 1 Judah Rand 1 Raúl Cumplido Roadmap Future development will expand the capabilities of the existing drivers, as well as adding drivers for more targets. Drivers for Flight SQL in C/C++ and Go are being developed. See the 0.2.0 milestone for details. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Introducing ADBC: Database Access for Apache Arrow</title><link href="https://arrow.apache.org/blog/2023/01/05/introducing-arrow-adbc/" rel="alternate" type="text/html" title="Introducing ADBC: Database Access for Apache Arrow" /><published>2023-01-05T00:00:00-05:00</published><updated>2023-01-05T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/01/05/introducing-arrow-adbc</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/01/05/introducing-arrow-adbc/"><![CDATA[<!--

-->

<p>The Arrow community would like to introduce version 1.0.0 of the <a href="https://github.com/apache/arrow-adbc">Arrow Database Connectivity (ADBC)</a> specification.
ADBC is a columnar, minimal-overhead alternative to JDBC/ODBC for analytical applications.
Or in other words: <strong>ADBC is a single API for getting Arrow data in and out of different databases</strong>.</p>

<h2 id="motivation">Motivation</h2>

<p>Applications often use API standards like <a href="https://docs.oracle.com/javase/tutorial/jdbc/overview/index.html">JDBC</a> and <a href="https://learn.microsoft.com/en-us/sql/odbc/reference/what-is-odbc?view=sql-server-ver16">ODBC</a> to work with databases.
That way, they can code to the same API regardless of the underlying database, saving on development time.
Roughly speaking, when an application executes a query with these APIs:</p>

<figure style="text-align: center;">
  <img src="/img/ADBCFlow1.svg" width="90%" class="img-responsive" alt="A diagram showing the query execution flow." />
  <figcaption>The query execution flow.</figcaption>
</figure>

<ol>
  <li>The application submits a SQL query via the JDBC/ODBC API.</li>
  <li>The query is passed on to the driver.</li>
  <li>The driver translates the query to a database-specific protocol and sends it to the database.</li>
  <li>The database executes the query and returns the result set in a database-specific format.</li>
  <li>The driver translates the result into the format required by the JDBC/ODBC API.</li>
  <li>The application iterates over the result rows using the JDBC/ODBC API.</li>
</ol>

<p>When columnar data comes into play, however, problems arise.
JDBC is a row-oriented API, and while ODBC can support columnar data, the type system and data representation is not a perfect match with Arrow.
So generally, columnar data must be converted to rows in step 5, spending resources without performing “useful” work.</p>

<p>This mismatch is problematic for columnar database systems, such as ClickHouse, Dremio, DuckDB, and Google BigQuery.
On the client side, tools such as Apache Spark and pandas would be better off getting columnar data directly, skipping that conversion.
Otherwise, they’re leaving performance on the table.
At the same time, that conversion isn’t always avoidable.
Row-oriented database systems like PostgreSQL aren’t going away, and these clients will still want to consume data from them.</p>

<p>Developers have a few options:</p>

<ul>
  <li><em>Just use JDBC/ODBC</em>.
These standards are here to stay, and it makes sense for databases to support them for applications that want them.
But when both the database and the application are columnar, that means converting data into rows for JDBC/ODBC, only for the client to convert them right back into columns!
Performance suffers, and developers have to spend time implementing the conversions.</li>
  <li><em>Use JDBC/ODBC-to-Arrow conversion libraries</em>.
Libraries like <a href="https://turbodbc.readthedocs.io/en/latest/">Turbodbc</a> and <a href="https://arrow.apache.org/docs/java/jdbc.html">arrow-jdbc</a> handle row-to-columnar conversions for clients.
But this doesn’t fundamentally solve the problem.
Unnecessary data conversions are still required.</li>
  <li><em>Use vendor-specific protocols</em>.
For some databases, applications can use a database-specific protocol or SDK to directly get Arrow data.
For example, applications could use Dremio via <a href="/blog/2022/02/16/introducing-arrow-flight-sql/">Arrow Flight SQL</a>.
But client applications that want to support multiple database vendors would need to integrate with each of them.
(Look at all the <a href="https://trino.io/docs/current/connector.html">connectors</a> that Trino implements.)
And databases like PostgreSQL don’t offer an option supporting Arrow in the first place.</li>
</ul>

<p>As is, clients must choose between either tedious integration work or leaving performance on the table. We can make this better.</p>

<h2 id="introducing-adbc">Introducing ADBC</h2>

<p>ADBC is an Arrow-based, vendor-neutral API for interacting with databases.
Applications that use ADBC simply receive Arrow data.
They don’t have to do any conversions themselves, and they don’t have to integrate each database’s specific SDK.</p>

<p>Just like JDBC/ODBC, underneath the ADBC API are drivers that translate the API for specific databases.</p>

<ul>
  <li>A driver for an Arrow-native database just passes Arrow data through without conversion.</li>
  <li>A driver for a non-Arrow-native database must convert the data to Arrow.
This saves the application from doing that, and the driver can optimize the conversion for its database.</li>
</ul>

<figure style="text-align: center;">
  <img src="/img/ADBCFlow2.svg" alt="A diagram showing the query execution flow with ADBC." width="90%" class="img-responsive" />
  <figcaption>The query execution flow with two different ADBC drivers.</figcaption>
</figure>

<ol>
  <li>The application submits a SQL query via the ADBC API.</li>
  <li>The query is passed on to the ADBC driver.</li>
  <li>The driver translates the query to a database-specific protocol and sends the query to the database.</li>
  <li>The database executes the query and returns the result set in a database-specific format, which is ideally Arrow data.</li>
  <li>If needed: the driver translates the result into Arrow data.</li>
  <li>The application iterates over batches of Arrow data.</li>
</ol>

<p>The application only deals with one API, and only works with Arrow data.</p>

<p>ADBC API and driver implementations are under development. For example, in Python, the ADBC packages offer a familiar <a href="https://www.python.org/dev/peps/pep-0249/">DBAPI 2.0 (PEP 249)</a>-style interface, with extensions to get Arrow data.
We can get Arrow data out of PostgreSQL easily:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">adbc_driver_postgresql.dbapi</span>

<span class="n">uri</span> <span class="o">=</span> <span class="s">"postgresql://localhost:5432/postgres?user=postgres&amp;password=password"</span>
<span class="k">with</span> <span class="n">adbc_driver_postgresql</span><span class="p">.</span><span class="n">dbapi</span><span class="p">.</span><span class="n">connect</span><span class="p">(</span><span class="n">uri</span><span class="p">)</span> <span class="k">as</span> <span class="n">conn</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">conn</span><span class="p">.</span><span class="n">cursor</span><span class="p">()</span> <span class="k">as</span> <span class="n">cur</span><span class="p">:</span>
        <span class="n">cur</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="s">"SELECT * FROM customer"</span><span class="p">)</span>
        <span class="n">table</span> <span class="o">=</span> <span class="n">cur</span><span class="p">.</span><span class="n">fetch_arrow_table</span><span class="p">()</span>
        <span class="c1"># Process the results
</span></code></pre></div></div>

<p>Or SQLite:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">adbc_driver_sqlite.dbapi</span>

<span class="n">uri</span> <span class="o">=</span> <span class="s">"file:mydb.sqlite"</span>
<span class="k">with</span> <span class="n">adbc_driver_sqlite</span><span class="p">.</span><span class="n">dbapi</span><span class="p">.</span><span class="n">connect</span><span class="p">(</span><span class="n">uri</span><span class="p">)</span> <span class="k">as</span> <span class="n">conn</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">conn</span><span class="p">.</span><span class="n">cursor</span><span class="p">()</span> <span class="k">as</span> <span class="n">cur</span><span class="p">:</span>
        <span class="n">cur</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="s">"SELECT * FROM customer"</span><span class="p">)</span>
        <span class="n">table</span> <span class="o">=</span> <span class="n">cur</span><span class="p">.</span><span class="n">fetch_arrow_table</span><span class="p">()</span>
        <span class="c1"># Process the results
</span></code></pre></div></div>

<p><em>Note: implementations are still under development. See the <a href="https://arrow.apache.org/adbc/">documentation</a> for up-to-date examples.</em></p>

<h2 id="what-about-flight-sql-jdbc-odbc-">What about {Flight SQL, JDBC, ODBC, …}?</h2>

<p>ADBC fills a specific niche that related projects do not address. It is both:</p>

<ul>
  <li><strong>Arrow-native</strong>: ADBC can pass through Arrow data with no overhead thanks to the <a href="/blog/2020/05/03/introducing-arrow-c-data-interface/">C Data Interface</a>.
JDBC is row-oriented, and ODBC has implementation caveats, as discussed, that make it hard to use with Arrow.</li>
  <li><strong>Vendor-agnostic</strong>: ADBC drivers can implement the API using any underlying protocol, while Flight SQL requires server-side support that may not be easy to add.</li>
</ul>

<table class="table table-hover" style="table-layout: fixed">
  <caption>Comparing database APIs and protocols</caption>
  <thead class="thead-dark">
    <tr>
      <th></th>
      <th class="align-top" style="width: 40%" scope="col">Vendor-neutral (database APIs)</th>
      <th class="align-top" style="width: 40%" scope="col">Vendor-specific (database protocols)</th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <th scope="row">Arrow-native</th>
      <td class="table-success"><strong>ADBC</strong></td>
      <td>Arrow Flight SQL<br />BigQuery Storage gRPC protocol</td>
    </tr>
    <tr>
      <th scope="row">Row-oriented</th>
      <td>JDBC<br />ODBC (typically row-oriented)</td>
      <td>PostgreSQL wire protocol<br />Tabular Data Stream (Microsoft SQL Server)</td>
    </tr>
  </tbody>
</table>

<p><strong>ADBC doesn’t intend to replace JDBC or ODBC in general</strong>.
But for applications that just want bulk columnar data access, ADBC lets them avoid data conversion overhead and tedious integration work.</p>

<p>Similarly, within the Arrow project, ADBC does not replace Flight SQL, but instead <em>complements</em> it.
ADBC is an <strong>API</strong> that lets <em>clients</em> work with different databases easily.
Meanwhile, Flight SQL is a <strong>wire protocol</strong> that <em>database servers</em> can implement to simultaneously support ADBC, <a href="/blog/2022/11/01/arrow-flight-sql-jdbc/">JDBC</a>, and ODBC users.</p>

<figure style="text-align: center;">
  <img src="/img/ADBC.svg" alt="ADBC abstracts over protocols and APIs like Flight SQL and JDBC for client applications. Flight SQL provides implementations of APIs like ADBC and JDBC for database servers." width="90%" class="img-responsive" />
</figure>

<h2 id="getting-involved">Getting Involved</h2>

<p>ADBC works as part of the Arrow ecosystem to “cover the bases” for database interaction:</p>

<ul>
  <li>Arrow offers a universal columnar data format,</li>
  <li>Arrow Flight SQL offers a universal wire protocol for database servers,</li>
  <li>and ADBC offers a universal API for database clients.</li>
</ul>

<p>To start using ADBC, see the <a href="https://arrow.apache.org/adbc/">documentation</a> for build instructions and a short tutorial.
(A formal release of the packages is still under way.)
If you’re interested in learning more or contributing, please reach out on the <a href="https://arrow.apache.org/community/">mailing list</a> or on <a href="https://github.com/apache/arrow-adbc/issues">GitHub Issues</a>.</p>

<p>ADBC was only possible with the help and involvement of several Arrow community members and projects.
In particular, we would like to thank members of the <a href="https://duckdb.org/">DuckDB project</a> and the <a href="https://www.r-dbi.org/">R DBI project</a>, who constructed prototypes based on early revisions of the standard and provided feedback on the design.
And ADBC builds on existing Arrow projects, including the <a href="/blog/2020/05/03/introducing-arrow-c-data-interface/">Arrow C Data Interface</a> and <a href="https://github.com/apache/arrow-nanoarrow">nanoarrow</a>.</p>

<p>Thanks to Fernanda Foertter for assistance with some of the diagrams.</p>]]></content><author><name>pmc</name></author><category term="application" /><summary type="html"><![CDATA[The Arrow community would like to introduce version 1.0.0 of the Arrow Database Connectivity (ADBC) specification. ADBC is a columnar, minimal-overhead alternative to JDBC/ODBC for analytical applications. Or in other words: ADBC is a single API for getting Arrow data in and out of different databases. Motivation Applications often use API standards like JDBC and ODBC to work with databases. That way, they can code to the same API regardless of the underlying database, saving on development time. Roughly speaking, when an application executes a query with these APIs: The query execution flow. The application submits a SQL query via the JDBC/ODBC API. The query is passed on to the driver. The driver translates the query to a database-specific protocol and sends it to the database. The database executes the query and returns the result set in a database-specific format. The driver translates the result into the format required by the JDBC/ODBC API. The application iterates over the result rows using the JDBC/ODBC API. When columnar data comes into play, however, problems arise. JDBC is a row-oriented API, and while ODBC can support columnar data, the type system and data representation is not a perfect match with Arrow. So generally, columnar data must be converted to rows in step 5, spending resources without performing “useful” work. This mismatch is problematic for columnar database systems, such as ClickHouse, Dremio, DuckDB, and Google BigQuery. On the client side, tools such as Apache Spark and pandas would be better off getting columnar data directly, skipping that conversion. Otherwise, they’re leaving performance on the table. At the same time, that conversion isn’t always avoidable. Row-oriented database systems like PostgreSQL aren’t going away, and these clients will still want to consume data from them. Developers have a few options: Just use JDBC/ODBC. These standards are here to stay, and it makes sense for databases to support them for applications that want them. But when both the database and the application are columnar, that means converting data into rows for JDBC/ODBC, only for the client to convert them right back into columns! Performance suffers, and developers have to spend time implementing the conversions. Use JDBC/ODBC-to-Arrow conversion libraries. Libraries like Turbodbc and arrow-jdbc handle row-to-columnar conversions for clients. But this doesn’t fundamentally solve the problem. Unnecessary data conversions are still required. Use vendor-specific protocols. For some databases, applications can use a database-specific protocol or SDK to directly get Arrow data. For example, applications could use Dremio via Arrow Flight SQL. But client applications that want to support multiple database vendors would need to integrate with each of them. (Look at all the connectors that Trino implements.) And databases like PostgreSQL don’t offer an option supporting Arrow in the first place. As is, clients must choose between either tedious integration work or leaving performance on the table. We can make this better. Introducing ADBC ADBC is an Arrow-based, vendor-neutral API for interacting with databases. Applications that use ADBC simply receive Arrow data. They don’t have to do any conversions themselves, and they don’t have to integrate each database’s specific SDK. Just like JDBC/ODBC, underneath the ADBC API are drivers that translate the API for specific databases. A driver for an Arrow-native database just passes Arrow data through without conversion. A driver for a non-Arrow-native database must convert the data to Arrow. This saves the application from doing that, and the driver can optimize the conversion for its database. The query execution flow with two different ADBC drivers. The application submits a SQL query via the ADBC API. The query is passed on to the ADBC driver. The driver translates the query to a database-specific protocol and sends the query to the database. The database executes the query and returns the result set in a database-specific format, which is ideally Arrow data. If needed: the driver translates the result into Arrow data. The application iterates over batches of Arrow data. The application only deals with one API, and only works with Arrow data. ADBC API and driver implementations are under development. For example, in Python, the ADBC packages offer a familiar DBAPI 2.0 (PEP 249)-style interface, with extensions to get Arrow data. We can get Arrow data out of PostgreSQL easily: import adbc_driver_postgresql.dbapi uri = "postgresql://localhost:5432/postgres?user=postgres&amp;password=password" with adbc_driver_postgresql.dbapi.connect(uri) as conn: with conn.cursor() as cur: cur.execute("SELECT * FROM customer") table = cur.fetch_arrow_table() # Process the results Or SQLite: import adbc_driver_sqlite.dbapi uri = "file:mydb.sqlite" with adbc_driver_sqlite.dbapi.connect(uri) as conn: with conn.cursor() as cur: cur.execute("SELECT * FROM customer") table = cur.fetch_arrow_table() # Process the results Note: implementations are still under development. See the documentation for up-to-date examples. What about {Flight SQL, JDBC, ODBC, …}? ADBC fills a specific niche that related projects do not address. It is both: Arrow-native: ADBC can pass through Arrow data with no overhead thanks to the C Data Interface. JDBC is row-oriented, and ODBC has implementation caveats, as discussed, that make it hard to use with Arrow. Vendor-agnostic: ADBC drivers can implement the API using any underlying protocol, while Flight SQL requires server-side support that may not be easy to add. Comparing database APIs and protocols Vendor-neutral (database APIs) Vendor-specific (database protocols) Arrow-native ADBC Arrow Flight SQLBigQuery Storage gRPC protocol Row-oriented JDBCODBC (typically row-oriented) PostgreSQL wire protocolTabular Data Stream (Microsoft SQL Server) ADBC doesn’t intend to replace JDBC or ODBC in general. But for applications that just want bulk columnar data access, ADBC lets them avoid data conversion overhead and tedious integration work. Similarly, within the Arrow project, ADBC does not replace Flight SQL, but instead complements it. ADBC is an API that lets clients work with different databases easily. Meanwhile, Flight SQL is a wire protocol that database servers can implement to simultaneously support ADBC, JDBC, and ODBC users. Getting Involved ADBC works as part of the Arrow ecosystem to “cover the bases” for database interaction: Arrow offers a universal columnar data format, Arrow Flight SQL offers a universal wire protocol for database servers, and ADBC offers a universal API for database clients. To start using ADBC, see the documentation for build instructions and a short tutorial. (A formal release of the packages is still under way.) If you’re interested in learning more or contributing, please reach out on the mailing list or on GitHub Issues. ADBC was only possible with the help and involvement of several Arrow community members and projects. In particular, we would like to thank members of the DuckDB project and the R DBI project, who constructed prototypes based on early revisions of the standard and provided feedback on the design. And ADBC builds on existing Arrow projects, including the Arrow C Data Interface and nanoarrow. Thanks to Fernanda Foertter for assistance with some of the diagrams.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Querying Parquet with Millisecond Latency</title><link href="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/" rel="alternate" type="text/html" title="Querying Parquet with Millisecond Latency" /><published>2022-12-26T00:00:00-05:00</published><updated>2022-12-26T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/"><![CDATA[<!--

-->

<h1 id="querying-parquet-with-millisecond-latency">Querying Parquet with Millisecond Latency</h1>
<p><em>Note: this article was originally published on the <a href="https://www.influxdata.com/blog/querying-parquet-millisecond-latency">InfluxData Blog</a>.</em></p>

<p>We believe that querying data in <a href="https://parquet.apache.org/">Apache Parquet</a> files directly can achieve similar or better storage efficiency and query performance than most specialized file formats. While it requires significant engineering effort, the benefits of Parquet’s open format and broad ecosystem support make it the obvious choice for a wide class of data systems.</p>

<p>In this article we explain several advanced techniques needed to query data stored in the Parquet format quickly that we implemented in the <a href="https://docs.rs/parquet/27.0.0/parquet/">Apache Arrow Rust Parquet reader</a>. Together these techniques make the Rust implementation one of, if not the fastest implementation for querying Parquet files — be it on local disk or remote object storage. It is able to query GBs of Parquet in a <a href="https://github.com/tustvold/access-log-bench">matter of milliseconds</a>.</p>

<p>We would like to acknowledge and thank <a href="https://www.influxdata.com/">InfluxData</a> for their support of this work. InfluxData has a deep and continuing commitment to Open source software, and it sponsored much of our time for writing this blog post as well as many contributions as part of building the <a href="https://www.influxdata.com/blog/influxdb-engine/">InfluxDB IOx Storage Engine</a>.</p>

<h1 id="background">Background</h1>

<p><a href="https://parquet.apache.org/">Apache Parquet</a> is an increasingly popular open format for storing <a href="https://www.influxdata.com/glossary/olap/">analytic datasets</a>, and has become the de-facto standard for cost-effective, DBMS-agnostic data storage. Initially created for the Hadoop ecosystem, Parquet’s reach now expands broadly across the data analytics ecosystem due to its compelling combination of:</p>

<ul>
  <li>High compression ratios</li>
  <li>Amenability to commodity blob-storage such as S3</li>
  <li>Broad ecosystem and tooling support</li>
  <li>Portability across many different platforms and tools</li>
  <li>Support for <a href="https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/">arbitrarily structured data</a></li>
</ul>

<p>Increasingly other systems, such as <a href="https://duckdb.org/2021/06/25/querying-parquet.html">DuckDB</a> and <a href="https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html#c-spectrum-overview">Redshift</a> allow querying data stored in Parquet directly, but support is still often a secondary consideration compared to their native (custom) file formats. Such formats include the DuckDB <code class="language-plaintext highlighter-rouge">.duckdb</code> file format, the Apache IOT <a href="https://github.com/apache/iotdb/blob/master/tsfile/README.md">TsFile</a>, the <a href="https://www.vldb.org/pvldb/vol8/p1816-teller.pdf">Gorilla format</a>, and others.</p>

<p>For the first time, access to the same sophisticated query techniques, previously only available in closed source commercial implementations, are now available as open source. The required engineering capacity comes from large, well-run open source projects with global contributor communities, such as <a href="https://arrow.apache.org/">Apache Arrow</a> and <a href="https://impala.apache.org/">Apache Impala</a>.</p>

<h1 id="parquet-file-format">Parquet file format</h1>

<p>Before diving into the details of efficiently reading from <a href="https://www.influxdata.com/glossary/apache-parquet/">Parquet</a>, it is important to understand the file layout. The file format is carefully designed to quickly locate the desired information, skip irrelevant portions, and decode what remains efficiently.</p>

<ul>
  <li>The data in a Parquet file is broken into horizontal slices called <code class="language-plaintext highlighter-rouge">RowGroup</code>s</li>
  <li>Each <code class="language-plaintext highlighter-rouge">RowGroup</code> contains a single <code class="language-plaintext highlighter-rouge">ColumnChunk</code> for each column in the schema</li>
</ul>

<p>For example, the following diagram illustrates a Parquet file with three columns “A”, “B” and “C” stored in two <code class="language-plaintext highlighter-rouge">RowGroup</code>s for a total of 6 <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┓          ┃
┃┃┌ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ┐┌ ─ ─ ─ ─ ─ ─  ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃│             │             ││              ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃│             │             ││              ┃ RowGroup ┃
┃┃             │                            │ ┃     1    ┃
┃┃│             │             ││              ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃└ ─ ─ ─ ─ ─ ─ └ ─ ─ ─ ─ ─ ─ ┘└ ─ ─ ─ ─ ─ ─  ┃          ┃
┃┃ColumnChunk 1  ColumnChunk 2 ColumnChunk 3  ┃          ┃
┃┃ (Column "A")   (Column "B")  (Column "C")  ┃          ┃
┃┗━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┛          ┃
┃┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┓          ┃
┃┃┌ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ┐┌ ─ ─ ─ ─ ─ ─  ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃│             │             ││              ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃│             │             ││              ┃ RowGroup ┃
┃┃             │                            │ ┃     2    ┃
┃┃│             │             ││              ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃└ ─ ─ ─ ─ ─ ─ └ ─ ─ ─ ─ ─ ─ ┘└ ─ ─ ─ ─ ─ ─  ┃          ┃
┃┃ColumnChunk 4  ColumnChunk 5 ColumnChunk 6  ┃          ┃
┃┃ (Column "A")   (Column "B")  (Column "C")  ┃          ┃
┃┗━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┛          ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
</code></pre></div></div>

<p>The logical values for a <code class="language-plaintext highlighter-rouge">ColumnChunk</code> are written using one of the many <a href="https://parquet.apache.org/docs/file-format/data-pages/encodings/">available encodings</a> into one or more Data Pages appended sequentially in the file. At the end of a Parquet file is a footer, which contains important metadata, such as:</p>

<ul>
  <li>The file’s schema information such as column names and types</li>
  <li>The locations of the <code class="language-plaintext highlighter-rouge">RowGroup</code> and <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s in the file</li>
</ul>

<p>The footer may also contain other specialized data structures:</p>

<ul>
  <li>Optional statistics for each <code class="language-plaintext highlighter-rouge">ColumnChunk</code> including min/max values and null counts</li>
  <li>Optional pointers to <a href="https://github.com/apache/parquet-format/blob/54e53e5d7794d383529dd30746378f19a12afd58/src/main/thrift/parquet.thrift#L926-L932">OffsetIndexes</a> containing the location of each individual Page</li>
  <li>Optional pointers to <a href="https://github.com/apache/parquet-format/blob/54e53e5d7794d383529dd30746378f19a12afd58/src/main/thrift/parquet.thrift#L938">ColumnIndex</a> containing row counts and summary statistics for each Page</li>
  <li>Optional pointers to <a href="https://github.com/apache/parquet-format/blob/54e53e5d7794d383529dd30746378f19a12afd58/src/main/thrift/parquet.thrift#L621-L630">BloomFilterData</a>, which can quickly check if a value is present in a <code class="language-plaintext highlighter-rouge">ColumnChunk</code></li>
</ul>

<p>For example, the logical structure of 2 Row Groups and 6 <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s in the previous diagram might be stored in a Parquet file as shown in the following diagram (not to scale). The pages for the <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s come first, followed by the footer. The data, the effectiveness of the encoding scheme, and the settings of the Parquet encoder determine the number of and size of the pages needed for each <code class="language-plaintext highlighter-rouge">ColumnChunk</code>. In this case, <code class="language-plaintext highlighter-rouge">ColumnChunk</code> 1 required 2 pages while <code class="language-plaintext highlighter-rouge">ColumnChunk</code> 6 required only 1 page. In addition to other information, the footer contains the locations of each Data Page and the types of the columns.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 1 ("A")             ◀─┃─ ─ ─│
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃     │
┃ Data Page for ColumnChunk 1 ("A")               ┃
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃     │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 2 ("B")               ┃     │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃     │
┃ Data Page for ColumnChunk 3 ("C")               ┃
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃     │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 3 ("C")               ┃     │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃     │
┃ Data Page for ColumnChunk 3 ("C")               ┃
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃     │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 4 ("A")             ◀─┃─ ─ ─│─ ┐
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃     │  │
┃ Data Page for ColumnChunk 5 ("B")               ┃
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃     │  │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 5 ("B")               ┃     │  │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃     │  │
┃ Data Page for ColumnChunk 5 ("B")               ┃
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃     │  │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 6 ("C")               ┃     │  │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃     │  │
┃┃Footer                                        ┃ ┃
┃┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃     │  │
┃┃ ┃File Metadata                             ┃ ┃ ┃
┃┃ ┃ Schema, etc                              ┃ ┃ ┃     │  │
┃┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓     ┃ ┃ ┃
┃┃ ┃ ┃Row Group 1 Metadata              ┃     ┃ ┃ ┃     │  │
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓             ┃     ┃ ┃ ┃
┃┃ ┃ ┃┃Column "A" Metadata┃ Location of ┃     ┃ ┃ ┃     │  │
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ first Data  ┣ ─ ─ ╋ ╋ ╋ ─ ─
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ Page, row   ┃     ┃ ┃ ┃        │
┃┃ ┃ ┃┃Column "B" Metadata┃ counts,     ┃     ┃ ┃ ┃
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ sizes,      ┃     ┃ ┃ ┃        │
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ min/max     ┃     ┃ ┃ ┃
┃┃ ┃ ┃┃Column "C" Metadata┃ values, etc ┃     ┃ ┃ ┃        │
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛             ┃     ┃ ┃ ┃
┃┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛     ┃ ┃ ┃        │
┃┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓     ┃ ┃ ┃
┃┃ ┃ ┃Row Group 2 Metadata              ┃     ┃ ┃ ┃        │
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ Location of ┃     ┃ ┃ ┃
┃┃ ┃ ┃┃Column "A" Metadata┃ first Data  ┃     ┃ ┃ ┃        │
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ Page, row   ┣ ─ ─ ╋ ╋ ╋ ─ ─ ─ ─
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ counts,     ┃     ┃ ┃ ┃
┃┃ ┃ ┃┃Column "B" Metadata┃ sizes,      ┃     ┃ ┃ ┃
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ min/max     ┃     ┃ ┃ ┃
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ values, etc ┃     ┃ ┃ ┃
┃┃ ┃ ┃┃Column "C" Metadata┃             ┃     ┃ ┃ ┃
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛             ┃     ┃ ┃ ┃
┃┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛     ┃ ┃ ┃
┃┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃
┃┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
</code></pre></div></div>

<p>There are many important criteria to consider when creating Parquet files such as how to optimally order/cluster data and structure it into <code class="language-plaintext highlighter-rouge">RowGroup</code>s and Data Pages. Such “physical design” considerations are complex, worthy of their own series of articles, and not addressed in this blog post. Instead, we focus on how to use the available structure to make queries very fast.</p>

<h1 id="optimizing-queries">Optimizing queries</h1>

<p>In any query processing system, the following techniques generally improve performance:</p>

<ol>
  <li>Reduce the data that must be transferred from secondary storage for processing (reduce I/O)</li>
  <li>Reduce the computational load for decoding the data (reduce CPU)</li>
  <li>Interleave/pipeline the reading and decoding of the data (improve parallelism)</li>
</ol>

<p>The same principles apply to querying Parquet files, as we describe below:</p>

<h1 id="decode-optimization">Decode optimization</h1>

<p>Parquet achieves impressive compression ratios by using <a href="https://parquet.apache.org/docs/file-format/data-pages/encodings/">sophisticated encoding techniques</a> such as run length compression, dictionary encoding, delta encoding, and others. Consequently, the CPU-bound task of decoding can dominate query latency. Parquet readers can use a number of techniques to improve the latency and throughput of this task, as we have done in the Rust implementation.</p>

<h2 id="vectorized-decode">Vectorized decode</h2>

<p>Most analytic systems decode multiple values at a time to a columnar memory format, such as Apache Arrow, rather than processing data row-by-row. This is often called vectorized or columnar processing, and is beneficial because it:</p>

<ul>
  <li>Amortizes dispatch overheads to switch on the type of column being decoded</li>
  <li>Improves cache locality by reading consecutive values from a <code class="language-plaintext highlighter-rouge">ColumnChunk</code></li>
  <li>Often allows multiple values to be decoded in a single instruction.</li>
  <li>Avoid many small heap allocations with a single large allocation, yielding significant savings for variable length types such as strings and byte arrays</li>
</ul>

<p>Thus, Rust Parquet Reader implements specialized decoders for reading Parquet directly into a <a href="https://www.influxdata.com/glossary/column-database/">columnar</a> memory format (Arrow Arrays).</p>

<h2 id="streaming-decode">Streaming decode</h2>

<p>There is no relationship between which rows are stored in which Pages across <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s. For example, the logical values for the 10,000th row may be in the first page of column A and in the third page of column B.</p>

<p>The simplest approach to vectorized decoding, and the one often initially implemented in Parquet decoders, is to decode an entire <code class="language-plaintext highlighter-rouge">RowGroup</code> (or <code class="language-plaintext highlighter-rouge">ColumnChunk</code>) at a time.</p>

<p>However, given Parquet’s high compression ratios, a single <code class="language-plaintext highlighter-rouge">RowGroup</code> may well contain millions of rows. Decoding so many rows at once is non-optimal because it:</p>

<ul>
  <li><strong>Requires large amounts of intermediate RAM</strong>: typical in-memory formats optimized for processing, such as Apache Arrow, require much more than their Parquet-encoded form.</li>
  <li><strong>Increases query latency</strong>: Subsequent processing steps (like filtering or aggregation) can only begin once the entire <code class="language-plaintext highlighter-rouge">RowGroup</code> (or <code class="language-plaintext highlighter-rouge">ColumnChunk</code>) is decoded.</li>
</ul>

<p>As such, the best Parquet readers support “streaming” data out in by producing configurable sized batches of rows on demand. The batch size must be large enough to amortize decode overhead, but small enough for efficient memory usage and to allow downstream processing to begin concurrently while the subsequent batch is decoded.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃
┃ Data Page for ColumnChunk 1 │◀┃─                   ┌── ─── ─── ─── ─── ┐
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃ │   ┏━━━━━━━┓        ┌ ─ ┐ ┌ ─ ┐ ┌ ─ ┐ │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃     ┃       ┃      │                   │
┃ Data Page for ColumnChunk 1 │ ┃ │   ┃       ┃   ─ ▶│ │   │ │   │ │   │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃  ─ ─┃       ┃─ ┤   │  ─ ─   ─ ─   ─ ─  │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃ │   ┃       ┃           A    B     C   │
┃ Data Page for ColumnChunk 2 │◀┃─    ┗━━━━━━━┛  │   └── ─── ─── ─── ─── ┘
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃ │    Parquet
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃      Decoder   │            ...
┃ Data Page for ColumnChunk 3 │ ┃ │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃                │   ┌── ─── ─── ─── ─── ┐
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃ │                    ┌ ─ ┐ ┌ ─ ┐ ┌ ─ ┐ │
┃ Data Page for ColumnChunk 3 │◀┃─               │   │                   │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃                 ─ ▶│ │   │ │   │ │   │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃                    │  ─ ─   ─ ─   ─ ─  │
┃ Data Page for ColumnChunk 3 │ ┃                         A    B     C   │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃                    └── ─── ─── ─── ─── ┘
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

      Parquet file                                    Smaller in memory
                                                         batches for
                                                         processing
</code></pre></div></div>

<p>While streaming is not a complicated feature to explain, the stateful nature of decoding, especially across multiple columns and <a href="https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/">arbitrarily nested data</a>, where the relationship between rows and values is not fixed, requires <a href="https://github.com/apache/arrow-rs/blob/b7af85cb8dfe6887bb3fd43d1d76f659473b6927/parquet/src/arrow/record_reader/mod.rs">complex intermediate buffering</a> and significant engineering effort to handle correctly.</p>

<h2 id="dictionary-preservation">Dictionary preservation</h2>

<p>Dictionary Encoding, also called <a href="https://pandas.pydata.org/docs/user_guide/categorical.html">categorical</a> encoding, is a technique where each value in a column is not stored directly, but instead, an index in a separate list called a “Dictionary” is stored. This technique achieves many of the benefits of <a href="https://en.wikipedia.org/wiki/Third_normal_form#:~:text=Third%20normal%20form%20(3NF)%20is,in%201971%20by%20Edgar%20F.">third normal form</a> for columns that have repeated values (low <a href="https://www.influxdata.com/glossary/cardinality/">cardinality</a>) and is especially effective for columns of strings such as “City”.</p>

<p>The first page in a <code class="language-plaintext highlighter-rouge">ColumnChunk</code> can optionally be a dictionary page, containing a list of values of the column’s type. Subsequent pages within this <code class="language-plaintext highlighter-rouge">ColumnChunk</code> can then encode an index into this dictionary, instead of encoding the values directly.</p>

<p>Given the effectiveness of this encoding, if a Parquet decoder simply decodes dictionary data into the native type, it will inefficiently replicate the same value over and over again, which is especially disastrous for string data. To handle dictionary-encoded data efficiently, the encoding must be preserved during decode. Conveniently, many columnar formats, such as the Arrow <a href="https://docs.rs/arrow/27.0.0/arrow/array/struct.DictionaryArray.html">DictionaryArray</a>, support such compatible encodings.</p>

<p>Preserving dictionary encoding drastically improves performance when reading to an Arrow array, in some cases in excess of <a href="https://github.com/apache/arrow-rs/pull/1180">60x</a>, as well as using significantly less memory.</p>

<p>The major complicating factor for preserving dictionaries is that the dictionaries are stored per <code class="language-plaintext highlighter-rouge">ColumnChunk</code>, and therefore the dictionary changes between <code class="language-plaintext highlighter-rouge">RowGroup</code>s. The reader must automatically recompute a dictionary for batches that span multiple <code class="language-plaintext highlighter-rouge">RowGroup</code>s, while also optimizing for the case that batch sizes divide evenly into the number of rows per <code class="language-plaintext highlighter-rouge">RowGroup</code>. Additionally a column may be only <a href="https://github.com/apache/parquet-format/blob/111dbdcf8eff2e9f8e0d4e958cecbc7e00028aca/README.md?plain=1#L194-L199">partly dictionary encoded</a>, further complicating implementation. More information on this technique and its complications can be found in the <a href="https://arrow.apache.org/blog/2019/09/05/faster-strings-cpp-parquet/">blog post</a> on applying this technique to the C++ Parquet reader.</p>

<h1 id="projection-pushdown">Projection pushdown</h1>

<p>The most basic Parquet optimization, and the one most commonly described for Parquet files, is <em>projection pushdown</em>, which reduces both I/Oand CPU requirements. Projection in this context means “selecting some but not all of the columns.” Given how Parquet organizes data, it is straightforward to read and decode only the <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s required for the referenced columns.</p>

<p>For example, consider a SQL query of the form</p>

<pre><code class="language-SQL">SELECT B from table where A &gt; 35
</code></pre>

<p>This query only needs data for columns A and B (and not C) and the projection can be “pushed down” to the Parquet reader.</p>

<p>Specifically, using the information in the footer, the Parquet reader can entirely skip fetching (I/O) and decoding (CPU) the Data Pages that store data for column C (<code class="language-plaintext highlighter-rouge">ColumnChunk</code> 3 and <code class="language-plaintext highlighter-rouge">ColumnChunk</code> 6 in our example).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                             ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
                             ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ┌─────▶ Data Page for ColumnChunk 1 ("A") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ├─────▶ Data Page for ColumnChunk 1 ("A") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ├─────▶ Data Page for ColumnChunk 2 ("B") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       │     ┃ Data Page for ColumnChunk 3 ("C") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
   A query that        │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
  accesses only        │     ┃ Data Page for ColumnChunk 3 ("C") ┃
 columns A and B       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
can read only the      │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
 relevant pages,  ─────┤     ┃ Data Page for ColumnChunk 3 ("C") ┃
skipping any Data      │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
Page for column C      │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ├─────▶ Data Page for ColumnChunk 4 ("A") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ├─────▶ Data Page for ColumnChunk 5 ("B") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ├─────▶ Data Page for ColumnChunk 5 ("B") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       └─────▶ Data Page for ColumnChunk 5 ("B") ┃
                             ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                             ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                             ┃ Data Page for ColumnChunk 6 ("C") ┃
                             ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                             ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
</code></pre></div></div>

<h1 id="predicate-pushdown">Predicate pushdown</h1>

<p>Similar to projection pushdown, <strong>predicate</strong> pushdown also avoids fetching and decoding data from Parquet files, but does so using filter expressions. This technique typically requires closer integration with a query engine such as <a href="https://arrow.apache.org/datafusion/">DataFusion</a>, to determine valid predicates and evaluate them during the scan. Unfortunately without careful API design, the Parquet decoder and query engine can end up tightly coupled, preventing reuse (e.g. there are different Impala and Spark implementations in <a href="https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/cdh_ig_predicate_pushdown_parquet.html#concept_pgs_plb_mgb">Cloudera Parquet Predicate Pushdown docs</a>). The Rust Parquet reader uses the <a href="https://docs.rs/parquet/27.0.0/parquet/arrow/arrow_reader/struct.RowSelector.html">RowSelection</a> API to avoid this coupling.</p>

<h2 id="rowgroup-pruning"><code class="language-plaintext highlighter-rouge">RowGroup</code> pruning</h2>

<p>The simplest form of predicate pushdown, supported by many Parquet based query engines, uses the statistics stored in the footer to skip entire <code class="language-plaintext highlighter-rouge">RowGroup</code>s. We call this operation <code class="language-plaintext highlighter-rouge">RowGroup</code> <em>pruning</em>, and it is analogous to <a href="https://docs.oracle.com/database/121/VLDBG/GUID-E677C85E-C5E3-4927-B3DF-684007A7B05D.htm#VLDBG00401">partition pruning</a> in many classical data warehouse systems.</p>

<p>For the example query above, if the maximum value for A in a particular <code class="language-plaintext highlighter-rouge">RowGroup</code> is less than 35, the decoder can skip fetching and decoding any <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s from that <strong>entire</strong> <code class="language-plaintext highlighter-rouge">RowGroup</code>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃Row Group 1 Metadata                      ┃
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃
┃ ┃Column "A" Metadata    Min:0 Max:15   ┃◀╋ ┐
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃       Using the min
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ │     and max values
┃ ┃Column "B" Metadata                   ┃ ┃       from the
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ │     metadata,
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃       RowGroup 1  can
┃ ┃Column "C" Metadata                   ┃ ┃ ├ ─ ─ be entirely
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃       skipped
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │     (pruned) when
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓       searching for
┃Row Group 2 Metadata                      ┃ │     rows with A &gt;
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃       35,
┃ ┃Column "A" Metadata   Min:10 Max:50   ┃◀╋ ┘
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃
┃ ┃Column "B" Metadata                   ┃ ┃
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃
┃ ┃Column "C" Metadata                   ┃ ┃
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
</code></pre></div></div>

<p>Note that pruning on minimum and maximum values is effective for many data layouts and column types, but not all. Specifically, it is not as effective for columns with many distinct pseudo-random values (e.g. identifiers or uuids). Thankfully for this use case, Parquet also supports per <code class="language-plaintext highlighter-rouge">ColumnChunk</code> <a href="https://github.com/apache/parquet-format/blob/master/BloomFilter.md">Bloom Filters</a>. We are actively working on <a href="https://github.com/apache/arrow-rs/issues/3023">adding bloom filter</a> support in Apache Rust’s implementation.</p>

<h2 id="page-pruning">Page pruning</h2>

<p>A more sophisticated form of predicate pushdown uses the optional <a href="https://github.com/apache/parquet-format/blob/master/PageIndex.md">page index</a> in the footer metadata to rule out entire Data Pages. The decoder decodes only the corresponding rows from other columns, often skipping entire pages.</p>

<p>The fact that pages in different <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s often contain different numbers of rows, due to various reasons, complicates this optimization. While the page index may identify the needed pages from one column, pruning a page from one column doesn’t immediately rule out entire pages in other columns.</p>

<p>Page pruning proceeds as follows:</p>

<ul>
  <li>Uses the predicates in combination with the page index to identify pages to skip</li>
  <li>Uses the offset index to determine what row ranges correspond to non-skipped pages</li>
  <li>Computes the intersection of ranges across non-skipped pages, and decodes only those rows</li>
</ul>

<p>This last point is highly non-trivial to implement, especially for nested lists where <a href="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/">a single row may correspond to multiple values</a>. Fortunately, the Rust Parquet reader hides this complexity internally, and can decode arbitrary <a href="https://docs.rs/parquet/27.0.0/parquet/arrow/arrow_reader/struct.RowSelection.html">RowSelections</a>.</p>

<p>For example, to scan Columns A and B, stored in 5 Data Pages as shown in the figure below:</p>

<p>If the predicate is <code class="language-plaintext highlighter-rouge">A &gt; 35</code>,</p>

<ul>
  <li>Page 1 is pruned using the page index (max value is <code class="language-plaintext highlighter-rouge">20</code>), leaving a RowSelection of  [200-&gt;onwards],</li>
  <li>Parquet reader skips Page 3 entirely (as its last row index is <code class="language-plaintext highlighter-rouge">99</code>)</li>
  <li>(Only) the relevant rows are read by reading pages 2, 4, and 5.</li>
</ul>

<p>If the predicate is instead <code class="language-plaintext highlighter-rouge">A &gt; 35 AND B = "F"</code> the page index is even more effective</p>

<ul>
  <li>Using <code class="language-plaintext highlighter-rouge">A &gt; 35</code>, yields a RowSelection of <code class="language-plaintext highlighter-rouge">[200-&gt;onwards]</code> as before</li>
  <li>Using <code class="language-plaintext highlighter-rouge">B = "F"</code>, on the remaining Page 4 and Page 5 of B, yields a RowSelection of <code class="language-plaintext highlighter-rouge">[100-244]</code></li>
  <li>Intersecting the two RowSelections leaves a combined RowSelection <code class="language-plaintext highlighter-rouge">[200-244]</code></li>
  <li>Parquet reader only decodes those 50 rows from Page 2 and Page 4.</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━
   ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   ┃
┃     ┌──────────────┐  │     ┌──────────────┐  │  ┃
┃  │  │              │     │  │              │     ┃
┃     │              │  │     │     Page     │  │
   │  │              │     │  │      3       │     ┃
┃     │              │  │     │   min: "A"   │  │  ┃
┃  │  │              │     │  │   max: "C"   │     ┃
┃     │     Page     │  │     │ first_row: 0 │  │
   │  │      1       │     │  │              │     ┃
┃     │   min: 10    │  │     └──────────────┘  │  ┃
┃  │  │   max: 20    │     │  ┌──────────────┐     ┃
┃     │ first_row: 0 │  │     │              │  │
   │  │              │     │  │     Page     │     ┃
┃     │              │  │     │      4       │  │  ┃
┃  │  │              │     │  │   min: "D"   │     ┃
┃     │              │  │     │   max: "G"   │  │
   │  │              │     │  │first_row: 100│     ┃
┃     └──────────────┘  │     │              │  │  ┃
┃  │  ┌──────────────┐     │  │              │     ┃
┃     │              │  │     └──────────────┘  │
   │  │     Page     │     │  ┌──────────────┐     ┃
┃     │      2       │  │     │              │  │  ┃
┃  │  │   min: 30    │     │  │     Page     │     ┃
┃     │   max: 40    │  │     │      5       │  │
   │  │first_row: 200│     │  │   min: "H"   │     ┃
┃     │              │  │     │   max: "Z"   │  │  ┃
┃  │  │              │     │  │first_row: 250│     ┃
┃     └──────────────┘  │     │              │  │
   │                       │  └──────────────┘     ┃
┃   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃       ColumnChunk            ColumnChunk         ┃
┃            A                      B
 ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━┛
</code></pre></div></div>

<p>Support for reading and writing these indexes from Arrow C++, and by extension pyarrow/pandas, is tracked in <a href="https://issues.apache.org/jira/browse/PARQUET-1404">PARQUET-1404</a>.</p>

<h2 id="late-materialization">Late materialization</h2>

<p>The two previous forms of predicate pushdown only operated on metadata stored for <code class="language-plaintext highlighter-rouge">RowGroup</code>s, <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s, and Data Pages prior to decoding values. However, the same techniques also extend to values of one or more columns <em>after</em> decoding them but prior to decoding other columns, which is often called “late materialization”.</p>

<p>This technique is especially effective when:</p>

<ul>
  <li>The predicate is very selective, i.e. filters out large numbers of rows</li>
  <li>Each row is large, either due to wide rows (e.g. JSON blobs) or many columns</li>
  <li>The selected data is clustered together</li>
  <li>The columns required by the predicate are relatively inexpensive to decode, e.g. PrimitiveArray / DictionaryArray</li>
</ul>

<p>There is additional discussion about the benefits of this technique in <a href="https://issues.apache.org/jira/browse/SPARK-36527">SPARK-36527</a> and<a href="https://docs.cloudera.com/cdw-runtime/cloud/impala-reference/topics/impala-lazy-materialization.html"> Impala</a>.</p>

<p>For example, given the predicate <code class="language-plaintext highlighter-rouge">A &gt; 35 AND B = "F"</code> from above where the engine uses the page index to determine only 50 rows within RowSelection of [100-244] could match, using late materialization, the Parquet decoder:</p>

<ul>
  <li>Decodes the 50 values of Column A</li>
  <li>Evaluates  <code class="language-plaintext highlighter-rouge">A &gt; 35 </code> on those 50 values</li>
  <li>In this case, only 5 rows pass, resulting in the RowSelection:
    <ul>
      <li>RowSelection[205-206]</li>
      <li>RowSelection[238-240]</li>
    </ul>
  </li>
  <li>Only decodes the 5 rows for Column B for those selections</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  Row Index
             ┌────────────────────┐            ┌────────────────────┐
       200   │         30         │            │        "F"         │
             └────────────────────┘            └────────────────────┘
                      ...                               ...
             ┌────────────────────┐            ┌────────────────────┐
       205   │         37         │─ ─ ─ ─ ─ ─▶│        "F"         │
             ├────────────────────┤            ├────────────────────┤
       206   │         36         │─ ─ ─ ─ ─ ─▶│        "G"         │
             └────────────────────┘            └────────────────────┘
                      ...                               ...
             ┌────────────────────┐            ┌────────────────────┐
       238   │         36         │─ ─ ─ ─ ─ ─▶│        "F"         │
             ├────────────────────┤            ├────────────────────┤
       239   │         36         │─ ─ ─ ─ ─ ─▶│        "G"         │
             ├────────────────────┤            ├────────────────────┤
       240   │         40         │─ ─ ─ ─ ─ ─▶│        "G"         │
             └────────────────────┘            └────────────────────┘
                      ...                               ...
             ┌────────────────────┐            ┌────────────────────┐
      244    │         26         │            │        "D"         │
             └────────────────────┘            └────────────────────┘


                   Column A                          Column B
                    Values                            Values
</code></pre></div></div>

<p>In certain cases, such as our example where B stores single character values, the cost of late materialization machinery can outweigh the savings in decoding. However, the savings can be substantial when some of the conditions listed above are fulfilled. The query engine must decide which predicates to push down and in which order to apply them for optimal results.</p>

<p>While it is outside the scope of this document, the same technique can be applied for multiple predicates as well as predicates on multiple columns. See the <a href="https://docs.rs/parquet/latest/parquet/arrow/arrow_reader/struct.RowFilter.html">RowFilter</a> interface in the Parquet crate for more information, and the <a href="https://github.com/apache/arrow-datafusion/blob/58b43f5c0b629be49a3efa0e37052ec51d9ba3fe/datafusion/core/src/physical_plan/file_format/parquet/row_filter.rs#L40-L70">row_filter</a> implementation in DataFusion.</p>

<h1 id="io-pushdown">I/O pushdown</h1>

<p>While Parquet was designed for efficient access on the <a href="https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html">HDFS distributed file system</a>, it works very well with commodity blob storage systems such as AWS S3 as they have very similar characteristics:</p>

<ul>
  <li><strong>Relatively slow “random access” reads</strong>: it is much more efficient to read large (MBs) sections of data in each request than issue many requests for smaller portions</li>
  <li><strong>Significant latency before retrieving the first byte</strong></li>
  <li><strong>High per-request cost:</strong> Often billed per request, regardless of number of bytes read, which incentivizes fewer requests that each read a large contiguous section of data.</li>
</ul>

<p>To read optimally from such systems, a Parquet reader must:</p>

<ol>
  <li>Minimize the number of I/O requests, while also applying the various pushdown techniques to avoid fetching large amounts of unused data.</li>
  <li>Integrate with the appropriate task scheduling mechanism to interleave I/O and processing on the data that is fetched to avoid pipeline bottlenecks.</li>
</ol>

<p>As these are substantial engineering and integration challenges, many Parquet readers still require the files to be fetched in their entirety to local storage.</p>

<p>Fetching the entire files in order to process them is not ideal for several reasons:</p>

<ol>
  <li><strong>High Latency</strong>: Decoding cannot begin until the entire file is fetched (Parquet metadata is at the end of the file, so the decoder must see the end prior to decoding the rest)</li>
  <li><strong>Wasted work</strong>: Fetching the entire file fetches all necessary data, but also potentially lots of unnecessary data that will be skipped after reading the footer. This increases the cost unnecessarily.</li>
  <li><strong>Requires costly “locally attached” storage (or memory)</strong>: Many cloud environments do not offer computing resources with locally attached storage – they either rely on expensive network block storage such as AWS EBS or else restrict local storage to certain classes of VMs.</li>
</ol>

<p>Avoiding the need to buffer the entire file requires a sophisticated Parquet decoder, integrated with the I/O subsystem, that can initially fetch and decode the metadata followed by ranged fetches for the relevant data blocks, interleaved with the decoding of Parquet data. This optimization requires careful engineering to fetch large enough blocks of data from the object store that the per request overhead doesn’t dominate gains from reducing the bytes transferred. <a href="https://issues.apache.org/jira/browse/SPARK-36529">SPARK-36529</a> describes the challenges of sequential processing in more detail.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                       ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
                                                                │
                       │
               Step 1: Fetch                                    │
 Parquet       Parquet metadata
 file on ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━▼━━━━━━━┓
 Remote  ┃      ▒▒▒▒▒▒▒▒▒▒          ▒▒▒▒▒▒▒▒▒▒               ░░░░░░░░░░ ┃
 Object  ┃      ▒▒▒data▒▒▒          ▒▒▒data▒▒▒               ░metadata░ ┃
  Store  ┃      ▒▒▒▒▒▒▒▒▒▒          ▒▒▒▒▒▒▒▒▒▒               ░░░░░░░░░░ ┃
         ┗━━━━━━━━━━━▲━━━━━━━━━━━━━━━━━━━━━▲━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
                     │                     └ ─ ─ ─
                                                  │
                     │                   Step 2: Fetch only
                      ─ ─ ─ ─ ─ ─ ─ ─ ─ relevant data blocks
</code></pre></div></div>

<p>Not included in this diagram picture are details like coalescing requests and ensuring minimum request sizes needed for an actual implementation.</p>

<p>The Rust Parquet crate provides an async Parquet reader, to efficiently read from any <a href="https://docs.rs/parquet/latest/parquet/arrow/async_reader/trait.AsyncFileReader.html">AsyncFileReader</a> that:</p>

<ul>
  <li>Efficiently reads from any storage medium that supports range requests</li>
  <li>Integrates with Rust’s futures ecosystem to avoid blocking threads waiting on network I/O <a href="https://www.influxdata.com/blog/using-rustlangs-async-tokio-runtime-for-cpu-bound-tasks/">and easily can interleave CPU and network </a></li>
  <li>Requests multiple ranges simultaneously, to allow the implementation to coalesce adjacent ranges, fetch ranges in parallel, etc.</li>
  <li>Uses the pushdown techniques described previously to eliminate fetching data where possible</li>
  <li>Integrates easily with the Apache Arrow <a href="https://docs.rs/object_store/latest/object_store/">object_store</a> crate which you can read more about <a href="https://www.influxdata.com/blog/rust-object-store-donation/">here</a></li>
</ul>

<p>To give a sense of what is possible, the following picture shows a timeline of fetching the footer metadata from remote files, using that metadata to determine what Data Pages to read, and then fetching data and decoding simultaneously. This process often must be done for more than one file at a time in order to match network latency, bandwidth, and available CPU.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                           begin
          metadata        read of   end read
            read  ─ ─ ─ ┐   data    of data          │
 begin    complete         block     block
read of                 │   │        │               │
metadata  ─ ─ ─ ┐                                       At any time, there are
             │          │   │        │               │     multiple network
             │  ▼       ▼   ▼        ▼                  requests outstanding to
  file 1     │ ░░░░░░░░░░   ▒▒▒read▒▒▒   ▒▒▒read▒▒▒  │    hide the individual
             │ ░░░read░░░   ▒▒▒data▒▒▒   ▒▒▒data▒▒▒        request latency
             │ ░metadata░                         ▓▓decode▓▓
             │ ░░░░░░░░░░                         ▓▓▓data▓▓▓
             │                                       │
             │
             │ ░░░░░░░░░░  ▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒read▒▒▒▒│▒▒▒▒▒▒▒▒▒▒▒▒▒▒
   file 2    │ ░░░read░░░  ▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒data▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒
             │ ░metadata░                            │              ▓▓▓▓▓decode▓▓▓▓▓▓
             │ ░░░░░░░░░░                                           ▓▓▓▓▓▓data▓▓▓▓▓▓▓
             │                                       │
             │
             │                                     ░░│░░░░░░░  ▒▒▒read▒▒▒  ▒▒▒▒read▒▒▒▒▒
   file 3    │                                     ░░░read░░░  ▒▒▒data▒▒▒  ▒▒▒▒data▒▒▒▒▒      ...
             │                                     ░m│tadata░            ▓▓decode▓▓
             │                                     ░░░░░░░░░░            ▓▓▓data▓▓▓
             └───────────────────────────────────────┼──────────────────────────────▶Time


                                                     │
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>We hope you enjoyed reading about the Parquet file format, and the various techniques used to quickly query parquet files.</p>

<p>We believe that the reason most open source implementations of Parquet do not have the breadth of features described in this post is that it takes a monumental effort, that was previously only possible at well-financed commercial enterprises which kept their implementations closed source.</p>

<p>However, with the growth and quality of the Apache Arrow community, both Rust practitioners and the wider Arrow community, our ability to collaborate and build a cutting-edge open source implementation is exhilarating and immensely satisfying. The technology described in this blog is the result of the contributions of many engineers spread across companies, hobbyists, and the world in several repositories, notably <a href="https://github.com/apache/arrow-datafusion">Apache Arrow DataFusion</a>, <a href="https://github.com/apache/arrow-rs">Apache Arrow</a> and <a href="https://github.com/apache/arrow-ballista">Apache Arrow Ballista.</a></p>

<p>If you are interested in joining the DataFusion Community, please <a href="https://arrow.apache.org/datafusion/contributor-guide/communication.html">get in touch</a>.</p>]]></content><author><name>tustvold and alamb</name></author><category term="parquet" /><summary type="html"><![CDATA[Querying Parquet with Millisecond Latency Note: this article was originally published on the InfluxData Blog. We believe that querying data in Apache Parquet files directly can achieve similar or better storage efficiency and query performance than most specialized file formats. While it requires significant engineering effort, the benefits of Parquet’s open format and broad ecosystem support make it the obvious choice for a wide class of data systems. In this article we explain several advanced techniques needed to query data stored in the Parquet format quickly that we implemented in the Apache Arrow Rust Parquet reader. Together these techniques make the Rust implementation one of, if not the fastest implementation for querying Parquet files — be it on local disk or remote object storage. It is able to query GBs of Parquet in a matter of milliseconds. We would like to acknowledge and thank InfluxData for their support of this work. InfluxData has a deep and continuing commitment to Open source software, and it sponsored much of our time for writing this blog post as well as many contributions as part of building the InfluxDB IOx Storage Engine. Background Apache Parquet is an increasingly popular open format for storing analytic datasets, and has become the de-facto standard for cost-effective, DBMS-agnostic data storage. Initially created for the Hadoop ecosystem, Parquet’s reach now expands broadly across the data analytics ecosystem due to its compelling combination of: High compression ratios Amenability to commodity blob-storage such as S3 Broad ecosystem and tooling support Portability across many different platforms and tools Support for arbitrarily structured data Increasingly other systems, such as DuckDB and Redshift allow querying data stored in Parquet directly, but support is still often a secondary consideration compared to their native (custom) file formats. Such formats include the DuckDB .duckdb file format, the Apache IOT TsFile, the Gorilla format, and others. For the first time, access to the same sophisticated query techniques, previously only available in closed source commercial implementations, are now available as open source. The required engineering capacity comes from large, well-run open source projects with global contributor communities, such as Apache Arrow and Apache Impala. Parquet file format Before diving into the details of efficiently reading from Parquet, it is important to understand the file layout. The file format is carefully designed to quickly locate the desired information, skip irrelevant portions, and decode what remains efficiently. The data in a Parquet file is broken into horizontal slices called RowGroups Each RowGroup contains a single ColumnChunk for each column in the schema For example, the following diagram illustrates a Parquet file with three columns “A”, “B” and “C” stored in two RowGroups for a total of 6 ColumnChunks. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┓ ┃ ┃┃┌ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ┐┌ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ RowGroup ┃ ┃┃ │ │ ┃ 1 ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃└ ─ ─ ─ ─ ─ ─ └ ─ ─ ─ ─ ─ ─ ┘└ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ColumnChunk 1 ColumnChunk 2 ColumnChunk 3 ┃ ┃ ┃┃ (Column "A") (Column "B") (Column "C") ┃ ┃ ┃┗━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┛ ┃ ┃┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┓ ┃ ┃┃┌ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ┐┌ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ RowGroup ┃ ┃┃ │ │ ┃ 2 ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃└ ─ ─ ─ ─ ─ ─ └ ─ ─ ─ ─ ─ ─ ┘└ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ColumnChunk 4 ColumnChunk 5 ColumnChunk 6 ┃ ┃ ┃┃ (Column "A") (Column "B") (Column "C") ┃ ┃ ┃┗━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┛ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ The logical values for a ColumnChunk are written using one of the many available encodings into one or more Data Pages appended sequentially in the file. At the end of a Parquet file is a footer, which contains important metadata, such as: The file’s schema information such as column names and types The locations of the RowGroup and ColumnChunks in the file The footer may also contain other specialized data structures: Optional statistics for each ColumnChunk including min/max values and null counts Optional pointers to OffsetIndexes containing the location of each individual Page Optional pointers to ColumnIndex containing row counts and summary statistics for each Page Optional pointers to BloomFilterData, which can quickly check if a value is present in a ColumnChunk For example, the logical structure of 2 Row Groups and 6 ColumnChunks in the previous diagram might be stored in a Parquet file as shown in the following diagram (not to scale). The pages for the ColumnChunks come first, followed by the footer. The data, the effectiveness of the encoding scheme, and the settings of the Parquet encoder determine the number of and size of the pages needed for each ColumnChunk. In this case, ColumnChunk 1 required 2 pages while ColumnChunk 6 required only 1 page. In addition to other information, the footer contains the locations of each Data Page and the types of the columns. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 1 ("A") ◀─┃─ ─ ─│ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ ┃ Data Page for ColumnChunk 1 ("A") ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 2 ("B") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ ┃ Data Page for ColumnChunk 3 ("C") ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 3 ("C") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ ┃ Data Page for ColumnChunk 3 ("C") ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 4 ("A") ◀─┃─ ─ ─│─ ┐ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ │ ┃ Data Page for ColumnChunk 5 ("B") ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 5 ("B") ┃ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ │ ┃ Data Page for ColumnChunk 5 ("B") ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 6 ("C") ┃ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ │ │ ┃┃Footer ┃ ┃ ┃┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ │ │ ┃┃ ┃File Metadata ┃ ┃ ┃ ┃┃ ┃ Schema, etc ┃ ┃ ┃ │ │ ┃┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃ ┃┃ ┃ ┃Row Group 1 Metadata ┃ ┃ ┃ ┃ │ │ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column "A" Metadata┃ Location of ┃ ┃ ┃ ┃ │ │ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ first Data ┣ ─ ─ ╋ ╋ ╋ ─ ─ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ Page, row ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┃Column "B" Metadata┃ counts, ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ sizes, ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ min/max ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column "C" Metadata┃ values, etc ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ ┃ ┃┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ │ ┃┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃ ┃┃ ┃ ┃Row Group 2 Metadata ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ Location of ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column "A" Metadata┃ first Data ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ Page, row ┣ ─ ─ ╋ ╋ ╋ ─ ─ ─ ─ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ counts, ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column "B" Metadata┃ sizes, ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ min/max ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ values, etc ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column "C" Metadata┃ ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ ┃ ┃┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ ┃┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ There are many important criteria to consider when creating Parquet files such as how to optimally order/cluster data and structure it into RowGroups and Data Pages. Such “physical design” considerations are complex, worthy of their own series of articles, and not addressed in this blog post. Instead, we focus on how to use the available structure to make queries very fast. Optimizing queries In any query processing system, the following techniques generally improve performance: Reduce the data that must be transferred from secondary storage for processing (reduce I/O) Reduce the computational load for decoding the data (reduce CPU) Interleave/pipeline the reading and decoding of the data (improve parallelism) The same principles apply to querying Parquet files, as we describe below: Decode optimization Parquet achieves impressive compression ratios by using sophisticated encoding techniques such as run length compression, dictionary encoding, delta encoding, and others. Consequently, the CPU-bound task of decoding can dominate query latency. Parquet readers can use a number of techniques to improve the latency and throughput of this task, as we have done in the Rust implementation. Vectorized decode Most analytic systems decode multiple values at a time to a columnar memory format, such as Apache Arrow, rather than processing data row-by-row. This is often called vectorized or columnar processing, and is beneficial because it: Amortizes dispatch overheads to switch on the type of column being decoded Improves cache locality by reading consecutive values from a ColumnChunk Often allows multiple values to be decoded in a single instruction. Avoid many small heap allocations with a single large allocation, yielding significant savings for variable length types such as strings and byte arrays Thus, Rust Parquet Reader implements specialized decoders for reading Parquet directly into a columnar memory format (Arrow Arrays). Streaming decode There is no relationship between which rows are stored in which Pages across ColumnChunks. For example, the logical values for the 10,000th row may be in the first page of column A and in the third page of column B. The simplest approach to vectorized decoding, and the one often initially implemented in Parquet decoders, is to decode an entire RowGroup (or ColumnChunk) at a time. However, given Parquet’s high compression ratios, a single RowGroup may well contain millions of rows. Decoding so many rows at once is non-optimal because it: Requires large amounts of intermediate RAM: typical in-memory formats optimized for processing, such as Apache Arrow, require much more than their Parquet-encoded form. Increases query latency: Subsequent processing steps (like filtering or aggregation) can only begin once the entire RowGroup (or ColumnChunk) is decoded. As such, the best Parquet readers support “streaming” data out in by producing configurable sized batches of rows on demand. The batch size must be large enough to amortize decode overhead, but small enough for efficient memory usage and to allow downstream processing to begin concurrently while the subsequent batch is decoded. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ┃ Data Page for ColumnChunk 1 │◀┃─ ┌── ─── ─── ─── ─── ┐ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┏━━━━━━━┓ ┌ ─ ┐ ┌ ─ ┐ ┌ ─ ┐ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃ │ │ ┃ Data Page for ColumnChunk 1 │ ┃ │ ┃ ┃ ─ ▶│ │ │ │ │ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ─ ─┃ ┃─ ┤ │ ─ ─ ─ ─ ─ ─ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┃ ┃ A B C │ ┃ Data Page for ColumnChunk 2 │◀┃─ ┗━━━━━━━┛ │ └── ─── ─── ─── ─── ┘ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ Parquet ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ Decoder │ ... ┃ Data Page for ColumnChunk 3 │ ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┌── ─── ─── ─── ─── ┐ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┌ ─ ┐ ┌ ─ ┐ ┌ ─ ┐ │ ┃ Data Page for ColumnChunk 3 │◀┃─ │ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ─ ▶│ │ │ │ │ │ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ─ ─ ─ ─ ─ ─ │ ┃ Data Page for ColumnChunk 3 │ ┃ A B C │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ └── ─── ─── ─── ─── ┘ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ Parquet file Smaller in memory batches for processing While streaming is not a complicated feature to explain, the stateful nature of decoding, especially across multiple columns and arbitrarily nested data, where the relationship between rows and values is not fixed, requires complex intermediate buffering and significant engineering effort to handle correctly. Dictionary preservation Dictionary Encoding, also called categorical encoding, is a technique where each value in a column is not stored directly, but instead, an index in a separate list called a “Dictionary” is stored. This technique achieves many of the benefits of third normal form for columns that have repeated values (low cardinality) and is especially effective for columns of strings such as “City”. The first page in a ColumnChunk can optionally be a dictionary page, containing a list of values of the column’s type. Subsequent pages within this ColumnChunk can then encode an index into this dictionary, instead of encoding the values directly. Given the effectiveness of this encoding, if a Parquet decoder simply decodes dictionary data into the native type, it will inefficiently replicate the same value over and over again, which is especially disastrous for string data. To handle dictionary-encoded data efficiently, the encoding must be preserved during decode. Conveniently, many columnar formats, such as the Arrow DictionaryArray, support such compatible encodings. Preserving dictionary encoding drastically improves performance when reading to an Arrow array, in some cases in excess of 60x, as well as using significantly less memory. The major complicating factor for preserving dictionaries is that the dictionaries are stored per ColumnChunk, and therefore the dictionary changes between RowGroups. The reader must automatically recompute a dictionary for batches that span multiple RowGroups, while also optimizing for the case that batch sizes divide evenly into the number of rows per RowGroup. Additionally a column may be only partly dictionary encoded, further complicating implementation. More information on this technique and its complications can be found in the blog post on applying this technique to the C++ Parquet reader. Projection pushdown The most basic Parquet optimization, and the one most commonly described for Parquet files, is projection pushdown, which reduces both I/Oand CPU requirements. Projection in this context means “selecting some but not all of the columns.” Given how Parquet organizes data, it is straightforward to read and decode only the ColumnChunks required for the referenced columns. For example, consider a SQL query of the form SELECT B from table where A &gt; 35 This query only needs data for columns A and B (and not C) and the projection can be “pushed down” to the Parquet reader. Specifically, using the information in the footer, the Parquet reader can entirely skip fetching (I/O) and decoding (CPU) the Data Pages that store data for column C (ColumnChunk 3 and ColumnChunk 6 in our example). ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ┌─────▶ Data Page for ColumnChunk 1 ("A") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 1 ("A") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 2 ("B") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ │ ┃ Data Page for ColumnChunk 3 ("C") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ A query that │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ accesses only │ ┃ Data Page for ColumnChunk 3 ("C") ┃ columns A and B │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ can read only the │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ relevant pages, ─────┤ ┃ Data Page for ColumnChunk 3 ("C") ┃ skipping any Data │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ Page for column C │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 4 ("A") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 5 ("B") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 5 ("B") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ └─────▶ Data Page for ColumnChunk 5 ("B") ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ┃ Data Page for ColumnChunk 6 ("C") ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ Predicate pushdown Similar to projection pushdown, predicate pushdown also avoids fetching and decoding data from Parquet files, but does so using filter expressions. This technique typically requires closer integration with a query engine such as DataFusion, to determine valid predicates and evaluate them during the scan. Unfortunately without careful API design, the Parquet decoder and query engine can end up tightly coupled, preventing reuse (e.g. there are different Impala and Spark implementations in Cloudera Parquet Predicate Pushdown docs). The Rust Parquet reader uses the RowSelection API to avoid this coupling. RowGroup pruning The simplest form of predicate pushdown, supported by many Parquet based query engines, uses the statistics stored in the footer to skip entire RowGroups. We call this operation RowGroup pruning, and it is analogous to partition pruning in many classical data warehouse systems. For the example query above, if the maximum value for A in a particular RowGroup is less than 35, the decoder can skip fetching and decoding any ColumnChunks from that entire RowGroup. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃Row Group 1 Metadata ┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃Column "A" Metadata Min:0 Max:15 ┃◀╋ ┐ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ Using the min ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ │ and max values ┃ ┃Column "B" Metadata ┃ ┃ from the ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ │ metadata, ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ RowGroup 1 can ┃ ┃Column "C" Metadata ┃ ┃ ├ ─ ─ be entirely ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ skipped ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │ (pruned) when ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ searching for ┃Row Group 2 Metadata ┃ │ rows with A &gt; ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ 35, ┃ ┃Column "A" Metadata Min:10 Max:50 ┃◀╋ ┘ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃Column "B" Metadata ┃ ┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃Column "C" Metadata ┃ ┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ Note that pruning on minimum and maximum values is effective for many data layouts and column types, but not all. Specifically, it is not as effective for columns with many distinct pseudo-random values (e.g. identifiers or uuids). Thankfully for this use case, Parquet also supports per ColumnChunk Bloom Filters. We are actively working on adding bloom filter support in Apache Rust’s implementation. Page pruning A more sophisticated form of predicate pushdown uses the optional page index in the footer metadata to rule out entire Data Pages. The decoder decodes only the corresponding rows from other columns, often skipping entire pages. The fact that pages in different ColumnChunks often contain different numbers of rows, due to various reasons, complicates this optimization. While the page index may identify the needed pages from one column, pruning a page from one column doesn’t immediately rule out entire pages in other columns. Page pruning proceeds as follows: Uses the predicates in combination with the page index to identify pages to skip Uses the offset index to determine what row ranges correspond to non-skipped pages Computes the intersection of ranges across non-skipped pages, and decodes only those rows This last point is highly non-trivial to implement, especially for nested lists where a single row may correspond to multiple values. Fortunately, the Rust Parquet reader hides this complexity internally, and can decode arbitrary RowSelections. For example, to scan Columns A and B, stored in 5 Data Pages as shown in the figure below: If the predicate is A &gt; 35, Page 1 is pruned using the page index (max value is 20), leaving a RowSelection of [200-&gt;onwards], Parquet reader skips Page 3 entirely (as its last row index is 99) (Only) the relevant rows are read by reading pages 2, 4, and 5. If the predicate is instead A &gt; 35 AND B = "F" the page index is even more effective Using A &gt; 35, yields a RowSelection of [200-&gt;onwards] as before Using B = "F", on the remaining Page 4 and Page 5 of B, yields a RowSelection of [100-244] Intersecting the two RowSelections leaves a combined RowSelection [200-244] Parquet reader only decodes those 50 rows from Page 2 and Page 4. ┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ┃ ┌──────────────┐ │ ┌──────────────┐ │ ┃ ┃ │ │ │ │ │ │ ┃ ┃ │ │ │ │ Page │ │ │ │ │ │ │ 3 │ ┃ ┃ │ │ │ │ min: "A" │ │ ┃ ┃ │ │ │ │ │ max: "C" │ ┃ ┃ │ Page │ │ │ first_row: 0 │ │ │ │ 1 │ │ │ │ ┃ ┃ │ min: 10 │ │ └──────────────┘ │ ┃ ┃ │ │ max: 20 │ │ ┌──────────────┐ ┃ ┃ │ first_row: 0 │ │ │ │ │ │ │ │ │ │ Page │ ┃ ┃ │ │ │ │ 4 │ │ ┃ ┃ │ │ │ │ │ min: "D" │ ┃ ┃ │ │ │ │ max: "G" │ │ │ │ │ │ │first_row: 100│ ┃ ┃ └──────────────┘ │ │ │ │ ┃ ┃ │ ┌──────────────┐ │ │ │ ┃ ┃ │ │ │ └──────────────┘ │ │ │ Page │ │ ┌──────────────┐ ┃ ┃ │ 2 │ │ │ │ │ ┃ ┃ │ │ min: 30 │ │ │ Page │ ┃ ┃ │ max: 40 │ │ │ 5 │ │ │ │first_row: 200│ │ │ min: "H" │ ┃ ┃ │ │ │ │ max: "Z" │ │ ┃ ┃ │ │ │ │ │first_row: 250│ ┃ ┃ └──────────────┘ │ │ │ │ │ │ └──────────────┘ ┃ ┃ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃ ColumnChunk ColumnChunk ┃ ┃ A B ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━┛ Support for reading and writing these indexes from Arrow C++, and by extension pyarrow/pandas, is tracked in PARQUET-1404. Late materialization The two previous forms of predicate pushdown only operated on metadata stored for RowGroups, ColumnChunks, and Data Pages prior to decoding values. However, the same techniques also extend to values of one or more columns after decoding them but prior to decoding other columns, which is often called “late materialization”. This technique is especially effective when: The predicate is very selective, i.e. filters out large numbers of rows Each row is large, either due to wide rows (e.g. JSON blobs) or many columns The selected data is clustered together The columns required by the predicate are relatively inexpensive to decode, e.g. PrimitiveArray / DictionaryArray There is additional discussion about the benefits of this technique in SPARK-36527 and Impala. For example, given the predicate A &gt; 35 AND B = "F" from above where the engine uses the page index to determine only 50 rows within RowSelection of [100-244] could match, using late materialization, the Parquet decoder: Decodes the 50 values of Column A Evaluates A &gt; 35 on those 50 values In this case, only 5 rows pass, resulting in the RowSelection: RowSelection[205-206] RowSelection[238-240] Only decodes the 5 rows for Column B for those selections Row Index ┌────────────────────┐ ┌────────────────────┐ 200 │ 30 │ │ "F" │ └────────────────────┘ └────────────────────┘ ... ... ┌────────────────────┐ ┌────────────────────┐ 205 │ 37 │─ ─ ─ ─ ─ ─▶│ "F" │ ├────────────────────┤ ├────────────────────┤ 206 │ 36 │─ ─ ─ ─ ─ ─▶│ "G" │ └────────────────────┘ └────────────────────┘ ... ... ┌────────────────────┐ ┌────────────────────┐ 238 │ 36 │─ ─ ─ ─ ─ ─▶│ "F" │ ├────────────────────┤ ├────────────────────┤ 239 │ 36 │─ ─ ─ ─ ─ ─▶│ "G" │ ├────────────────────┤ ├────────────────────┤ 240 │ 40 │─ ─ ─ ─ ─ ─▶│ "G" │ └────────────────────┘ └────────────────────┘ ... ... ┌────────────────────┐ ┌────────────────────┐ 244 │ 26 │ │ "D" │ └────────────────────┘ └────────────────────┘ Column A Column B Values Values In certain cases, such as our example where B stores single character values, the cost of late materialization machinery can outweigh the savings in decoding. However, the savings can be substantial when some of the conditions listed above are fulfilled. The query engine must decide which predicates to push down and in which order to apply them for optimal results. While it is outside the scope of this document, the same technique can be applied for multiple predicates as well as predicates on multiple columns. See the RowFilter interface in the Parquet crate for more information, and the row_filter implementation in DataFusion. I/O pushdown While Parquet was designed for efficient access on the HDFS distributed file system, it works very well with commodity blob storage systems such as AWS S3 as they have very similar characteristics: Relatively slow “random access” reads: it is much more efficient to read large (MBs) sections of data in each request than issue many requests for smaller portions Significant latency before retrieving the first byte High per-request cost: Often billed per request, regardless of number of bytes read, which incentivizes fewer requests that each read a large contiguous section of data. To read optimally from such systems, a Parquet reader must: Minimize the number of I/O requests, while also applying the various pushdown techniques to avoid fetching large amounts of unused data. Integrate with the appropriate task scheduling mechanism to interleave I/O and processing on the data that is fetched to avoid pipeline bottlenecks. As these are substantial engineering and integration challenges, many Parquet readers still require the files to be fetched in their entirety to local storage. Fetching the entire files in order to process them is not ideal for several reasons: High Latency: Decoding cannot begin until the entire file is fetched (Parquet metadata is at the end of the file, so the decoder must see the end prior to decoding the rest) Wasted work: Fetching the entire file fetches all necessary data, but also potentially lots of unnecessary data that will be skipped after reading the footer. This increases the cost unnecessarily. Requires costly “locally attached” storage (or memory): Many cloud environments do not offer computing resources with locally attached storage – they either rely on expensive network block storage such as AWS EBS or else restrict local storage to certain classes of VMs. Avoiding the need to buffer the entire file requires a sophisticated Parquet decoder, integrated with the I/O subsystem, that can initially fetch and decode the metadata followed by ranged fetches for the relevant data blocks, interleaved with the decoding of Parquet data. This optimization requires careful engineering to fetch large enough blocks of data from the object store that the per request overhead doesn’t dominate gains from reducing the bytes transferred. SPARK-36529 describes the challenges of sequential processing in more detail. ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ │ Step 1: Fetch │ Parquet Parquet metadata file on ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━▼━━━━━━━┓ Remote ┃ ▒▒▒▒▒▒▒▒▒▒ ▒▒▒▒▒▒▒▒▒▒ ░░░░░░░░░░ ┃ Object ┃ ▒▒▒data▒▒▒ ▒▒▒data▒▒▒ ░metadata░ ┃ Store ┃ ▒▒▒▒▒▒▒▒▒▒ ▒▒▒▒▒▒▒▒▒▒ ░░░░░░░░░░ ┃ ┗━━━━━━━━━━━▲━━━━━━━━━━━━━━━━━━━━━▲━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │ └ ─ ─ ─ │ │ Step 2: Fetch only ─ ─ ─ ─ ─ ─ ─ ─ ─ relevant data blocks Not included in this diagram picture are details like coalescing requests and ensuring minimum request sizes needed for an actual implementation. The Rust Parquet crate provides an async Parquet reader, to efficiently read from any AsyncFileReader that: Efficiently reads from any storage medium that supports range requests Integrates with Rust’s futures ecosystem to avoid blocking threads waiting on network I/O and easily can interleave CPU and network Requests multiple ranges simultaneously, to allow the implementation to coalesce adjacent ranges, fetch ranges in parallel, etc. Uses the pushdown techniques described previously to eliminate fetching data where possible Integrates easily with the Apache Arrow object_store crate which you can read more about here To give a sense of what is possible, the following picture shows a timeline of fetching the footer metadata from remote files, using that metadata to determine what Data Pages to read, and then fetching data and decoding simultaneously. This process often must be done for more than one file at a time in order to match network latency, bandwidth, and available CPU. begin metadata read of end read read ─ ─ ─ ┐ data of data │ begin complete block block read of │ │ │ │ metadata ─ ─ ─ ┐ At any time, there are │ │ │ │ │ multiple network │ ▼ ▼ ▼ ▼ requests outstanding to file 1 │ ░░░░░░░░░░ ▒▒▒read▒▒▒ ▒▒▒read▒▒▒ │ hide the individual │ ░░░read░░░ ▒▒▒data▒▒▒ ▒▒▒data▒▒▒ request latency │ ░metadata░ ▓▓decode▓▓ │ ░░░░░░░░░░ ▓▓▓data▓▓▓ │ │ │ │ ░░░░░░░░░░ ▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒read▒▒▒▒│▒▒▒▒▒▒▒▒▒▒▒▒▒▒ file 2 │ ░░░read░░░ ▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒data▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ │ ░metadata░ │ ▓▓▓▓▓decode▓▓▓▓▓▓ │ ░░░░░░░░░░ ▓▓▓▓▓▓data▓▓▓▓▓▓▓ │ │ │ │ ░░│░░░░░░░ ▒▒▒read▒▒▒ ▒▒▒▒read▒▒▒▒▒ file 3 │ ░░░read░░░ ▒▒▒data▒▒▒ ▒▒▒▒data▒▒▒▒▒ ... │ ░m│tadata░ ▓▓decode▓▓ │ ░░░░░░░░░░ ▓▓▓data▓▓▓ └───────────────────────────────────────┼──────────────────────────────▶Time │ Conclusion We hope you enjoyed reading about the Parquet file format, and the various techniques used to quickly query parquet files. We believe that the reason most open source implementations of Parquet do not have the breadth of features described in this post is that it takes a monumental effort, that was previously only possible at well-financed commercial enterprises which kept their implementations closed source. However, with the growth and quality of the Apache Arrow community, both Rust practitioners and the wider Arrow community, our ability to collaborate and build a cutting-edge open source implementation is exhilarating and immensely satisfying. The technology described in this blog is the result of the contributions of many engineers spread across companies, hobbyists, and the world in several repositories, notably Apache Arrow DataFusion, Apache Arrow and Apache Arrow Ballista. If you are interested in joining the DataFusion Community, please get in touch.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>