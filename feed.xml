<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://arrow.apache.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arrow.apache.org/" rel="alternate" type="text/html" /><updated>2023-01-12T14:19:57-05:00</updated><id>https://arrow.apache.org/feed.xml</id><title type="html">Apache Arrow</title><subtitle>Apache Arrow is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, C++, C#, Go, Java, JavaScript, MATLAB, Python, R, Ruby, and Rust.</subtitle><entry><title type="html">Apache Arrow ADBC 0.1.0 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/01/12/adbc-0.1.0-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.1.0 (Libraries) Release" /><published>2023-01-12T00:00:00-05:00</published><updated>2023-01-12T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/01/12/adbc-0.1.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/01/12/adbc-0.1.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.1.0 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/1"><strong>63
resolved issues</strong></a> from <a href="#contributors"><strong>7 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version
0.1.0.  The <strong>API specification</strong> is versioned separately and is
at version 1.0.0.  For more, see the <a href="/blog/2023/01/05/introducing-arrow-adbc/">introduction to ADBC</a>.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.1.0/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>This initial release includes the following:</p>

<ul>
  <li>Driver manager libraries for C/C++, Go, Java, Python, and Ruby.</li>
  <li>ADBC drivers for SQLite and PostgreSQL, available in C/C++, Go, Python, and Ruby.</li>
  <li>ADBC drivers for Arrow FLight SQL and JDBC, available in Java.</li>
</ul>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn fe96ea9186194af06f4a229b6e5a57815f05f6bd..apache-arrow-adbc-0.1.0
   169	David Li
    12	Sutou Kouhei
     5	Matt Topol
     2	Dewey Dunnington
     1	Ash
     1	Judah Rand
     1	Raúl Cumplido
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>Future development will expand the capabilities of the existing
drivers, as well as adding drivers for more targets.  Drivers for
Flight SQL in C/C++ and Go are being developed.  See the <a href="https://github.com/apache/arrow-adbc/milestone/2">0.2.0
milestone</a> for details.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.1.0 release of the Apache Arrow ADBC libraries. This covers includes 63 resolved issues from 7 distinct contributors. This is a release of the libraries, which are at version 0.1.0. The API specification is versioned separately and is at version 1.0.0. For more, see the introduction to ADBC. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights This initial release includes the following: Driver manager libraries for C/C++, Go, Java, Python, and Ruby. ADBC drivers for SQLite and PostgreSQL, available in C/C++, Go, Python, and Ruby. ADBC drivers for Arrow FLight SQL and JDBC, available in Java. Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn fe96ea9186194af06f4a229b6e5a57815f05f6bd..apache-arrow-adbc-0.1.0 169 David Li 12 Sutou Kouhei 5 Matt Topol 2 Dewey Dunnington 1 Ash 1 Judah Rand 1 Raúl Cumplido Roadmap Future development will expand the capabilities of the existing drivers, as well as adding drivers for more targets. Drivers for Flight SQL in C/C++ and Go are being developed. See the 0.2.0 milestone for details. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Introducing ADBC: Database Access for Apache Arrow</title><link href="https://arrow.apache.org/blog/2023/01/05/introducing-arrow-adbc/" rel="alternate" type="text/html" title="Introducing ADBC: Database Access for Apache Arrow" /><published>2023-01-05T00:00:00-05:00</published><updated>2023-01-05T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/01/05/introducing-arrow-adbc</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/01/05/introducing-arrow-adbc/"><![CDATA[<!--

-->

<p>The Arrow community would like to introduce version 1.0.0 of the <a href="https://github.com/apache/arrow-adbc">Arrow Database Connectivity (ADBC)</a> specification.
ADBC is a columnar, minimal-overhead alternative to JDBC/ODBC for analytical applications.
Or in other words: <strong>ADBC is a single API for getting Arrow data in and out of different databases</strong>.</p>

<h2 id="motivation">Motivation</h2>

<p>Applications often use API standards like <a href="https://docs.oracle.com/javase/tutorial/jdbc/overview/index.html">JDBC</a> and <a href="https://learn.microsoft.com/en-us/sql/odbc/reference/what-is-odbc?view=sql-server-ver16">ODBC</a> to work with databases.
That way, they can code to the same API regardless of the underlying database, saving on development time.
Roughly speaking, when an application executes a query with these APIs:</p>

<figure style="text-align: center;">
  <img src="/img/ADBCFlow1.svg" width="90%" class="img-responsive" alt="A diagram showing the query execution flow." />
  <figcaption>The query execution flow.</figcaption>
</figure>

<ol>
  <li>The application submits a SQL query via the JDBC/ODBC API.</li>
  <li>The query is passed on to the driver.</li>
  <li>The driver translates the query to a database-specific protocol and sends it to the database.</li>
  <li>The database executes the query and returns the result set in a database-specific format.</li>
  <li>The driver translates the result into the format required by the JDBC/ODBC API.</li>
  <li>The application iterates over the result rows using the JDBC/ODBC API.</li>
</ol>

<p>When columnar data comes into play, however, problems arise.
JDBC is a row-oriented API, and while ODBC can support columnar data, the type system and data representation is not a perfect match with Arrow.
So generally, columnar data must be converted to rows in step 5, spending resources without performing “useful” work.</p>

<p>This mismatch is problematic for columnar database systems, such as ClickHouse, Dremio, DuckDB, and Google BigQuery.
On the client side, tools such as Apache Spark and pandas would be better off getting columnar data directly, skipping that conversion.
Otherwise, they’re leaving performance on the table.
At the same time, that conversion isn’t always avoidable.
Row-oriented database systems like PostgreSQL aren’t going away, and these clients will still want to consume data from them.</p>

<p>Developers have a few options:</p>

<ul>
  <li><em>Just use JDBC/ODBC</em>.
These standards are here to stay, and it makes sense for databases to support them for applications that want them.
But when both the database and the application are columnar, that means converting data into rows for JDBC/ODBC, only for the client to convert them right back into columns!
Performance suffers, and developers have to spend time implementing the conversions.</li>
  <li><em>Use JDBC/ODBC-to-Arrow conversion libraries</em>.
Libraries like <a href="https://turbodbc.readthedocs.io/en/latest/">Turbodbc</a> and <a href="https://arrow.apache.org/docs/java/jdbc.html">arrow-jdbc</a> handle row-to-columnar conversions for clients.
But this doesn’t fundamentally solve the problem.
Unnecessary data conversions are still required.</li>
  <li><em>Use vendor-specific protocols</em>.
For some databases, applications can use a database-specific protocol or SDK to directly get Arrow data.
For example, applications could use Dremio via <a href="/blog/2022/02/16/introducing-arrow-flight-sql/">Arrow Flight SQL</a>.
But client applications that want to support multiple database vendors would need to integrate with each of them.
(Look at all the <a href="https://trino.io/docs/current/connector.html">connectors</a> that Trino implements.)
And databases like PostgreSQL don’t offer an option supporting Arrow in the first place.</li>
</ul>

<p>As is, clients must choose between either tedious integration work or leaving performance on the table. We can make this better.</p>

<h2 id="introducing-adbc">Introducing ADBC</h2>

<p>ADBC is an Arrow-based, vendor-neutral API for interacting with databases.
Applications that use ADBC simply receive Arrow data.
They don’t have to do any conversions themselves, and they don’t have to integrate each database’s specific SDK.</p>

<p>Just like JDBC/ODBC, underneath the ADBC API are drivers that translate the API for specific databases.</p>

<ul>
  <li>A driver for an Arrow-native database just passes Arrow data through without conversion.</li>
  <li>A driver for a non-Arrow-native database must convert the data to Arrow.
This saves the application from doing that, and the driver can optimize the conversion for its database.</li>
</ul>

<figure style="text-align: center;">
  <img src="/img/ADBCFlow2.svg" alt="A diagram showing the query execution flow with ADBC." width="90%" class="img-responsive" />
  <figcaption>The query execution flow with two different ADBC drivers.</figcaption>
</figure>

<ol>
  <li>The application submits a SQL query via the ADBC API.</li>
  <li>The query is passed on to the ADBC driver.</li>
  <li>The driver translates the query to a database-specific protocol and sends the query to the database.</li>
  <li>The database executes the query and returns the result set in a database-specific format, which is ideally Arrow data.</li>
  <li>If needed: the driver translates the result into Arrow data.</li>
  <li>The application iterates over batches of Arrow data.</li>
</ol>

<p>The application only deals with one API, and only works with Arrow data.</p>

<p>ADBC API and driver implementations are under development. For example, in Python, the ADBC packages offer a familiar <a href="https://www.python.org/dev/peps/pep-0249/">DBAPI 2.0 (PEP 249)</a>-style interface, with extensions to get Arrow data.
We can get Arrow data out of PostgreSQL easily:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">adbc_driver_postgresql.dbapi</span>

<span class="n">uri</span> <span class="o">=</span> <span class="s">"postgresql://localhost:5432/postgres?user=postgres&amp;password=password"</span>
<span class="k">with</span> <span class="n">adbc_driver_postgresql</span><span class="p">.</span><span class="n">dbapi</span><span class="p">.</span><span class="n">connect</span><span class="p">(</span><span class="n">uri</span><span class="p">)</span> <span class="k">as</span> <span class="n">conn</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">conn</span><span class="p">.</span><span class="n">cursor</span><span class="p">()</span> <span class="k">as</span> <span class="n">cur</span><span class="p">:</span>
        <span class="n">cur</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="s">"SELECT * FROM customer"</span><span class="p">)</span>
        <span class="n">table</span> <span class="o">=</span> <span class="n">cur</span><span class="p">.</span><span class="n">fetch_arrow_table</span><span class="p">()</span>
        <span class="c1"># Process the results
</span></code></pre></div></div>

<p>Or SQLite:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">adbc_driver_sqlite.dbapi</span>

<span class="n">uri</span> <span class="o">=</span> <span class="s">"file:mydb.sqlite"</span>
<span class="k">with</span> <span class="n">adbc_driver_sqlite</span><span class="p">.</span><span class="n">dbapi</span><span class="p">.</span><span class="n">connect</span><span class="p">(</span><span class="n">uri</span><span class="p">)</span> <span class="k">as</span> <span class="n">conn</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">conn</span><span class="p">.</span><span class="n">cursor</span><span class="p">()</span> <span class="k">as</span> <span class="n">cur</span><span class="p">:</span>
        <span class="n">cur</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="s">"SELECT * FROM customer"</span><span class="p">)</span>
        <span class="n">table</span> <span class="o">=</span> <span class="n">cur</span><span class="p">.</span><span class="n">fetch_arrow_table</span><span class="p">()</span>
        <span class="c1"># Process the results
</span></code></pre></div></div>

<p><em>Note: implementations are still under development. See the <a href="https://arrow.apache.org/adbc/">documentation</a> for up-to-date examples.</em></p>

<h2 id="what-about-flight-sql-jdbc-odbc-">What about {Flight SQL, JDBC, ODBC, …}?</h2>

<p>ADBC fills a specific niche that related projects do not address. It is both:</p>

<ul>
  <li><strong>Arrow-native</strong>: ADBC can pass through Arrow data with no overhead thanks to the <a href="/blog/2020/05/03/introducing-arrow-c-data-interface/">C Data Interface</a>.
JDBC is row-oriented, and ODBC has implementation caveats, as discussed, that make it hard to use with Arrow.</li>
  <li><strong>Vendor-agnostic</strong>: ADBC drivers can implement the API using any underlying protocol, while Flight SQL requires server-side support that may not be easy to add.</li>
</ul>

<table class="table table-hover" style="table-layout: fixed">
  <caption>Comparing database APIs and protocols</caption>
  <thead class="thead-dark">
    <tr>
      <th></th>
      <th class="align-top" style="width: 40%" scope="col">Vendor-neutral (database APIs)</th>
      <th class="align-top" style="width: 40%" scope="col">Vendor-specific (database protocols)</th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <th scope="row">Arrow-native</th>
      <td class="table-success"><strong>ADBC</strong></td>
      <td>Arrow Flight SQL<br />BigQuery Storage gRPC protocol</td>
    </tr>
    <tr>
      <th scope="row">Row-oriented</th>
      <td>JDBC<br />ODBC (typically row-oriented)</td>
      <td>PostgreSQL wire protocol<br />Tabular Data Stream (Microsoft SQL Server)</td>
    </tr>
  </tbody>
</table>

<p><strong>ADBC doesn’t intend to replace JDBC or ODBC in general</strong>.
But for applications that just want bulk columnar data access, ADBC lets them avoid data conversion overhead and tedious integration work.</p>

<p>Similarly, within the Arrow project, ADBC does not replace Flight SQL, but instead <em>complements</em> it.
ADBC is an <strong>API</strong> that lets <em>clients</em> work with different databases easily.
Meanwhile, Flight SQL is a <strong>wire protocol</strong> that <em>database servers</em> can implement to simultaneously support ADBC, <a href="/blog/2022/11/01/arrow-flight-sql-jdbc/">JDBC</a>, and ODBC users.</p>

<figure style="text-align: center;">
  <img src="/img/ADBC.svg" alt="ADBC abstracts over protocols and APIs like Flight SQL and JDBC for client applications. Flight SQL provides implementations of APIs like ADBC and JDBC for database servers." width="90%" class="img-responsive" />
</figure>

<h2 id="getting-involved">Getting Involved</h2>

<p>ADBC works as part of the Arrow ecosystem to “cover the bases” for database interaction:</p>

<ul>
  <li>Arrow offers a universal columnar data format,</li>
  <li>Arrow Flight SQL offers a universal wire protocol for database servers,</li>
  <li>and ADBC offers a universal API for database clients.</li>
</ul>

<p>To start using ADBC, see the <a href="https://arrow.apache.org/adbc/">documentation</a> for build instructions and a short tutorial.
(A formal release of the packages is still under way.)
If you’re interested in learning more or contributing, please reach out on the <a href="https://arrow.apache.org/community/">mailing list</a> or on <a href="https://github.com/apache/arrow-adbc/issues">GitHub Issues</a>.</p>

<p>ADBC was only possible with the help and involvement of several Arrow community members and projects.
In particular, we would like to thank members of the <a href="https://duckdb.org/">DuckDB project</a> and the <a href="https://www.r-dbi.org/">R DBI project</a>, who constructed prototypes based on early revisions of the standard and provided feedback on the design.
And ADBC builds on existing Arrow projects, including the <a href="/blog/2020/05/03/introducing-arrow-c-data-interface/">Arrow C Data Interface</a> and <a href="https://github.com/apache/arrow-nanoarrow">nanoarrow</a>.</p>

<p>Thanks to Fernanda Foertter for assistance with some of the diagrams.</p>]]></content><author><name>pmc</name></author><category term="application" /><summary type="html"><![CDATA[The Arrow community would like to introduce version 1.0.0 of the Arrow Database Connectivity (ADBC) specification. ADBC is a columnar, minimal-overhead alternative to JDBC/ODBC for analytical applications. Or in other words: ADBC is a single API for getting Arrow data in and out of different databases. Motivation Applications often use API standards like JDBC and ODBC to work with databases. That way, they can code to the same API regardless of the underlying database, saving on development time. Roughly speaking, when an application executes a query with these APIs: The query execution flow. The application submits a SQL query via the JDBC/ODBC API. The query is passed on to the driver. The driver translates the query to a database-specific protocol and sends it to the database. The database executes the query and returns the result set in a database-specific format. The driver translates the result into the format required by the JDBC/ODBC API. The application iterates over the result rows using the JDBC/ODBC API. When columnar data comes into play, however, problems arise. JDBC is a row-oriented API, and while ODBC can support columnar data, the type system and data representation is not a perfect match with Arrow. So generally, columnar data must be converted to rows in step 5, spending resources without performing “useful” work. This mismatch is problematic for columnar database systems, such as ClickHouse, Dremio, DuckDB, and Google BigQuery. On the client side, tools such as Apache Spark and pandas would be better off getting columnar data directly, skipping that conversion. Otherwise, they’re leaving performance on the table. At the same time, that conversion isn’t always avoidable. Row-oriented database systems like PostgreSQL aren’t going away, and these clients will still want to consume data from them. Developers have a few options: Just use JDBC/ODBC. These standards are here to stay, and it makes sense for databases to support them for applications that want them. But when both the database and the application are columnar, that means converting data into rows for JDBC/ODBC, only for the client to convert them right back into columns! Performance suffers, and developers have to spend time implementing the conversions. Use JDBC/ODBC-to-Arrow conversion libraries. Libraries like Turbodbc and arrow-jdbc handle row-to-columnar conversions for clients. But this doesn’t fundamentally solve the problem. Unnecessary data conversions are still required. Use vendor-specific protocols. For some databases, applications can use a database-specific protocol or SDK to directly get Arrow data. For example, applications could use Dremio via Arrow Flight SQL. But client applications that want to support multiple database vendors would need to integrate with each of them. (Look at all the connectors that Trino implements.) And databases like PostgreSQL don’t offer an option supporting Arrow in the first place. As is, clients must choose between either tedious integration work or leaving performance on the table. We can make this better. Introducing ADBC ADBC is an Arrow-based, vendor-neutral API for interacting with databases. Applications that use ADBC simply receive Arrow data. They don’t have to do any conversions themselves, and they don’t have to integrate each database’s specific SDK. Just like JDBC/ODBC, underneath the ADBC API are drivers that translate the API for specific databases. A driver for an Arrow-native database just passes Arrow data through without conversion. A driver for a non-Arrow-native database must convert the data to Arrow. This saves the application from doing that, and the driver can optimize the conversion for its database. The query execution flow with two different ADBC drivers. The application submits a SQL query via the ADBC API. The query is passed on to the ADBC driver. The driver translates the query to a database-specific protocol and sends the query to the database. The database executes the query and returns the result set in a database-specific format, which is ideally Arrow data. If needed: the driver translates the result into Arrow data. The application iterates over batches of Arrow data. The application only deals with one API, and only works with Arrow data. ADBC API and driver implementations are under development. For example, in Python, the ADBC packages offer a familiar DBAPI 2.0 (PEP 249)-style interface, with extensions to get Arrow data. We can get Arrow data out of PostgreSQL easily: import adbc_driver_postgresql.dbapi uri = "postgresql://localhost:5432/postgres?user=postgres&amp;password=password" with adbc_driver_postgresql.dbapi.connect(uri) as conn: with conn.cursor() as cur: cur.execute("SELECT * FROM customer") table = cur.fetch_arrow_table() # Process the results Or SQLite: import adbc_driver_sqlite.dbapi uri = "file:mydb.sqlite" with adbc_driver_sqlite.dbapi.connect(uri) as conn: with conn.cursor() as cur: cur.execute("SELECT * FROM customer") table = cur.fetch_arrow_table() # Process the results Note: implementations are still under development. See the documentation for up-to-date examples. What about {Flight SQL, JDBC, ODBC, …}? ADBC fills a specific niche that related projects do not address. It is both: Arrow-native: ADBC can pass through Arrow data with no overhead thanks to the C Data Interface. JDBC is row-oriented, and ODBC has implementation caveats, as discussed, that make it hard to use with Arrow. Vendor-agnostic: ADBC drivers can implement the API using any underlying protocol, while Flight SQL requires server-side support that may not be easy to add. Comparing database APIs and protocols Vendor-neutral (database APIs) Vendor-specific (database protocols) Arrow-native ADBC Arrow Flight SQLBigQuery Storage gRPC protocol Row-oriented JDBCODBC (typically row-oriented) PostgreSQL wire protocolTabular Data Stream (Microsoft SQL Server) ADBC doesn’t intend to replace JDBC or ODBC in general. But for applications that just want bulk columnar data access, ADBC lets them avoid data conversion overhead and tedious integration work. Similarly, within the Arrow project, ADBC does not replace Flight SQL, but instead complements it. ADBC is an API that lets clients work with different databases easily. Meanwhile, Flight SQL is a wire protocol that database servers can implement to simultaneously support ADBC, JDBC, and ODBC users. Getting Involved ADBC works as part of the Arrow ecosystem to “cover the bases” for database interaction: Arrow offers a universal columnar data format, Arrow Flight SQL offers a universal wire protocol for database servers, and ADBC offers a universal API for database clients. To start using ADBC, see the documentation for build instructions and a short tutorial. (A formal release of the packages is still under way.) If you’re interested in learning more or contributing, please reach out on the mailing list or on GitHub Issues. ADBC was only possible with the help and involvement of several Arrow community members and projects. In particular, we would like to thank members of the DuckDB project and the R DBI project, who constructed prototypes based on early revisions of the standard and provided feedback on the design. And ADBC builds on existing Arrow projects, including the Arrow C Data Interface and nanoarrow. Thanks to Fernanda Foertter for assistance with some of the diagrams.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Querying Parquet with Millisecond Latency</title><link href="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/" rel="alternate" type="text/html" title="Querying Parquet with Millisecond Latency" /><published>2022-12-26T00:00:00-05:00</published><updated>2022-12-26T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/"><![CDATA[<!--

-->

<h1 id="querying-parquet-with-millisecond-latency">Querying Parquet with Millisecond Latency</h1>
<p><em>Note: this article was originally published on the <a href="https://www.influxdata.com/blog/querying-parquet-millisecond-latency">InfluxData Blog</a>.</em></p>

<p>We believe that querying data in <a href="https://parquet.apache.org/">Apache Parquet</a> files directly can achieve similar or better storage efficiency and query performance than most specialized file formats. While it requires significant engineering effort, the benefits of Parquet’s open format and broad ecosystem support make it the obvious choice for a wide class of data systems.</p>

<p>In this article we explain several advanced techniques needed to query data stored in the Parquet format quickly that we implemented in the <a href="https://docs.rs/parquet/27.0.0/parquet/">Apache Arrow Rust Parquet reader</a>. Together these techniques make the Rust implementation one of, if not the, fastest implementation for querying Parquet files — be it on local disk or remote object storage. It is able to query GBs of Parquet in a <a href="https://github.com/tustvold/access-log-bench">matter of milliseconds</a>.</p>

<p>We would like to acknowledge and thank <a href="https://www.influxdata.com/">InfluxData</a> for their support of this work. InfluxData has a deep and continuing commitment to Open source software, and it sponsored much of our time for writing this blog post as well as many contributions as part of building the <a href="https://www.influxdata.com/blog/influxdb-engine/">InfluxDB IOx Storage Engine</a>.</p>

<h1 id="background">Background</h1>

<p><a href="https://parquet.apache.org/">Apache Parquet</a> is an increasingly popular open format for storing <a href="https://www.influxdata.com/glossary/olap/">analytic datasets</a>, and has become the de-facto standard for cost-effective, DBMS-agnostic data storage. Initially created for the Hadoop ecosystem, Parquet’s reach now expands broadly across the data analytics ecosystem due to its compelling combination of:</p>

<ul>
  <li>High compression ratios</li>
  <li>Amenability to commodity blob-storage such as S3</li>
  <li>Broad ecosystem and tooling support</li>
  <li>Portability across many different platforms and tools</li>
  <li>Support for <a href="https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/">arbitrarily structured data</a></li>
</ul>

<p>Increasingly other systems, such as <a href="https://duckdb.org/2021/06/25/querying-parquet.html">DuckDB</a> and <a href="https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html#c-spectrum-overview">Redshift</a> allow querying data stored in Parquet directly, but support is still often a secondary consideration compared to their native (custom) file formats. Such formats include the DuckDB <code class="language-plaintext highlighter-rouge">.duckdb</code> file format, the Apache IOT <a href="https://github.com/apache/iotdb/blob/master/tsfile/README.md">TsFile</a>, the <a href="https://www.vldb.org/pvldb/vol8/p1816-teller.pdf">Gorilla format</a>, and others.</p>

<p>For the first time, access to the same sophisticated query techniques, previously only available in closed source commercial implementations, are now available as open source. The required engineering capacity comes from large, well-run open source projects with global contributor communities, such as <a href="https://arrow.apache.org/">Apache Arrow</a> and <a href="https://impala.apache.org/">Apache Impala</a>.</p>

<h1 id="parquet-file-format">Parquet file format</h1>

<p>Before diving into the details of efficiently reading from <a href="https://www.influxdata.com/glossary/apache-parquet/">Parquet</a>, it is important to understand the file layout. The file format is carefully designed to quickly locate the desired information, skip irrelevant portions, and decode what remains efficiently.</p>

<ul>
  <li>The data in a Parquet file is broken into horizontal slices called <code class="language-plaintext highlighter-rouge">RowGroup</code>s</li>
  <li>Each <code class="language-plaintext highlighter-rouge">RowGroup</code> contains a single <code class="language-plaintext highlighter-rouge">ColumnChunk</code> for each column in the schema</li>
</ul>

<p>For example, the following diagram illustrates a Parquet file with three columns “A”, “B” and “C” stored in two <code class="language-plaintext highlighter-rouge">RowGroup</code>s for a total of 6 <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┓          ┃
┃┃┌ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ┐┌ ─ ─ ─ ─ ─ ─  ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃│             │             ││              ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃│             │             ││              ┃ RowGroup ┃
┃┃             │                            │ ┃     1    ┃
┃┃│             │             ││              ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃└ ─ ─ ─ ─ ─ ─ └ ─ ─ ─ ─ ─ ─ ┘└ ─ ─ ─ ─ ─ ─  ┃          ┃
┃┃ColumnChunk 1  ColumnChunk 2 ColumnChunk 3  ┃          ┃
┃┃ (Column "A")   (Column "B")  (Column "C")  ┃          ┃
┃┗━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┛          ┃
┃┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┓          ┃
┃┃┌ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ┐┌ ─ ─ ─ ─ ─ ─  ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃│             │             ││              ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃│             │             ││              ┃ RowGroup ┃
┃┃             │                            │ ┃     2    ┃
┃┃│             │             ││              ┃          ┃
┃┃             │                            │ ┃          ┃
┃┃└ ─ ─ ─ ─ ─ ─ └ ─ ─ ─ ─ ─ ─ ┘└ ─ ─ ─ ─ ─ ─  ┃          ┃
┃┃ColumnChunk 4  ColumnChunk 5 ColumnChunk 6  ┃          ┃
┃┃ (Column "A")   (Column "B")  (Column "C")  ┃          ┃
┃┗━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┛          ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
</code></pre></div></div>

<p>The logical values for a <code class="language-plaintext highlighter-rouge">ColumnChunk</code> are written using one of the many <a href="https://parquet.apache.org/docs/file-format/data-pages/encodings/">available encodings</a> into one or more Data Pages appended sequentially in the file. At the end of a Parquet file is a footer, which contains important metadata, such as:</p>

<ul>
  <li>The file’s schema information such as column names and types</li>
  <li>The locations of the <code class="language-plaintext highlighter-rouge">RowGroup</code> and <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s in the file</li>
</ul>

<p>The footer may also contain other specialized data structures:</p>

<ul>
  <li>Optional statistics for each <code class="language-plaintext highlighter-rouge">ColumnChunk</code> including min/max values and null counts</li>
  <li>Optional pointers to <a href="https://github.com/apache/parquet-format/blob/54e53e5d7794d383529dd30746378f19a12afd58/src/main/thrift/parquet.thrift#L926-L932">OffsetIndexes</a> containing the location of each individual Page</li>
  <li>Optional pointers to <a href="https://github.com/apache/parquet-format/blob/54e53e5d7794d383529dd30746378f19a12afd58/src/main/thrift/parquet.thrift#L938">ColumnIndex</a> containing row counts and summary statistics for each Page</li>
  <li>Optional pointers to <a href="https://github.com/apache/parquet-format/blob/54e53e5d7794d383529dd30746378f19a12afd58/src/main/thrift/parquet.thrift#L621-L630">BloomFilterData</a>, which can quickly check if a value is present in a <code class="language-plaintext highlighter-rouge">ColumnChunk</code></li>
</ul>

<p>For example, the logical structure of 2 Row Groups and 6 <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s in the previous diagram might be stored in a Parquet file as shown in the following diagram (not to scale). The pages for the <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s come first, followed by the footer. The data, the effectiveness of the encoding scheme, and the settings of the Parquet encoder determine the number of and size of the pages needed for each <code class="language-plaintext highlighter-rouge">ColumnChunk</code>. In this case, <code class="language-plaintext highlighter-rouge">ColumnChunk</code> 1 required 2 pages while <code class="language-plaintext highlighter-rouge">ColumnChunk</code> 6 required only 1 page. In addition to other information, the footer contains the locations of each Data Page and the types of the columns.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 1 ("A")             ◀─┃─ ─ ─│
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃     │
┃ Data Page for ColumnChunk 1 ("A")               ┃
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃     │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 2 ("B")               ┃     │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃     │
┃ Data Page for ColumnChunk 3 ("C")               ┃
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃     │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 3 ("C")               ┃     │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃     │
┃ Data Page for ColumnChunk 3 ("C")               ┃
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃     │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 4 ("A")             ◀─┃─ ─ ─│─ ┐
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃     │  │
┃ Data Page for ColumnChunk 5 ("B")               ┃
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃     │  │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 5 ("B")               ┃     │  │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃     │  │
┃ Data Page for ColumnChunk 5 ("B")               ┃
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃     │  │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  ┃
┃ Data Page for ColumnChunk 6 ("C")               ┃     │  │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃     │  │
┃┃Footer                                        ┃ ┃
┃┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃     │  │
┃┃ ┃File Metadata                             ┃ ┃ ┃
┃┃ ┃ Schema, etc                              ┃ ┃ ┃     │  │
┃┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓     ┃ ┃ ┃
┃┃ ┃ ┃Row Group 1 Metadata              ┃     ┃ ┃ ┃     │  │
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓             ┃     ┃ ┃ ┃
┃┃ ┃ ┃┃Column "A" Metadata┃ Location of ┃     ┃ ┃ ┃     │  │
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ first Data  ┣ ─ ─ ╋ ╋ ╋ ─ ─
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ Page, row   ┃     ┃ ┃ ┃        │
┃┃ ┃ ┃┃Column "B" Metadata┃ counts,     ┃     ┃ ┃ ┃
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ sizes,      ┃     ┃ ┃ ┃        │
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ min/max     ┃     ┃ ┃ ┃
┃┃ ┃ ┃┃Column "C" Metadata┃ values, etc ┃     ┃ ┃ ┃        │
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛             ┃     ┃ ┃ ┃
┃┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛     ┃ ┃ ┃        │
┃┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓     ┃ ┃ ┃
┃┃ ┃ ┃Row Group 2 Metadata              ┃     ┃ ┃ ┃        │
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ Location of ┃     ┃ ┃ ┃
┃┃ ┃ ┃┃Column "A" Metadata┃ first Data  ┃     ┃ ┃ ┃        │
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ Page, row   ┣ ─ ─ ╋ ╋ ╋ ─ ─ ─ ─
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ counts,     ┃     ┃ ┃ ┃
┃┃ ┃ ┃┃Column "B" Metadata┃ sizes,      ┃     ┃ ┃ ┃
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ min/max     ┃     ┃ ┃ ┃
┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ values, etc ┃     ┃ ┃ ┃
┃┃ ┃ ┃┃Column "C" Metadata┃             ┃     ┃ ┃ ┃
┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛             ┃     ┃ ┃ ┃
┃┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛     ┃ ┃ ┃
┃┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃
┃┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
</code></pre></div></div>

<p>There are many important criteria to consider when creating Parquet files such as how to optimally order/cluster data and structure it into <code class="language-plaintext highlighter-rouge">RowGroup</code>s and Data Pages. Such “physical design” considerations are complex, worthy of their own series of articles, and not addressed in this blog post. Instead, we focus on how to use the available structure to make queries very fast.</p>

<h1 id="optimizing-queries">Optimizing queries</h1>

<p>In any query processing system, the following techniques generally improve performance:</p>

<ol>
  <li>Reduce the data that must be transferred from secondary storage for processing (reduce I/O)</li>
  <li>Reduce the computational load for decoding the data (reduce CPU)</li>
  <li>Interleave/pipeline the reading and decoding of the data (improve parallelism)</li>
</ol>

<p>The same principles apply to querying Parquet files, as we describe below:</p>

<h1 id="decode-optimization">Decode optimization</h1>

<p>Parquet achieves impressive compression ratios by using <a href="https://parquet.apache.org/docs/file-format/data-pages/encodings/">sophisticated encoding techniques</a> such as run length compression, dictionary encoding, delta encoding, and others. Consequently, the CPU-bound task of decoding can dominate query latency. Parquet readers can use a number of techniques to improve the latency and throughput of this task, as we have done in the Rust implementation.</p>

<h2 id="vectorized-decode">Vectorized decode</h2>

<p>Most analytic systems decode multiple values at a time to a columnar memory format, such as Apache Arrow, rather than processing data row-by-row. This is often called vectorized or columnar processing, and is beneficial because it:</p>

<ul>
  <li>Amortizes dispatch overheads to switch on the type of column being decoded</li>
  <li>Improves cache locality by reading consecutive values from a <code class="language-plaintext highlighter-rouge">ColumnChunk</code></li>
  <li>Often allows multiple values to be decoded in a single instruction.</li>
  <li>Avoid many small heap allocations with a single large allocation, yielding significant savings for variable length types such as strings and byte arrays</li>
</ul>

<p>Thus, Rust Parquet Reader implements specialized decoders for reading Parquet directly into a <a href="https://www.influxdata.com/glossary/column-database/">columnar</a> memory format (Arrow Arrays).</p>

<h2 id="streaming-decode">Streaming decode</h2>

<p>There is no relationship between which rows are stored in which Pages across <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s. For example, the logical values for the 10,000th row may be in the first page of column A and in the third page of column B.</p>

<p>The simplest approach to vectorized decoding, and the one often initially implemented in Parquet decoders, is to decode an entire <code class="language-plaintext highlighter-rouge">RowGroup</code> (or <code class="language-plaintext highlighter-rouge">ColumnChunk</code>) at a time.</p>

<p>However, given Parquet’s high compression ratios, a single <code class="language-plaintext highlighter-rouge">RowGroup</code> may well contain millions of rows. Decoding so many rows at once is non-optimal because it:</p>

<ul>
  <li><strong>Requires large amounts of intermediate RAM</strong>: typical in-memory formats optimized for processing, such as Apache Arrow, require much more than their Parquet-encoded form.</li>
  <li><strong>Increases query latency</strong>: Subsequent processing steps (like filtering or aggregation) can only begin once the entire <code class="language-plaintext highlighter-rouge">RowGroup</code> (or <code class="language-plaintext highlighter-rouge">ColumnChunk</code>) is decoded.</li>
</ul>

<p>As such, the best Parquet readers support “streaming” data out in by producing configurable sized batches of rows on demand. The batch size must be large enough to amortize decode overhead, but small enough for efficient memory usage and to allow downstream processing to begin concurrently while the subsequent batch is decoded.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃
┃ Data Page for ColumnChunk 1 │◀┃─                   ┌── ─── ─── ─── ─── ┐
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃ │   ┏━━━━━━━┓        ┌ ─ ┐ ┌ ─ ┐ ┌ ─ ┐ │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃     ┃       ┃      │                   │
┃ Data Page for ColumnChunk 1 │ ┃ │   ┃       ┃   ─ ▶│ │   │ │   │ │   │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃  ─ ─┃       ┃─ ┤   │  ─ ─   ─ ─   ─ ─  │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃ │   ┃       ┃           A    B     C   │
┃ Data Page for ColumnChunk 2 │◀┃─    ┗━━━━━━━┛  │   └── ─── ─── ─── ─── ┘
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃ │    Parquet
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃      Decoder   │            ...
┃ Data Page for ColumnChunk 3 │ ┃ │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃                │   ┌── ─── ─── ─── ─── ┐
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃ │                    ┌ ─ ┐ ┌ ─ ┐ ┌ ─ ┐ │
┃ Data Page for ColumnChunk 3 │◀┃─               │   │                   │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃                 ─ ▶│ │   │ │   │ │   │
┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃                    │  ─ ─   ─ ─   ─ ─  │
┃ Data Page for ColumnChunk 3 │ ┃                         A    B     C   │
┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ┃                    └── ─── ─── ─── ─── ┘
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

      Parquet file                                    Smaller in memory
                                                         batches for
                                                         processing
</code></pre></div></div>

<p>While streaming is not a complicated feature to explain, the stateful nature of decoding, especially across multiple columns and <a href="https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/">arbitrarily nested data</a>, where the relationship between rows and values is not fixed, requires <a href="https://github.com/apache/arrow-rs/blob/b7af85cb8dfe6887bb3fd43d1d76f659473b6927/parquet/src/arrow/record_reader/mod.rs">complex intermediate buffering</a> and significant engineering effort to handle correctly.</p>

<h2 id="dictionary-preservation">Dictionary preservation</h2>

<p>Dictionary Encoding, also called <a href="https://pandas.pydata.org/docs/user_guide/categorical.html">categorical</a> encoding, is a technique where each value in a column is not stored directly, but instead, an index in a separate list called a “Dictionary” is stored. This technique achieves many of the benefits of <a href="https://en.wikipedia.org/wiki/Third_normal_form#:~:text=Third%20normal%20form%20(3NF)%20is,in%201971%20by%20Edgar%20F.">third normal form</a> for columns that have repeated values (low <a href="https://www.influxdata.com/glossary/cardinality/">cardinality</a>) and is especially effective for columns of strings such as “City”.</p>

<p>The first page in a <code class="language-plaintext highlighter-rouge">ColumnChunk</code> can optionally be a dictionary page, containing a list of values of the column’s type. Subsequent pages within this <code class="language-plaintext highlighter-rouge">ColumnChunk</code> can then encode an index into this dictionary, instead of encoding the values directly.</p>

<p>Given the effectiveness of this encoding, if a Parquet decoder simply decodes dictionary data into the native type, it will inefficiently replicate the same value over and over again, which is especially disastrous for string data. To handle dictionary-encoded data efficiently, the encoding must be preserved during decode. Conveniently, many columnar formats, such as the Arrow <a href="https://docs.rs/arrow/27.0.0/arrow/array/struct.DictionaryArray.html">DictionaryArray</a>, support such compatible encodings.</p>

<p>Preserving dictionary encoding drastically improves performance when reading to an Arrow array, in some cases in excess of <a href="https://github.com/apache/arrow-rs/pull/1180">60x</a>, as well as using significantly less memory.</p>

<p>The major complicating factor for preserving dictionaries is that the dictionaries are stored per <code class="language-plaintext highlighter-rouge">ColumnChunk</code>, and therefore the dictionary changes between <code class="language-plaintext highlighter-rouge">RowGroup</code>s. The reader must automatically recompute a dictionary for batches that span multiple <code class="language-plaintext highlighter-rouge">RowGroup</code>s, while also optimizing for the case that batch sizes divide evenly into the number of rows per <code class="language-plaintext highlighter-rouge">RowGroup</code>. Additionally a column may be only <a href="https://github.com/apache/parquet-format/blob/111dbdcf8eff2e9f8e0d4e958cecbc7e00028aca/README.md?plain=1#L194-L199">partly dictionary encoded</a>, further complicating implementation. More information on this technique and its complications can be found in the <a href="https://arrow.apache.org/blog/2019/09/05/faster-strings-cpp-parquet/">blog post</a> on applying this technique to the C++ Parquet reader.</p>

<h1 id="projection-pushdown">Projection pushdown</h1>

<p>The most basic Parquet optimization, and the one most commonly described for Parquet files, is <em>projection pushdown</em>, which reduces both I/Oand CPU requirements. Projection in this context means “selecting some but not all of the columns.” Given how Parquet organizes data, it is straightforward to read and decode only the <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s required for the referenced columns.</p>

<p>For example, consider a SQL query of the form</p>

<pre><code class="language-SQL">SELECT B from table where A &gt; 35
</code></pre>

<p>This query only needs data for columns A and B (and not C) and the projection can be “pushed down” to the Parquet reader.</p>

<p>Specifically, using the information in the footer, the Parquet reader can entirely skip fetching (I/O) and decoding (CPU) the Data Pages that store data for column C (<code class="language-plaintext highlighter-rouge">ColumnChunk</code> 3 and <code class="language-plaintext highlighter-rouge">ColumnChunk</code> 6 in our example).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                             ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
                             ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ┌─────▶ Data Page for ColumnChunk 1 ("A") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ├─────▶ Data Page for ColumnChunk 1 ("A") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ├─────▶ Data Page for ColumnChunk 2 ("B") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       │     ┃ Data Page for ColumnChunk 3 ("C") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
   A query that        │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
  accesses only        │     ┃ Data Page for ColumnChunk 3 ("C") ┃
 columns A and B       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
can read only the      │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
 relevant pages,  ─────┤     ┃ Data Page for ColumnChunk 3 ("C") ┃
skipping any Data      │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
Page for column C      │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ├─────▶ Data Page for ColumnChunk 4 ("A") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ├─────▶ Data Page for ColumnChunk 5 ("B") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       ├─────▶ Data Page for ColumnChunk 5 ("B") ┃
                       │     ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                       │     ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                       └─────▶ Data Page for ColumnChunk 5 ("B") ┃
                             ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                             ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃
                             ┃ Data Page for ColumnChunk 6 ("C") ┃
                             ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃
                             ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
</code></pre></div></div>

<h1 id="predicate-pushdown">Predicate pushdown</h1>

<p>Similar to projection pushdown, <strong>predicate</strong> pushdown also avoids fetching and decoding data from Parquet files, but does so using filter expressions. This technique typically requires closer integration with a query engine such as <a href="https://arrow.apache.org/datafusion/">DataFusion</a>, to determine valid predicates and evaluate them during the scan. Unfortunately without careful API design, the Parquet decoder and query engine can end up tightly coupled, preventing reuse (e.g. there are different Impala and Spark implementations in <a href="https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/cdh_ig_predicate_pushdown_parquet.html#concept_pgs_plb_mgb">Cloudera Parquet Predicate Pushdown docs</a>). The Rust Parquet reader uses the <a href="https://docs.rs/parquet/27.0.0/parquet/arrow/arrow_reader/struct.RowSelector.html">RowSelection</a> API to avoid this coupling.</p>

<h2 id="rowgroup-pruning"><code class="language-plaintext highlighter-rouge">RowGroup</code> pruning</h2>

<p>The simplest form of predicate pushdown, supported by many Parquet based query engines, uses the statistics stored in the footer to skip entire <code class="language-plaintext highlighter-rouge">RowGroup</code>s. We call this operation <code class="language-plaintext highlighter-rouge">RowGroup</code> <em>pruning</em>, and it is analogous to <a href="https://docs.oracle.com/database/121/VLDBG/GUID-E677C85E-C5E3-4927-B3DF-684007A7B05D.htm#VLDBG00401">partition pruning</a> in many classical data warehouse systems.</p>

<p>For the example query above, if the maximum value for A in a particular <code class="language-plaintext highlighter-rouge">RowGroup</code> is less than 35, the decoder can skip fetching and decoding any <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s from that <strong>entire</strong> <code class="language-plaintext highlighter-rouge">RowGroup</code>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃Row Group 1 Metadata                      ┃
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃
┃ ┃Column "A" Metadata    Min:0 Max:15   ┃◀╋ ┐
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃       Using the min
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ │     and max values
┃ ┃Column "B" Metadata                   ┃ ┃       from the
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ │     metadata,
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃       RowGroup 1  can
┃ ┃Column "C" Metadata                   ┃ ┃ ├ ─ ─ be entirely
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃       skipped
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │     (pruned) when
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓       searching for
┃Row Group 2 Metadata                      ┃ │     rows with A &gt;
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃       35,
┃ ┃Column "A" Metadata   Min:10 Max:50   ┃◀╋ ┘
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃
┃ ┃Column "B" Metadata                   ┃ ┃
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃
┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃
┃ ┃Column "C" Metadata                   ┃ ┃
┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
</code></pre></div></div>

<p>Note that pruning on minimum and maximum values is effective for many data layouts and column types, but not all. Specifically, it is not as effective for columns with many distinct pseudo-random values (e.g. identifiers or uuids). Thankfully for this use case, Parquet also supports per <code class="language-plaintext highlighter-rouge">ColumnChunk</code> <a href="https://github.com/apache/parquet-format/blob/master/BloomFilter.md">Bloom Filters</a>. We are actively working on <a href="https://github.com/apache/arrow-rs/issues/3023">adding bloom filter</a> support in Apache Rust’s implementation.</p>

<h2 id="page-pruning">Page pruning</h2>

<p>A more sophisticated form of predicate pushdown uses the optional <a href="https://github.com/apache/parquet-format/blob/master/PageIndex.md">page index</a> in the footer metadata to rule out entire Data Pages. The decoder decodes only the corresponding rows from other columns, often skipping entire pages.</p>

<p>The fact that pages in different <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s often contain different numbers of rows, due to various reasons, complicates this optimization. While the page index may identify the needed pages from one column, pruning a page from one column doesn’t immediately rule out entire pages in other columns.</p>

<p>Page pruning proceeds as follows:</p>

<ul>
  <li>Uses the predicates in combination with the page index to identify pages to skip</li>
  <li>Uses the offset index to determine what row ranges correspond to non-skipped pages</li>
  <li>Computes the intersection of ranges across non-skipped pages, and decodes only those rows</li>
</ul>

<p>This last point is highly non-trivial to implement, especially for nested lists where <a href="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/">a single row may correspond to multiple values</a>. Fortunately, the Rust Parquet reader hides this complexity internally, and can decode arbitrary <a href="https://docs.rs/parquet/27.0.0/parquet/arrow/arrow_reader/struct.RowSelection.html">RowSelections</a>.</p>

<p>For example, to scan Columns A and B, stored in 5 Data Pages as shown in the figure below:</p>

<p>If the predicate is <code class="language-plaintext highlighter-rouge">A &gt; 35</code>,</p>

<ul>
  <li>Page 1 is pruned using the page index (max value is <code class="language-plaintext highlighter-rouge">20</code>), leaving a RowSelection of  [200-&gt;onwards],</li>
  <li>Parquet reader skips Page 3 entirely (as its last row index is <code class="language-plaintext highlighter-rouge">99</code>)</li>
  <li>(Only) the relevant rows are read by reading pages 2, 4, and 5.</li>
</ul>

<p>If the predicate is instead <code class="language-plaintext highlighter-rouge">A &gt; 35 AND B = "F"</code> the page index is even more effective</p>

<ul>
  <li>Using <code class="language-plaintext highlighter-rouge">A &gt; 35</code>, yields a RowSelection of <code class="language-plaintext highlighter-rouge">[200-&gt;onwards]</code> as before</li>
  <li>Using <code class="language-plaintext highlighter-rouge">B = "F"</code>, on the remaining Page 4 and Page 5 of B, yields a RowSelection of <code class="language-plaintext highlighter-rouge">[100-244]</code></li>
  <li>Intersecting the two RowSelections leaves a combined RowSelection <code class="language-plaintext highlighter-rouge">[200-244]</code></li>
  <li>Parquet reader only decodes those 50 rows from Page 2 and Page 4.</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━
   ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   ┃
┃     ┌──────────────┐  │     ┌──────────────┐  │  ┃
┃  │  │              │     │  │              │     ┃
┃     │              │  │     │     Page     │  │
   │  │              │     │  │      3       │     ┃
┃     │              │  │     │   min: "A"   │  │  ┃
┃  │  │              │     │  │   max: "C"   │     ┃
┃     │     Page     │  │     │ first_row: 0 │  │
   │  │      1       │     │  │              │     ┃
┃     │   min: 10    │  │     └──────────────┘  │  ┃
┃  │  │   max: 20    │     │  ┌──────────────┐     ┃
┃     │ first_row: 0 │  │     │              │  │
   │  │              │     │  │     Page     │     ┃
┃     │              │  │     │      4       │  │  ┃
┃  │  │              │     │  │   min: "D"   │     ┃
┃     │              │  │     │   max: "G"   │  │
   │  │              │     │  │first_row: 100│     ┃
┃     └──────────────┘  │     │              │  │  ┃
┃  │  ┌──────────────┐     │  │              │     ┃
┃     │              │  │     └──────────────┘  │
   │  │     Page     │     │  ┌──────────────┐     ┃
┃     │      2       │  │     │              │  │  ┃
┃  │  │   min: 30    │     │  │     Page     │     ┃
┃     │   max: 40    │  │     │      5       │  │
   │  │first_row: 200│     │  │   min: "H"   │     ┃
┃     │              │  │     │   max: "Z"   │  │  ┃
┃  │  │              │     │  │first_row: 250│     ┃
┃     └──────────────┘  │     │              │  │
   │                       │  └──────────────┘     ┃
┃   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘  ┃
┃       ColumnChunk            ColumnChunk         ┃
┃            A                      B
 ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━┛
</code></pre></div></div>

<p>Support for reading and writing these indexes from Arrow C++, and by extension pyarrow/pandas, is tracked in <a href="https://issues.apache.org/jira/browse/PARQUET-1404">PARQUET-1404</a>.</p>

<h2 id="late-materialization">Late materialization</h2>

<p>The two previous forms of predicate pushdown only operated on metadata stored for <code class="language-plaintext highlighter-rouge">RowGroup</code>s, <code class="language-plaintext highlighter-rouge">ColumnChunk</code>s, and Data Pages prior to decoding values. However, the same techniques also extend to values of one or more columns <em>after</em> decoding them but prior to decoding other columns, which is often called “late materialization”.</p>

<p>This technique is especially effective when:</p>

<ul>
  <li>The predicate is very selective, i.e. filters out large numbers of rows</li>
  <li>Each row is large, either due to wide rows (e.g. JSON blobs) or many columns</li>
  <li>The selected data is clustered together</li>
  <li>The columns required by the predicate are relatively inexpensive to decode, e.g. PrimitiveArray / DictionaryArray</li>
</ul>

<p>There is additional discussion about the benefits of this technique in <a href="https://issues.apache.org/jira/browse/SPARK-36527">SPARK-36527</a> and<a href="https://docs.cloudera.com/cdw-runtime/cloud/impala-reference/topics/impala-lazy-materialization.html"> Impala</a>.</p>

<p>For example, given the predicate <code class="language-plaintext highlighter-rouge">A &gt; 35 AND B = "F"</code> from above where the engine uses the page index to determine only 50 rows within RowSelection of [100-244] could match, using late materialization, the Parquet decoder:</p>

<ul>
  <li>Decodes the 50 values of Column A</li>
  <li>Evaluates  <code class="language-plaintext highlighter-rouge">A &gt; 35 </code> on those 50 values</li>
  <li>In this case, only 5 rows pass, resulting in the RowSelection:
    <ul>
      <li>RowSelection[205-206]</li>
      <li>RowSelection[238-240]</li>
    </ul>
  </li>
  <li>Only decodes the 5 rows for Column B for those selections</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  Row Index
             ┌────────────────────┐            ┌────────────────────┐
       200   │         30         │            │        "F"         │
             └────────────────────┘            └────────────────────┘
                      ...                               ...
             ┌────────────────────┐            ┌────────────────────┐
       205   │         37         │─ ─ ─ ─ ─ ─▶│        "F"         │
             ├────────────────────┤            ├────────────────────┤
       206   │         36         │─ ─ ─ ─ ─ ─▶│        "G"         │
             └────────────────────┘            └────────────────────┘
                      ...                               ...
             ┌────────────────────┐            ┌────────────────────┐
       238   │         36         │─ ─ ─ ─ ─ ─▶│        "F"         │
             ├────────────────────┤            ├────────────────────┤
       239   │         36         │─ ─ ─ ─ ─ ─▶│        "G"         │
             ├────────────────────┤            ├────────────────────┤
       240   │         40         │─ ─ ─ ─ ─ ─▶│        "G"         │
             └────────────────────┘            └────────────────────┘
                      ...                               ...
             ┌────────────────────┐            ┌────────────────────┐
      244    │         26         │            │        "D"         │
             └────────────────────┘            └────────────────────┘


                   Column A                          Column B
                    Values                            Values
</code></pre></div></div>

<p>In certain cases, such as our example where B stores single character values, the cost of late materialization machinery can outweigh the savings in decoding. However, the savings can be substantial when some of the conditions listed above are fulfilled. The query engine must decide which predicates to push down and in which order to apply them for optimal results.</p>

<p>While it is outside the scope of this document, the same technique can be applied for multiple predicates as well as predicates on multiple columns. See the <a href="https://docs.rs/parquet/latest/parquet/arrow/arrow_reader/struct.RowFilter.html">RowFilter</a> interface in the Parquet crate for more information, and the <a href="https://github.com/apache/arrow-datafusion/blob/58b43f5c0b629be49a3efa0e37052ec51d9ba3fe/datafusion/core/src/physical_plan/file_format/parquet/row_filter.rs#L40-L70">row_filter</a> implementation in DataFusion.</p>

<h1 id="io-pushdown">I/O pushdown</h1>

<p>While Parquet was designed for efficient access on the <a href="https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html">HDFS distributed file system</a>, it works very well with commodity blob storage systems such as AWS S3 as they have very similar characteristics:</p>

<ul>
  <li><strong>Relatively slow “random access” reads</strong>: it is much more efficient to read large (MBs) sections of data in each request than issue many requests for smaller portions</li>
  <li>**Significant latency before retrieving the first byte **</li>
  <li>**High per-request cost: **Often billed per request, regardless of number of bytes read, which incentivizes fewer requests that each read a large contiguous section of data.</li>
</ul>

<p>To read optimally from such systems, a Parquet reader must:</p>

<ol>
  <li>Minimize the number of I/O requests, while also applying the various pushdown techniques to avoid fetching large amounts of unused data.</li>
  <li>Integrate with the appropriate task scheduling mechanism to interleave I/O and processing on the data that is fetched to avoid pipeline bottlenecks.</li>
</ol>

<p>As these are substantial engineering and integration challenges, many Parquet readers still require the files to be fetched in their entirety to local storage.</p>

<p>Fetching the entire files in order to process them is not ideal for several reasons:</p>

<ol>
  <li><strong>High Latency</strong>: Decoding cannot begin until the entire file is fetched (Parquet metadata is at the end of the file, so the decoder must see the end prior to decoding the rest)</li>
  <li><strong>Wasted work</strong>: Fetching the entire file fetches all necessary data, but also potentially lots of unnecessary data that will be skipped after reading the footer. This increases the cost unnecessarily.</li>
  <li><strong>Requires costly “locally attached” storage (or memory)</strong>: Many cloud environments do not offer computing resources with locally attached storage – they either rely on expensive network block storage such as AWS EBS or else restrict local storage to certain classes of VMs.</li>
</ol>

<p>Avoiding the need to buffer the entire file requires a sophisticated Parquet decoder, integrated with the I/O subsystem, that can initially fetch and decode the metadata followed by ranged fetches for the relevant data blocks, interleaved with the decoding of Parquet data. This optimization requires careful engineering to fetch large enough blocks of data from the object store that the per request overhead doesn’t dominate gains from reducing the bytes transferred. <a href="https://issues.apache.org/jira/browse/SPARK-36529">SPARK-36529</a> describes the challenges of sequential processing in more detail.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                       ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
                                                                │
                       │
               Step 1: Fetch                                    │
 Parquet       Parquet metadata
 file on ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━▼━━━━━━━┓
 Remote  ┃      ▒▒▒▒▒▒▒▒▒▒          ▒▒▒▒▒▒▒▒▒▒               ░░░░░░░░░░ ┃
 Object  ┃      ▒▒▒data▒▒▒          ▒▒▒data▒▒▒               ░metadata░ ┃
  Store  ┃      ▒▒▒▒▒▒▒▒▒▒          ▒▒▒▒▒▒▒▒▒▒               ░░░░░░░░░░ ┃
         ┗━━━━━━━━━━━▲━━━━━━━━━━━━━━━━━━━━━▲━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
                     │                     └ ─ ─ ─
                                                  │
                     │                   Step 2: Fetch only
                      ─ ─ ─ ─ ─ ─ ─ ─ ─ relevant data blocks
</code></pre></div></div>

<p>Not included in this diagram picture are details like coalescing requests and ensuring minimum request sizes needed for an actual implementation.</p>

<p>The Rust Parquet crate provides an async Parquet reader, to efficiently read from any <a href="https://docs.rs/parquet/latest/parquet/arrow/async_reader/trait.AsyncFileReader.html">AsyncFileReader</a> that:</p>

<ul>
  <li>Efficiently reads from any storage medium that supports range requests</li>
  <li>Integrates with Rust’s futures ecosystem to avoid blocking threads waiting on network I/O <a href="https://www.influxdata.com/blog/using-rustlangs-async-tokio-runtime-for-cpu-bound-tasks/">and easily can interleave CPU and network </a></li>
  <li>Requests multiple ranges simultaneously, to allow the implementation to coalesce adjacent ranges, fetch ranges in parallel, etc.</li>
  <li>Uses the pushdown techniques described previously to eliminate fetching data where possible</li>
  <li>Integrates easily with the Apache Arrow <a href="https://docs.rs/object_store/latest/object_store/">object_store</a> crate which you can read more about <a href="https://www.influxdata.com/blog/rust-object-store-donation/">here</a></li>
</ul>

<p>To give a sense of what is possible, the following picture shows a timeline of fetching the footer metadata from remote files, using that metadata to determine what Data Pages to read, and then fetching data and decoding simultaneously. This process often must be done for more than one file at a time in order to match network latency, bandwidth, and available CPU.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                           begin
          metadata        read of   end read
            read  ─ ─ ─ ┐   data    of data          │
 begin    complete         block     block
read of                 │   │        │               │
metadata  ─ ─ ─ ┐                                       At any time, there are
             │          │   │        │               │     multiple network
             │  ▼       ▼   ▼        ▼                  requests outstanding to
  file 1     │ ░░░░░░░░░░   ▒▒▒read▒▒▒   ▒▒▒read▒▒▒  │    hide the individual
             │ ░░░read░░░   ▒▒▒data▒▒▒   ▒▒▒data▒▒▒        request latency
             │ ░metadata░                         ▓▓decode▓▓
             │ ░░░░░░░░░░                         ▓▓▓data▓▓▓
             │                                       │
             │
             │ ░░░░░░░░░░  ▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒read▒▒▒▒│▒▒▒▒▒▒▒▒▒▒▒▒▒▒
   file 2    │ ░░░read░░░  ▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒data▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒
             │ ░metadata░                            │              ▓▓▓▓▓decode▓▓▓▓▓▓
             │ ░░░░░░░░░░                                           ▓▓▓▓▓▓data▓▓▓▓▓▓▓
             │                                       │
             │
             │                                     ░░│░░░░░░░  ▒▒▒read▒▒▒  ▒▒▒▒read▒▒▒▒▒
   file 3    │                                     ░░░read░░░  ▒▒▒data▒▒▒  ▒▒▒▒data▒▒▒▒▒      ...
             │                                     ░m│tadata░            ▓▓decode▓▓
             │                                     ░░░░░░░░░░            ▓▓▓data▓▓▓
             └───────────────────────────────────────┼──────────────────────────────▶Time


                                                     │
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>We hope you enjoyed reading about the Parquet file format, and the various techniques used to quickly query parquet files.</p>

<p>We believe that the reason most open source implementations of Parquet do not have the breadth of features described in this post is that it takes a monumental effort, that was previously only possible at well-financed commercial enterprises which kept their implementations closed source.</p>

<p>However, with the growth and quality of the Apache Arrow community, both Rust practitioners and the wider Arrow community, our ability to collaborate and build a cutting-edge open source implementation is exhilarating and immensely satisfying. The technology described in this blog is the result of the contributions of many engineers spread across companies, hobbyists, and the world in several repositories, notably <a href="https://github.com/apache/arrow-datafusion">Apache Arrow DataFusion</a>, <a href="https://github.com/apache/arrow-rs">Apache Arrow</a> and <a href="https://github.com/apache/arrow-ballista">Apache Arrow Ballista.</a></p>

<p>If you are interested in joining the DataFusion Community, please <a href="https://arrow.apache.org/datafusion/contributor-guide/communication.html">get in touch</a>.</p>]]></content><author><name>tustvold and alamb</name></author><category term="parquet" /><summary type="html"><![CDATA[Querying Parquet with Millisecond Latency Note: this article was originally published on the InfluxData Blog. We believe that querying data in Apache Parquet files directly can achieve similar or better storage efficiency and query performance than most specialized file formats. While it requires significant engineering effort, the benefits of Parquet’s open format and broad ecosystem support make it the obvious choice for a wide class of data systems. In this article we explain several advanced techniques needed to query data stored in the Parquet format quickly that we implemented in the Apache Arrow Rust Parquet reader. Together these techniques make the Rust implementation one of, if not the, fastest implementation for querying Parquet files — be it on local disk or remote object storage. It is able to query GBs of Parquet in a matter of milliseconds. We would like to acknowledge and thank InfluxData for their support of this work. InfluxData has a deep and continuing commitment to Open source software, and it sponsored much of our time for writing this blog post as well as many contributions as part of building the InfluxDB IOx Storage Engine. Background Apache Parquet is an increasingly popular open format for storing analytic datasets, and has become the de-facto standard for cost-effective, DBMS-agnostic data storage. Initially created for the Hadoop ecosystem, Parquet’s reach now expands broadly across the data analytics ecosystem due to its compelling combination of: High compression ratios Amenability to commodity blob-storage such as S3 Broad ecosystem and tooling support Portability across many different platforms and tools Support for arbitrarily structured data Increasingly other systems, such as DuckDB and Redshift allow querying data stored in Parquet directly, but support is still often a secondary consideration compared to their native (custom) file formats. Such formats include the DuckDB .duckdb file format, the Apache IOT TsFile, the Gorilla format, and others. For the first time, access to the same sophisticated query techniques, previously only available in closed source commercial implementations, are now available as open source. The required engineering capacity comes from large, well-run open source projects with global contributor communities, such as Apache Arrow and Apache Impala. Parquet file format Before diving into the details of efficiently reading from Parquet, it is important to understand the file layout. The file format is carefully designed to quickly locate the desired information, skip irrelevant portions, and decode what remains efficiently. The data in a Parquet file is broken into horizontal slices called RowGroups Each RowGroup contains a single ColumnChunk for each column in the schema For example, the following diagram illustrates a Parquet file with three columns “A”, “B” and “C” stored in two RowGroups for a total of 6 ColumnChunks. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┓ ┃ ┃┃┌ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ┐┌ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ RowGroup ┃ ┃┃ │ │ ┃ 1 ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃└ ─ ─ ─ ─ ─ ─ └ ─ ─ ─ ─ ─ ─ ┘└ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ColumnChunk 1 ColumnChunk 2 ColumnChunk 3 ┃ ┃ ┃┃ (Column "A") (Column "B") (Column "C") ┃ ┃ ┃┗━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┛ ┃ ┃┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┓ ┃ ┃┃┌ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ┐┌ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃│ │ ││ ┃ RowGroup ┃ ┃┃ │ │ ┃ 2 ┃ ┃┃│ │ ││ ┃ ┃ ┃┃ │ │ ┃ ┃ ┃┃└ ─ ─ ─ ─ ─ ─ └ ─ ─ ─ ─ ─ ─ ┘└ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃┃ColumnChunk 4 ColumnChunk 5 ColumnChunk 6 ┃ ┃ ┃┃ (Column "A") (Column "B") (Column "C") ┃ ┃ ┃┗━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━┛ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ The logical values for a ColumnChunk are written using one of the many available encodings into one or more Data Pages appended sequentially in the file. At the end of a Parquet file is a footer, which contains important metadata, such as: The file’s schema information such as column names and types The locations of the RowGroup and ColumnChunks in the file The footer may also contain other specialized data structures: Optional statistics for each ColumnChunk including min/max values and null counts Optional pointers to OffsetIndexes containing the location of each individual Page Optional pointers to ColumnIndex containing row counts and summary statistics for each Page Optional pointers to BloomFilterData, which can quickly check if a value is present in a ColumnChunk For example, the logical structure of 2 Row Groups and 6 ColumnChunks in the previous diagram might be stored in a Parquet file as shown in the following diagram (not to scale). The pages for the ColumnChunks come first, followed by the footer. The data, the effectiveness of the encoding scheme, and the settings of the Parquet encoder determine the number of and size of the pages needed for each ColumnChunk. In this case, ColumnChunk 1 required 2 pages while ColumnChunk 6 required only 1 page. In addition to other information, the footer contains the locations of each Data Page and the types of the columns. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 1 ("A") ◀─┃─ ─ ─│ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ ┃ Data Page for ColumnChunk 1 ("A") ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 2 ("B") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ ┃ Data Page for ColumnChunk 3 ("C") ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 3 ("C") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ ┃ Data Page for ColumnChunk 3 ("C") ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 4 ("A") ◀─┃─ ─ ─│─ ┐ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ │ ┃ Data Page for ColumnChunk 5 ("B") ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 5 ("B") ┃ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ │ │ ┃ Data Page for ColumnChunk 5 ("B") ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ │ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┃ ┃ Data Page for ColumnChunk 6 ("C") ┃ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ │ │ ┃┃Footer ┃ ┃ ┃┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ │ │ ┃┃ ┃File Metadata ┃ ┃ ┃ ┃┃ ┃ Schema, etc ┃ ┃ ┃ │ │ ┃┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃ ┃┃ ┃ ┃Row Group 1 Metadata ┃ ┃ ┃ ┃ │ │ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column "A" Metadata┃ Location of ┃ ┃ ┃ ┃ │ │ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ first Data ┣ ─ ─ ╋ ╋ ╋ ─ ─ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ Page, row ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┃Column "B" Metadata┃ counts, ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ sizes, ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ min/max ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column "C" Metadata┃ values, etc ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ ┃ ┃┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ │ ┃┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃ ┃┃ ┃ ┃Row Group 2 Metadata ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ Location of ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column "A" Metadata┃ first Data ┃ ┃ ┃ ┃ │ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ Page, row ┣ ─ ─ ╋ ╋ ╋ ─ ─ ─ ─ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ counts, ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column "B" Metadata┃ sizes, ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ min/max ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┏━━━━━━━━━━━━━━━━━━━┓ values, etc ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┃Column "C" Metadata┃ ┃ ┃ ┃ ┃ ┃┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ ┃ ┃┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃ ┃┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┃┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ There are many important criteria to consider when creating Parquet files such as how to optimally order/cluster data and structure it into RowGroups and Data Pages. Such “physical design” considerations are complex, worthy of their own series of articles, and not addressed in this blog post. Instead, we focus on how to use the available structure to make queries very fast. Optimizing queries In any query processing system, the following techniques generally improve performance: Reduce the data that must be transferred from secondary storage for processing (reduce I/O) Reduce the computational load for decoding the data (reduce CPU) Interleave/pipeline the reading and decoding of the data (improve parallelism) The same principles apply to querying Parquet files, as we describe below: Decode optimization Parquet achieves impressive compression ratios by using sophisticated encoding techniques such as run length compression, dictionary encoding, delta encoding, and others. Consequently, the CPU-bound task of decoding can dominate query latency. Parquet readers can use a number of techniques to improve the latency and throughput of this task, as we have done in the Rust implementation. Vectorized decode Most analytic systems decode multiple values at a time to a columnar memory format, such as Apache Arrow, rather than processing data row-by-row. This is often called vectorized or columnar processing, and is beneficial because it: Amortizes dispatch overheads to switch on the type of column being decoded Improves cache locality by reading consecutive values from a ColumnChunk Often allows multiple values to be decoded in a single instruction. Avoid many small heap allocations with a single large allocation, yielding significant savings for variable length types such as strings and byte arrays Thus, Rust Parquet Reader implements specialized decoders for reading Parquet directly into a columnar memory format (Arrow Arrays). Streaming decode There is no relationship between which rows are stored in which Pages across ColumnChunks. For example, the logical values for the 10,000th row may be in the first page of column A and in the third page of column B. The simplest approach to vectorized decoding, and the one often initially implemented in Parquet decoders, is to decode an entire RowGroup (or ColumnChunk) at a time. However, given Parquet’s high compression ratios, a single RowGroup may well contain millions of rows. Decoding so many rows at once is non-optimal because it: Requires large amounts of intermediate RAM: typical in-memory formats optimized for processing, such as Apache Arrow, require much more than their Parquet-encoded form. Increases query latency: Subsequent processing steps (like filtering or aggregation) can only begin once the entire RowGroup (or ColumnChunk) is decoded. As such, the best Parquet readers support “streaming” data out in by producing configurable sized batches of rows on demand. The batch size must be large enough to amortize decode overhead, but small enough for efficient memory usage and to allow downstream processing to begin concurrently while the subsequent batch is decoded. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ┃ Data Page for ColumnChunk 1 │◀┃─ ┌── ─── ─── ─── ─── ┐ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┏━━━━━━━┓ ┌ ─ ┐ ┌ ─ ┐ ┌ ─ ┐ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ┃ ┃ │ │ ┃ Data Page for ColumnChunk 1 │ ┃ │ ┃ ┃ ─ ▶│ │ │ │ │ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ─ ─┃ ┃─ ┤ │ ─ ─ ─ ─ ─ ─ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┃ ┃ A B C │ ┃ Data Page for ColumnChunk 2 │◀┃─ ┗━━━━━━━┛ │ └── ─── ─── ─── ─── ┘ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ Parquet ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ Decoder │ ... ┃ Data Page for ColumnChunk 3 │ ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┌── ─── ─── ─── ─── ┐ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ┌ ─ ┐ ┌ ─ ┐ ┌ ─ ┐ │ ┃ Data Page for ColumnChunk 3 │◀┃─ │ │ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ─ ▶│ │ │ │ │ │ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ │ ─ ─ ─ ─ ─ ─ │ ┃ Data Page for ColumnChunk 3 │ ┃ A B C │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ └── ─── ─── ─── ─── ┘ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ Parquet file Smaller in memory batches for processing While streaming is not a complicated feature to explain, the stateful nature of decoding, especially across multiple columns and arbitrarily nested data, where the relationship between rows and values is not fixed, requires complex intermediate buffering and significant engineering effort to handle correctly. Dictionary preservation Dictionary Encoding, also called categorical encoding, is a technique where each value in a column is not stored directly, but instead, an index in a separate list called a “Dictionary” is stored. This technique achieves many of the benefits of third normal form for columns that have repeated values (low cardinality) and is especially effective for columns of strings such as “City”. The first page in a ColumnChunk can optionally be a dictionary page, containing a list of values of the column’s type. Subsequent pages within this ColumnChunk can then encode an index into this dictionary, instead of encoding the values directly. Given the effectiveness of this encoding, if a Parquet decoder simply decodes dictionary data into the native type, it will inefficiently replicate the same value over and over again, which is especially disastrous for string data. To handle dictionary-encoded data efficiently, the encoding must be preserved during decode. Conveniently, many columnar formats, such as the Arrow DictionaryArray, support such compatible encodings. Preserving dictionary encoding drastically improves performance when reading to an Arrow array, in some cases in excess of 60x, as well as using significantly less memory. The major complicating factor for preserving dictionaries is that the dictionaries are stored per ColumnChunk, and therefore the dictionary changes between RowGroups. The reader must automatically recompute a dictionary for batches that span multiple RowGroups, while also optimizing for the case that batch sizes divide evenly into the number of rows per RowGroup. Additionally a column may be only partly dictionary encoded, further complicating implementation. More information on this technique and its complications can be found in the blog post on applying this technique to the C++ Parquet reader. Projection pushdown The most basic Parquet optimization, and the one most commonly described for Parquet files, is projection pushdown, which reduces both I/Oand CPU requirements. Projection in this context means “selecting some but not all of the columns.” Given how Parquet organizes data, it is straightforward to read and decode only the ColumnChunks required for the referenced columns. For example, consider a SQL query of the form SELECT B from table where A &gt; 35 This query only needs data for columns A and B (and not C) and the projection can be “pushed down” to the Parquet reader. Specifically, using the information in the footer, the Parquet reader can entirely skip fetching (I/O) and decoding (CPU) the Data Pages that store data for column C (ColumnChunk 3 and ColumnChunk 6 in our example). ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ┌─────▶ Data Page for ColumnChunk 1 ("A") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 1 ("A") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 2 ("B") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ │ ┃ Data Page for ColumnChunk 3 ("C") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ A query that │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ accesses only │ ┃ Data Page for ColumnChunk 3 ("C") ┃ columns A and B │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ can read only the │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ relevant pages, ─────┤ ┃ Data Page for ColumnChunk 3 ("C") ┃ skipping any Data │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ Page for column C │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 4 ("A") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 5 ("B") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ├─────▶ Data Page for ColumnChunk 5 ("B") ┃ │ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ │ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ └─────▶ Data Page for ColumnChunk 5 ("B") ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ ┃┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐┃ ┃ Data Page for ColumnChunk 6 ("C") ┃ ┃└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ Predicate pushdown Similar to projection pushdown, predicate pushdown also avoids fetching and decoding data from Parquet files, but does so using filter expressions. This technique typically requires closer integration with a query engine such as DataFusion, to determine valid predicates and evaluate them during the scan. Unfortunately without careful API design, the Parquet decoder and query engine can end up tightly coupled, preventing reuse (e.g. there are different Impala and Spark implementations in Cloudera Parquet Predicate Pushdown docs). The Rust Parquet reader uses the RowSelection API to avoid this coupling. RowGroup pruning The simplest form of predicate pushdown, supported by many Parquet based query engines, uses the statistics stored in the footer to skip entire RowGroups. We call this operation RowGroup pruning, and it is analogous to partition pruning in many classical data warehouse systems. For the example query above, if the maximum value for A in a particular RowGroup is less than 35, the decoder can skip fetching and decoding any ColumnChunks from that entire RowGroup. ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃Row Group 1 Metadata ┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃Column "A" Metadata Min:0 Max:15 ┃◀╋ ┐ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ Using the min ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ │ and max values ┃ ┃Column "B" Metadata ┃ ┃ from the ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ │ metadata, ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ RowGroup 1 can ┃ ┃Column "C" Metadata ┃ ┃ ├ ─ ─ be entirely ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ skipped ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │ (pruned) when ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ searching for ┃Row Group 2 Metadata ┃ │ rows with A &gt; ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ 35, ┃ ┃Column "A" Metadata Min:10 Max:50 ┃◀╋ ┘ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃Column "B" Metadata ┃ ┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┃ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃Column "C" Metadata ┃ ┃ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ ┃ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ Note that pruning on minimum and maximum values is effective for many data layouts and column types, but not all. Specifically, it is not as effective for columns with many distinct pseudo-random values (e.g. identifiers or uuids). Thankfully for this use case, Parquet also supports per ColumnChunk Bloom Filters. We are actively working on adding bloom filter support in Apache Rust’s implementation. Page pruning A more sophisticated form of predicate pushdown uses the optional page index in the footer metadata to rule out entire Data Pages. The decoder decodes only the corresponding rows from other columns, often skipping entire pages. The fact that pages in different ColumnChunks often contain different numbers of rows, due to various reasons, complicates this optimization. While the page index may identify the needed pages from one column, pruning a page from one column doesn’t immediately rule out entire pages in other columns. Page pruning proceeds as follows: Uses the predicates in combination with the page index to identify pages to skip Uses the offset index to determine what row ranges correspond to non-skipped pages Computes the intersection of ranges across non-skipped pages, and decodes only those rows This last point is highly non-trivial to implement, especially for nested lists where a single row may correspond to multiple values. Fortunately, the Rust Parquet reader hides this complexity internally, and can decode arbitrary RowSelections. For example, to scan Columns A and B, stored in 5 Data Pages as shown in the figure below: If the predicate is A &gt; 35, Page 1 is pruned using the page index (max value is 20), leaving a RowSelection of [200-&gt;onwards], Parquet reader skips Page 3 entirely (as its last row index is 99) (Only) the relevant rows are read by reading pages 2, 4, and 5. If the predicate is instead A &gt; 35 AND B = "F" the page index is even more effective Using A &gt; 35, yields a RowSelection of [200-&gt;onwards] as before Using B = "F", on the remaining Page 4 and Page 5 of B, yields a RowSelection of [100-244] Intersecting the two RowSelections leaves a combined RowSelection [200-244] Parquet reader only decodes those 50 rows from Page 2 and Page 4. ┏━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃ ┃ ┌──────────────┐ │ ┌──────────────┐ │ ┃ ┃ │ │ │ │ │ │ ┃ ┃ │ │ │ │ Page │ │ │ │ │ │ │ 3 │ ┃ ┃ │ │ │ │ min: "A" │ │ ┃ ┃ │ │ │ │ │ max: "C" │ ┃ ┃ │ Page │ │ │ first_row: 0 │ │ │ │ 1 │ │ │ │ ┃ ┃ │ min: 10 │ │ └──────────────┘ │ ┃ ┃ │ │ max: 20 │ │ ┌──────────────┐ ┃ ┃ │ first_row: 0 │ │ │ │ │ │ │ │ │ │ Page │ ┃ ┃ │ │ │ │ 4 │ │ ┃ ┃ │ │ │ │ │ min: "D" │ ┃ ┃ │ │ │ │ max: "G" │ │ │ │ │ │ │first_row: 100│ ┃ ┃ └──────────────┘ │ │ │ │ ┃ ┃ │ ┌──────────────┐ │ │ │ ┃ ┃ │ │ │ └──────────────┘ │ │ │ Page │ │ ┌──────────────┐ ┃ ┃ │ 2 │ │ │ │ │ ┃ ┃ │ │ min: 30 │ │ │ Page │ ┃ ┃ │ max: 40 │ │ │ 5 │ │ │ │first_row: 200│ │ │ min: "H" │ ┃ ┃ │ │ │ │ max: "Z" │ │ ┃ ┃ │ │ │ │ │first_row: 250│ ┃ ┃ └──────────────┘ │ │ │ │ │ │ └──────────────┘ ┃ ┃ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┃ ┃ ColumnChunk ColumnChunk ┃ ┃ A B ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━━ ━━┛ Support for reading and writing these indexes from Arrow C++, and by extension pyarrow/pandas, is tracked in PARQUET-1404. Late materialization The two previous forms of predicate pushdown only operated on metadata stored for RowGroups, ColumnChunks, and Data Pages prior to decoding values. However, the same techniques also extend to values of one or more columns after decoding them but prior to decoding other columns, which is often called “late materialization”. This technique is especially effective when: The predicate is very selective, i.e. filters out large numbers of rows Each row is large, either due to wide rows (e.g. JSON blobs) or many columns The selected data is clustered together The columns required by the predicate are relatively inexpensive to decode, e.g. PrimitiveArray / DictionaryArray There is additional discussion about the benefits of this technique in SPARK-36527 and Impala. For example, given the predicate A &gt; 35 AND B = "F" from above where the engine uses the page index to determine only 50 rows within RowSelection of [100-244] could match, using late materialization, the Parquet decoder: Decodes the 50 values of Column A Evaluates A &gt; 35 on those 50 values In this case, only 5 rows pass, resulting in the RowSelection: RowSelection[205-206] RowSelection[238-240] Only decodes the 5 rows for Column B for those selections Row Index ┌────────────────────┐ ┌────────────────────┐ 200 │ 30 │ │ "F" │ └────────────────────┘ └────────────────────┘ ... ... ┌────────────────────┐ ┌────────────────────┐ 205 │ 37 │─ ─ ─ ─ ─ ─▶│ "F" │ ├────────────────────┤ ├────────────────────┤ 206 │ 36 │─ ─ ─ ─ ─ ─▶│ "G" │ └────────────────────┘ └────────────────────┘ ... ... ┌────────────────────┐ ┌────────────────────┐ 238 │ 36 │─ ─ ─ ─ ─ ─▶│ "F" │ ├────────────────────┤ ├────────────────────┤ 239 │ 36 │─ ─ ─ ─ ─ ─▶│ "G" │ ├────────────────────┤ ├────────────────────┤ 240 │ 40 │─ ─ ─ ─ ─ ─▶│ "G" │ └────────────────────┘ └────────────────────┘ ... ... ┌────────────────────┐ ┌────────────────────┐ 244 │ 26 │ │ "D" │ └────────────────────┘ └────────────────────┘ Column A Column B Values Values In certain cases, such as our example where B stores single character values, the cost of late materialization machinery can outweigh the savings in decoding. However, the savings can be substantial when some of the conditions listed above are fulfilled. The query engine must decide which predicates to push down and in which order to apply them for optimal results. While it is outside the scope of this document, the same technique can be applied for multiple predicates as well as predicates on multiple columns. See the RowFilter interface in the Parquet crate for more information, and the row_filter implementation in DataFusion. I/O pushdown While Parquet was designed for efficient access on the HDFS distributed file system, it works very well with commodity blob storage systems such as AWS S3 as they have very similar characteristics: Relatively slow “random access” reads: it is much more efficient to read large (MBs) sections of data in each request than issue many requests for smaller portions **Significant latency before retrieving the first byte ** **High per-request cost: **Often billed per request, regardless of number of bytes read, which incentivizes fewer requests that each read a large contiguous section of data. To read optimally from such systems, a Parquet reader must: Minimize the number of I/O requests, while also applying the various pushdown techniques to avoid fetching large amounts of unused data. Integrate with the appropriate task scheduling mechanism to interleave I/O and processing on the data that is fetched to avoid pipeline bottlenecks. As these are substantial engineering and integration challenges, many Parquet readers still require the files to be fetched in their entirety to local storage. Fetching the entire files in order to process them is not ideal for several reasons: High Latency: Decoding cannot begin until the entire file is fetched (Parquet metadata is at the end of the file, so the decoder must see the end prior to decoding the rest) Wasted work: Fetching the entire file fetches all necessary data, but also potentially lots of unnecessary data that will be skipped after reading the footer. This increases the cost unnecessarily. Requires costly “locally attached” storage (or memory): Many cloud environments do not offer computing resources with locally attached storage – they either rely on expensive network block storage such as AWS EBS or else restrict local storage to certain classes of VMs. Avoiding the need to buffer the entire file requires a sophisticated Parquet decoder, integrated with the I/O subsystem, that can initially fetch and decode the metadata followed by ranged fetches for the relevant data blocks, interleaved with the decoding of Parquet data. This optimization requires careful engineering to fetch large enough blocks of data from the object store that the per request overhead doesn’t dominate gains from reducing the bytes transferred. SPARK-36529 describes the challenges of sequential processing in more detail. ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ │ Step 1: Fetch │ Parquet Parquet metadata file on ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━▼━━━━━━━┓ Remote ┃ ▒▒▒▒▒▒▒▒▒▒ ▒▒▒▒▒▒▒▒▒▒ ░░░░░░░░░░ ┃ Object ┃ ▒▒▒data▒▒▒ ▒▒▒data▒▒▒ ░metadata░ ┃ Store ┃ ▒▒▒▒▒▒▒▒▒▒ ▒▒▒▒▒▒▒▒▒▒ ░░░░░░░░░░ ┃ ┗━━━━━━━━━━━▲━━━━━━━━━━━━━━━━━━━━━▲━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │ └ ─ ─ ─ │ │ Step 2: Fetch only ─ ─ ─ ─ ─ ─ ─ ─ ─ relevant data blocks Not included in this diagram picture are details like coalescing requests and ensuring minimum request sizes needed for an actual implementation. The Rust Parquet crate provides an async Parquet reader, to efficiently read from any AsyncFileReader that: Efficiently reads from any storage medium that supports range requests Integrates with Rust’s futures ecosystem to avoid blocking threads waiting on network I/O and easily can interleave CPU and network Requests multiple ranges simultaneously, to allow the implementation to coalesce adjacent ranges, fetch ranges in parallel, etc. Uses the pushdown techniques described previously to eliminate fetching data where possible Integrates easily with the Apache Arrow object_store crate which you can read more about here To give a sense of what is possible, the following picture shows a timeline of fetching the footer metadata from remote files, using that metadata to determine what Data Pages to read, and then fetching data and decoding simultaneously. This process often must be done for more than one file at a time in order to match network latency, bandwidth, and available CPU. begin metadata read of end read read ─ ─ ─ ┐ data of data │ begin complete block block read of │ │ │ │ metadata ─ ─ ─ ┐ At any time, there are │ │ │ │ │ multiple network │ ▼ ▼ ▼ ▼ requests outstanding to file 1 │ ░░░░░░░░░░ ▒▒▒read▒▒▒ ▒▒▒read▒▒▒ │ hide the individual │ ░░░read░░░ ▒▒▒data▒▒▒ ▒▒▒data▒▒▒ request latency │ ░metadata░ ▓▓decode▓▓ │ ░░░░░░░░░░ ▓▓▓data▓▓▓ │ │ │ │ ░░░░░░░░░░ ▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒read▒▒▒▒│▒▒▒▒▒▒▒▒▒▒▒▒▒▒ file 2 │ ░░░read░░░ ▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒data▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ │ ░metadata░ │ ▓▓▓▓▓decode▓▓▓▓▓▓ │ ░░░░░░░░░░ ▓▓▓▓▓▓data▓▓▓▓▓▓▓ │ │ │ │ ░░│░░░░░░░ ▒▒▒read▒▒▒ ▒▒▒▒read▒▒▒▒▒ file 3 │ ░░░read░░░ ▒▒▒data▒▒▒ ▒▒▒▒data▒▒▒▒▒ ... │ ░m│tadata░ ▓▓decode▓▓ │ ░░░░░░░░░░ ▓▓▓data▓▓▓ └───────────────────────────────────────┼──────────────────────────────▶Time │ Conclusion We hope you enjoyed reading about the Parquet file format, and the various techniques used to quickly query parquet files. We believe that the reason most open source implementations of Parquet do not have the breadth of features described in this post is that it takes a monumental effort, that was previously only possible at well-financed commercial enterprises which kept their implementations closed source. However, with the growth and quality of the Apache Arrow community, both Rust practitioners and the wider Arrow community, our ability to collaborate and build a cutting-edge open source implementation is exhilarating and immensely satisfying. The technology described in this blog is the result of the contributions of many engineers spread across companies, hobbyists, and the world in several repositories, notably Apache Arrow DataFusion, Apache Arrow and Apache Arrow Ballista. If you are interested in joining the DataFusion Community, please get in touch.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 10.0.1 Release</title><link href="https://arrow.apache.org/blog/2022/11/22/10.0.1-release/" rel="alternate" type="text/html" title="Apache Arrow 10.0.1 Release" /><published>2022-11-22T00:00:00-05:00</published><updated>2022-11-22T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2022/11/22/10.0.1-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/11/22/10.0.1-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 10.0.1 release.
This is mostly a bugfix release that includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%2010.0.1"><strong>30 resolved issues</strong></a>
from <a href="/release/10.0.1.html#contributors"><strong>15 distinct contributors</strong></a>. See the Install Page to learn how to
get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/10.0.1.html#changelog">complete changelog</a>.</p>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>
<p>The Arrow Flight SQL JDBC driver now properly handles <code class="language-plaintext highlighter-rouge">executeUpdate</code> (<a href="https://issues.apache.org/jira/browse/ARROW-18294">ARROW-18294</a>), and will no longer try to handle URIs that it does not recognize (<a href="https://issues.apache.org/jira/browse/ARROW-18296">ARROW-18296</a>).</p>

<h2 id="c-notes">C++ notes</h2>
<ul>
  <li>Add support for ARMv6 (<a href="https://issues.apache.org/jira/browse/ARROW-18255">ARROW-18255</a>)
And some other minor fixes.</li>
</ul>

<h2 id="go-notes">Go notes</h2>
<ul>
  <li>Added option to support dictionary deltas with IPC (<a href="https://issues.apache.org/jira/browse/ARROW-18326">ARROW-18326</a>)</li>
  <li>Fix dictionary replacement during IPC stream (<a href="https://issues.apache.org/jira/browse/ARROW-18317">ARROW-18317</a>)</li>
  <li>Fix StructBuilder premature release fields (<a href="https://issues.apache.org/jira/browse/ARROW-18274">ARROW-18274</a>)</li>
</ul>

<h2 id="python-notes">Python notes</h2>

<p>New features and improvements:</p>

<ul>
  <li>Added support and wheels for Python 3.11
(<a href="https://issues.apache.org/jira/browse/ARROW-17487">ARROW-17487</a>).</li>
  <li>Updated OpenSSL bundled on wheels to 3.0.7 due to CVE-2022-3786
(<a href="https://issues.apache.org/jira/browse/ARROW-18302">ARROW-18302</a>).</li>
</ul>

<h2 id="r-notes">R notes</h2>
<ul>
  <li>Fix for failing test after lubridate 1.9 release (<a href="https://issues.apache.org/jira/browse/ARROW-18285">ARROW-18285</a>)</li>
  <li>Add deprecation cycle for pull() change (<a href="https://issues.apache.org/jira/browse/ARROW-18132">ARROW-18132</a>)</li>
  <li>Fix to correctly handle .data pronoun in group_by() (<a href="https://issues.apache.org/jira/browse/ARROW-18131">ARROW-18131</a>)</li>
  <li>Fix for dev purrr (<a href="https://issues.apache.org/jira/browse/ARROW-18305">ARROW-18305</a>)</li>
</ul>

<h2 id="other-modules-and-languages">Other modules and languages</h2>

<p>No general changes were made to the other libraries or languages.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 10.0.1 release. This is mostly a bugfix release that includes 30 resolved issues from 15 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Arrow Flight RPC notes The Arrow Flight SQL JDBC driver now properly handles executeUpdate (ARROW-18294), and will no longer try to handle URIs that it does not recognize (ARROW-18296). C++ notes Add support for ARMv6 (ARROW-18255) And some other minor fixes. Go notes Added option to support dictionary deltas with IPC (ARROW-18326) Fix dictionary replacement during IPC stream (ARROW-18317) Fix StructBuilder premature release fields (ARROW-18274) Python notes New features and improvements: Added support and wheels for Python 3.11 (ARROW-17487). Updated OpenSSL bundled on wheels to 3.0.7 due to CVE-2022-3786 (ARROW-18302). R notes Fix for failing test after lubridate 1.9 release (ARROW-18285) Add deprecation cycle for pull() change (ARROW-18132) Fix to correctly handle .data pronoun in group_by() (ARROW-18131) Fix for dev purrr (ARROW-18305) Other modules and languages No general changes were made to the other libraries or languages.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Fast and Memory Efficient Multi-Column Sorts in Apache Arrow Rust, Part 1</title><link href="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1/" rel="alternate" type="text/html" title="Fast and Memory Efficient Multi-Column Sorts in Apache Arrow Rust, Part 1" /><published>2022-11-07T00:00:00-05:00</published><updated>2022-11-07T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1/"><![CDATA[<!--

-->

<h2 id="introduction">Introduction</h2>

<p>Sorting is one of the most fundamental operations in modern databases and other analytic systems, underpinning important operators such as aggregates, joins, window functions, merge, and more. By some estimates, more than half of the execution time in data processing systems is spent sorting. Optimizing sorts is therefore vital to improving query performance and overall system efficiency.</p>

<p>Sorting is also one of the most well studied topics in computer science. The classic survey paper for databases is <a href="https://dl.acm.org/doi/10.1145/1132960.1132964">Implementing Sorting in Database Systems</a> by Goetz Graefe which provides a thorough academic treatment and is still very applicable today. However, it may not be obvious how to apply the wisdom and advanced techniques described in that paper to modern systems. In addition, the excellent <a href="https://duckdb.org/2021/08/27/external-sorting.html">DuckDB blog on sorting</a> highlights many sorting techniques, and mentions a comparable row format, but it does not explain how to efficiently sort variable length strings or dictionary encoded data.</p>

<p>In this series we explain in detail the new <a href="https://docs.rs/arrow/27.0.0/arrow/row/index.html">row format</a> in the <a href="https://github.com/apache/arrow-rs">Rust implementation</a> of <a href="https://arrow.apache.org/">Apache Arrow</a>, and how we used to make sorting more than <a href="https://github.com/apache/arrow-rs/pull/2929">3x</a> faster than an alternate comparator based approach. The benefits are especially pronounced for strings, dictionary encoded data, and sorts with large numbers of columns.</p>

<h2 id="multicolumn--lexicographical-sort-problem">Multicolumn / Lexicographical Sort Problem</h2>

<p>Most languages have native, optimized operations to sort a single column (array) of data, which are specialized based on the type of data being sorted. The reason that sorting is typically more challenging in analytic systems is that:</p>

<ol>
  <li>They must support multiple columns of data</li>
  <li>The column types are not knowable at compile time, and thus the compiler can not typically generate optimized code</li>
</ol>

<p>Multicolumn sorting is also referred to as lexicographical sorting in some libraries.</p>

<p>For example, given sales data for various customers and their state of residence, a user might want to find the lowest 10 orders for each state.</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Customer | State | Orders
—--------+-------+-------
12345    |  MA   |  10.12
532432   |  MA   |  8.44
12345    |  CA   |  3.25
56232    |  WA   |  6.00
23442    |  WA   |  132.50
7844     |  CA   |  9.33
852353   |  MA   |  1.30
</code></pre></div></div>

<p>One way to do so is to order the data first by <code class="language-plaintext highlighter-rouge">State</code> and then by <code class="language-plaintext highlighter-rouge">Orders</code>:</p>
<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Customer | State | Orders
—--------+-------+-------
12345    |  CA   |  3.25
7844     |  CA   |  9.33
852353   |  MA   |  1.30
532432   |  MA   |  8.44
12345    |  MA   |  10.12
56232    |  WA   |  6.00
23442    |  WA   |  132.50
</code></pre></div></div>

<p>(Note: While there are specialized ways for computing this particular query other than fully sorting the entire input (e.g. “TopK”), they typically need the same multi-column comparison operation described below. Thus while we will use the simplified example in this series, it applies much more broadly)</p>

<h2 id="basic-implementation">Basic Implementation</h2>

<p>Let us take the example of a basic sort kernel which takes a set of columns as input, and returns a list of indices identifying a sorted order.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="n">lexsort_to_indices</span><span class="p">([</span>
    <span class="p">[</span><span class="s">"MA"</span><span class="p">,</span> <span class="s">"MA"</span><span class="p">,</span> <span class="s">"CA"</span><span class="p">,</span> <span class="s">"WA"</span><span class="p">,</span> <span class="s">"WA"</span><span class="p">,</span> <span class="s">"CA"</span><span class="p">,</span> <span class="s">"MA"</span><span class="p">]</span>
  <span class="p">])</span>

<span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>

<span class="o">&gt;</span> <span class="n">lexsort_to_indices</span><span class="p">([</span>
    <span class="p">[</span><span class="s">"MA"</span><span class="p">,</span> <span class="s">"MA"</span><span class="p">,</span> <span class="s">"CA"</span><span class="p">,</span> <span class="s">"WA"</span><span class="p">,</span> <span class="s">"WA"</span><span class="p">,</span>   <span class="s">"CA"</span><span class="p">,</span> <span class="s">"MA"</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">10.10</span><span class="p">,</span> <span class="mf">8.44</span><span class="p">,</span> <span class="mf">3.25</span><span class="p">,</span> <span class="mf">6.00</span><span class="p">,</span> <span class="mf">132.50</span><span class="p">,</span> <span class="mf">9.33</span><span class="p">,</span> <span class="mf">1.30</span><span class="p">]</span>
  <span class="p">])</span>

<span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
</code></pre></div></div>

<p>This function returns a list of indices instead of sorting the columns directly because it:</p>
<ol>
  <li>Avoids expensive copying data during the sorting process</li>
  <li>Allows deferring copying of values until the latest possible moment</li>
  <li>Can be used to reorder additional columns that weren’t part of the sort key</li>
</ol>

<p>A straightforward implementation of lexsort_to_indices uses a comparator function,</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   row
  index
        ┌─────┐   ┌─────┐   ┌─────┐     compare(left_index, right_index)
      0 │     │   │     │   │     │
       ┌├─────┤─ ─├─────┤─ ─├─────┤┐                   │             │
        │     │   │     │   │     │ ◀──────────────────┘             │
       └├─────┤─ ─├─────┤─ ─├─────┤┘                                 │
        │     │   │     │   │     │Comparator function compares one  │
        ├─────┤   ├─────┤   ├─────┤ multi-column row with another.   │
        │     │   │     │   │     │                                  │
        ├─────┤   ├─────┤   ├─────┤ The data types of the columns    │
        │     │   │     │   │     │  and the sort options are not    │
        └─────┘   └─────┘   └─────┘  known at compile time, only     │
                    ...                        runtime               │
                                                                     │
       ┌┌─────┐─ ─┌─────┐─ ─┌─────┐┐                                 │
        │     │   │     │   │     │ ◀────────────────────────────────┘
       └├─────┤─ ─├─────┤─ ─├─────┤┘
        │     │   │     │   │     │
        ├─────┤   ├─────┤   ├─────┤
    N-1 │     │   │     │   │     │
        └─────┘   └─────┘   └─────┘
        Customer    State    Orders
         UInt64      Utf8     F64
</code></pre></div></div>

<p>The comparator function compares each row a column at a time, based on the column types</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                         ┌────────────────────────────────┐
                         │                                │
                         ▼                                │
                     ┌ ─ ─ ─ ┐ ┌ ─ ─ ─ ┐                  │
                                                          │
            ┌─────┐  │┌─────┐│ │┌─────┐│                  │
left_index  │     │   │     │   │     │                   │
            └─────┘  │└─────┘│ │└─────┘│   Step 1: Compare State
                                                    (UInt64)
                     │       │ │       │

                     │       │ │       │
            ┌─────┐   ┌─────┐   ┌─────┐
 right_index│     │  ││     ││ ││     ││
            └─────┘   └─────┘   └─────┘    Step 2: If State values equal
                     │       │ │       │   compare Orders (F64)
            Customer   State     Orders                     │
             UInt64  │  Utf8 │ │  F64  │                    │
                      ─ ─ ─ ─   ─ ─ ─ ─                     │
                                    ▲                       │
                                    │                       │
                                    └───────────────────────┘
</code></pre></div></div>

<p>Pseudocode for this operation might look something like</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Takes a list of columns and returns the lexicographically
# sorted order as a list of indices
</span><span class="k">def</span> <span class="nf">lexsort_to_indices</span><span class="p">(</span><span class="n">columns</span><span class="p">):</span>
  <span class="n">comparator</span> <span class="o">=</span> <span class="n">build_comparator</span><span class="p">(</span><span class="n">columns</span><span class="p">)</span>

  <span class="c1"># Construct a list of integers from 0 to the number of rows
</span>  <span class="c1"># and sort it according to the comparator
</span>  <span class="p">[</span><span class="mf">0.</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">num_rows</span><span class="p">()].</span><span class="n">sort_by</span><span class="p">(</span><span class="n">comparator</span><span class="p">)</span>

<span class="c1"># Build a function that given indexes (left_idx, right_idx)
# returns the comparison of the sort keys at the left
# and right indices respectively
</span><span class="k">def</span> <span class="nf">build_comparator</span><span class="p">(</span><span class="n">columns</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">comparator</span><span class="p">(</span><span class="n">left_idx</span><span class="p">,</span> <span class="n">right_idx</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">:</span>
      <span class="c1"># call a compare function which performs
</span>      <span class="c1"># dynamic dispatch on type of left and right columns
</span>      <span class="n">ordering</span> <span class="o">=</span> <span class="n">compare</span><span class="p">(</span><span class="n">column</span><span class="p">,</span> <span class="n">left_idx</span><span class="p">,</span><span class="n">right_idx</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">ordering</span> <span class="o">!=</span> <span class="n">Equal</span> <span class="p">{</span>
        <span class="k">return</span> <span class="n">ordering</span>
      <span class="p">}</span>
    <span class="c1"># All values equal
</span>    <span class="n">Equal</span>
  <span class="c1"># Return comparator function
</span>  <span class="n">comparator</span>

  <span class="c1"># compares the values in a single column at left_idx and right_idx
</span>  <span class="k">def</span> <span class="nf">compare</span><span class="p">(</span><span class="n">column</span><span class="p">,</span> <span class="n">left_idx</span><span class="p">,</span> <span class="n">right_idx</span><span class="p">):</span>
    <span class="c1"># Choose comparison based on type of column ("dynamic dispatch")
</span>    <span class="k">if</span> <span class="n">column</span><span class="p">.</span><span class="nb">type</span> <span class="o">==</span> <span class="n">Int</span><span class="p">:</span>
     <span class="nb">cmp</span><span class="p">(</span><span class="n">column</span><span class="p">[</span><span class="n">left_idx</span><span class="p">].</span><span class="n">as_int</span><span class="p">(),</span> <span class="n">column</span><span class="p">[</span><span class="n">right_idx</span><span class="p">].</span><span class="n">as_int</span><span class="p">())</span>
    <span class="k">elif</span> <span class="n">column</span><span class="p">.</span><span class="nb">type</span> <span class="o">==</span> <span class="n">Float</span><span class="p">:</span>
     <span class="nb">cmp</span><span class="p">(</span><span class="n">column</span><span class="p">[</span><span class="n">left_idx</span><span class="p">].</span><span class="n">as_float</span><span class="p">(),</span> <span class="n">column</span><span class="p">[</span><span class="n">right_idx</span><span class="p">].</span><span class="n">as_float</span><span class="p">())</span>
    <span class="p">...</span>
</code></pre></div></div>

<p>Greater detail is beyond the scope of this post, but in general the more predictable the behavior of a block of code, the better its performance will be. In the case of this pseudocode,  there is clear room for improvement:</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">comparator</code> performs a large number of unpredictable conditional branches, where the path execution takes depends on the data values</li>
  <li><code class="language-plaintext highlighter-rouge">comparator</code> and <code class="language-plaintext highlighter-rouge">compare</code> use dynamic dispatch, which not only adds further conditional branches, but also function call overhead</li>
  <li><code class="language-plaintext highlighter-rouge">comparator</code> performs a large number of reads of memory at unpredictable locations</li>
</ol>

<p>You can find the complete implementation of multi-column comparator construction in arrow-rs in <a href="https://github.com/apache/arrow-rs/blob/f629a2ebe08033e7b78585d82e98c50a4439e7a2/arrow/src/compute/kernels/sort.rs#L905-L1036">sort.rs</a> and <a href="https://github.com/apache/arrow-rs/blob/f629a2e/arrow/src/array/ord.rs#L178-L313">ord.rs</a>.</p>

<h1 id="normalized-keys--byte-array-comparisons">Normalized Keys / Byte Array Comparisons</h1>

<p>Now imagine we had a way to represent each logical row of data as a sequence of bytes, and that byte-wise comparison of that sequence yielded the same result as comparing the actual column values using the code above. Such a representation would require no switching on column types, and the kernel would become</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">lexsort_to_indices</span><span class="p">(</span><span class="n">columns</span><span class="p">):</span>
  <span class="n">rows</span> <span class="o">=</span> <span class="n">convert_to_rows</span><span class="p">(</span><span class="n">columns</span><span class="p">)</span>
  <span class="p">[</span><span class="mf">0.</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">num_rows</span><span class="p">()].</span><span class="n">sort_by</span><span class="p">(</span><span class="k">lambda</span> <span class="n">l</span><span class="p">,</span> <span class="n">r</span><span class="p">:</span> <span class="nb">cmp</span><span class="p">(</span><span class="n">rows</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">rows</span><span class="p">[</span><span class="n">r</span><span class="p">]))</span>
</code></pre></div></div>

<p>While this approach does require converting to/from the byte array representation, it has some major advantages:</p>

<ul>
  <li>Rows can be compared by comparing bytes in memory, which modern computer hardware excels at with the extremely well optimized <a href="https://www.man7.org/linux/man-pages/man3/memcmp.3.html">memcmp</a></li>
  <li>Memory accesses are largely predictable</li>
  <li>There is no dynamic dispatch overhead</li>
  <li>Extends straightforwardly to more sophisticated sorting strategies such as
    <ul>
      <li>Distribution-based sorting techniques such as radix sort</li>
      <li>Parallel merge sort</li>
      <li>External sort</li>
      <li>…</li>
    </ul>
  </li>
</ul>

<p>You can find more information on how to leverage such representation in the “Binary String Comparison” section of the <a href="https://duckdb.org/2021/08/27/external-sorting.html">DuckDB blog post</a> on the topic as well as <a href="https://dl.acm.org/doi/10.1145/1132960.1132964">Graefe’s paper</a>. However, we found it wasn’t immediately obvious how to apply this technique to variable length string or dictionary encoded data, which we will explain in the next post in this series.</p>

<h2 id="next-up-row-format">Next up: Row Format</h2>

<p>This post has introduced the concept and challenges of multi column sorting, and shown why a comparable byte array representation, such as the <a href="https://docs.rs/arrow/27.0.0/arrow/row/index.html">row format</a> introduced to the <a href="https://github.com/apache/arrow-rs">Rust implementation</a> of <a href="https://arrow.apache.org/">Apache Arrow</a>, is such a compelling primitive.</p>

<p>In <a href="/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-2/">the next post</a> we explain how this encoding works, but if you just want to use it, check out the <a href="https://docs.rs/arrow/27.0.0/arrow/row/index.html">docs</a> for getting started, and report any issues on our <a href="https://github.com/apache/arrow-rs/issues">bugtracker</a>. As always, the <a href="https://github.com/apache/arrow-rs#arrow-rust-community">Arrow community</a> very much looks forward to seeing what you build with it!</p>]]></content><author><name>tustvold and alamb</name></author><category term="arrow" /><summary type="html"><![CDATA[Introduction Sorting is one of the most fundamental operations in modern databases and other analytic systems, underpinning important operators such as aggregates, joins, window functions, merge, and more. By some estimates, more than half of the execution time in data processing systems is spent sorting. Optimizing sorts is therefore vital to improving query performance and overall system efficiency. Sorting is also one of the most well studied topics in computer science. The classic survey paper for databases is Implementing Sorting in Database Systems by Goetz Graefe which provides a thorough academic treatment and is still very applicable today. However, it may not be obvious how to apply the wisdom and advanced techniques described in that paper to modern systems. In addition, the excellent DuckDB blog on sorting highlights many sorting techniques, and mentions a comparable row format, but it does not explain how to efficiently sort variable length strings or dictionary encoded data. In this series we explain in detail the new row format in the Rust implementation of Apache Arrow, and how we used to make sorting more than 3x faster than an alternate comparator based approach. The benefits are especially pronounced for strings, dictionary encoded data, and sorts with large numbers of columns. Multicolumn / Lexicographical Sort Problem Most languages have native, optimized operations to sort a single column (array) of data, which are specialized based on the type of data being sorted. The reason that sorting is typically more challenging in analytic systems is that: They must support multiple columns of data The column types are not knowable at compile time, and thus the compiler can not typically generate optimized code Multicolumn sorting is also referred to as lexicographical sorting in some libraries. For example, given sales data for various customers and their state of residence, a user might want to find the lowest 10 orders for each state. Customer | State | Orders —--------+-------+------- 12345 | MA | 10.12 532432 | MA | 8.44 12345 | CA | 3.25 56232 | WA | 6.00 23442 | WA | 132.50 7844 | CA | 9.33 852353 | MA | 1.30 One way to do so is to order the data first by State and then by Orders: Customer | State | Orders —--------+-------+------- 12345 | CA | 3.25 7844 | CA | 9.33 852353 | MA | 1.30 532432 | MA | 8.44 12345 | MA | 10.12 56232 | WA | 6.00 23442 | WA | 132.50 (Note: While there are specialized ways for computing this particular query other than fully sorting the entire input (e.g. “TopK”), they typically need the same multi-column comparison operation described below. Thus while we will use the simplified example in this series, it applies much more broadly) Basic Implementation Let us take the example of a basic sort kernel which takes a set of columns as input, and returns a list of indices identifying a sorted order. &gt; lexsort_to_indices([ ["MA", "MA", "CA", "WA", "WA", "CA", "MA"] ]) [2, 5, 0, 1, 6, 3, 4] &gt; lexsort_to_indices([ ["MA", "MA", "CA", "WA", "WA", "CA", "MA"], [10.10, 8.44, 3.25, 6.00, 132.50, 9.33, 1.30] ]) [2, 5, 6, 1, 0, 3, 4] This function returns a list of indices instead of sorting the columns directly because it: Avoids expensive copying data during the sorting process Allows deferring copying of values until the latest possible moment Can be used to reorder additional columns that weren’t part of the sort key A straightforward implementation of lexsort_to_indices uses a comparator function, row index ┌─────┐ ┌─────┐ ┌─────┐ compare(left_index, right_index) 0 │ │ │ │ │ │ ┌├─────┤─ ─├─────┤─ ─├─────┤┐ │ │ │ │ │ │ │ │ ◀──────────────────┘ │ └├─────┤─ ─├─────┤─ ─├─────┤┘ │ │ │ │ │ │ │Comparator function compares one │ ├─────┤ ├─────┤ ├─────┤ multi-column row with another. │ │ │ │ │ │ │ │ ├─────┤ ├─────┤ ├─────┤ The data types of the columns │ │ │ │ │ │ │ and the sort options are not │ └─────┘ └─────┘ └─────┘ known at compile time, only │ ... runtime │ │ ┌┌─────┐─ ─┌─────┐─ ─┌─────┐┐ │ │ │ │ │ │ │ ◀────────────────────────────────┘ └├─────┤─ ─├─────┤─ ─├─────┤┘ │ │ │ │ │ │ ├─────┤ ├─────┤ ├─────┤ N-1 │ │ │ │ │ │ └─────┘ └─────┘ └─────┘ Customer State Orders UInt64 Utf8 F64 The comparator function compares each row a column at a time, based on the column types ┌────────────────────────────────┐ │ │ ▼ │ ┌ ─ ─ ─ ┐ ┌ ─ ─ ─ ┐ │ │ ┌─────┐ │┌─────┐│ │┌─────┐│ │ left_index │ │ │ │ │ │ │ └─────┘ │└─────┘│ │└─────┘│ Step 1: Compare State (UInt64) │ │ │ │ │ │ │ │ ┌─────┐ ┌─────┐ ┌─────┐ right_index│ │ ││ ││ ││ ││ └─────┘ └─────┘ └─────┘ Step 2: If State values equal │ │ │ │ compare Orders (F64) Customer State Orders │ UInt64 │ Utf8 │ │ F64 │ │ ─ ─ ─ ─ ─ ─ ─ ─ │ ▲ │ │ │ └───────────────────────┘ Pseudocode for this operation might look something like # Takes a list of columns and returns the lexicographically # sorted order as a list of indices def lexsort_to_indices(columns): comparator = build_comparator(columns) # Construct a list of integers from 0 to the number of rows # and sort it according to the comparator [0..columns.num_rows()].sort_by(comparator) # Build a function that given indexes (left_idx, right_idx) # returns the comparison of the sort keys at the left # and right indices respectively def build_comparator(columns): def comparator(left_idx, right_idx): for column in columns: # call a compare function which performs # dynamic dispatch on type of left and right columns ordering = compare(column, left_idx,right_idx) if ordering != Equal { return ordering } # All values equal Equal # Return comparator function comparator # compares the values in a single column at left_idx and right_idx def compare(column, left_idx, right_idx): # Choose comparison based on type of column ("dynamic dispatch") if column.type == Int: cmp(column[left_idx].as_int(), column[right_idx].as_int()) elif column.type == Float: cmp(column[left_idx].as_float(), column[right_idx].as_float()) ... Greater detail is beyond the scope of this post, but in general the more predictable the behavior of a block of code, the better its performance will be. In the case of this pseudocode, there is clear room for improvement: comparator performs a large number of unpredictable conditional branches, where the path execution takes depends on the data values comparator and compare use dynamic dispatch, which not only adds further conditional branches, but also function call overhead comparator performs a large number of reads of memory at unpredictable locations You can find the complete implementation of multi-column comparator construction in arrow-rs in sort.rs and ord.rs. Normalized Keys / Byte Array Comparisons Now imagine we had a way to represent each logical row of data as a sequence of bytes, and that byte-wise comparison of that sequence yielded the same result as comparing the actual column values using the code above. Such a representation would require no switching on column types, and the kernel would become def lexsort_to_indices(columns): rows = convert_to_rows(columns) [0..columns.num_rows()].sort_by(lambda l, r: cmp(rows[l], rows[r])) While this approach does require converting to/from the byte array representation, it has some major advantages: Rows can be compared by comparing bytes in memory, which modern computer hardware excels at with the extremely well optimized memcmp Memory accesses are largely predictable There is no dynamic dispatch overhead Extends straightforwardly to more sophisticated sorting strategies such as Distribution-based sorting techniques such as radix sort Parallel merge sort External sort … You can find more information on how to leverage such representation in the “Binary String Comparison” section of the DuckDB blog post on the topic as well as Graefe’s paper. However, we found it wasn’t immediately obvious how to apply this technique to variable length string or dictionary encoded data, which we will explain in the next post in this series. Next up: Row Format This post has introduced the concept and challenges of multi column sorting, and shown why a comparable byte array representation, such as the row format introduced to the Rust implementation of Apache Arrow, is such a compelling primitive. In the next post we explain how this encoding works, but if you just want to use it, check out the docs for getting started, and report any issues on our bugtracker. As always, the Arrow community very much looks forward to seeing what you build with it!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Fast and Memory Efficient Multi-Column Sorts in Apache Arrow Rust, Part 2</title><link href="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-2/" rel="alternate" type="text/html" title="Fast and Memory Efficient Multi-Column Sorts in Apache Arrow Rust, Part 2" /><published>2022-11-07T00:00:00-05:00</published><updated>2022-11-07T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-2</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-2/"><![CDATA[<!--

-->

<h2 id="introduction">Introduction</h2>

<p>In <a href="/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1/">Part 1</a> of this post, we described the problem of Multi-Column Sorting and the challenges of implementing it efficiently. This second post explains how the new <a href="https://docs.rs/arrow/27.0.0/arrow/row/index.html">row format</a> in the <a href="https://github.com/apache/arrow-rs">Rust implementation</a> of <a href="https://arrow.apache.org/">Apache Arrow</a> works and is constructed.</p>

<h2 id="row-format">Row Format</h2>

<p>The row format is a variable length byte sequence created by concatenating the encoded form of each column. The encoding for each column depends on its datatype (and sort options).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   ┌─────┐   ┌─────┐   ┌─────┐
   │     │   │     │   │     │
   ├─────┤ ┌ ┼─────┼ ─ ┼─────┼ ┐              ┏━━━━━━━━━━━━━┓
   │     │   │     │   │     │  ─────────────▶┃             ┃
   ├─────┤ └ ┼─────┼ ─ ┼─────┼ ┘              ┗━━━━━━━━━━━━━┛
   │     │   │     │   │     │
   └─────┘   └─────┘   └─────┘
               ...
   ┌─────┐ ┌ ┬─────┬ ─ ┬─────┬ ┐              ┏━━━━━━━━┓
   │     │   │     │   │     │  ─────────────▶┃        ┃
   └─────┘ └ ┴─────┴ ─ ┴─────┴ ┘              ┗━━━━━━━━┛
   Customer    State    Orders
    UInt64      Utf8     F64

          Input Arrays                          Row Format
           (Columns)
</code></pre></div></div>

<p>The encoding is carefully designed in such a way that escaping is unnecessary: it is never ambiguous as to whether a byte is part of a sentinel (e.g. null) or a value.</p>

<h3 id="unsigned-integers">Unsigned Integers</h3>

<p>To encode a non-null unsigned integer, the byte <code class="language-plaintext highlighter-rouge">0x01</code> is written, followed by the integer’s bytes starting with the most significant, i.e. big endian. A null is encoded as a <code class="language-plaintext highlighter-rouge">0x00</code> byte, followed by the encoded bytes of the integer’s zero value</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>              ┌──┬──┬──┬──┐      ┌──┬──┬──┬──┬──┐
   3          │03│00│00│00│      │01│00│00│00│03│
              └──┴──┴──┴──┘      └──┴──┴──┴──┴──┘
              ┌──┬──┬──┬──┐      ┌──┬──┬──┬──┬──┐
  258         │02│01│00│00│      │01│00│00│01│02│
              └──┴──┴──┴──┘      └──┴──┴──┴──┴──┘
              ┌──┬──┬──┬──┐      ┌──┬──┬──┬──┬──┐
 23423        │7F│5B│00│00│      │01│00│00│5B│7F│
              └──┴──┴──┴──┘      └──┴──┴──┴──┴──┘
              ┌──┬──┬──┬──┐      ┌──┬──┬──┬──┬──┐
 NULL         │??│??│??│??│      │00│00│00│00│00│
              └──┴──┴──┴──┘      └──┴──┴──┴──┴──┘

             32-bit (4 bytes)        Row Format
 Value        Little Endian
</code></pre></div></div>

<h3 id="signed-integers">Signed Integers</h3>

<p>In Rust and most modern computer architectures, signed integers are encoded using <a href="https://en.wikipedia.org/wiki/Two%27s_complement">two’s complement</a>, where a number is negated by flipping all the bits, and adding 1. Therefore, flipping the top-most bit and treating the result as an unsigned integer preserves the order. This unsigned integer can then be encoded using the same encoding for unsigned integers described in the previous section. For example</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>       ┌──┬──┬──┬──┐       ┌──┬──┬──┬──┐       ┌──┬──┬──┬──┬──┐
    5  │05│00│00│00│       │05│00│00│80│       │01│80│00│00│05│
       └──┴──┴──┴──┘       └──┴──┴──┴──┘       └──┴──┴──┴──┴──┘
       ┌──┬──┬──┬──┐       ┌──┬──┬──┬──┐       ┌──┬──┬──┬──┬──┐
   -5  │FB│FF│FF│FF│       │FB│FF│FF│7F│       │01│7F│FF│FF│FB│
       └──┴──┴──┴──┘       └──┴──┴──┴──┘       └──┴──┴──┴──┴──┘

 Value  32-bit (4 bytes)    High bit flipped      Row Format
         Little Endian
</code></pre></div></div>

<h3 id="floating-point">Floating Point</h3>

<p>Floating point values can be ordered according to the <a href="https://en.wikipedia.org/wiki/IEEE_754#Total-ordering_predicate">IEEE 754 totalOrder predicate</a> (implemented in Rust by <a href="https://doc.rust-lang.org/std/primitive.f32.html#method.total_cmp">f32::total_cmp</a>). This ordering interprets the bytes of the floating point value as the correspondingly sized, signed, little-endian integer, flipping all the bits except the sign bit in the case of negatives.</p>

<p>Floating point values are therefore encoded to row format by converting them to the appropriate sized signed integer representation, and then using the same encoding for signed integers described in the previous section.</p>

<h3 id="byte-arrays-including-strings">Byte Arrays (Including Strings)</h3>

<p>Unlike primitive types above, byte arrays are variable length. For short strings, such as <code class="language-plaintext highlighter-rouge">state</code> in our example above, it is possible to pad all values to the length of the longest one with some fixed value such as <code class="language-plaintext highlighter-rouge">0x00</code> and produce a fixed length row. This is the approach described in the DuckDB blog for encoding <code class="language-plaintext highlighter-rouge">c_birth_country</code>.</p>

<p>However, often values in string columns differ substantially in length or the maximum length is not known at the start of execution, making it inadvisable and/or impractical to pad the strings to a fixed length. The Rust Arrow row format therefore uses a variable length encoding.</p>

<p>We need an encoding that unambiguously terminates the end of the byte array. This not only permits recovering the original value from the row format, but ensures that bytes of a longer byte array are not compared against bytes from a different column when compared against a row containing a shorter byte array.</p>

<p>A null byte array is encoded as a single <code class="language-plaintext highlighter-rouge">0x00</code> byte. Similarly, an empty byte array is encoded as a single <code class="language-plaintext highlighter-rouge">0x01</code> byte.</p>

<p>To encode a non-null, non-empty array, first a single <code class="language-plaintext highlighter-rouge">0x02</code> byte  is written. Then the array is written in 32-byte blocks, with each complete block followed by a <code class="language-plaintext highlighter-rouge">0xFF</code> byte as a continuation token. The final block is padded to 32-bytes with <code class="language-plaintext highlighter-rouge">0x00</code>, and is then followed by the unpadded length of this final block as a single byte in place of a continuation token</p>

<p>Note the following example encodings use a block size of 4 bytes, as opposed to 32 bytes for brevity</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                      ┌───┬───┬───┬───┬───┬───┐
 "MEEP"               │02 │'M'│'E'│'E'│'P'│04 │
                      └───┴───┴───┴───┴───┴───┘

                      ┌───┐
 ""                   │01 |
                      └───┘

 NULL                 ┌───┐
                      │00 │
                      └───┘

"Defenestration"      ┌───┬───┬───┬───┬───┬───┐
                      │02 │'D'│'e'│'f'│'e'│FF │
                      └───┼───┼───┼───┼───┼───┤
                          │'n'│'e'│'s'│'t'│FF │
                          ├───┼───┼───┼───┼───┤
                          │'r'│'a'│'t'│'r'│FF │
                          ├───┼───┼───┼───┼───┤
                          │'a'│'t'│'i'│'o'│FF │
                          ├───┼───┼───┼───┼───┤
                          │'n'│00 │00 │00 │01 │
                          └───┴───┴───┴───┴───┘
</code></pre></div></div>

<p>This approach is loosely inspired by <a href="https://en.wikipedia.org/wiki/Consistent_Overhead_Byte_Stuffing">COBS encoding</a>, and chosen over more traditional <a href="https://en.wikipedia.org/wiki/High-Level_Data_Link_Control#Asynchronous_framing">byte stuffing</a> as it is more amenable to vectorization, in particular hardware with AVX-256 can copy a 32-byte block in a single instruction.</p>

<h3 id="dictionary-arrays">Dictionary Arrays</h3>
<p>Dictionary Encoded Data (called <a href="https://pandas.pydata.org/docs/user_guide/categorical.html">categorical</a> in pandas) is increasingly important because they can store and process low cardinality data very efficiently.</p>

<p>A simple approach to encoding dictionary arrays would be to encode the logical values directly using the encodings for primitive values described previously. However, this would lose the benefits of dictionary encoding to reduce memory and CPU consumption.</p>

<p>To further complicate matters, the <a href="https://arrow.apache.org/docs/format/Columnar.html#dictionary-encoded-layout">Arrow implementation of Dictionary encoding</a> is quite general, and we can make no assumptions about the contents of the dictionaries. In particular, we cannot assume that the dictionary values are sorted, nor that the same dictionary is used for all arrays within a column</p>

<p>The following example shows how a string column might be encoded in two arrays using two different dictionaries. The dictionary keys <code class="language-plaintext highlighter-rouge">0</code>, <code class="language-plaintext highlighter-rouge">1</code>, and <code class="language-plaintext highlighter-rouge">2</code> in the first batch correspond to different values than the same keys in the second dictionary.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
  ┌───────────┐ ┌─────┐    │
│ │"Fabulous" │ │  0  │
  ├───────────┤ ├─────┤    │
│ │   "Bar"   │ │  2  │
  ├───────────┤ ├─────┤    │       ┌───────────┐
│ │  "Soup"   │ │  2  │            │"Fabulous" │
  └───────────┘ ├─────┤    │       ├───────────┤
│               │  0  │            │  "Soup"   │
                ├─────┤    │       ├───────────┤
│               │  1  │            │  "Soup"   │
                └─────┘    │       ├───────────┤
│                                  │"Fabulous" │
                 Values    │       ├───────────┤
│ Dictionary   (indexes in         │   "Bar"   │
               dictionary) │       ├───────────┤
│                                  │   "ZZ"    │
 ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘       ├───────────┤
┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─        │   "Bar"   │
                           │       ├───────────┤
│ ┌───────────┐ ┌─────┐            │   "ZZ"    │
  │"Fabulous" │ │  1  │    │       ├───────────┤
│ ├───────────┤ ├─────┤            │"Fabulous" │
  │   "ZZ"    │ │  2  │    │       └───────────┘
│ ├───────────┤ ├─────┤
  │   "Bar"   │ │  1  │    │
│ └───────────┘ ├─────┤
                │  0  │    │      Logical column
│               └─────┘               values
                Values     │
│  Dictionary (indexes in
              dictionary)  │
 ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘
</code></pre></div></div>

<p>The key observation which allows us to efficiently create a row format for this kind of data is that given a byte array, a new byte array can always be created which comes before or after it in the sort order by adding an additional byte.</p>

<p>Therefore we can incrementally build an order-preserving mapping from dictionary values to variable length byte arrays, without needing to know all possible dictionary values beforehand, instead introducing mappings for new dictionary values as we encounter them.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌──────────┐                 ┌─────┐
│  "Bar"   │ ───────────────▶│ 01  │
└──────────┘                 └─────┘
┌──────────┐                 ┌─────┬─────┐
│"Fabulous"│ ───────────────▶│ 01  │ 02  │
└──────────┘                 └─────┴─────┘
┌──────────┐                 ┌─────┐
│  "Soup"  │ ───────────────▶│ 05  │
└──────────┘                 └─────┘
┌──────────┐                 ┌─────┐
│   "ZZ"   │ ───────────────▶│ 07  │
└──────────┘                 └─────┘

    Example Order Preserving Mapping
</code></pre></div></div>

<p>The details of the data structure used to generate this mapping are beyond the scope of this blog post, but may be the topic of a future post. You can find <a href="https://github.com/apache/arrow-rs/blob/07024f6a16b870fda81cba5779b8817b20386ebf/arrow/src/row/interner.rs">the code here</a>.</p>

<p>The data structure also ensures that no values contain <code class="language-plaintext highlighter-rouge">0x00</code> and therefore we can encode the arrays directly using <code class="language-plaintext highlighter-rouge">0x00</code> as an end-delimiter.</p>

<p>A null value is encoded as a single <code class="language-plaintext highlighter-rouge">0x00</code> byte, and a non-null value encoded as a single <code class="language-plaintext highlighter-rouge">0x01</code> byte, followed by the <code class="language-plaintext highlighter-rouge">0x00</code> terminated byte array determined by the order preserving mapping</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                          ┌─────┬─────┬─────┬─────┐
   "Fabulous"             │ 01  │ 03  │ 05  │ 00  │
                          └─────┴─────┴─────┴─────┘

                          ┌─────┬─────┬─────┐
   "ZZ"                   │ 01  │ 07  │ 00  │
                          └─────┴─────┴─────┘

                          ┌─────┐
    NULL                  │ 00  │
                          └─────┘

     Input                  Row Format
</code></pre></div></div>

<h3 id="sort-options">Sort Options</h3>

<p>One detail we have so far ignored over is how to support ascending and descending sorts (e.g. <code class="language-plaintext highlighter-rouge">ASC</code> or <code class="language-plaintext highlighter-rouge">DESC</code> in SQL). The Arrow Rust row format supports these options by simply inverting the bytes of the encoded representation, except the initial byte used for nullability encoding, on a per column basis.</p>

<p>Similarly, supporting SQL compatible sorting also requires a format that can specify the order of <code class="language-plaintext highlighter-rouge">NULL</code>s (before or after all non <code class="language-plaintext highlighter-rouge">NULL</code> values). The row format supports this option by optionally encoding nulls as <code class="language-plaintext highlighter-rouge">0xFF</code> instead of <code class="language-plaintext highlighter-rouge">0x00</code> on a per column basis.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Hopefully these two articles have given you a flavor of what is possible with a comparable row format and how it works. Feel free to check out the <a href="https://docs.rs/arrow/27.0.0/arrow/row/index.html">docs</a> for instructions on getting started, and report any issues on our <a href="https://github.com/apache/arrow-rs/issues">bugtracker</a>.</p>

<p>Using this format for lexicographic sorting is more than <a href="https://github.com/apache/arrow-rs/pull/2929">3x</a> faster than the comparator based approach, with the benefits especially pronounced for strings, dictionaries and sorts with large numbers of columns.</p>

<p>We have also already used it to more than <a href="https://github.com/apache/arrow-datafusion/pull/3386">double</a> the performance of sort preserving merge in the <a href="https://arrow.apache.org/datafusion/">DataFusion project</a>, and expect similar or greater performance uplift as we apply it to sort, grouping, join, and window function operators as well.</p>

<p>As always, the <a href="https://github.com/apache/arrow-rs#arrow-rust-community">Arrow community</a> very much looks forward to seeing what you build with it!</p>]]></content><author><name>tustvold and alamb</name></author><category term="arrow" /><summary type="html"><![CDATA[Introduction In Part 1 of this post, we described the problem of Multi-Column Sorting and the challenges of implementing it efficiently. This second post explains how the new row format in the Rust implementation of Apache Arrow works and is constructed. Row Format The row format is a variable length byte sequence created by concatenating the encoded form of each column. The encoding for each column depends on its datatype (and sort options). ┌─────┐ ┌─────┐ ┌─────┐ │ │ │ │ │ │ ├─────┤ ┌ ┼─────┼ ─ ┼─────┼ ┐ ┏━━━━━━━━━━━━━┓ │ │ │ │ │ │ ─────────────▶┃ ┃ ├─────┤ └ ┼─────┼ ─ ┼─────┼ ┘ ┗━━━━━━━━━━━━━┛ │ │ │ │ │ │ └─────┘ └─────┘ └─────┘ ... ┌─────┐ ┌ ┬─────┬ ─ ┬─────┬ ┐ ┏━━━━━━━━┓ │ │ │ │ │ │ ─────────────▶┃ ┃ └─────┘ └ ┴─────┴ ─ ┴─────┴ ┘ ┗━━━━━━━━┛ Customer State Orders UInt64 Utf8 F64 Input Arrays Row Format (Columns) The encoding is carefully designed in such a way that escaping is unnecessary: it is never ambiguous as to whether a byte is part of a sentinel (e.g. null) or a value. Unsigned Integers To encode a non-null unsigned integer, the byte 0x01 is written, followed by the integer’s bytes starting with the most significant, i.e. big endian. A null is encoded as a 0x00 byte, followed by the encoded bytes of the integer’s zero value ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┬──┐ 3 │03│00│00│00│ │01│00│00│00│03│ └──┴──┴──┴──┘ └──┴──┴──┴──┴──┘ ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┬──┐ 258 │02│01│00│00│ │01│00│00│01│02│ └──┴──┴──┴──┘ └──┴──┴──┴──┴──┘ ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┬──┐ 23423 │7F│5B│00│00│ │01│00│00│5B│7F│ └──┴──┴──┴──┘ └──┴──┴──┴──┴──┘ ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┬──┐ NULL │??│??│??│??│ │00│00│00│00│00│ └──┴──┴──┴──┘ └──┴──┴──┴──┴──┘ 32-bit (4 bytes) Row Format Value Little Endian Signed Integers In Rust and most modern computer architectures, signed integers are encoded using two’s complement, where a number is negated by flipping all the bits, and adding 1. Therefore, flipping the top-most bit and treating the result as an unsigned integer preserves the order. This unsigned integer can then be encoded using the same encoding for unsigned integers described in the previous section. For example ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┬──┐ 5 │05│00│00│00│ │05│00│00│80│ │01│80│00│00│05│ └──┴──┴──┴──┘ └──┴──┴──┴──┘ └──┴──┴──┴──┴──┘ ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┐ ┌──┬──┬──┬──┬──┐ -5 │FB│FF│FF│FF│ │FB│FF│FF│7F│ │01│7F│FF│FF│FB│ └──┴──┴──┴──┘ └──┴──┴──┴──┘ └──┴──┴──┴──┴──┘ Value 32-bit (4 bytes) High bit flipped Row Format Little Endian Floating Point Floating point values can be ordered according to the IEEE 754 totalOrder predicate (implemented in Rust by f32::total_cmp). This ordering interprets the bytes of the floating point value as the correspondingly sized, signed, little-endian integer, flipping all the bits except the sign bit in the case of negatives. Floating point values are therefore encoded to row format by converting them to the appropriate sized signed integer representation, and then using the same encoding for signed integers described in the previous section. Byte Arrays (Including Strings) Unlike primitive types above, byte arrays are variable length. For short strings, such as state in our example above, it is possible to pad all values to the length of the longest one with some fixed value such as 0x00 and produce a fixed length row. This is the approach described in the DuckDB blog for encoding c_birth_country. However, often values in string columns differ substantially in length or the maximum length is not known at the start of execution, making it inadvisable and/or impractical to pad the strings to a fixed length. The Rust Arrow row format therefore uses a variable length encoding. We need an encoding that unambiguously terminates the end of the byte array. This not only permits recovering the original value from the row format, but ensures that bytes of a longer byte array are not compared against bytes from a different column when compared against a row containing a shorter byte array. A null byte array is encoded as a single 0x00 byte. Similarly, an empty byte array is encoded as a single 0x01 byte. To encode a non-null, non-empty array, first a single 0x02 byte is written. Then the array is written in 32-byte blocks, with each complete block followed by a 0xFF byte as a continuation token. The final block is padded to 32-bytes with 0x00, and is then followed by the unpadded length of this final block as a single byte in place of a continuation token Note the following example encodings use a block size of 4 bytes, as opposed to 32 bytes for brevity ┌───┬───┬───┬───┬───┬───┐ "MEEP" │02 │'M'│'E'│'E'│'P'│04 │ └───┴───┴───┴───┴───┴───┘ ┌───┐ "" │01 | └───┘ NULL ┌───┐ │00 │ └───┘ "Defenestration" ┌───┬───┬───┬───┬───┬───┐ │02 │'D'│'e'│'f'│'e'│FF │ └───┼───┼───┼───┼───┼───┤ │'n'│'e'│'s'│'t'│FF │ ├───┼───┼───┼───┼───┤ │'r'│'a'│'t'│'r'│FF │ ├───┼───┼───┼───┼───┤ │'a'│'t'│'i'│'o'│FF │ ├───┼───┼───┼───┼───┤ │'n'│00 │00 │00 │01 │ └───┴───┴───┴───┴───┘ This approach is loosely inspired by COBS encoding, and chosen over more traditional byte stuffing as it is more amenable to vectorization, in particular hardware with AVX-256 can copy a 32-byte block in a single instruction. Dictionary Arrays Dictionary Encoded Data (called categorical in pandas) is increasingly important because they can store and process low cardinality data very efficiently. A simple approach to encoding dictionary arrays would be to encode the logical values directly using the encodings for primitive values described previously. However, this would lose the benefits of dictionary encoding to reduce memory and CPU consumption. To further complicate matters, the Arrow implementation of Dictionary encoding is quite general, and we can make no assumptions about the contents of the dictionaries. In particular, we cannot assume that the dictionary values are sorted, nor that the same dictionary is used for all arrays within a column The following example shows how a string column might be encoded in two arrays using two different dictionaries. The dictionary keys 0, 1, and 2 in the first batch correspond to different values than the same keys in the second dictionary. ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌───────────┐ ┌─────┐ │ │ │"Fabulous" │ │ 0 │ ├───────────┤ ├─────┤ │ │ │ "Bar" │ │ 2 │ ├───────────┤ ├─────┤ │ ┌───────────┐ │ │ "Soup" │ │ 2 │ │"Fabulous" │ └───────────┘ ├─────┤ │ ├───────────┤ │ │ 0 │ │ "Soup" │ ├─────┤ │ ├───────────┤ │ │ 1 │ │ "Soup" │ └─────┘ │ ├───────────┤ │ │"Fabulous" │ Values │ ├───────────┤ │ Dictionary (indexes in │ "Bar" │ dictionary) │ ├───────────┤ │ │ "ZZ" │ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ├───────────┤ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ "Bar" │ │ ├───────────┤ │ ┌───────────┐ ┌─────┐ │ "ZZ" │ │"Fabulous" │ │ 1 │ │ ├───────────┤ │ ├───────────┤ ├─────┤ │"Fabulous" │ │ "ZZ" │ │ 2 │ │ └───────────┘ │ ├───────────┤ ├─────┤ │ "Bar" │ │ 1 │ │ │ └───────────┘ ├─────┤ │ 0 │ │ Logical column │ └─────┘ values Values │ │ Dictionary (indexes in dictionary) │ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ The key observation which allows us to efficiently create a row format for this kind of data is that given a byte array, a new byte array can always be created which comes before or after it in the sort order by adding an additional byte. Therefore we can incrementally build an order-preserving mapping from dictionary values to variable length byte arrays, without needing to know all possible dictionary values beforehand, instead introducing mappings for new dictionary values as we encounter them. ┌──────────┐ ┌─────┐ │ "Bar" │ ───────────────▶│ 01 │ └──────────┘ └─────┘ ┌──────────┐ ┌─────┬─────┐ │"Fabulous"│ ───────────────▶│ 01 │ 02 │ └──────────┘ └─────┴─────┘ ┌──────────┐ ┌─────┐ │ "Soup" │ ───────────────▶│ 05 │ └──────────┘ └─────┘ ┌──────────┐ ┌─────┐ │ "ZZ" │ ───────────────▶│ 07 │ └──────────┘ └─────┘ Example Order Preserving Mapping The details of the data structure used to generate this mapping are beyond the scope of this blog post, but may be the topic of a future post. You can find the code here. The data structure also ensures that no values contain 0x00 and therefore we can encode the arrays directly using 0x00 as an end-delimiter. A null value is encoded as a single 0x00 byte, and a non-null value encoded as a single 0x01 byte, followed by the 0x00 terminated byte array determined by the order preserving mapping ┌─────┬─────┬─────┬─────┐ "Fabulous" │ 01 │ 03 │ 05 │ 00 │ └─────┴─────┴─────┴─────┘ ┌─────┬─────┬─────┐ "ZZ" │ 01 │ 07 │ 00 │ └─────┴─────┴─────┘ ┌─────┐ NULL │ 00 │ └─────┘ Input Row Format Sort Options One detail we have so far ignored over is how to support ascending and descending sorts (e.g. ASC or DESC in SQL). The Arrow Rust row format supports these options by simply inverting the bytes of the encoded representation, except the initial byte used for nullability encoding, on a per column basis. Similarly, supporting SQL compatible sorting also requires a format that can specify the order of NULLs (before or after all non NULL values). The row format supports this option by optionally encoding nulls as 0xFF instead of 0x00 on a per column basis. Conclusion Hopefully these two articles have given you a flavor of what is possible with a comparable row format and how it works. Feel free to check out the docs for instructions on getting started, and report any issues on our bugtracker. Using this format for lexicographic sorting is more than 3x faster than the comparator based approach, with the benefits especially pronounced for strings, dictionaries and sorts with large numbers of columns. We have also already used it to more than double the performance of sort preserving merge in the DataFusion project, and expect similar or greater performance uplift as we apply it to sort, grouping, join, and window function operators as well. As always, the Arrow community very much looks forward to seeing what you build with it!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Expanding Arrow’s Reach with a JDBC Driver for Arrow Flight SQL</title><link href="https://arrow.apache.org/blog/2022/11/01/arrow-flight-sql-jdbc/" rel="alternate" type="text/html" title="Expanding Arrow’s Reach with a JDBC Driver for Arrow Flight SQL" /><published>2022-11-01T00:00:00-04:00</published><updated>2022-11-01T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/11/01/arrow-flight-sql-jdbc</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/11/01/arrow-flight-sql-jdbc/"><![CDATA[<!--

-->

<p>We’re excited to announce that as of version 10.0.0, the Arrow project
now includes a <a href="https://docs.oracle.com/javase/tutorial/jdbc/overview/index.html">JDBC</a> driver <a href="https://github.com/apache/arrow/tree/master/java/flight/flight-sql-jdbc-driver">implementation</a> based on
<a href="/docs/format/FlightSql.html">Arrow Flight SQL</a>.  This is courtesy of a software grant
from <a href="https://www.dremio.com/resources/guides/apache-arrow/">Dremio</a>, a data lakehouse platform. Contributors
from Dremio developed and open-sourced this driver implementation, in
addition to designing and contributing Flight SQL itself.</p>

<p>Flight SQL is a protocol for client-server database interactions.  It
defines how a client should talk to a server and execute queries,
fetch result sets, and so on.  Note that despite the name, Flight SQL
is <em>not</em> a SQL dialect, or even specific to SQL itself.  Underneath,
it builds on <a href="/docs/format/Flight.html">Arrow Flight RPC</a>, a framework for efficient
transfer of Arrow data across the network.  While Flight RPC is
flexible and can be used in any type of application, from the
beginning, it was designed with an eye towards the kinds of use cases
that Flight SQL supports.</p>

<p>With this new JDBC driver, applications can talk to any database
server implementing the Flight SQL protocol using familiar JDBC APIs.
Underneath, the driver sends queries to the server via Flight SQL and
adapts the Arrow result set to the JDBC interface, so that the
database can support JDBC users without implementing additional APIs
or its own JDBC driver.</p>

<h2 id="why-use-jdbc-with-flight-sql">Why use JDBC with Flight SQL?</h2>

<p>JDBC offers a row-oriented API, which is opposite of Arrow’s columnar
structure.  However, it is a popular and time-tested choice for many
applications.  For example, many business intelligence (BI) tools take
advantage of JDBC to interoperate generically with multiple databases.
An Arrow-native database may still wish to be accessible to all of
this existing software, without having to implement multiple client
drivers itself.  Additionally, columnar data transfer alone can be a
<a href="https://ir.cwi.nl/pub/26415">significant speedup</a> for analytical use cases.</p>

<p>This JDBC driver implementation demonstrates the generality of Arrow
and Flight SQL, and increases the reach of Arrow-based applications.
Additionally, an <a href="https://docs.dremio.com/software/drivers/arrow-flight-sql-odbc-driver/">ODBC driver implementation</a> based on
Flight SQL is also available courtesy of Dremio, though it is not yet
part of the Arrow project due to dependency licensing issues.</p>

<p>Now, a database can support the vast body of existing code that uses
JDBC or ODBC, as well as Arrow-native applications, just by
implementing a single wire protocol: Flight SQL.  Some projects
instead do things like reimplementing the Postgres wire protocol to
benefit from its existing drivers.  But for Arrow-native databases,
this gives up the benefits of columnar data.  On the other hand,
Flight SQL is:</p>

<ol>
  <li>Columnar and Arrow-native, using Arrow for result sets to avoid
unnecessary data copies and transformations;</li>
  <li>Designed for implementation by multiple databases, with high-level
C++ and Java libraries and a Protobuf protocol definition; and</li>
  <li>Usable both through APIs like JDBC and ODBC thanks to this software
grant, as well as directly (or via <a href="htttps://github.com/apache/arrow-adbc">ADBC</a>) for applications
that want columnar data.</li>
</ol>

<h2 id="getting-involved">Getting Involved</h2>

<p>The JDBC driver was merged for the <a href="/blog/2022/10/31/10.0.0-release/">Arrow 10.0.0 release</a>, and
the <a href="https://github.com/apache/arrow/tree/master/java/flight/flight-sql-jdbc-driver">source code</a> can be found in the Arrow repository.
Official builds of the driver are <a href="https://search.maven.org/search?q=a:flight-sql-jdbc-driver">available on Maven Central</a>.
Dremio is already making use of the driver, and we’re looking forward
to seeing what else gets built on top.  Of course, there are still
improvements to be made.  If you’re interested in contributing, or
have feedback or questions, please reach out on the <a href="/community/">mailing list</a>
or <a href="htttps://github.com/apache/arrow">GitHub</a>.</p>

<p>To learn more about when to use the Flight SQL JDBC driver vs the
Flight SQL native client libraries, see this section of Dremio’s
presentation, <a href="https://www.youtube.com/watch?v=6q8AMrQV3vE&amp;t=1343s">“Apache Arrow Flight SQL: a universal standard for high
performance data transfers from databases”</a>
(starting at 22:23).  For more about how Dremio uses Apache Arrow, see
their <a href="https://www.dremio.com/resources/guides/apache-arrow/">guide</a>.</p>]]></content><author><name>pmc</name></author><category term="application" /><summary type="html"><![CDATA[We’re excited to announce that as of version 10.0.0, the Arrow project now includes a JDBC driver implementation based on Arrow Flight SQL. This is courtesy of a software grant from Dremio, a data lakehouse platform. Contributors from Dremio developed and open-sourced this driver implementation, in addition to designing and contributing Flight SQL itself. Flight SQL is a protocol for client-server database interactions. It defines how a client should talk to a server and execute queries, fetch result sets, and so on. Note that despite the name, Flight SQL is not a SQL dialect, or even specific to SQL itself. Underneath, it builds on Arrow Flight RPC, a framework for efficient transfer of Arrow data across the network. While Flight RPC is flexible and can be used in any type of application, from the beginning, it was designed with an eye towards the kinds of use cases that Flight SQL supports. With this new JDBC driver, applications can talk to any database server implementing the Flight SQL protocol using familiar JDBC APIs. Underneath, the driver sends queries to the server via Flight SQL and adapts the Arrow result set to the JDBC interface, so that the database can support JDBC users without implementing additional APIs or its own JDBC driver. Why use JDBC with Flight SQL? JDBC offers a row-oriented API, which is opposite of Arrow’s columnar structure. However, it is a popular and time-tested choice for many applications. For example, many business intelligence (BI) tools take advantage of JDBC to interoperate generically with multiple databases. An Arrow-native database may still wish to be accessible to all of this existing software, without having to implement multiple client drivers itself. Additionally, columnar data transfer alone can be a significant speedup for analytical use cases. This JDBC driver implementation demonstrates the generality of Arrow and Flight SQL, and increases the reach of Arrow-based applications. Additionally, an ODBC driver implementation based on Flight SQL is also available courtesy of Dremio, though it is not yet part of the Arrow project due to dependency licensing issues. Now, a database can support the vast body of existing code that uses JDBC or ODBC, as well as Arrow-native applications, just by implementing a single wire protocol: Flight SQL. Some projects instead do things like reimplementing the Postgres wire protocol to benefit from its existing drivers. But for Arrow-native databases, this gives up the benefits of columnar data. On the other hand, Flight SQL is: Columnar and Arrow-native, using Arrow for result sets to avoid unnecessary data copies and transformations; Designed for implementation by multiple databases, with high-level C++ and Java libraries and a Protobuf protocol definition; and Usable both through APIs like JDBC and ODBC thanks to this software grant, as well as directly (or via ADBC) for applications that want columnar data. Getting Involved The JDBC driver was merged for the Arrow 10.0.0 release, and the source code can be found in the Arrow repository. Official builds of the driver are available on Maven Central. Dremio is already making use of the driver, and we’re looking forward to seeing what else gets built on top. Of course, there are still improvements to be made. If you’re interested in contributing, or have feedback or questions, please reach out on the mailing list or GitHub. To learn more about when to use the Flight SQL JDBC driver vs the Flight SQL native client libraries, see this section of Dremio’s presentation, “Apache Arrow Flight SQL: a universal standard for high performance data transfers from databases” (starting at 22:23). For more about how Dremio uses Apache Arrow, see their guide.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 10.0.0 Release</title><link href="https://arrow.apache.org/blog/2022/10/31/10.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 10.0.0 Release" /><published>2022-10-31T00:00:00-04:00</published><updated>2022-10-31T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/10/31/10.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/10/31/10.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 10.0.0 release. This covers
over 3 months of development work and includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%2010.0.0"><strong>473 resolved issues</strong></a>
from <a href="/release/10.0.0.html#contributors"><strong>100 distinct contributors</strong></a>. See the <a href="https://arrow.apache.org/install/">Install Page</a>
to learn how to get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/10.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 9.0.0 release, Yanghong Zhong, Remzi Yang, Dan Harris and
Bogumił Kamińsk have been invited to be committers.
L.C. Hsieh, Weston Pace, Raphael Taylor-Davies, Nicola Crane and
Jacob Quinn have joined the Project Management Committee (PMC).
Thanks for your contributions and participation in the project!</p>

<h2 id="columnar-format-notes">Columnar Format Notes</h2>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>

<p>A JDBC driver based on <a href="https://arrow.apache.org/docs/format/FlightSql.html">Arrow Flight SQL</a> is now available, courtesy of a code donation from Dremio (<a href="https://issues.apache.org/jira/browse/ARROW-7744">ARROW-7744</a>).
For more details, see <a href="/blog/2022/11/01/arrow-flight-sql-jdbc/">“Expanding Arrow’s Reach with a JDBC Driver for Arrow Flight SQL”</a>.
Flight SQL is now supported in Go (<a href="https://issues.apache.org/jira/browse/ARROW-17326">ARROW-17326</a>).
Protocol definitions for transactions and <a href="https://substrait.io">Substrait</a> plans were added to Flight SQL and are implemented in C++ and Java (<a href="https://issues.apache.org/jira/browse/ARROW-17688">ARROW-17688</a>).
General “best practices” documentation was added for C++ (<a href="https://issues.apache.org/jira/browse/ARROW-17407">ARROW-17407</a>).
The C++ implementation now has basic <a href="https://opentelemetry.io/">OpenTelemetry</a> integration (<a href="https://issues.apache.org/jira/browse/ARROW-14958">ARROW-14958</a>).</p>

<h2 id="c-notes">C++ notes</h2>

<h3 id="c11-is-no-longer-supported">C++11 is no longer supported</h3>

<p>The Arrow C++ codebase has moved to C++17 as its language compatibility
standard (<a href="https://issues.apache.org/jira/browse/ARROW-17545">ARROW-17545</a>). This means Arrow C++, including its header files,
now requires a C++17-compliant compiler and standard library to be used.
Such compilers are widely available on most platforms.</p>

<p>Compatibility backports of C++14 and C++17 features, such as <code class="language-plaintext highlighter-rouge">std::string_view</code>
or <code class="language-plaintext highlighter-rouge">std::variant</code>, have been removed in favor of the standard library version
of these APIs (<a href="https://issues.apache.org/jira/browse/ARROW-17546">ARROW-17546</a>). This will also make integration of Arrow C++
with other codebases easier.</p>

<p>It is expected that the Arrow C++ codebase will be gradually modernized to use
C++17 features in subsequent releases, when the need arises.</p>

<h3 id="plasma-is-deprecated">Plasma is deprecated</h3>

<p>The Plasma module is deprecated and will be removed in a future release.</p>

<h3 id="compute--acero">Compute / Acero</h3>

<p>Extension types are now supported in hash joins (<a href="https://issues.apache.org/jira/browse/ARROW-16695">ARROW-16695</a>).</p>

<h3 id="datasets">Datasets</h3>

<p>The fragments of a dataset can now be iterated in an asynchronous fashion,
using <code class="language-plaintext highlighter-rouge">Dataset::GetFragmentsAsync</code> (<a href="https://issues.apache.org/jira/browse/ARROW-17318">ARROW-17318</a>).</p>

<h3 id="filesystems">Filesystems</h3>

<p>It is now possible to configure a timeout policy for S3 (<a href="https://issues.apache.org/jira/browse/ARROW-16521">ARROW-16521</a>).</p>

<p>Error messages for S3 have been improved to give more context about the
error (<a href="https://issues.apache.org/jira/browse/ARROW-17079">ARROW-17079</a>).</p>

<p><code class="language-plaintext highlighter-rouge">GetFileInfoGenerator</code> has been optimized for local filesystems, with
dedicated options to tune chunking and readahead (<a href="https://issues.apache.org/jira/browse/ARROW-17306">ARROW-17306</a>).</p>

<h3 id="json">JSON</h3>

<p>Previously, the JSON reader could only read Decimal fields from JSON strings
(i.e. quoted). Now it can also read Decimal fields from JSON numbers as well
(<a href="https://issues.apache.org/jira/browse/ARROW-17847">ARROW-17847</a>).</p>

<h3 id="parquet">Parquet</h3>

<p>Before Arrow 3.0.0, data pages version 2 were incorrectly written out, making
them unreadable with spec-compliant readers. A compatibility fix has been
introduced so that they can still be read with contemporary versions of
Arrow (<a href="https://issues.apache.org/jira/browse/ARROW-17100">ARROW-17100</a>).</p>

<h3 id="substrait">Substrait</h3>

<p>The Substrait consumer, which allows Substrait plans to be executed by the
Acero execution engine, has received some improvements:</p>

<ul>
  <li>
    <p>Aggregations are now supported (<a href="https://issues.apache.org/jira/browse/ARROW-15591">ARROW-15591</a>).</p>
  </li>
  <li>
    <p>Conversion options have been added so that the level of compliance and
rountrippability can be chosen when converting between Substrait and
Acero representations of a plan (<a href="https://issues.apache.org/jira/browse/ARROW-16988">ARROW-16988</a>).</p>
  </li>
  <li>
    <p>Support for many standard Substrait functions has been added
(<a href="https://issues.apache.org/jira/browse/ARROW-15582">ARROW-15582</a>, <a href="https://issues.apache.org/jira/browse/ARROW-17523">ARROW-17523</a>)</p>
  </li>
</ul>

<p>Some work has also been done in the reverse direction, to allow Acero execution
plans to be serialized as Substrait plans (<a href="https://issues.apache.org/jira/browse/ARROW-16855">ARROW-16855</a>).</p>

<h3 id="other-changes">Other changes</h3>

<p>Our CMake package files have been overhauled (<a href="https://issues.apache.org/jira/browse/ARROW-12175">ARROW-12175</a>).  As a result,
namespaced targets are now exported, such as <code class="language-plaintext highlighter-rouge">Arrow::arrow_shared</code>.
Legacy (non-namespaced) names are still available, for example <code class="language-plaintext highlighter-rouge">arrow_shared</code>.</p>

<p>Compiling in release mode now uses <code class="language-plaintext highlighter-rouge">-O2</code>, not <code class="language-plaintext highlighter-rouge">-O3</code>, by default (<a href="https://issues.apache.org/jira/browse/ARROW-17436">ARROW-17436</a>).</p>

<p>The RISC-V architecture is now recongnized at build time (<a href="https://issues.apache.org/jira/browse/ARROW-17440">ARROW-17440</a>).</p>

<p>The PyArrow-specific C++ code was moved into the PyArrow source tree
(see below, <a href="https://issues.apache.org/jira/browse/ARROW-16340">ARROW-16340</a>). The <code class="language-plaintext highlighter-rouge">ARROW_PYTHON</code> CMake variable has been deprecated
and will be removed in a later release; you should instead enable the necessary
components explicitly (<a href="https://issues.apache.org/jira/browse/ARROW-17868">ARROW-17868</a>).</p>

<p>Some classes with a <code class="language-plaintext highlighter-rouge">Equals</code> method now also support <code class="language-plaintext highlighter-rouge">operator==</code> (<a href="https://issues.apache.org/jira/browse/ARROW-6772">ARROW-6772</a>).
It was decided to only do this when equality is computationally cheap (i.e.
not on data collections such as Array, RecordBatch…).</p>

<h2 id="c-notes-1">C# notes</h2>

<h4 id="bug-fixes">Bug Fixes</h4>

<ul>
  <li>DecimalArray incorrectly appends values very large and very small values. (<a href="https://github.com/apache/arrow/pull/13732">ARROW-17223</a>)</li>
</ul>

<h2 id="gandiva-notes">Gandiva notes</h2>

<p>Gandiva has been migrated to use LLVM opaque pointer types, as typed
pointers had been deprecated (<a href="https://issues.apache.org/jira/browse/ARROW-17790">ARROW-17790</a>).</p>

<h2 id="go-notes">Go notes</h2>

<ul>
  <li>A new CI job has been added to run all of the tests with the <code class="language-plaintext highlighter-rouge">-asan</code> option using go1.18 (<a href="https://issues.apache.org/jira/browse/ARROW-17324">ARROW-17324</a>)</li>
  <li>Go now passes all integration tests on data types and IPC handling.</li>
  <li>The Go Arrow and Parquet packages now require go1.17+ (<a href="https://issues.apache.org/jira/browse/ARROW-17646">ARROW-17646</a>)</li>
</ul>

<h3 id="compute">Compute</h3>

<p>The compute package that was importable via <code class="language-plaintext highlighter-rouge">github.com/apache/arrow/go/v9/arrow/compute</code> is now a separate module which requires go1.18+ (only the compute module, the rest of the packages still work fine under go1.17). (<a href="https://issues.apache.org/jira/browse/ARROW-17456">ARROW-17456</a>).</p>

<p>Scalar and Vector kernel infrastructure has been implemented for performing compute operations providing the following functionality:</p>

<ul>
  <li>casting Arrow Arrays from one type to another (<a href="https://issues.apache.org/jira/browse/ARROW-17454">ARROW-17454</a>)</li>
  <li>Using Filter and Take functions on an Arrow Array, Record Batch, or Table (<a href="https://issues.apache.org/jira/browse/ARROW-17669">ARROW-17669</a>)</li>
</ul>

<h3 id="arrow">Arrow</h3>

<ul>
  <li>Sparse and Dense Union Arrays have been implemented along with appropriate builders and data type support including in IPC reading and writing. (<a href="https://issues.apache.org/jira/browse/ARROW-3678">ARROW-3678</a>, <a href="https://issues.apache.org/jira/browse/ARROW-17276">ARROW-17276</a>). This includes scalar types for Dense and Sparse union (<a href="https://issues.apache.org/jira/browse/ARROW-17390">ARROW-17390</a>)</li>
  <li>LargeBinary, LargeString and LargeList arrays have been implemented for handling arrays with 64-bit offsets. This also included fixing a bug so that binary builders are correctly resettable. (<a href="https://issues.apache.org/jira/browse/ARROW-8226">ARROW-8226</a>, <a href="https://issues.apache.org/jira/browse/ARROW-17275">ARROW-17275</a>)</li>
  <li>Support for Decimal256 arrays has been implemented (<a href="https://issues.apache.org/jira/browse/ARROW-10600">ARROW-10600</a>)</li>
  <li>Automatic Endianness Conversion for non-native endianness is now an option for IPC streams (<a href="https://issues.apache.org/jira/browse/ARROW-17219">ARROW-17219</a>)</li>
  <li>CSV Writer now supports Timestamp, Date32 and Date64 types (<a href="https://issues.apache.org/jira/browse/ARROW-17273">ARROW-17273</a>)</li>
  <li>CSV Writer now supports custom formatting for boolean values (<a href="https://issues.apache.org/jira/browse/ARROW-17277">ARROW-17277</a>)</li>
  <li>The Go Arrow Library now provides a FlightSQL client and server implementation (<a href="https://issues.apache.org/jira/browse/ARROW-17326">ARROW-17326</a>). An example server implementation is provided for a FlightSQL server using SQLite (<a href="https://issues.apache.org/jira/browse/ARROW-17359">ARROW-17359</a>)</li>
  <li>CSV Reader now supports schema type inference via NewInferringReader, along with options for specifying the type of some columns and skipping columns (<a href="https://issues.apache.org/jira/browse/ARROW-17778">ARROW-17778</a>)</li>
</ul>

<h3 id="parquet-1">Parquet</h3>

<ul>
  <li>RowGroupReader.Column(index int) no longer panics if provided an invalid column index. Instead the signature has been changed to now return (PageReader, error) similar to other methods in the codebase. (<a href="https://issues.apache.org/jira/browse/ARROW-17274">ARROW-17274</a>)</li>
  <li>Bitpacking and other internal required implementations for ppc64le have been added for the Parquet library (<a href="https://issues.apache.org/jira/browse/ARROW-17372">ARROW-17372</a>)</li>
  <li>A bug has been fixed that caused inconsistent row information data from a table written by Athena (<a href="https://issues.apache.org/jira/browse/ARROW-17453">ARROW-17453</a>)</li>
  <li>Fixed a bug that caused panics when writing a Nullable List of Structs (<a href="https://issues.apache.org/jira/browse/ARROW-17169">ARROW-17169</a>)</li>
  <li>Key Value metadata in an Arrow Schema will be propagated to the Parquet file when using pqarrow even when not using StoreSchema (<a href="https://issues.apache.org/jira/browse/ARROW-17627">ARROW-17627</a>)</li>
  <li>A memory leak when using statistics on ByteArray columns has been fixed (<a href="https://issues.apache.org/jira/browse/ARROW-17573">ARROW-17573</a>)</li>
</ul>

<h2 id="java-notes">Java notes</h2>
<p>Many important features, enhancements, and bug fixes are included in this release, as are documentation enhancements, and a large number of improvements to build processes and project infrastructure. Selected highlights can be found below.</p>

<h4 id="new-features-and-enhancements">New Features and Enhancements</h4>

<ul>
  <li>JDBC Driver for Arrow Flight SQL (<a href="https://github.com/apache/arrow/pull/13800">13800</a>)</li>
  <li>Initial implementation of immutable Table API (<a href="https://github.com/apache/arrow/pull/14316">14316</a>)</li>
  <li>Substrait, transaction, cancellation for Flight SQL (<a href="https://github.com/apache/arrow/pull/13492">13492</a>)</li>
  <li>Read Arrow IPC, CSV, and ORC files by NativeDatasetFactory (<a href="https://github.com/apache/arrow/pull/13811">13811</a>, <a href="https://github.com/apache/arrow/pull/13973">13973</a>, <a href="https://github.com/apache/arrow/pull/14182">14182</a>)</li>
  <li>Add utility to bind Arrow data to JDBC parameters (<a href="https://github.com/apache/arrow/pull/13589">13589</a>)</li>
</ul>

<h4 id="build-enhancements">Build enhancements</h4>

<ul>
  <li>Add Windows build script that produces DLLs (<a href="https://github.com/apache/arrow/pull/14203">14203</a>)</li>
  <li>C Data Interface and Dataset libraries can now be built with mvn commands (<a href="https://github.com/apache/arrow/pull/13881">13881</a>, <a href="https://github.com/apache/arrow/pull/13889">13889</a>)</li>
</ul>

<h4 id="java-notes-1">Java notes:</h4>

<ul>
  <li>Java Plasma JNI bindings have been deprecated (<a href="https://github.com/apache/arrow/pull/14262">14262</a>)
    <h2 id="javascript-notes">JavaScript notes</h2>
  </li>
  <li>No major changes.</li>
</ul>

<h2 id="python-notes">Python notes</h2>

<p>Compatibility notes:</p>

<ul>
  <li>Some deprecated APIs (deprecated at least since pyarrow 1.0.0) have been removed: the
<code class="language-plaintext highlighter-rouge">RecordBatchReader.get_next_batch</code> method, <code class="language-plaintext highlighter-rouge">DataType.num_children</code> attribute, etc
(<a href="https://issues.apache.org/jira/browse/ARROW-17649">ARROW-17649</a>).</li>
  <li>When writing to Arrow IPC file format with <code class="language-plaintext highlighter-rouge">pyarrow.dataset.write_dataset</code> using
<code class="language-plaintext highlighter-rouge">format="ipc"</code> or <code class="language-plaintext highlighter-rouge">format="arrow"</code>, the default extension for the resulting files is
changed to <code class="language-plaintext highlighter-rouge">.arrow</code> instead of <code class="language-plaintext highlighter-rouge">.feather</code>. You can still use <code class="language-plaintext highlighter-rouge">format="feather"</code> to
write identical files but using the <code class="language-plaintext highlighter-rouge">.feather</code> extension
(<a href="https://issues.apache.org/jira/browse/ARROW-17089">ARROW-17089</a>).</li>
</ul>

<p>New features and improvements:</p>

<ul>
  <li>Filters in <code class="language-plaintext highlighter-rouge">pq.read_table()</code> can be passed as an expression in addition to the legacy
list of tuples. For example, <code class="language-plaintext highlighter-rouge">filters=pc.field("col") &lt; 4</code> is equivalent to
<code class="language-plaintext highlighter-rouge">filters=[("col", "&lt;", 4)]</code>
(<a href="https://issues.apache.org/jira/browse/ARROW-17483">ARROW-17483</a>).</li>
  <li>The <code class="language-plaintext highlighter-rouge">batch_readahead</code> and <code class="language-plaintext highlighter-rouge">fragment_readahead</code> arguments for scanning Datasets are
exposed in Python (<a href="https://issues.apache.org/jira/browse/ARROW-17299">ARROW-17299</a>).</li>
  <li>ExtensionArrays can now be created from a storage array through the <code class="language-plaintext highlighter-rouge">pa.array(..)</code>
constructor (<a href="https://issues.apache.org/jira/browse/ARROW-17834">ARROW-17834</a>).</li>
  <li>Converting ListArrays containing ExtensionArray values to numpy or pandas works by
falling back to the storage array
(<a href="https://issues.apache.org/jira/browse/ARROW-17813">ARROW-17813</a>).</li>
  <li>The <code class="language-plaintext highlighter-rouge">pyarrow.substrait.run_query()</code> function gained a <code class="language-plaintext highlighter-rouge">table_provider</code> keyword to run
the query against in-memory tables
(<a href="https://issues.apache.org/jira/browse/ARROW-17521">ARROW-17521</a>).</li>
  <li>The <code class="language-plaintext highlighter-rouge">StructType</code> class gained a <code class="language-plaintext highlighter-rouge">field()</code> method to retrieve a child field
(<a href="https://issues.apache.org/jira/browse/ARROW-17131">ARROW-17131</a>).</li>
  <li>Casting Tables to a new schema now honors the nullability flag in the target schema
(<a href="https://issues.apache.org/jira/browse/ARROW-16651">ARROW-16651</a>).</li>
  <li>Parquet files are now explicitly closed after reading
(<a href="https://issues.apache.org/jira/browse/ARROW-13763">ARROW-13763</a>).</li>
</ul>

<p>Further, the Python bindings benefit from improvements in the C++ library
(e.g. new compute functions); see the C++ notes above for additional details.</p>

<p><strong>Build notes</strong></p>

<p>The PyArrow-specific C++ code, previously part of Arrow C++ codebase, is now integrated
into PyArrow. The tests are run automatically as part of the PyArrow test suite. See:
<a href="https://issues.apache.org/jira/browse/ARROW-16340">ARROW-16340</a>,
<a href="https://issues.apache.org/jira/browse/ARROW-17122">ARROW-17122</a> and
<a href="https://arrow.apache.org/docs/dev/python/integration/extending.html#c-api">PyArrow C++ API notes</a>).</p>

<p>The build process is generally not affected by the change, but the <code class="language-plaintext highlighter-rouge">ARROW_PYTHON</code> CMake
variable has been deprecated and will be removed in a later release; you should instead
enable the necessary components explicitly
(<a href="https://issues.apache.org/jira/browse/ARROW-17868">ARROW-17868</a>).</p>

<h2 id="r-notes">R notes</h2>
<p>Many improvements to Arrow dplyr queries are added in this version, including:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">dplyr::across()</code> can be used to apply the same computation across multiple columns;</li>
  <li>long-running queries can now be cancelled;</li>
  <li>the data source file name can be added as a column when reading multi-file datasets with <code class="language-plaintext highlighter-rouge">add_filename()</code>;</li>
  <li>joins now support extension arrays;</li>
  <li>and all supported Arrow dplyr functions are now documented on the <a href="https://arrow.apache.org/docs/r/reference/acero.html">R documentation site</a>.</li>
</ul>

<p>For more on what’s in the 10.0.0 R package, see the <a href="/docs/r/news/">R changelog</a>.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<h3 id="ruby">Ruby</h3>

<ul>
  <li>Plasma binding has been deprecated (<a href="https://issues.apache.org/jira/browse/ARROW-17864">ARROW-17864</a>)</li>
</ul>

<h3 id="c-glib">C GLib</h3>

<ul>
  <li>Plasma binding has been deprecated (<a href="https://issues.apache.org/jira/browse/ARROW-17862">ARROW-17862</a>)</li>
</ul>

<h2 id="rust-notes">Rust notes</h2>

<p>The Rust projects have moved to separate repositories outside the
main Arrow monorepo. For notes on the latest release of the Rust
implementation, see the latest <a href="https://github.com/apache/arrow-rs/tags">Arrow Rust changelog</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 10.0.0 release. This covers over 3 months of development work and includes 473 resolved issues from 100 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 9.0.0 release, Yanghong Zhong, Remzi Yang, Dan Harris and Bogumił Kamińsk have been invited to be committers. L.C. Hsieh, Weston Pace, Raphael Taylor-Davies, Nicola Crane and Jacob Quinn have joined the Project Management Committee (PMC). Thanks for your contributions and participation in the project! Columnar Format Notes Arrow Flight RPC notes A JDBC driver based on Arrow Flight SQL is now available, courtesy of a code donation from Dremio (ARROW-7744). For more details, see “Expanding Arrow’s Reach with a JDBC Driver for Arrow Flight SQL”. Flight SQL is now supported in Go (ARROW-17326). Protocol definitions for transactions and Substrait plans were added to Flight SQL and are implemented in C++ and Java (ARROW-17688). General “best practices” documentation was added for C++ (ARROW-17407). The C++ implementation now has basic OpenTelemetry integration (ARROW-14958). C++ notes C++11 is no longer supported The Arrow C++ codebase has moved to C++17 as its language compatibility standard (ARROW-17545). This means Arrow C++, including its header files, now requires a C++17-compliant compiler and standard library to be used. Such compilers are widely available on most platforms. Compatibility backports of C++14 and C++17 features, such as std::string_view or std::variant, have been removed in favor of the standard library version of these APIs (ARROW-17546). This will also make integration of Arrow C++ with other codebases easier. It is expected that the Arrow C++ codebase will be gradually modernized to use C++17 features in subsequent releases, when the need arises. Plasma is deprecated The Plasma module is deprecated and will be removed in a future release. Compute / Acero Extension types are now supported in hash joins (ARROW-16695). Datasets The fragments of a dataset can now be iterated in an asynchronous fashion, using Dataset::GetFragmentsAsync (ARROW-17318). Filesystems It is now possible to configure a timeout policy for S3 (ARROW-16521). Error messages for S3 have been improved to give more context about the error (ARROW-17079). GetFileInfoGenerator has been optimized for local filesystems, with dedicated options to tune chunking and readahead (ARROW-17306). JSON Previously, the JSON reader could only read Decimal fields from JSON strings (i.e. quoted). Now it can also read Decimal fields from JSON numbers as well (ARROW-17847). Parquet Before Arrow 3.0.0, data pages version 2 were incorrectly written out, making them unreadable with spec-compliant readers. A compatibility fix has been introduced so that they can still be read with contemporary versions of Arrow (ARROW-17100). Substrait The Substrait consumer, which allows Substrait plans to be executed by the Acero execution engine, has received some improvements: Aggregations are now supported (ARROW-15591). Conversion options have been added so that the level of compliance and rountrippability can be chosen when converting between Substrait and Acero representations of a plan (ARROW-16988). Support for many standard Substrait functions has been added (ARROW-15582, ARROW-17523) Some work has also been done in the reverse direction, to allow Acero execution plans to be serialized as Substrait plans (ARROW-16855). Other changes Our CMake package files have been overhauled (ARROW-12175). As a result, namespaced targets are now exported, such as Arrow::arrow_shared. Legacy (non-namespaced) names are still available, for example arrow_shared. Compiling in release mode now uses -O2, not -O3, by default (ARROW-17436). The RISC-V architecture is now recongnized at build time (ARROW-17440). The PyArrow-specific C++ code was moved into the PyArrow source tree (see below, ARROW-16340). The ARROW_PYTHON CMake variable has been deprecated and will be removed in a later release; you should instead enable the necessary components explicitly (ARROW-17868). Some classes with a Equals method now also support operator== (ARROW-6772). It was decided to only do this when equality is computationally cheap (i.e. not on data collections such as Array, RecordBatch…). C# notes Bug Fixes DecimalArray incorrectly appends values very large and very small values. (ARROW-17223) Gandiva notes Gandiva has been migrated to use LLVM opaque pointer types, as typed pointers had been deprecated (ARROW-17790). Go notes A new CI job has been added to run all of the tests with the -asan option using go1.18 (ARROW-17324) Go now passes all integration tests on data types and IPC handling. The Go Arrow and Parquet packages now require go1.17+ (ARROW-17646) Compute The compute package that was importable via github.com/apache/arrow/go/v9/arrow/compute is now a separate module which requires go1.18+ (only the compute module, the rest of the packages still work fine under go1.17). (ARROW-17456). Scalar and Vector kernel infrastructure has been implemented for performing compute operations providing the following functionality: casting Arrow Arrays from one type to another (ARROW-17454) Using Filter and Take functions on an Arrow Array, Record Batch, or Table (ARROW-17669) Arrow Sparse and Dense Union Arrays have been implemented along with appropriate builders and data type support including in IPC reading and writing. (ARROW-3678, ARROW-17276). This includes scalar types for Dense and Sparse union (ARROW-17390) LargeBinary, LargeString and LargeList arrays have been implemented for handling arrays with 64-bit offsets. This also included fixing a bug so that binary builders are correctly resettable. (ARROW-8226, ARROW-17275) Support for Decimal256 arrays has been implemented (ARROW-10600) Automatic Endianness Conversion for non-native endianness is now an option for IPC streams (ARROW-17219) CSV Writer now supports Timestamp, Date32 and Date64 types (ARROW-17273) CSV Writer now supports custom formatting for boolean values (ARROW-17277) The Go Arrow Library now provides a FlightSQL client and server implementation (ARROW-17326). An example server implementation is provided for a FlightSQL server using SQLite (ARROW-17359) CSV Reader now supports schema type inference via NewInferringReader, along with options for specifying the type of some columns and skipping columns (ARROW-17778) Parquet RowGroupReader.Column(index int) no longer panics if provided an invalid column index. Instead the signature has been changed to now return (PageReader, error) similar to other methods in the codebase. (ARROW-17274) Bitpacking and other internal required implementations for ppc64le have been added for the Parquet library (ARROW-17372) A bug has been fixed that caused inconsistent row information data from a table written by Athena (ARROW-17453) Fixed a bug that caused panics when writing a Nullable List of Structs (ARROW-17169) Key Value metadata in an Arrow Schema will be propagated to the Parquet file when using pqarrow even when not using StoreSchema (ARROW-17627) A memory leak when using statistics on ByteArray columns has been fixed (ARROW-17573) Java notes Many important features, enhancements, and bug fixes are included in this release, as are documentation enhancements, and a large number of improvements to build processes and project infrastructure. Selected highlights can be found below. New Features and Enhancements JDBC Driver for Arrow Flight SQL (13800) Initial implementation of immutable Table API (14316) Substrait, transaction, cancellation for Flight SQL (13492) Read Arrow IPC, CSV, and ORC files by NativeDatasetFactory (13811, 13973, 14182) Add utility to bind Arrow data to JDBC parameters (13589) Build enhancements Add Windows build script that produces DLLs (14203) C Data Interface and Dataset libraries can now be built with mvn commands (13881, 13889) Java notes: Java Plasma JNI bindings have been deprecated (14262) JavaScript notes No major changes. Python notes Compatibility notes: Some deprecated APIs (deprecated at least since pyarrow 1.0.0) have been removed: the RecordBatchReader.get_next_batch method, DataType.num_children attribute, etc (ARROW-17649). When writing to Arrow IPC file format with pyarrow.dataset.write_dataset using format="ipc" or format="arrow", the default extension for the resulting files is changed to .arrow instead of .feather. You can still use format="feather" to write identical files but using the .feather extension (ARROW-17089). New features and improvements: Filters in pq.read_table() can be passed as an expression in addition to the legacy list of tuples. For example, filters=pc.field("col") &lt; 4 is equivalent to filters=[("col", "&lt;", 4)] (ARROW-17483). The batch_readahead and fragment_readahead arguments for scanning Datasets are exposed in Python (ARROW-17299). ExtensionArrays can now be created from a storage array through the pa.array(..) constructor (ARROW-17834). Converting ListArrays containing ExtensionArray values to numpy or pandas works by falling back to the storage array (ARROW-17813). The pyarrow.substrait.run_query() function gained a table_provider keyword to run the query against in-memory tables (ARROW-17521). The StructType class gained a field() method to retrieve a child field (ARROW-17131). Casting Tables to a new schema now honors the nullability flag in the target schema (ARROW-16651). Parquet files are now explicitly closed after reading (ARROW-13763). Further, the Python bindings benefit from improvements in the C++ library (e.g. new compute functions); see the C++ notes above for additional details. Build notes The PyArrow-specific C++ code, previously part of Arrow C++ codebase, is now integrated into PyArrow. The tests are run automatically as part of the PyArrow test suite. See: ARROW-16340, ARROW-17122 and PyArrow C++ API notes). The build process is generally not affected by the change, but the ARROW_PYTHON CMake variable has been deprecated and will be removed in a later release; you should instead enable the necessary components explicitly (ARROW-17868). R notes Many improvements to Arrow dplyr queries are added in this version, including: dplyr::across() can be used to apply the same computation across multiple columns; long-running queries can now be cancelled; the data source file name can be added as a column when reading multi-file datasets with add_filename(); joins now support extension arrays; and all supported Arrow dplyr functions are now documented on the R documentation site. For more on what’s in the 10.0.0 R package, see the R changelog. Ruby and C GLib notes Ruby Plasma binding has been deprecated (ARROW-17864) C GLib Plasma binding has been deprecated (ARROW-17862) Rust notes The Rust projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Rust implementation, see the latest Arrow Rust changelog.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow Ballista 0.9.0 Release</title><link href="https://arrow.apache.org/blog/2022/10/28/ballista-0.9.0/" rel="alternate" type="text/html" title="Apache Arrow Ballista 0.9.0 Release" /><published>2022-10-28T00:00:00-04:00</published><updated>2022-10-28T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/10/28/ballista-0.9.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/10/28/ballista-0.9.0/"><![CDATA[<!--

-->

<h1 id="introduction">Introduction</h1>

<p><a href="https://github.com/apache/arrow-ballista">Ballista</a> is an Arrow-native distributed SQL query engine implemented in Rust.</p>

<p>Ballista 0.9.0 is now available and is the most significant release since the project was <a href="http://arrow.apache.org/blog/2021/04/12/ballista-donation/">donated</a> to Apache
Arrow in 2021.</p>

<p>This release represents 4 weeks of work, with 66 commits from 14 contributors:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    22  Andy Grove
    12  yahoNanJing
     6  Daniël Heres
     4  Brent Gardner
     4  dependabot[bot]
     4  r.4ntix
     3  Stefan Stanciulescu
     3  mingmwang
     2  Ken Suenobu
     2  Yang Jiang
     1  Metehan Yıldırım
     1  Trent Feda
     1  askoa
     1  yangzhong
</code></pre></div></div>

<h2 id="release-highlights">Release Highlights</h2>

<p>The release notes below are not exhaustive and only expose selected highlights of the release. Many other bug fixes
and improvements have been made: we refer you to the <a href="https://github.com/apache/arrow-ballista/blob/0.9.0-rc2/ballista/CHANGELOG.md">complete changelog</a>.</p>

<h3 id="support-for-cloud-object-stores-and-distributed-file-systems">Support for Cloud Object Stores and Distributed File Systems</h3>

<p>This is the first release of Ballista to have documented support for querying data from distributed file systems and
object stores. Currently, S3 and HDFS are supported. Support for Google Cloud Storage and Azure Blob Storage is planned
for the next release.</p>

<h3 id="flight-sql--jdbc-support">Flight SQL &amp; JDBC support</h3>

<p>The Ballista scheduler now implements the <a href="https://arrow.apache.org/blog/2022/02/16/introducing-arrow-flight-sql/">Flight SQL protocol</a>, enabling any compliant Flight SQL client
to connect to and run queries against a Ballista cluster.</p>

<p>The Apache Arrow Flight SQL JDBC driver can be used to connect Business Intelligence tools to a Ballista cluster.</p>

<h3 id="python-bindings">Python Bindings</h3>

<p>It is now possible to connect to a Ballista cluster from Python and execute queries using both the DataFrame and SQL
interfaces.</p>

<h3 id="scheduler-web-user-interface-and-rest-api">Scheduler Web User Interface and REST API</h3>

<p>The scheduler now has a web user interface for monitoring queries. It is also possible to view graphical query plans
that show how the query was executed, along with metrics.</p>

<p><img src="/img/2022-10-28-ballista-web-ui.png" width="800" /></p>

<p>The REST API that powers the user interface can also be accessed directly.</p>

<h3 id="simplified-kubernetes-deployment">Simplified Kubernetes Deployment</h3>

<p>Ballista now provides a <a href="https://github.com/apache/arrow-ballista/tree/master/helm">Helm chart</a> for simplified Kubernetes deployment.</p>

<h3 id="user-guide">User Guide</h3>

<p>The user guide is published at <a href="https://arrow.apache.org/ballista/">https://arrow.apache.org/ballista/</a> and provides
deployment instructions for Docker, Docker Compose, and Kubernetes, as well as references for configuring and
tuning Ballista.</p>

<h2 id="roadmap">Roadmap</h2>

<p>The Ballista community is currently focused on the following tasks for the next release:</p>

<ul>
  <li>Support for Azure Blob Storage and Google Cloud Storage</li>
  <li>Improve benchmark performance by implementing more query optimizations</li>
  <li>Improve scheduler web user interface</li>
  <li>Publish Docker images to GitHub Container Registry</li>
</ul>

<p>The detailed list of issues planned for the 0.10.0 release can be found in the <a href="https://github.com/apache/arrow-ballista/issues/361">tracking issue</a>.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>Ballista has a friendly community and we welcome contributions. A good place to start is to following the instructions
in the <a href="https://arrow.apache.org/ballista/">user guide</a> and try using Ballista with your own SQL queries and ETL pipelines, and file issues
for any bugs or feature suggestions.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[Introduction Ballista is an Arrow-native distributed SQL query engine implemented in Rust. Ballista 0.9.0 is now available and is the most significant release since the project was donated to Apache Arrow in 2021. This release represents 4 weeks of work, with 66 commits from 14 contributors: 22 Andy Grove 12 yahoNanJing 6 Daniël Heres 4 Brent Gardner 4 dependabot[bot] 4 r.4ntix 3 Stefan Stanciulescu 3 mingmwang 2 Ken Suenobu 2 Yang Jiang 1 Metehan Yıldırım 1 Trent Feda 1 askoa 1 yangzhong Release Highlights The release notes below are not exhaustive and only expose selected highlights of the release. Many other bug fixes and improvements have been made: we refer you to the complete changelog. Support for Cloud Object Stores and Distributed File Systems This is the first release of Ballista to have documented support for querying data from distributed file systems and object stores. Currently, S3 and HDFS are supported. Support for Google Cloud Storage and Azure Blob Storage is planned for the next release. Flight SQL &amp; JDBC support The Ballista scheduler now implements the Flight SQL protocol, enabling any compliant Flight SQL client to connect to and run queries against a Ballista cluster. The Apache Arrow Flight SQL JDBC driver can be used to connect Business Intelligence tools to a Ballista cluster. Python Bindings It is now possible to connect to a Ballista cluster from Python and execute queries using both the DataFrame and SQL interfaces. Scheduler Web User Interface and REST API The scheduler now has a web user interface for monitoring queries. It is also possible to view graphical query plans that show how the query was executed, along with metrics. The REST API that powers the user interface can also be accessed directly. Simplified Kubernetes Deployment Ballista now provides a Helm chart for simplified Kubernetes deployment. User Guide The user guide is published at https://arrow.apache.org/ballista/ and provides deployment instructions for Docker, Docker Compose, and Kubernetes, as well as references for configuring and tuning Ballista. Roadmap The Ballista community is currently focused on the following tasks for the next release: Support for Azure Blob Storage and Google Cloud Storage Improve benchmark performance by implementing more query optimizations Improve scheduler web user interface Publish Docker images to GitHub Container Registry The detailed list of issues planned for the 0.10.0 release can be found in the tracking issue. Getting Involved Ballista has a friendly community and we welcome contributions. A good place to start is to following the instructions in the user guide and try using Ballista with your own SQL queries and ETL pipelines, and file issues for any bugs or feature suggestions.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow DataFusion 13.0.0 Project Update</title><link href="https://arrow.apache.org/blog/2022/10/25/datafusion-13.0.0/" rel="alternate" type="text/html" title="Apache Arrow DataFusion 13.0.0 Project Update" /><published>2022-10-25T00:00:00-04:00</published><updated>2022-10-25T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/10/25/datafusion-13.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/10/25/datafusion-13.0.0/"><![CDATA[<!--

-->

<h1 id="introduction">Introduction</h1>

<p><a href="https://arrow.apache.org/datafusion/">Apache Arrow DataFusion</a> <a href="https://crates.io/crates/datafusion"><code class="language-plaintext highlighter-rouge">13.0.0</code></a> is released, and this blog contains an update on the project for the 5 months since our <a href="https://arrow.apache.org/blog/2022/05/16/datafusion-8.0.0/">last update in May 2022</a>.</p>

<p>DataFusion is an extensible and embeddable query engine, written in Rust used to create modern, fast and efficient data pipelines, ETL processes, and database systems. You may want to check out DataFusion to extend your Rust project to:</p>

<ul>
  <li>Support <a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html">SQL support</a></li>
  <li>Support <a href="https://docs.rs/datafusion/13.0.0/datafusion/dataframe/struct.DataFrame.html">DataFrame API</a></li>
  <li>Support a Domain Specific Query Language</li>
  <li>Easily and quickly read and process Parquet, JSON, Avro or CSV data.</li>
  <li>Read from remote object stores such as AWS S3, Azure Blob Storage, GCP.</li>
</ul>

<p>Even though DataFusion is 4 years “young,” it has seen significant community growth in the last few months and the momentum continues to accelerate.</p>

<h1 id="background">Background</h1>

<p>DataFusion is used as the engine in <a href="https://github.com/apache/arrow-datafusion#known-uses">many open source and commercial projects</a> and was one of the early open source projects to provide this capability. 2022 has validated our belief in the need for such a <a href="https://docs.google.com/presentation/d/1iNX_35sWUakee2q3zMFPyHE4IV2nC3lkCK_H6Y2qK84/edit#slide=id.p">“LLVM for database and AI systems”</a><a href="https://www.slideshare.net/AndrewLamb32/20220623-apache-arrow-and-datafusion-changing-the-game-for-implementing-database-systemspdf">(alternate link)</a> with announcements such as the <a href="https://engineering.fb.com/2022/08/31/open-source/velox/">release of FaceBook’s Velox</a> engine, the major investments in <a href="https://arrow.apache.org/docs/cpp/streaming_execution.html">Acero</a> as well as the continued popularity of <a href="https://calcite.apache.org/">Apache Calcite</a> and other similar technologies.</p>

<p>While Velox and Acero focus on execution engines, DataFusion provides the entire suite of components needed to build most analytic systems, including a SQL frontend, a dataframe API, and  extension points for just about everything. Some <a href="https://github.com/apache/arrow-datafusion#known-uses">DataFusion users</a> use a subset of the features such as the frontend (e.g. <a href="https://dask-sql.readthedocs.io/en/latest/">dask-sql</a>) or the execution engine, (e.g.  <a href="https://github.com/blaze-init/blaze">Blaze</a>), and some use many different components to build both SQL based and customized DSL based systems such as <a href="https://github.com/influxdata/influxdb_iox/pulls">InfluxDB IOx</a> and <a href="https://github.com/vegafusion/vegafusion">VegaFusion</a>.</p>

<p>One of DataFusion’s advantages is its implementation in <a href="https://www.rust-lang.org/">Rust</a> and thus its easy integration with the broader Rust ecosystem. Rust continues to be a major source of benefit, from the <a href="https://www.influxdata.com/blog/using-rustlangs-async-tokio-runtime-for-cpu-bound-tasks/">ease of parallelization with the high quality and standardized <code class="language-plaintext highlighter-rouge">async</code> ecosystem</a> , as well as its modern dependency management system and wonderful performance. <!-- I wonder if we should link to clickbench?? -->
<!--While we haven’t invested in the benchmarking ratings game datafusion continues to be quite speedy (todo quantity this, with some evidence) – maybe clickbench?--></p>

<!--
Maybe we can do this un a future post
# DataFusion in Action

While DataFusion really shines as an embeddable query engine, if you want to try it out and get a feel for its power, you can use the basic[`datafusion-cli`](https://docs.rs/datafusion-cli/13.0.0/datafusion_cli/) tool to get a sense for what is possible to add in your application

(TODO example here of using datafusion-cli to query from local parquet files on disk)

TODO: also mention you can use the same thing to query data from S3
-->

<h1 id="summary">Summary</h1>

<p>We have increased the frequency of DataFusion releases to monthly instead of quarterly. This
makes it easier for the increasing number of projects that now depend on DataFusion.</p>

<p>We have also completed the “graduation” of <a href="https://github.com/apache/arrow-ballista">Ballista to its own top-level arrow-ballista repository</a>
which decouples the two projects and allows each project to move even faster.</p>

<p>Along with numerous other bug fixes and smaller improvements, here are some of the major advances:</p>

<h1 id="improved-support-for-cloud-object-stores">Improved Support for Cloud Object Stores</h1>

<p>DataFusion now supports many major cloud object stores (Amazon S3, Azure Blob Storage, and Google Cloud Storage) “out of the box” via the <a href="https://crates.io/crates/object_store">object_store</a> crate. Using this integration, DataFusion optimizes reading parquet files by reading only the parts of the files that are needed.</p>

<h2 id="advanced-sql">Advanced SQL</h2>

<p>DataFusion now supports correlated subqueries, by rewriting them as joins. See the <a href="https://arrow.apache.org/datafusion/user-guide/sql/subqueries.html">Subquery</a> page in the User Guide for more information.</p>

<p>In addition to numerous other small improvements, the following SQL features are now supported:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ROWS</code>, <code class="language-plaintext highlighter-rouge">RANGE</code>, <code class="language-plaintext highlighter-rouge">PRECEDING</code> and <code class="language-plaintext highlighter-rouge">FOLLOWING</code> in <code class="language-plaintext highlighter-rouge">OVER</code> clauses <a href="https://github.com/apache/arrow-datafusion/issues/3570">#3570</a></li>
  <li><code class="language-plaintext highlighter-rouge">ROLLUP</code> and <code class="language-plaintext highlighter-rouge">CUBE</code> grouping set expressions  <a href="https://github.com/apache/arrow-datafusion/issues/2446">#2446</a></li>
  <li><code class="language-plaintext highlighter-rouge">SUM DISTINCT</code> aggregate support  <a href="https://github.com/apache/arrow-datafusion/issues/2405">#2405</a></li>
  <li><code class="language-plaintext highlighter-rouge">IN</code> and <code class="language-plaintext highlighter-rouge">NOT IN</code> Subqueries by rewriting them to <code class="language-plaintext highlighter-rouge">SEMI</code> / <code class="language-plaintext highlighter-rouge">ANTI</code> <a href="https://github.com/apache/arrow-datafusion/issues/2885">#2421</a></li>
  <li>Non equality predicates in  <code class="language-plaintext highlighter-rouge">ON</code> clause of  <code class="language-plaintext highlighter-rouge">LEFT</code>, <code class="language-plaintext highlighter-rouge">RIGHT, </code>and <code class="language-plaintext highlighter-rouge">FULL</code> joins <a href="https://github.com/apache/arrow-datafusion/issues/2591">#2591</a></li>
  <li>Exact <code class="language-plaintext highlighter-rouge">MEDIAN</code> <a href="https://github.com/apache/arrow-datafusion/issues/3009">#3009</a></li>
  <li><code class="language-plaintext highlighter-rouge">GROUPING SETS</code>/<code class="language-plaintext highlighter-rouge">CUBE</code>/<code class="language-plaintext highlighter-rouge">ROLLUP</code> <a href="https://github.com/apache/arrow-datafusion/issues/2716">#2716</a></li>
</ul>

<h1 id="more-ddl-support">More DDL Support</h1>

<p>Just as it is important to query, it is also important to give users the ability to define their data sources. We have added:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">CREATE VIEW</code> <a href="https://github.com/apache/arrow-datafusion/issues/2279">#2279</a></li>
  <li><code class="language-plaintext highlighter-rouge">DESCRIBE &lt;table&gt;</code> <a href="https://github.com/apache/arrow-datafusion/issues/2642">#2642</a></li>
  <li>Custom / Dynamic table provider factories <a href="https://github.com/apache/arrow-datafusion/issues/3311">#3311</a></li>
  <li><code class="language-plaintext highlighter-rouge">SHOW CREATE TABLE</code> for support for views <a href="https://github.com/apache/arrow-datafusion/issues/2830">#2830</a></li>
</ul>

<h1 id="faster-execution">Faster Execution</h1>
<p>Performance is always an important goal for DataFusion, and there are a number of significant new optimizations such as</p>

<ul>
  <li>Optimizations of TopK (queries with a <code class="language-plaintext highlighter-rouge">LIMIT</code> or <code class="language-plaintext highlighter-rouge">OFFSET</code> clause):  <a href="https://github.com/apache/arrow-datafusion/issues/3527">#3527</a>, <a href="https://github.com/apache/arrow-datafusion/issues/2521">#2521</a></li>
  <li>Reduce <code class="language-plaintext highlighter-rouge">left</code>/<code class="language-plaintext highlighter-rouge">right</code>/<code class="language-plaintext highlighter-rouge">full</code> joins to <code class="language-plaintext highlighter-rouge">inner</code> join <a href="https://github.com/apache/arrow-datafusion/issues/2750">#2750</a></li>
  <li>Convert  cross joins to inner joins when possible <a href="https://github.com/apache/arrow-datafusion/issues/3482">#3482</a></li>
  <li>Sort preserving <code class="language-plaintext highlighter-rouge">SortMergeJoin</code> <a href="https://github.com/apache/arrow-datafusion/issues/2699">#2699</a></li>
  <li>Improvements in group by and sort performance <a href="https://github.com/apache/arrow-datafusion/issues/2375">#2375</a></li>
  <li>Adaptive <code class="language-plaintext highlighter-rouge">regex_replace</code> implementation <a href="https://github.com/apache/arrow-datafusion/issues/3518">#3518</a></li>
</ul>

<h1 id="optimizer-enhancements">Optimizer Enhancements</h1>
<p>Internally the optimizer has been significantly enhanced as well.</p>

<ul>
  <li>Casting / coercion now happens during logical planning <a href="https://github.com/apache/arrow-datafusion/issues/3396">#3185</a> <a href="https://github.com/apache/arrow-datafusion/issues/3636">#3636</a></li>
  <li>More sophisticated expression analysis and simplification is available</li>
</ul>

<h1 id="parquet">Parquet</h1>
<ul>
  <li>The parquet reader can now read directly from parquet files on remote object storage <a href="https://github.com/apache/arrow-datafusion/issues/2677">#2489</a> <a href="https://github.com/apache/arrow-datafusion/issues/3051">#3051</a></li>
  <li>Experimental support for “predicate pushdown” with late materialization after filtering during the scan (another blog post on this topic is coming soon).</li>
  <li>Support reading directly from AWS S3 and other object stores via <code class="language-plaintext highlighter-rouge">datafusion-cli </code> <a href="https://github.com/apache/arrow-datafusion/issues/3631">#3631</a></li>
</ul>

<h1 id="datatype-support">DataType Support</h1>
<ul>
  <li>Support for <code class="language-plaintext highlighter-rouge">TimestampTz</code> <a href="https://github.com/apache/arrow-datafusion/issues/3660">#3660</a></li>
  <li>Expanded support for the <code class="language-plaintext highlighter-rouge">Decimal</code> type, including  <code class="language-plaintext highlighter-rouge">IN</code> list and better built in coercion.</li>
  <li>Expanded support for date/time manipulation such as  <code class="language-plaintext highlighter-rouge">date_bin</code> built-in function , timestamp <code class="language-plaintext highlighter-rouge">+/-</code> interval, <code class="language-plaintext highlighter-rouge">TIME</code> literal values <a href="https://github.com/apache/arrow-datafusion/issues/3010">#3010</a>, <a href="https://github.com/apache/arrow-datafusion/issues/3110">#3110</a>, <a href="https://github.com/apache/arrow-datafusion/issues/3034">#3034</a></li>
  <li>Binary operations (<code class="language-plaintext highlighter-rouge">AND</code>, <code class="language-plaintext highlighter-rouge">XOR</code>, etc):  <a href="https://github.com/apache/arrow-datafusion/issues/1619">#3037</a> <a href="https://github.com/apache/arrow-datafusion/issues/3430">#3420</a></li>
  <li><code class="language-plaintext highlighter-rouge">IS TRUE/FALSE</code> and <code class="language-plaintext highlighter-rouge">IS [NOT] UNKNOWN</code> <a href="https://github.com/apache/arrow-datafusion/issues/3235">#3235</a>, <a href="https://github.com/apache/arrow-datafusion/issues/3246">#3246</a></li>
</ul>

<h2 id="upcoming-work">Upcoming Work</h2>
<p>With the community growing and code accelerating, there is so much great stuff on the horizon. Some features we expect to land in the next few months:</p>

<ul>
  <li><a href="https://github.com/apache/arrow-datafusion/issues/3462">Complete Parquet Pushdown</a></li>
  <li><a href="https://github.com/apache/arrow-datafusion/issues/3148">Additional date/time support</a></li>
  <li>Cost models, Nested Join Optimizations, analysis framework <a href="https://github.com/apache/arrow-datafusion/issues/128">#128</a>, <a href="https://github.com/apache/arrow-datafusion/issues/3843">#3843</a>, <a href="https://github.com/apache/arrow-datafusion/issues/3845">#3845</a></li>
</ul>

<h1 id="community-growth">Community Growth</h1>

<p>The DataFusion 9.0.0 and 13.0.0 releases consists of 433 PRs from 64 distinct contributors. This does not count all the work that goes into our dependencies such as <a href="https://crates.io/crates/arrow">arrow</a>,  <a href="https://crates.io/crates/parquet">parquet</a>, and <a href="https://crates.io/crates/object_store">object_store</a>, that much of the same community helps nurture.</p>

<!--
$ git log --pretty=oneline 9.0.0..13.0.0 . | wc -l
433

$ git shortlog -sn 9.0.0..13.0.0 . | wc -l
65
-->

<h1 id="how-to-get-involved">How to Get Involved</h1>

<p>Kudos to everyone in the community who contributed ideas, discussions, bug reports, documentation and code. It is exciting to be building something so cool together!</p>

<p>If you are interested in contributing to DataFusion, we would love to
have you join us on our journey to create the most advanced open
source query engine. You can try out DataFusion on some of your own
data and projects and let us know how it goes or contribute a PR with
documentation, tests or code. A list of open issues suitable for
beginners is
<a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a>.</p>

<p>Check out our <a href="https://arrow.apache.org/datafusion/community/communication.html">Communication Doc</a> on more
ways to engage with the community.</p>

<h2 id="appendix-contributor-shoutout">Appendix: Contributor Shoutout</h2>

<p>To give a sense of the number of people who contribute to this project regularly, we present for your consideration the following list derived from <code class="language-plaintext highlighter-rouge">git shortlog -sn 9.0.0..13.0.0 .</code> Thank you all again!</p>

<!-- Note: combined kmitchener and Kirk Mitchener -->

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    87	Andy Grove
    71	Andrew Lamb
    29	Kun Liu
    29	Kirk Mitchener
    17	Wei-Ting Kuo
    14	Yang Jiang
    12	Raphael Taylor-Davies
    11	Batuhan Taskaya
    10	Brent Gardner
    10	Remzi Yang
    10	comphead
    10	xudong.w
     8	AssHero
     7	Ruihang Xia
     6	Dan Harris
     6	Daniël Heres
     6	Ian Alexander Joiner
     6	Mike Roberts
     6	askoa
     4	BaymaxHWY
     4	gorkem
     4	jakevin
     3	George Andronchik
     3	Sarah Yurick
     3	Stuart Carnie
     2	Dalton Modlin
     2	Dmitry Patsura
     2	JasonLi
     2	Jon Mease
     2	Marco Neumann
     2	yahoNanJing
     1	Adilet Sarsembayev
     1	Ayush Dattagupta
     1	Dezhi Wu
     1	Dhamotharan Sritharan
     1	Eduard Karacharov
     1	Francis Du
     1	Harbour Zheng
     1	Ismaël Mejía
     1	Jack Klamer
     1	Jeremy Dyer
     1	Jiayu Liu
     1	Kamil Konior
     1	Liang-Chi Hsieh
     1	Martin Grigorov
     1	Matthijs Brobbel
     1	Mehmet Ozan Kabak
     1	Metehan Yıldırım
     1	Morgan Cassels
     1	Nitish Tiwari
     1	Renjie Liu
     1	Rito Takeuchi
     1	Robert Pack
     1	Thomas Cameron
     1	Vrishabh
     1	Xin Hao
     1	Yijie Shen
     1	byteink
     1	kamille
     1	mateuszkj
     1	nvartolomei
     1	yourenawo
     1	Özgür Akkurt
</code></pre></div></div>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[Introduction Apache Arrow DataFusion 13.0.0 is released, and this blog contains an update on the project for the 5 months since our last update in May 2022. DataFusion is an extensible and embeddable query engine, written in Rust used to create modern, fast and efficient data pipelines, ETL processes, and database systems. You may want to check out DataFusion to extend your Rust project to: Support SQL support Support DataFrame API Support a Domain Specific Query Language Easily and quickly read and process Parquet, JSON, Avro or CSV data. Read from remote object stores such as AWS S3, Azure Blob Storage, GCP. Even though DataFusion is 4 years “young,” it has seen significant community growth in the last few months and the momentum continues to accelerate. Background DataFusion is used as the engine in many open source and commercial projects and was one of the early open source projects to provide this capability. 2022 has validated our belief in the need for such a “LLVM for database and AI systems”(alternate link) with announcements such as the release of FaceBook’s Velox engine, the major investments in Acero as well as the continued popularity of Apache Calcite and other similar technologies. While Velox and Acero focus on execution engines, DataFusion provides the entire suite of components needed to build most analytic systems, including a SQL frontend, a dataframe API, and extension points for just about everything. Some DataFusion users use a subset of the features such as the frontend (e.g. dask-sql) or the execution engine, (e.g. Blaze), and some use many different components to build both SQL based and customized DSL based systems such as InfluxDB IOx and VegaFusion. One of DataFusion’s advantages is its implementation in Rust and thus its easy integration with the broader Rust ecosystem. Rust continues to be a major source of benefit, from the ease of parallelization with the high quality and standardized async ecosystem , as well as its modern dependency management system and wonderful performance. Summary We have increased the frequency of DataFusion releases to monthly instead of quarterly. This makes it easier for the increasing number of projects that now depend on DataFusion. We have also completed the “graduation” of Ballista to its own top-level arrow-ballista repository which decouples the two projects and allows each project to move even faster. Along with numerous other bug fixes and smaller improvements, here are some of the major advances: Improved Support for Cloud Object Stores DataFusion now supports many major cloud object stores (Amazon S3, Azure Blob Storage, and Google Cloud Storage) “out of the box” via the object_store crate. Using this integration, DataFusion optimizes reading parquet files by reading only the parts of the files that are needed. Advanced SQL DataFusion now supports correlated subqueries, by rewriting them as joins. See the Subquery page in the User Guide for more information. In addition to numerous other small improvements, the following SQL features are now supported: ROWS, RANGE, PRECEDING and FOLLOWING in OVER clauses #3570 ROLLUP and CUBE grouping set expressions #2446 SUM DISTINCT aggregate support #2405 IN and NOT IN Subqueries by rewriting them to SEMI / ANTI #2421 Non equality predicates in ON clause of LEFT, RIGHT, and FULL joins #2591 Exact MEDIAN #3009 GROUPING SETS/CUBE/ROLLUP #2716 More DDL Support Just as it is important to query, it is also important to give users the ability to define their data sources. We have added: CREATE VIEW #2279 DESCRIBE &lt;table&gt; #2642 Custom / Dynamic table provider factories #3311 SHOW CREATE TABLE for support for views #2830 Faster Execution Performance is always an important goal for DataFusion, and there are a number of significant new optimizations such as Optimizations of TopK (queries with a LIMIT or OFFSET clause): #3527, #2521 Reduce left/right/full joins to inner join #2750 Convert cross joins to inner joins when possible #3482 Sort preserving SortMergeJoin #2699 Improvements in group by and sort performance #2375 Adaptive regex_replace implementation #3518 Optimizer Enhancements Internally the optimizer has been significantly enhanced as well. Casting / coercion now happens during logical planning #3185 #3636 More sophisticated expression analysis and simplification is available Parquet The parquet reader can now read directly from parquet files on remote object storage #2489 #3051 Experimental support for “predicate pushdown” with late materialization after filtering during the scan (another blog post on this topic is coming soon). Support reading directly from AWS S3 and other object stores via datafusion-cli #3631 DataType Support Support for TimestampTz #3660 Expanded support for the Decimal type, including IN list and better built in coercion. Expanded support for date/time manipulation such as date_bin built-in function , timestamp +/- interval, TIME literal values #3010, #3110, #3034 Binary operations (AND, XOR, etc): #3037 #3420 IS TRUE/FALSE and IS [NOT] UNKNOWN #3235, #3246 Upcoming Work With the community growing and code accelerating, there is so much great stuff on the horizon. Some features we expect to land in the next few months: Complete Parquet Pushdown Additional date/time support Cost models, Nested Join Optimizations, analysis framework #128, #3843, #3845 Community Growth The DataFusion 9.0.0 and 13.0.0 releases consists of 433 PRs from 64 distinct contributors. This does not count all the work that goes into our dependencies such as arrow, parquet, and object_store, that much of the same community helps nurture. How to Get Involved Kudos to everyone in the community who contributed ideas, discussions, bug reports, documentation and code. It is exciting to be building something so cool together! If you are interested in contributing to DataFusion, we would love to have you join us on our journey to create the most advanced open source query engine. You can try out DataFusion on some of your own data and projects and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable for beginners is here. Check out our Communication Doc on more ways to engage with the community. Appendix: Contributor Shoutout To give a sense of the number of people who contribute to this project regularly, we present for your consideration the following list derived from git shortlog -sn 9.0.0..13.0.0 . Thank you all again! 87 Andy Grove 71 Andrew Lamb 29 Kun Liu 29 Kirk Mitchener 17 Wei-Ting Kuo 14 Yang Jiang 12 Raphael Taylor-Davies 11 Batuhan Taskaya 10 Brent Gardner 10 Remzi Yang 10 comphead 10 xudong.w 8 AssHero 7 Ruihang Xia 6 Dan Harris 6 Daniël Heres 6 Ian Alexander Joiner 6 Mike Roberts 6 askoa 4 BaymaxHWY 4 gorkem 4 jakevin 3 George Andronchik 3 Sarah Yurick 3 Stuart Carnie 2 Dalton Modlin 2 Dmitry Patsura 2 JasonLi 2 Jon Mease 2 Marco Neumann 2 yahoNanJing 1 Adilet Sarsembayev 1 Ayush Dattagupta 1 Dezhi Wu 1 Dhamotharan Sritharan 1 Eduard Karacharov 1 Francis Du 1 Harbour Zheng 1 Ismaël Mejía 1 Jack Klamer 1 Jeremy Dyer 1 Jiayu Liu 1 Kamil Konior 1 Liang-Chi Hsieh 1 Martin Grigorov 1 Matthijs Brobbel 1 Mehmet Ozan Kabak 1 Metehan Yıldırım 1 Morgan Cassels 1 Nitish Tiwari 1 Renjie Liu 1 Rito Takeuchi 1 Robert Pack 1 Thomas Cameron 1 Vrishabh 1 Xin Hao 1 Yijie Shen 1 byteink 1 kamille 1 mateuszkj 1 nvartolomei 1 yourenawo 1 Özgür Akkurt]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>