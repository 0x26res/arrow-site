<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://arrow.apache.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arrow.apache.org/" rel="alternate" type="text/html" /><updated>2023-08-23T09:29:30-04:00</updated><id>https://arrow.apache.org/feed.xml</id><title type="html">Apache Arrow</title><subtitle>Apache Arrow is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, C++, C#, Go, Java, JavaScript, MATLAB, Python, R, Ruby, and Rust.</subtitle><entry><title type="html">Aggregating Millions of Groups Fast in Apache Arrow DataFusion 28.0.0</title><link href="https://arrow.apache.org/blog/2023/08/05/datafusion_fast_grouping/" rel="alternate" type="text/html" title="Aggregating Millions of Groups Fast in Apache Arrow DataFusion 28.0.0" /><published>2023-08-05T00:00:00-04:00</published><updated>2023-08-05T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/08/05/datafusion_fast_grouping</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/08/05/datafusion_fast_grouping/"><![CDATA[<!--

-->

<!--- Converted from Google Docs using https://www.buymeacoffee.com/docstomarkdown --->

<h2 id="aggregating-millions-of-groups-fast-in-apache-arrow-datafusion">Aggregating Millions of Groups Fast in Apache Arrow DataFusion</h2>

<p>Andrew Lamb, Daniël Heres, Raphael Taylor-Davies,</p>

<p><em>Note: this article was originally published on the <a href="https://www.influxdata.com/blog/aggregating-millions-groups-fast-apache-arrow-datafusion">InfluxData Blog</a></em></p>

<h2 id="tldr">TLDR</h2>

<p>Grouped aggregations are a core part of any analytic tool, creating understandable summaries of huge data volumes. <a href="https://arrow.apache.org/datafusion/">Apache Arrow DataFusion</a>’s parallel aggregation capability is 2-3x faster in the <a href="https://crates.io/crates/datafusion/28.0.0">newly released version <code class="language-plaintext highlighter-rouge">28.0.0</code></a> for queries with a large number (10,000 or more) of groups.</p>

<p>Improving aggregation performance matters to all users of DataFusion. For example, both InfluxDB, a <a href="https://github.com/influxdata/influxdb">time series data platform</a> and Coralogix, a <a href="https://coralogix.com/?utm_source=InfluxDB&amp;utm_medium=Blog&amp;utm_campaign=organic">full-stack observability</a> platform, aggregate vast amounts of raw data to monitor and create insights for our customers. Improving DataFusion’s performance lets us provide better user experiences by generating insights faster with fewer resources. Because DataFusion is open source and released under the permissive <a href="https://github.com/apache/arrow-datafusion/blob/main/LICENSE.txt">Apache 2.0</a> license, the whole DataFusion community benefits as well.</p>

<p>With the new optimizations, DataFusion’s grouping speed is now close to DuckDB, a system that regularly reports <a href="https://duckdblabs.github.io/db-benchmark/">great</a> <a href="https://duckdb.org/2022/03/07/aggregate-hashtable.html#experiments">grouping</a> benchmark performance numbers. Figure 1 contains a representative sample of <a href="https://github.com/ClickHouse/ClickBench/tree/main">ClickBench</a> on a single Parquet file, and the full results are at the end of this article.</p>

<p><img src="/assets/datafusion_fast_grouping/summary.png" width="700" /></p>

<p><strong>Figure 1</strong>: Query performance for ClickBench queries on queries 16, 17, 18 and 19 on a single Parquet file for DataFusion <code class="language-plaintext highlighter-rouge">27.0.0</code>, DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code> and DuckDB <code class="language-plaintext highlighter-rouge">0.8.1</code>.</p>

<h2 id="introduction-to-high-cardinality-grouping">Introduction to high cardinality grouping</h2>

<p>Aggregation is a fancy word for computing summary statistics across many rows that have the same value in one or more columns. We call the rows with the same values <em>groups</em> and “high cardinality” means there are a large number of distinct groups in the dataset. At the time of writing, a “large” number of groups in analytic engines is around 10,000.</p>

<p>For example the <a href="https://github.com/ClickHouse/ClickBench">ClickBench</a> <em>hits</em> dataset contains 100 million anonymized user clicks across a set of websites. ClickBench Query 17 is:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="nv">"UserID"</span><span class="p">,</span> <span class="nv">"SearchPhrase"</span><span class="p">,</span> <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span>
<span class="k">FROM</span> <span class="n">hits</span>
<span class="k">GROUP</span> <span class="k">BY</span> <span class="nv">"UserID"</span><span class="p">,</span> <span class="nv">"SearchPhrase"</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span>
<span class="k">DESC</span> <span class="k">LIMIT</span> <span class="mi">10</span><span class="p">;</span>
</code></pre></div></div>

<p>In English, this query finds “the top ten (user, search phrase) combinations, across all clicks” and produces the following results (there are no search phrases for the top ten users):</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+---------------------+--------------+-----------------+
| UserID              | SearchPhrase | COUNT(UInt8(1)) |
+---------------------+--------------+-----------------+
| 1313338681122956954 |              | 29097           |
| 1907779576417363396 |              | 25333           |
| 2305303682471783379 |              | 10597           |
| 7982623143712728547 |              | 6669            |
| 7280399273658728997 |              | 6408            |
| 1090981537032625727 |              | 6196            |
| 5730251990344211405 |              | 6019            |
| 6018350421959114808 |              | 5990            |
| 835157184735512989  |              | 5209            |
| 770542365400669095  |              | 4906            |
+---------------------+--------------+-----------------+
</code></pre></div></div>

<p>The ClickBench dataset contains</p>

<ul>
  <li>99,997,497 total rows<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></li>
  <li>17,630,976 different users (distinct UserIDs)<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></li>
  <li>6,019,103 different search phrases<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup></li>
  <li>24,070,560 distinct combinations<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup> of (UserID, SearchPhrase)
Thus, to answer the query, DataFusion must map each of the 100M different input rows into one of the <strong>24 million different groups</strong>, and keep count of how many such rows there are in each group.</li>
</ul>

<h2 id="the-solution">The solution</h2>

<p>Like most concepts in databases and other analytic systems, the basic ideas of this algorithm are straightforward and taught in introductory computer science courses. You could compute the query with a program such as this<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>

<span class="c1"># read file
</span><span class="n">hits</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="s">'hits.parquet'</span><span class="p">,</span> <span class="n">engine</span><span class="o">=</span><span class="s">'pyarrow'</span><span class="p">)</span>

<span class="c1"># build groups
</span><span class="n">counts</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">hits</span><span class="p">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">group</span> <span class="o">=</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s">'UserID'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s">'SearchPhrase'</span><span class="p">]);</span>
    <span class="c1"># update the dict entry for the corresponding key
</span>    <span class="n">counts</span><span class="p">[</span><span class="n">group</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># Print the top 10 values
</span><span class="k">print</span> <span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">counts</span><span class="p">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]))</span>
</code></pre></div></div>

<p>This approach, while simple, is both slow and very memory inefficient. It requires over 40 seconds to compute the results for less than 1% of the dataset<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup>. Both DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code> and DuckDB <code class="language-plaintext highlighter-rouge">0.8.1</code> compute results in under 10 seconds for the <em>entire</em> dataset.</p>

<p>To answer this query quickly and efficiently, you have to write your code such that it:</p>

<ol>
  <li>Keeps all cores busy aggregating via parallelized computation</li>
  <li>Updates aggregate values quickly, using vectorizable loops that are easy for compilers to translate into the high performance <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data">SIMD</a> instructions available in modern CPUs.</li>
</ol>

<p>The rest of this article explains how grouping works in DataFusion and the improvements we made in <code class="language-plaintext highlighter-rouge">28.0.0</code>.</p>

<h3 id="two-phase-parallel-partitioned-grouping">Two phase parallel partitioned grouping</h3>

<p>Both DataFusion <code class="language-plaintext highlighter-rouge">27.0.</code> and <code class="language-plaintext highlighter-rouge">28.0.0</code> use state-of-the-art, two phase parallel hash partitioned grouping, similar to other high-performance vectorized engines like <a href="https://duckdb.org/2022/03/07/aggregate-hashtable.html">DuckDB’s Parallel Grouped Aggregates</a>. In pictures this looks like:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            ▲                        ▲
            │                        │
            │                        │
            │                        │
┌───────────────────────┐  ┌───────────────────┐
│        GroupBy        │  │      GroupBy      │      Step 4
│        (Final)        │  │      (Final)      │
└───────────────────────┘  └───────────────────┘
            ▲                        ▲
            │                        │
            └────────────┬───────────┘
                         │
                         │
            ┌─────────────────────────┐
            │       Repartition       │               Step 3
            │         HASH(x)         │
            └─────────────────────────┘
                         ▲
                         │
            ┌────────────┴──────────┐
            │                       │
            │                       │
 ┌────────────────────┐  ┌─────────────────────┐
 │      GroupyBy      │  │       GroupBy       │      Step 2
 │     (Partial)      │  │      (Partial)      │
 └────────────────────┘  └─────────────────────┘
            ▲                       ▲
         ┌──┘                       └─┐
         │                            │
    .─────────.                  .─────────.
 ,─'           '─.            ,─'           '─.
;      Input      :          ;      Input      :      Step 1
:    Stream 1     ;          :    Stream 2     ;
 ╲               ╱            ╲               ╱
  '─.         ,─'              '─.         ,─'
     `───────'                    `───────'
</code></pre></div></div>

<p><strong>Figure 2</strong>: Two phase repartitioned grouping: data flows from bottom (source) to top (results) in two phases. First (Steps 1 and 2), each core reads the data into a core-specific hash table, computing intermediate aggregates without any cross-core coordination. Then (Steps 3 and 4) DataFusion divides the data (“repartitions”) into distinct subsets by group value, and each subset is sent to a specific core which computes the final aggregate.</p>

<p>The two phases are critical for keeping cores busy in a multi-core system. Both phases use the same hash table approach (explained in the next section), but differ in how the groups are distributed and the partial results emitted from the accumulators. The first phase aggregates data as soon as possible after it is produced. However, as shown in Figure 2, the groups can be anywhere in any input, so the same group is often found on many different cores. The second phase uses a hash function to redistribute data evenly across the cores, so each group value is processed by exactly one core which emits the final results for that group.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ┌─────┐    ┌─────┐
    │  1  │    │  3  │
    │  2  │    │  4  │   2. After Repartitioning: each
    └─────┘    └─────┘   group key  appears in exactly
    ┌─────┐    ┌─────┐   one partition
    │  1  │    │  3  │
    │  2  │    │  4  │
    └─────┘    └─────┘

─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─

    ┌─────┐    ┌─────┐
    │  2  │    │  2  │
    │  1  │    │  2  │
    │  3  │    │  3  │
    │  4  │    │  1  │
    └─────┘    └─────┘    1. Input Stream: groups
      ...        ...      values are spread
    ┌─────┐    ┌─────┐    arbitrarily over each input
    │  1  │    │  4  │
    │  4  │    │  3  │
    │  1  │    │  1  │
    │  4  │    │  3  │
    │  3  │    │  2  │
    │  2  │    │  2  │
    │  2  │    └─────┘
    └─────┘

    Core A      Core B

</code></pre></div></div>

<p><strong>Figure 3</strong>: Group value distribution across 2 cores during aggregation phases. In the first phase, every group value <code class="language-plaintext highlighter-rouge">1</code>, <code class="language-plaintext highlighter-rouge">2</code>, <code class="language-plaintext highlighter-rouge">3</code>, <code class="language-plaintext highlighter-rouge">4</code>, is present in the input stream processed by each core. In the second phase, after repartitioning, the group values <code class="language-plaintext highlighter-rouge">1</code> and <code class="language-plaintext highlighter-rouge">2</code> are processed by core A, and values <code class="language-plaintext highlighter-rouge">3</code> and <code class="language-plaintext highlighter-rouge">4</code> are processed only by core B.</p>

<p>There are some additional subtleties in the <a href="https://github.com/apache/arrow-datafusion/blob/main/datafusion/core/src/physical_plan/aggregates/row_hash.rs">DataFusion implementation</a> not mentioned above due to space constraints, such as:</p>

<ol>
  <li>The policy of when to emit data from the first phase’s hash table (e.g. because the data is partially sorted)</li>
  <li>Handling specific filters per aggregate (due to the <code class="language-plaintext highlighter-rouge">FILTER</code> SQL clause)</li>
  <li>Data types of intermediate values (which may not be the same as the final output for some aggregates such as <code class="language-plaintext highlighter-rouge">AVG</code>).</li>
  <li>Action taken when memory use exceeds its budget.</li>
</ol>

<h3 id="hash-grouping">Hash grouping</h3>

<p>DataFusion queries can compute many different aggregate functions for each group, both <a href="https://arrow.apache.org/datafusion/user-guide/sql/aggregate_functions.html">built in</a> and/or user defined <a href="https://docs.rs/datafusion/latest/datafusion/logical_expr/struct.AggregateUDF.html"><code class="language-plaintext highlighter-rouge">AggregateUDFs</code></a>. The state for each aggregate function, called an <em>accumulator</em>, is tracked with a hash table (DataFusion uses the excellent <a href="https://docs.rs/hashbrown/latest/hashbrown/index.html">HashBrown</a> <a href="https://docs.rs/hashbrown/latest/hashbrown/raw/struct.RawTable.html">RawTable API</a>), which logically stores the “index”  identifying the specific group value.</p>

<h3 id="hash-grouping-in-2700">Hash grouping in <code class="language-plaintext highlighter-rouge">27.0.0</code></h3>

<p>As shown in Figure 3, DataFusion <code class="language-plaintext highlighter-rouge">27.0.0</code> stores the data in a <a href="https://github.com/apache/arrow-datafusion/blob/4d93b6a3802151865b68967bdc4c7d7ef425b49a/datafusion/core/src/physical_plan/aggregates/utils.rs#L38-L50"><code class="language-plaintext highlighter-rouge">GroupState</code></a> structure which, unsurprisingly, tracks the state for each group. The state for each group consists of:</p>

<ol>
  <li>The actual value of the group columns, in <a href="https://docs.rs/arrow-row/latest/arrow_row/index.html">Arrow Row</a> format.</li>
  <li>In-progress accumulations (e.g. the running counts for the <code class="language-plaintext highlighter-rouge">COUNT</code> aggregate) for each group, in one of two possible formats (<a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/expr/src/accumulator.rs#L24-L49"><code class="language-plaintext highlighter-rouge">Accumulator</code></a>  or <a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/physical-expr/src/aggregate/row_accumulator.rs#L26-L46"><code class="language-plaintext highlighter-rouge">RowAccumulator</code></a>).</li>
  <li>Scratch space for tracking which rows match each aggregate in each batch.</li>
</ol>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                           ┌──────────────────────────────────────┐
                           │                                      │
                           │                  ...                 │
                           │ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ │
                           │ ┃                                  ┃ │
    ┌─────────┐            │ ┃ ┌──────────────────────────────┐ ┃ │
    │         │            │ ┃ │group values: OwnedRow        │ ┃ │
    │ ┌─────┐ │            │ ┃ └──────────────────────────────┘ ┃ │
    │ │  5  │ │            │ ┃ ┌──────────────────────────────┐ ┃ │
    │ ├─────┤ │            │ ┃ │Row accumulator:              │ ┃ │
    │ │  9  │─┼────┐       │ ┃ │Vec&lt;u8&gt;                       │ ┃ │
    │ ├─────┤ │    │       │ ┃ └──────────────────────────────┘ ┃ │
    │ │ ... │ │    │       │ ┃ ┌──────────────────────┐         ┃ │
    │ ├─────┤ │    │       │ ┃ │┌──────────────┐      │         ┃ │
    │ │  1  │ │    │       │ ┃ ││Accumulator 1 │      │         ┃ │
    │ ├─────┤ │    │       │ ┃ │└──────────────┘      │         ┃ │
    │ │ ... │ │    │       │ ┃ │┌──────────────┐      │         ┃ │
    │ └─────┘ │    │       │ ┃ ││Accumulator 2 │      │         ┃ │
    │         │    │       │ ┃ │└──────────────┘      │         ┃ │
    └─────────┘    │       │ ┃ │ Box&lt;dyn Accumulator&gt; │         ┃ │
    Hash Table     │       │ ┃ └──────────────────────┘         ┃ │
                   │       │ ┃ ┌─────────────────────────┐      ┃ │
                   │       │ ┃ │scratch indices: Vec&lt;u32&gt;│      ┃ │
                   │       │ ┃ └─────────────────────────┘      ┃ │
                   │       │ ┃ GroupState                       ┃ │
                   └─────▶ │ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │
                           │                                      │
  Hash table tracks an     │                 ...                  │
  index into group_states  │                                      │
                           └──────────────────────────────────────┘
                           group_states: Vec&lt;GroupState&gt;

                           There is one GroupState PER GROUP

</code></pre></div></div>

<p><strong>Figure 4</strong>: Hash group operator structure in DataFusion <code class="language-plaintext highlighter-rouge">27.0.0</code>. A hash table maps each group to a GroupState which contains all the per-group states.</p>

<p>To compute the aggregate, DataFusion performs the following steps for each input batch:</p>

<ol>
  <li>Calculate hash using <a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/physical-expr/src/hash_utils.rs#L264-L307">efficient vectorized code</a>, specialized for each data type.</li>
  <li>Determine group indexes for each input row using the hash table (creating new entries for newly seen groups).</li>
  <li><a href="https://github.com/apache/arrow-datafusion/blob/4ab8be57dee3bfa72dd105fbd7b8901b873a4878/datafusion/core/src/physical_plan/aggregates/row_hash.rs#L562-L602">Update Accumulators for each group that had input rows,</a> assembling the rows into a contiguous range for vectorized accumulator if there are a sufficient number of them.</li>
</ol>

<p>DataFusion also stores the hash values in the table to avoid potentially costly hash recomputation when resizing the hash table.</p>

<p>This scheme works very well for a relatively small number of distinct groups: all accumulators are efficiently updated with large contiguous batches of rows.</p>

<p>However, this scheme is not ideal for high cardinality grouping due to:</p>

<ol>
  <li><strong>Multiple allocations per group</strong> for the group value row format, as well as for the <code class="language-plaintext highlighter-rouge">RowAccumulator</code>s and each  <code class="language-plaintext highlighter-rouge">Accumulator</code>. The <code class="language-plaintext highlighter-rouge">Accumulator</code> may have additional allocations within it as well.</li>
  <li><strong>Non-vectorized updates:</strong> Accumulator updates often fall back to a slower non-vectorized form because the number of distinct groups is large (and thus number of values per group is small) in each input batch.</li>
</ol>

<h3 id="hash-grouping-in-2800">Hash grouping in <code class="language-plaintext highlighter-rouge">28.0.0</code></h3>

<p>For <code class="language-plaintext highlighter-rouge">28.0.0</code>, we rewrote the core group by implementation following traditional system optimization principles: fewer allocations, type specialization, and aggressive vectorization.</p>

<p>DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code> uses the same RawTable and still stores group indexes. The major differences, as shown in Figure 4, are:</p>

<ol>
  <li>Group values are stored either
    <ol>
      <li>Inline in the <code class="language-plaintext highlighter-rouge">RawTable</code> (for single columns of primitive types), where the conversion to Row format costs more than its benefit</li>
      <li>In a separate <a href="https://docs.rs/arrow-row/latest/arrow_row/struct.Row.html">Rows</a> structure with a single contiguous allocation for all groups values, rather than an allocation per group. Accumulators manage the state for all the groups internally, so the code to update intermediate values is a tight type specialized loop. The new <a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/physical-expr/src/aggregate/groups_accumulator/mod.rs#L66-L75"><code class="language-plaintext highlighter-rouge">GroupsAccumulator</code></a> interface results in highly efficient type accumulator update loops.</li>
    </ol>
  </li>
</ol>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌───────────────────────────────────┐     ┌───────────────────────┐
│ ┌ ─ ─ ─ ─ ─ ┐  ┌─────────────────┐│     │ ┏━━━━━━━━━━━━━━━━━━━┓ │
│                │                 ││     │ ┃  ┌──────────────┐ ┃ │
│ │           │  │ ┌ ─ ─ ┐┌─────┐  ││     │ ┃  │┌───────────┐ │ ┃ │
│                │    X   │  5  │  ││     │ ┃  ││  value1   │ │ ┃ │
│ │           │  │ ├ ─ ─ ┤├─────┤  ││     │ ┃  │└───────────┘ │ ┃ │
│                │    Q   │  9  │──┼┼──┐  │ ┃  │     ...      │ ┃ │
│ │           │  │ ├ ─ ─ ┤├─────┤  ││  └──┼─╋─▶│              │ ┃ │
│                │   ...  │ ... │  ││     │ ┃  │┌───────────┐ │ ┃ │
│ │           │  │ ├ ─ ─ ┤├─────┤  ││     │ ┃  ││  valueN   │ │ ┃ │
│                │    H   │  1  │  ││     │ ┃  │└───────────┘ │ ┃ │
│ │           │  │ ├ ─ ─ ┤├─────┤  ││     │ ┃  │values: Vec&lt;T&gt;│ ┃ │
│     Rows       │   ...  │ ... │  ││     │ ┃  └──────────────┘ ┃ │
│ │           │  │ └ ─ ─ ┘└─────┘  ││     │ ┃                   ┃ │
│  ─ ─ ─ ─ ─ ─   │                 ││     │ ┃ GroupsAccumulator ┃ │
│                └─────────────────┘│     │ ┗━━━━━━━━━━━━━━━━━━━┛ │
│                  Hash Table       │     │                       │
│                                   │     │          ...          │
└───────────────────────────────────┘     └───────────────────────┘
  GroupState                               Accumulators


Hash table value stores group_indexes     One  GroupsAccumulator
and group values.                         per aggregate. Each
                                          stores the state for
Group values are stored either inline     *ALL* groups, typically
in the hash table or in a single          using a native Vec&lt;T&gt;
allocation using the arrow Row format
</code></pre></div></div>

<p><strong>Figure 5</strong>: Hash group operator structure in DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code>. Group values are stored either directly in the hash table, or in a single allocation using the arrow Row format. The hash table contains group indexes. A single <code class="language-plaintext highlighter-rouge">GroupsAccumulator</code> stores the per-aggregate state for <em>all</em> groups.</p>

<p>This new structure improves performance significantly for high cardinality groups due to:</p>

<ol>
  <li><strong>Reduced allocations</strong>: There are no longer any individual allocations per group.</li>
  <li><strong>Contiguous native accumulator states</strong>: Type-specialized accumulators store the values for all groups in a single contiguous allocation using a <a href="https://doc.rust-lang.org/std/vec/struct.Vec.html">Rust Vec&lt;T&gt;</a> of some native type.</li>
  <li><strong>Vectorized state update</strong>: The inner aggregate update loops, which are type-specialized and in terms of native <code class="language-plaintext highlighter-rouge">Vec</code>s, are well-vectorized by the Rust compiler (thanks <a href="https://llvm.org/">LLVM</a>!).</li>
</ol>

<h3 id="notes">Notes</h3>

<p>Some vectorized grouping implementations store the accumulator state row-wise directly in the hash table, which often uses modern CPU caches efficiently. Managing accumulator state in columnar fashion may sacrifice some cache locality, however it ensures the size of the hash table remains small, even when there are large numbers of groups and aggregates, making it easier for the compiler to vectorize the accumulator update.</p>

<p>Depending on the cost of recomputing hash values, DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code> may or may not store the hash values in the table. This optimizes the tradeoff between the cost of computing the hash value (which is expensive for strings, for example) vs. the cost of storing it in the hash table.</p>

<p>One subtlety that arises from pushing state updates into GroupsAccumulators is that each accumulator must handle similar variations with/without filtering and with/without nulls in the input. DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code> uses a templated <a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/physical-expr/src/aggregate/groups_accumulator/accumulate.rs#L28-L54"><code class="language-plaintext highlighter-rouge">NullState</code></a> which encapsulates these common patterns across accumulators.</p>

<p>The code structure is heavily influenced by the fact DataFusion is implemented using <a href="https://www.rust-lang.org/">Rust</a>, a new(ish) systems programming language focused on speed and safety. Rust heavily discourages many of the traditional pointer casting “tricks” used in C/C++ hash grouping implementations. The DataFusion aggregation code is almost entirely <a href="https://doc.rust-lang.org/nomicon/meet-safe-and-unsafe.html#:~:text=Safe%20Rust%20is%20the%20true,Undefined%20Behavior%20(a.k.a.%20UB)."><code class="language-plaintext highlighter-rouge">safe</code></a>, deviating into <code class="language-plaintext highlighter-rouge">unsafe</code> only when necessary. (Rust is a great choice because it makes DataFusion fast, easy to embed, and prevents many crashes and security issues often associated with multi-threaded C/C++ code).</p>

<h2 id="clickbench-results">ClickBench results</h2>

<p>The full results of running the <a href="https://github.com/ClickHouse/ClickBench/tree/main">ClickBench</a> queries against the single Parquet file with DataFusion <code class="language-plaintext highlighter-rouge">27.0.0</code>, DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code>, and DuckDB <code class="language-plaintext highlighter-rouge">0.8.1</code> are below. These numbers were run on a GCP <code class="language-plaintext highlighter-rouge">e2-standard-8 machine</code> with 8 cores and 32 GB of RAM, using the scripts <a href="https://github.com/alamb/datafusion-duckdb-benchmark">here</a>.</p>

<p>As the industry moves towards data systems assembled from components, it is increasingly important that they exchange data using open standards such as <a href="https://arrow.apache.org/">Apache Arrow</a> and <a href="https://parquet.apache.org/">Parquet</a> rather than custom storage and in-memory formats. Thus, this benchmark uses a single input Parquet file representative of many DataFusion users and aligned with the current trend in analytics of avoiding a costly load/transformation into a custom storage format prior to query.</p>

<p>DataFusion now reaches near-DuckDB-speeds querying Parquet data. While we don’t plan to engage in a benchmarking shootout with a team that literally wrote <a href="https://dl.acm.org/doi/abs/10.1145/3209950.3209955">Fair Benchmarking Considered Difficult</a>, hopefully everyone can agree that DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code> is a significant improvement.</p>

<p><img src="/assets/datafusion_fast_grouping/full.png" width="700" /></p>

<p><strong>Figure 6</strong>: Performance of DataFusion <code class="language-plaintext highlighter-rouge">27.0.0</code>, DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code>, and DuckDB <code class="language-plaintext highlighter-rouge">0.8.1</code> on all 43 ClickBench queries against a single <code class="language-plaintext highlighter-rouge">hits.parquet</code> file. Lower is better.</p>

<h3 id="notes-1">Notes</h3>

<p>DataFusion <code class="language-plaintext highlighter-rouge">27.0.0</code> was not able to run several queries due to either planner bugs (Q9, Q11, Q12, 14) or running out of memory (Q33). DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code> solves those issues.</p>

<p>DataFusion is faster than DuckDB for query 21 and 22, likely due to optimized implementations of string pattern matching.</p>

<h2 id="conclusion-performance-matters">Conclusion: performance matters</h2>

<p>Improving aggregation performance by more than a factor of two allows developers building products and projects with DataFusion to spend more time on value-added domain specific features. We believe building systems with DataFusion is much faster than trying to build something similar from scratch. DataFusion increases productivity because it eliminates the need to rebuild well-understood, but costly to implement, analytic database technology. While we’re pleased with the improvements in DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code>, we are by no means done and are pursuing <a href="https://github.com/apache/arrow-datafusion/issues/7000">(Even More) Aggregation Performance</a>. The future for performance is bright.</p>

<h2 id="acknowledgments">Acknowledgments</h2>

<p>DataFusion is a <a href="https://arrow.apache.org/datafusion/contributor-guide/communication.html">community effort</a> and this work was not possible without contributions from many in the community. A special shout out to <a href="https://github.com/sunchao">sunchao</a>, <a href="https://github.com/jyshen">yjshen</a>, <a href="https://github.com/yahoNanJing">yahoNanJing</a>, <a href="https://github.com/mingmwang">mingmwang</a>, <a href="https://github.com/ozankabak">ozankabak</a>, <a href="https://github.com/mustafasrepo">mustafasrepo</a>, and everyone else who contributed ideas, reviews, and encouragement <a href="https://github.com/apache/arrow-datafusion/pull/6800">during</a> this <a href="https://github.com/apache/arrow-datafusion/pull/6904">work</a>.</p>

<h2 id="about-datafusion">About DataFusion</h2>

<p><a href="https://arrow.apache.org/datafusion/">Apache Arrow DataFusion</a> is an extensible query engine and database toolkit, written in <a href="https://www.rust-lang.org/">Rust</a>, that uses <a href="https://arrow.apache.org/">Apache Arrow</a> as its in-memory format. DataFusion, along with <a href="https://calcite.apache.org/">Apache Calcite</a>, Facebook’s <a href="https://github.com/facebookincubator/velox">Velox</a>, and similar technology are part of the next generation “<a href="https://www.usenix.org/publications/login/winter2018/khurana">Deconstructed Database</a>” architectures, where new systems are built on a foundation of fast, modular components, rather than as a single tightly integrated system.</p>

<!-- Footnotes themselves at the bottom. -->
<h2 id="notes-2">Notes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p><code class="language-plaintext highlighter-rouge">SELECT COUNT(*) FROM 'hits.parquet';</code> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p><code class="language-plaintext highlighter-rouge">SELECT COUNT(DISTINCT "UserID") as num_users FROM 'hits.parquet';</code> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p><code class="language-plaintext highlighter-rouge">SELECT COUNT(DISTINCT "SearchPhrase") as num_phrases FROM 'hits.parquet';</code> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p><code class="language-plaintext highlighter-rouge">SELECT COUNT(*) FROM (SELECT DISTINCT "UserID", "SearchPhrase" FROM 'hits.parquet')</code> <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Full script at <a href="https://github.com/alamb/datafusion-duckdb-benchmark/blob/main/hash.py">hash.py</a> <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p><a href="https://datasets.clickhouse.com/hits_compatible/athena_partitioned/hits_%7B%7D.parquet">hits_0.parquet</a>, one of the files from the partitioned ClickBench dataset, which has <code class="language-plaintext highlighter-rouge">100,000</code> rows and is 117 MB in size. The entire dataset has <code class="language-plaintext highlighter-rouge">100,000,000</code> rows in a single 14 GB Parquet file. The script did not complete on the entire dataset after 40 minutes, and used 212 GB RAM at peak. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>alamb, Dandandan, tustvold</name></author><category term="release" /><summary type="html"><![CDATA[Aggregating Millions of Groups Fast in Apache Arrow DataFusion Andrew Lamb, Daniël Heres, Raphael Taylor-Davies, Note: this article was originally published on the InfluxData Blog TLDR Grouped aggregations are a core part of any analytic tool, creating understandable summaries of huge data volumes. Apache Arrow DataFusion’s parallel aggregation capability is 2-3x faster in the newly released version 28.0.0 for queries with a large number (10,000 or more) of groups. Improving aggregation performance matters to all users of DataFusion. For example, both InfluxDB, a time series data platform and Coralogix, a full-stack observability platform, aggregate vast amounts of raw data to monitor and create insights for our customers. Improving DataFusion’s performance lets us provide better user experiences by generating insights faster with fewer resources. Because DataFusion is open source and released under the permissive Apache 2.0 license, the whole DataFusion community benefits as well. With the new optimizations, DataFusion’s grouping speed is now close to DuckDB, a system that regularly reports great grouping benchmark performance numbers. Figure 1 contains a representative sample of ClickBench on a single Parquet file, and the full results are at the end of this article. Figure 1: Query performance for ClickBench queries on queries 16, 17, 18 and 19 on a single Parquet file for DataFusion 27.0.0, DataFusion 28.0.0 and DuckDB 0.8.1. Introduction to high cardinality grouping Aggregation is a fancy word for computing summary statistics across many rows that have the same value in one or more columns. We call the rows with the same values groups and “high cardinality” means there are a large number of distinct groups in the dataset. At the time of writing, a “large” number of groups in analytic engines is around 10,000. For example the ClickBench hits dataset contains 100 million anonymized user clicks across a set of websites. ClickBench Query 17 is: SELECT "UserID", "SearchPhrase", COUNT(*) FROM hits GROUP BY "UserID", "SearchPhrase" ORDER BY COUNT(*) DESC LIMIT 10; In English, this query finds “the top ten (user, search phrase) combinations, across all clicks” and produces the following results (there are no search phrases for the top ten users): +---------------------+--------------+-----------------+ | UserID | SearchPhrase | COUNT(UInt8(1)) | +---------------------+--------------+-----------------+ | 1313338681122956954 | | 29097 | | 1907779576417363396 | | 25333 | | 2305303682471783379 | | 10597 | | 7982623143712728547 | | 6669 | | 7280399273658728997 | | 6408 | | 1090981537032625727 | | 6196 | | 5730251990344211405 | | 6019 | | 6018350421959114808 | | 5990 | | 835157184735512989 | | 5209 | | 770542365400669095 | | 4906 | +---------------------+--------------+-----------------+ The ClickBench dataset contains 99,997,497 total rows1 17,630,976 different users (distinct UserIDs)2 6,019,103 different search phrases3 24,070,560 distinct combinations4 of (UserID, SearchPhrase) Thus, to answer the query, DataFusion must map each of the 100M different input rows into one of the 24 million different groups, and keep count of how many such rows there are in each group. The solution Like most concepts in databases and other analytic systems, the basic ideas of this algorithm are straightforward and taught in introductory computer science courses. You could compute the query with a program such as this5: import pandas as pd from collections import defaultdict from operator import itemgetter # read file hits = pd.read_parquet('hits.parquet', engine='pyarrow') # build groups counts = defaultdict(int) for index, row in hits.iterrows(): group = (row['UserID'], row['SearchPhrase']); # update the dict entry for the corresponding key counts[group] += 1 # Print the top 10 values print (dict(sorted(counts.items(), key=itemgetter(1), reverse=True)[:10])) This approach, while simple, is both slow and very memory inefficient. It requires over 40 seconds to compute the results for less than 1% of the dataset6. Both DataFusion 28.0.0 and DuckDB 0.8.1 compute results in under 10 seconds for the entire dataset. To answer this query quickly and efficiently, you have to write your code such that it: Keeps all cores busy aggregating via parallelized computation Updates aggregate values quickly, using vectorizable loops that are easy for compilers to translate into the high performance SIMD instructions available in modern CPUs. The rest of this article explains how grouping works in DataFusion and the improvements we made in 28.0.0. Two phase parallel partitioned grouping Both DataFusion 27.0. and 28.0.0 use state-of-the-art, two phase parallel hash partitioned grouping, similar to other high-performance vectorized engines like DuckDB’s Parallel Grouped Aggregates. In pictures this looks like: ▲ ▲ │ │ │ │ │ │ ┌───────────────────────┐ ┌───────────────────┐ │ GroupBy │ │ GroupBy │ Step 4 │ (Final) │ │ (Final) │ └───────────────────────┘ └───────────────────┘ ▲ ▲ │ │ └────────────┬───────────┘ │ │ ┌─────────────────────────┐ │ Repartition │ Step 3 │ HASH(x) │ └─────────────────────────┘ ▲ │ ┌────────────┴──────────┐ │ │ │ │ ┌────────────────────┐ ┌─────────────────────┐ │ GroupyBy │ │ GroupBy │ Step 2 │ (Partial) │ │ (Partial) │ └────────────────────┘ └─────────────────────┘ ▲ ▲ ┌──┘ └─┐ │ │ .─────────. .─────────. ,─' '─. ,─' '─. ; Input : ; Input : Step 1 : Stream 1 ; : Stream 2 ; ╲ ╱ ╲ ╱ '─. ,─' '─. ,─' `───────' `───────' Figure 2: Two phase repartitioned grouping: data flows from bottom (source) to top (results) in two phases. First (Steps 1 and 2), each core reads the data into a core-specific hash table, computing intermediate aggregates without any cross-core coordination. Then (Steps 3 and 4) DataFusion divides the data (“repartitions”) into distinct subsets by group value, and each subset is sent to a specific core which computes the final aggregate. The two phases are critical for keeping cores busy in a multi-core system. Both phases use the same hash table approach (explained in the next section), but differ in how the groups are distributed and the partial results emitted from the accumulators. The first phase aggregates data as soon as possible after it is produced. However, as shown in Figure 2, the groups can be anywhere in any input, so the same group is often found on many different cores. The second phase uses a hash function to redistribute data evenly across the cores, so each group value is processed by exactly one core which emits the final results for that group. ┌─────┐ ┌─────┐ │ 1 │ │ 3 │ │ 2 │ │ 4 │ 2. After Repartitioning: each └─────┘ └─────┘ group key appears in exactly ┌─────┐ ┌─────┐ one partition │ 1 │ │ 3 │ │ 2 │ │ 4 │ └─────┘ └─────┘ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌─────┐ ┌─────┐ │ 2 │ │ 2 │ │ 1 │ │ 2 │ │ 3 │ │ 3 │ │ 4 │ │ 1 │ └─────┘ └─────┘ 1. Input Stream: groups ... ... values are spread ┌─────┐ ┌─────┐ arbitrarily over each input │ 1 │ │ 4 │ │ 4 │ │ 3 │ │ 1 │ │ 1 │ │ 4 │ │ 3 │ │ 3 │ │ 2 │ │ 2 │ │ 2 │ │ 2 │ └─────┘ └─────┘ Core A Core B Figure 3: Group value distribution across 2 cores during aggregation phases. In the first phase, every group value 1, 2, 3, 4, is present in the input stream processed by each core. In the second phase, after repartitioning, the group values 1 and 2 are processed by core A, and values 3 and 4 are processed only by core B. There are some additional subtleties in the DataFusion implementation not mentioned above due to space constraints, such as: The policy of when to emit data from the first phase’s hash table (e.g. because the data is partially sorted) Handling specific filters per aggregate (due to the FILTER SQL clause) Data types of intermediate values (which may not be the same as the final output for some aggregates such as AVG). Action taken when memory use exceeds its budget. Hash grouping DataFusion queries can compute many different aggregate functions for each group, both built in and/or user defined AggregateUDFs. The state for each aggregate function, called an accumulator, is tracked with a hash table (DataFusion uses the excellent HashBrown RawTable API), which logically stores the “index” identifying the specific group value. Hash grouping in 27.0.0 As shown in Figure 3, DataFusion 27.0.0 stores the data in a GroupState structure which, unsurprisingly, tracks the state for each group. The state for each group consists of: The actual value of the group columns, in Arrow Row format. In-progress accumulations (e.g. the running counts for the COUNT aggregate) for each group, in one of two possible formats (Accumulator or RowAccumulator). Scratch space for tracking which rows match each aggregate in each batch. ┌──────────────────────────────────────┐ │ │ │ ... │ │ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ │ │ ┃ ┃ │ ┌─────────┐ │ ┃ ┌──────────────────────────────┐ ┃ │ │ │ │ ┃ │group values: OwnedRow │ ┃ │ │ ┌─────┐ │ │ ┃ └──────────────────────────────┘ ┃ │ │ │ 5 │ │ │ ┃ ┌──────────────────────────────┐ ┃ │ │ ├─────┤ │ │ ┃ │Row accumulator: │ ┃ │ │ │ 9 │─┼────┐ │ ┃ │Vec&lt;u8&gt; │ ┃ │ │ ├─────┤ │ │ │ ┃ └──────────────────────────────┘ ┃ │ │ │ ... │ │ │ │ ┃ ┌──────────────────────┐ ┃ │ │ ├─────┤ │ │ │ ┃ │┌──────────────┐ │ ┃ │ │ │ 1 │ │ │ │ ┃ ││Accumulator 1 │ │ ┃ │ │ ├─────┤ │ │ │ ┃ │└──────────────┘ │ ┃ │ │ │ ... │ │ │ │ ┃ │┌──────────────┐ │ ┃ │ │ └─────┘ │ │ │ ┃ ││Accumulator 2 │ │ ┃ │ │ │ │ │ ┃ │└──────────────┘ │ ┃ │ └─────────┘ │ │ ┃ │ Box&lt;dyn Accumulator&gt; │ ┃ │ Hash Table │ │ ┃ └──────────────────────┘ ┃ │ │ │ ┃ ┌─────────────────────────┐ ┃ │ │ │ ┃ │scratch indices: Vec&lt;u32&gt;│ ┃ │ │ │ ┃ └─────────────────────────┘ ┃ │ │ │ ┃ GroupState ┃ │ └─────▶ │ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │ │ │ Hash table tracks an │ ... │ index into group_states │ │ └──────────────────────────────────────┘ group_states: Vec&lt;GroupState&gt; There is one GroupState PER GROUP Figure 4: Hash group operator structure in DataFusion 27.0.0. A hash table maps each group to a GroupState which contains all the per-group states. To compute the aggregate, DataFusion performs the following steps for each input batch: Calculate hash using efficient vectorized code, specialized for each data type. Determine group indexes for each input row using the hash table (creating new entries for newly seen groups). Update Accumulators for each group that had input rows, assembling the rows into a contiguous range for vectorized accumulator if there are a sufficient number of them. DataFusion also stores the hash values in the table to avoid potentially costly hash recomputation when resizing the hash table. This scheme works very well for a relatively small number of distinct groups: all accumulators are efficiently updated with large contiguous batches of rows. However, this scheme is not ideal for high cardinality grouping due to: Multiple allocations per group for the group value row format, as well as for the RowAccumulators and each Accumulator. The Accumulator may have additional allocations within it as well. Non-vectorized updates: Accumulator updates often fall back to a slower non-vectorized form because the number of distinct groups is large (and thus number of values per group is small) in each input batch. Hash grouping in 28.0.0 For 28.0.0, we rewrote the core group by implementation following traditional system optimization principles: fewer allocations, type specialization, and aggressive vectorization. DataFusion 28.0.0 uses the same RawTable and still stores group indexes. The major differences, as shown in Figure 4, are: Group values are stored either Inline in the RawTable (for single columns of primitive types), where the conversion to Row format costs more than its benefit In a separate Rows structure with a single contiguous allocation for all groups values, rather than an allocation per group. Accumulators manage the state for all the groups internally, so the code to update intermediate values is a tight type specialized loop. The new GroupsAccumulator interface results in highly efficient type accumulator update loops. ┌───────────────────────────────────┐ ┌───────────────────────┐ │ ┌ ─ ─ ─ ─ ─ ┐ ┌─────────────────┐│ │ ┏━━━━━━━━━━━━━━━━━━━┓ │ │ │ ││ │ ┃ ┌──────────────┐ ┃ │ │ │ │ │ ┌ ─ ─ ┐┌─────┐ ││ │ ┃ │┌───────────┐ │ ┃ │ │ │ X │ 5 │ ││ │ ┃ ││ value1 │ │ ┃ │ │ │ │ │ ├ ─ ─ ┤├─────┤ ││ │ ┃ │└───────────┘ │ ┃ │ │ │ Q │ 9 │──┼┼──┐ │ ┃ │ ... │ ┃ │ │ │ │ │ ├ ─ ─ ┤├─────┤ ││ └──┼─╋─▶│ │ ┃ │ │ │ ... │ ... │ ││ │ ┃ │┌───────────┐ │ ┃ │ │ │ │ │ ├ ─ ─ ┤├─────┤ ││ │ ┃ ││ valueN │ │ ┃ │ │ │ H │ 1 │ ││ │ ┃ │└───────────┘ │ ┃ │ │ │ │ │ ├ ─ ─ ┤├─────┤ ││ │ ┃ │values: Vec&lt;T&gt;│ ┃ │ │ Rows │ ... │ ... │ ││ │ ┃ └──────────────┘ ┃ │ │ │ │ │ └ ─ ─ ┘└─────┘ ││ │ ┃ ┃ │ │ ─ ─ ─ ─ ─ ─ │ ││ │ ┃ GroupsAccumulator ┃ │ │ └─────────────────┘│ │ ┗━━━━━━━━━━━━━━━━━━━┛ │ │ Hash Table │ │ │ │ │ │ ... │ └───────────────────────────────────┘ └───────────────────────┘ GroupState Accumulators Hash table value stores group_indexes One GroupsAccumulator and group values. per aggregate. Each stores the state for Group values are stored either inline *ALL* groups, typically in the hash table or in a single using a native Vec&lt;T&gt; allocation using the arrow Row format Figure 5: Hash group operator structure in DataFusion 28.0.0. Group values are stored either directly in the hash table, or in a single allocation using the arrow Row format. The hash table contains group indexes. A single GroupsAccumulator stores the per-aggregate state for all groups. This new structure improves performance significantly for high cardinality groups due to: Reduced allocations: There are no longer any individual allocations per group. Contiguous native accumulator states: Type-specialized accumulators store the values for all groups in a single contiguous allocation using a Rust Vec&lt;T&gt; of some native type. Vectorized state update: The inner aggregate update loops, which are type-specialized and in terms of native Vecs, are well-vectorized by the Rust compiler (thanks LLVM!). Notes Some vectorized grouping implementations store the accumulator state row-wise directly in the hash table, which often uses modern CPU caches efficiently. Managing accumulator state in columnar fashion may sacrifice some cache locality, however it ensures the size of the hash table remains small, even when there are large numbers of groups and aggregates, making it easier for the compiler to vectorize the accumulator update. Depending on the cost of recomputing hash values, DataFusion 28.0.0 may or may not store the hash values in the table. This optimizes the tradeoff between the cost of computing the hash value (which is expensive for strings, for example) vs. the cost of storing it in the hash table. One subtlety that arises from pushing state updates into GroupsAccumulators is that each accumulator must handle similar variations with/without filtering and with/without nulls in the input. DataFusion 28.0.0 uses a templated NullState which encapsulates these common patterns across accumulators. The code structure is heavily influenced by the fact DataFusion is implemented using Rust, a new(ish) systems programming language focused on speed and safety. Rust heavily discourages many of the traditional pointer casting “tricks” used in C/C++ hash grouping implementations. The DataFusion aggregation code is almost entirely safe, deviating into unsafe only when necessary. (Rust is a great choice because it makes DataFusion fast, easy to embed, and prevents many crashes and security issues often associated with multi-threaded C/C++ code). ClickBench results The full results of running the ClickBench queries against the single Parquet file with DataFusion 27.0.0, DataFusion 28.0.0, and DuckDB 0.8.1 are below. These numbers were run on a GCP e2-standard-8 machine with 8 cores and 32 GB of RAM, using the scripts here. As the industry moves towards data systems assembled from components, it is increasingly important that they exchange data using open standards such as Apache Arrow and Parquet rather than custom storage and in-memory formats. Thus, this benchmark uses a single input Parquet file representative of many DataFusion users and aligned with the current trend in analytics of avoiding a costly load/transformation into a custom storage format prior to query. DataFusion now reaches near-DuckDB-speeds querying Parquet data. While we don’t plan to engage in a benchmarking shootout with a team that literally wrote Fair Benchmarking Considered Difficult, hopefully everyone can agree that DataFusion 28.0.0 is a significant improvement. Figure 6: Performance of DataFusion 27.0.0, DataFusion 28.0.0, and DuckDB 0.8.1 on all 43 ClickBench queries against a single hits.parquet file. Lower is better. Notes DataFusion 27.0.0 was not able to run several queries due to either planner bugs (Q9, Q11, Q12, 14) or running out of memory (Q33). DataFusion 28.0.0 solves those issues. DataFusion is faster than DuckDB for query 21 and 22, likely due to optimized implementations of string pattern matching. Conclusion: performance matters Improving aggregation performance by more than a factor of two allows developers building products and projects with DataFusion to spend more time on value-added domain specific features. We believe building systems with DataFusion is much faster than trying to build something similar from scratch. DataFusion increases productivity because it eliminates the need to rebuild well-understood, but costly to implement, analytic database technology. While we’re pleased with the improvements in DataFusion 28.0.0, we are by no means done and are pursuing (Even More) Aggregation Performance. The future for performance is bright. Acknowledgments DataFusion is a community effort and this work was not possible without contributions from many in the community. A special shout out to sunchao, yjshen, yahoNanJing, mingmwang, ozankabak, mustafasrepo, and everyone else who contributed ideas, reviews, and encouragement during this work. About DataFusion Apache Arrow DataFusion is an extensible query engine and database toolkit, written in Rust, that uses Apache Arrow as its in-memory format. DataFusion, along with Apache Calcite, Facebook’s Velox, and similar technology are part of the next generation “Deconstructed Database” architectures, where new systems are built on a foundation of fast, modular components, rather than as a single tightly integrated system. Notes SELECT COUNT(*) FROM 'hits.parquet'; &#8617; SELECT COUNT(DISTINCT "UserID") as num_users FROM 'hits.parquet'; &#8617; SELECT COUNT(DISTINCT "SearchPhrase") as num_phrases FROM 'hits.parquet'; &#8617; SELECT COUNT(*) FROM (SELECT DISTINCT "UserID", "SearchPhrase" FROM 'hits.parquet') &#8617; Full script at hash.py &#8617; hits_0.parquet, one of the files from the partitioned ClickBench dataset, which has 100,000 rows and is 117 MB in size. The entire dataset has 100,000,000 rows in a single 14 GB Parquet file. The script did not complete on the entire dataset after 40 minutes, and used 212 GB RAM at peak. &#8617;]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 0.5.1 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/06/27/adbc-0.5.1-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.5.1 (Libraries) Release" /><published>2023-06-27T00:00:00-04:00</published><updated>2023-06-27T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/06/27/adbc-0.5.1-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/06/27/adbc-0.5.1-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.5.1 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/7"><strong>8
resolved issues</strong></a> from <a href="#contributors"><strong>8 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version
0.5.1.  The <strong>API specification</strong> is versioned separately and is
at version 1.0.0.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.5.1/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>This is a patch release primarily aimed at fixing a <a href="https://github.com/apache/arrow-adbc/commit/f35485a5f3c9597668c0b4a8936621c97c4adc15">deadlock in the Snowflake driver</a> that was discovered post-release.</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.5.0..apache-arrow-adbc-0.5.1
     9	David Li
     5	Dewey Dunnington
     4	Matt Topol
     3	William Ayd
     2	davidhcoe
     1	Curt Hagenlocher
     1	Diego Fernández Giraldo
     1	Sutou Kouhei
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>Work for the proposed <a href="https://github.com/apache/arrow-adbc/milestone/3">1.1.0 API revision</a> is progressing on a <a href="https://github.com/apache/arrow-adbc/tree/spec-1.1.0">branch</a>, with several API proposals merged in and others awaiting review.  They will then receive prototype implementations before being submitted for review/voting.  This is not currently targeting any release of the ADBC libraries, although we hope to wrap up this effort in the next few months.</p>

<p>A milestone has been created to explore <a href="https://github.com/apache/arrow-adbc/milestone/9">asynchronous APIs</a>, which have been requested for a while now.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.5.1 release of the Apache Arrow ADBC libraries. This covers includes 8 resolved issues from 8 distinct contributors. This is a release of the libraries, which are at version 0.5.1. The API specification is versioned separately and is at version 1.0.0. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights This is a patch release primarily aimed at fixing a deadlock in the Snowflake driver that was discovered post-release. Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.5.0..apache-arrow-adbc-0.5.1 9 David Li 5 Dewey Dunnington 4 Matt Topol 3 William Ayd 2 davidhcoe 1 Curt Hagenlocher 1 Diego Fernández Giraldo 1 Sutou Kouhei Roadmap Work for the proposed 1.1.0 API revision is progressing on a branch, with several API proposals merged in and others awaiting review. They will then receive prototype implementations before being submitted for review/voting. This is not currently targeting any release of the ADBC libraries, although we hope to wrap up this effort in the next few months. A milestone has been created to explore asynchronous APIs, which have been requested for a while now. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Our journey at F5 with Apache Arrow (part 2): Adaptive Schemas and Sorting to Optimize Arrow Usage</title><link href="https://arrow.apache.org/blog/2023/06/26/our-journey-at-f5-with-apache-arrow-part-2/" rel="alternate" type="text/html" title="Our journey at F5 with Apache Arrow (part 2): Adaptive Schemas and Sorting to Optimize Arrow Usage" /><published>2023-06-26T00:00:00-04:00</published><updated>2023-06-26T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/06/26/our-journey-at-f5-with-apache-arrow-part-2</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/06/26/our-journey-at-f5-with-apache-arrow-part-2/"><![CDATA[<!--

-->

<p>In the previous <a href="https://arrow.apache.org/blog/2023/04/11/our-journey-at-f5-with-apache-arrow-part-1/">article</a>, we discussed our use of Apache Arrow within the context of the OpenTelemetry project. We investigated various techniques to maximize the efficiency of Apache Arrow, aiming to find the optimal balance between data compression ratio and queryability. The compression results speak for themselves, boasting improvements ranging from 1.5x to 5x better than the original OTLP protocol. In this article, we will delve into three techniques that have enabled us to enhance both the compression ratio and memory usage of Apache Arrow buffers within the current version of the <a href="https://github.com/f5/otel-arrow-adapter">OTel Arrow protocol</a>.</p>

<p>The first technique we’ll discuss aims to optimize schemas in terms of memory usage. As you’ll see, the gains can be substantial, potentially halving memory usage in certain cases. The second section will delve more deeply into the various approaches that can be used to handle recursive schema definitions. Lastly, we’ll emphasize that the design of your schema(s), coupled with the sorts you can apply at the record level, play a pivotal role in maximizing the benefits of Apache Arrow and its columnar representation.</p>

<h2 id="handling-dynamic-and-unknown-data-distributions">Handling dynamic and unknown data distributions</h2>

<p>In certain contexts, the comprehensive definition of an Arrow schema can end up being overly broad and complex in order to cover all possible cases that you intend to represent in columnar form. However, as is often the case with complex schemas, only a subset of this schema will actually be utilized for a specific deployment. Similarly, it’s not always possible to determine the optimal dictionary encoding for one or more fields in advance. Employing a broad and very general schema that covers all cases is usually more memory-intensive. This is because, for most implementations, a column without value still continues to consume memory space. Likewise, a column with dictionary encoding that indexes a uint64 will occupy four times more memory than the same column with a dictionary encoding based on a uint8.</p>

<p>To illustrate this more concretely, let’s consider an OTel collector positioned at the output of a production environment, receiving a telemetry data stream produced by a large and dynamic set of servers. Invariably, the content of this telemetry stream will change in volume and nature over time. It’s challenging to predict the optimal schema in such a scenario, and it’s equally difficult to know in advance the distribution of a particular attribute of the telemetry data passing through this point.</p>

<p>To optimize such scenarios, we have adopted an intermediary approach that we have named <strong>dynamic Arrow schema</strong>, aiming to gradually adapt the schema based on the observed data. The general principle is relatively simple. We start with a general schema defining the maximum envelope of what should be represented. Some fields of this schema will be declared optional, while other fields will be encoded with multiple possible options depending on the observed distribution. In theory, this principle can be applied to other types of transformations (e.g., recursive column creation) but we will let your imagination explore these other options. So if you encounter data streams where certain fields are not utilized, some union variants remain unused, and/or the value distribution of a field cannot be determined a priori, it may be worthwhile to invest time in implementing this model. This can lead to improved efficiency in terms of compression ratio, memory usage, and processing speed.</p>

<p>The following Go Arrow schema definition provides an example of such a schema, instrumented with a collection of annotations. These annotations will be processed by an enhanced Record Builder, equipped with the ability to dynamically adapt the schema. The structure of this system is illustrated in Figure 1.</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">var</span> <span class="p">(</span>
  <span class="c">// Arrow schema for the OTLP Arrow Traces record (without attributes, links, and events).</span>
  <span class="n">TracesSchema</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">.</span><span class="n">NewSchema</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
      <span class="c">// Nullabe:true means the field is optional, in this case of 16 bit unsigned integers</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">ID</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint16</span><span class="p">,</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">Resource</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">ID</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint16</span><span class="p">,</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
        <span class="c">// --- Use dictionary with 8 bit integers initially ----</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">SchemaUrl</span><span class="p">,</span><span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span><span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">DroppedAttributesCount</span><span class="p">,</span><span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint32</span><span class="p">,</span><span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">}</span><span class="o">...</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">Scope</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
          <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">ID</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint16</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">acommon</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">acommon</span><span class="o">.</span><span class="n">DeltaEncoding</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
          <span class="c">// --- Use dictionary with 8 bit integers initially ----</span>
          <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">Name</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">acommon</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">acommon</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
          <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">Version</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">acommon</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">acommon</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
          <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">DroppedAttributesCount</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint32</span><span class="p">,</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">}</span><span class="o">...</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">SchemaUrl</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">StartTimeUnixNano</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FixedWidthTypes</span><span class="o">.</span><span class="n">Timestamp_ns</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">DurationTimeUnixNano</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FixedWidthTypes</span><span class="o">.</span><span class="n">Duration_ms</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">)},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">TraceId</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">FixedSizeBinaryType</span><span class="p">{</span><span class="n">ByteWidth</span><span class="o">:</span> <span class="m">16</span><span class="p">}},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">SpanId</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">FixedSizeBinaryType</span><span class="p">{</span><span class="n">ByteWidth</span><span class="o">:</span> <span class="m">8</span><span class="p">}},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">TraceState</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">ParentSpanId</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">FixedSizeBinaryType</span><span class="p">{</span><span class="n">ByteWidth</span><span class="o">:</span> <span class="m">8</span><span class="p">},</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">Name</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">)},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">KIND</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Int32</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">DroppedAttributesCount</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint32</span><span class="p">,</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">DroppedEventsCount</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint32</span><span class="p">,</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">DroppedLinksCount</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint32</span><span class="p">,</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">Status</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">StatusCode</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Int32</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">StatusMessage</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">}</span><span class="o">...</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
    <span class="p">},</span> <span class="no">nil</span><span class="p">)</span>
  <span class="p">)</span>
</code></pre></div></div>

<p>In this example, Arrow field-level metadata are employed to designate when a field is optional (Nullable: true) or to specify the minimal dictionary encoding applicable to a particular field (Metadata Dictionary8/16/…). Now let’s imagine a scenario utilizing this schema in a straightforward scenario, wherein only a handful of fields are actually in use, and the cardinality of most dictionary-encoded fields is low (i.e., below 2^8). Ideally, we’d want a system capable of dynamically constructing the following simplified schema, which, in essence, is a strict subset of the original schema.</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">var</span> <span class="p">(</span>
  <span class="c">// Simplified schema definition generated by the Arrow Record encoder based on</span>
  <span class="c">// the data observed.</span>
  <span class="n">TracesSchema</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">.</span><span class="n">NewSchema</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">ID</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint16</span><span class="p">,</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">StartTimeUnixNano</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FixedWidthTypes</span><span class="o">.</span><span class="n">Timestamp_ns</span><span class="p">},</span>
    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">TraceId</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">FixedSizeBinaryType</span><span class="p">{</span><span class="n">ByteWidth</span><span class="o">:</span> <span class="m">16</span><span class="p">}},</span>
    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">SpanId</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">FixedSizeBinaryType</span><span class="p">{</span><span class="n">ByteWidth</span><span class="o">:</span> <span class="m">8</span><span class="p">}},</span>
    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">Name</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">DictionaryType</span> <span class="p">{</span>
      <span class="n">IndexType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint8</span><span class="p">,</span>
      <span class="n">ValueType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">}},</span>
    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">KIND</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">DictionaryType</span> <span class="p">{</span>
      <span class="n">IndexType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint8</span><span class="p">,</span>
      <span class="n">ValueType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Int32</span><span class="p">,</span>
    <span class="p">},</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
  <span class="p">},</span> <span class="no">nil</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Additionally, we desire a system capable of automatically adapting the aforementioned schema if it encounters new fields or existing fields with a cardinality exceeding the size of the current dictionary definition in future batches. In extreme scenarios, if the cardinality of a specific field surpasses a certain threshold, we would prefer the system to automatically revert to the non-dictionary representation (mechanism of dictionary overflow). That is precisely what we will elaborate on in the remainder of this section.</p>

<p>An overview of the different components and events used to implement this approach is depicted in figure 1.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/adaptive-schema-architecture.svg" width="100%" class="img-responsive" alt="Fig 1: Adaptive Arrow schema architecture overview." />
  <figcaption>Fig 1: Adaptive Arrow schema architecture overview.</figcaption>
</figure>

<p>The overall Adaptive Arrow schema component takes a data stream segmented into batches and produces one or multiple streams of Arrow Records (one schema per stream). Each of these records is defined with an Arrow schema, which is based both on the annotated Arrow schema and the shape of fields observed in the incoming data.</p>

<p>More specifically, the process of the Adaptive Arrow schema component consists of four main phases</p>

<p><strong>Initialization phase</strong></p>

<p>During the initialization phase, the Arrow Record Encoder reads the annotated Arrow schema (i.e. the reference schema) and generates a collection of transformations. When these transformations are applied to the reference schema, they yield the first minimal Arrow schema that adheres to the constraints depicted by these annotations. In this initial iteration, all optional fields are eliminated, and all dictionary-encoded fields are configured to utilize the smallest encoding as defined by the annotation (only <code class="language-plaintext highlighter-rouge">Dictionary8</code> in the previous example). These transformations form a tree, reflecting the structure of the reference schema.</p>

<p><strong>Feeding phase</strong></p>

<p>Following the initialization is the feeding phase. Here, the Arrow Record Encoder scans the batch and attempts to store all the fields in an Arrow Record Builder, which is defined by the schema created in the prior step. If a field exists in the data but is not included in the schema, the encoder will trigger a <code class="language-plaintext highlighter-rouge">missing field</code> event. This process continues until the current batch is completely processed. An additional internal check is conducted on all dictionary-encoded fields in the Arrow Record builder to ensure there’s no dictionary overflow (i.e. more unique entries than the cardinality of the index permits). <code class="language-plaintext highlighter-rouge">Dictionary overflow</code> events are generated if such a situation is detected. Consequently, by the end, all unknown fields and dictionary overflow would have been detected, or alternatively, no discrepancies would have surfaced if the data aligns perfectly with the schema.</p>

<p><strong>Corrective phase</strong></p>

<p>If at least one event has been generated, a corrective phase will be initiated to fix the schema. This optional stage considers all the events generated in the previous stage and adjusts the transformation tree accordingly to align with the observed data. A <code class="language-plaintext highlighter-rouge">missing field</code> event will remove a NoField transformation for the corresponding field. A <code class="language-plaintext highlighter-rouge">dictionary overflow</code> event will modify the dictionary transformation to mirror the event (e.g. changing the index type from uint8 to uint16, or if the maximum index size has been reached, the transformation will remove the dictionary-encoding and revert to the original non-dictionary-encoded type). The updated transformation tree is subsequently used to create a new schema and a fresh Arrow Record Builder. This Record Builder is then utilized to replay the preceding feeding phase with the batch that wasn’t processed correctly.</p>

<p><strong>Routing phase</strong></p>

<p>Once a Record Builder has been properly fed, an Arrow Record is created, and the system transitions into the routing phase. The router component calculates a schema signature of the record and utilizes this signature to route the record to an existing Arrow stream compatible with the signature, or it initiates a new stream if there is no match.</p>

<p>This four-phase process should gradually adapt and stabilize the schema to a structure and definition that is optimized for a specific data stream. Unused fields will never unnecessarily consume memory. Dictionary-encoded fields will be defined with the most optimal index size based on the observed data cardinality, and fields with a cardinality exceeding a certain threshold (defined by configuration) will automatically revert to their non-dictionary-encoded versions.</p>

<p>To effectively execute this approach, you must ensure that there is a sufficient level of flexibility on the receiver side. It’s crucial that your downstream pipeline remains functional even when some fields are missing in the schema or when various dictionary index configurations are employed. While this may not always be feasible without implementing additional transformations upon reception, it proves worthwhile in certain scenarios.</p>

<p>The following results highlight the significant memory usage reduction achieved through the application of various optimization techniques. These results were gathered using a schema akin to the one previously presented. The considerable memory efficiency underscores the effectiveness of this approach.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/memory-usage-25k-traces.png" width="100%" class="img-responsive" alt="Fig 2: Comparative analysis of memory usage for different schema optimizations." />
  <figcaption>Fig 2: Comparative analysis of memory usage for different schema optimizations.</figcaption>
</figure>

<p>The concept of a transformation tree enables a generalized approach to perform various types of schema optimizations based on the knowledge acquired from the data. This architecture is highly flexible; the current implementation allows for the removal of unused fields, the application of the most specific dictionary encoding, and the optimization of union type variants. In the future, there is potential for introducing additional optimizations that can be expressed as transformations on the initial schema. An implementation of this approach is available <a href="https://github.com/f5/otel-arrow-adapter/tree/main/pkg/otel/common/schema">here</a>.</p>

<h2 id="handling-recursive-schema-definition">Handling recursive schema definition</h2>

<p>Apache Arrow does not support recursive schema definitions, implying that data structures with variable depth cannot be directly represented. Figure 3 exemplifies such a recursive definition where the value of an attribute can either be a simple data type, a list of values, or a map of values. The depth of this definition cannot be predetermined.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/recursive-def-otel-attributes.svg" width="100%" class="img-responsive" alt="Fig 3: Recursive definition of OTel attributes." />
  <figcaption>Fig 3: Recursive definition of OTel attributes.</figcaption>
</figure>

<p>Several strategies can be employed to circumvent this limitation. Technically, the dynamic schema concept we’ve presented could be expanded to dynamically update the schema to include any missing level of recursion. However, for this use case, this method is complex and has the notable downside of not offering any assurance on the maximum size of the schema. This lack of constraint can pose security issues; hence, this approach isn’t elaborated upon.</p>

<p>The second approach consists of breaking the recursion by employing a serialization format that supports the definition of a recursive schema. The result of this serialization can then be integrated into the Arrow record as a binary type column, effectively halting the recursion at a specific level. To fully leverage the advantages of columnar representation, it is crucial to apply this ad-hoc serialization as deeply within the data structure as feasible. In the context of OpenTelemetry, this is performed at the attribute level – more specifically, at the second level of attributes.</p>

<p>A variety of serialization formats, such as protobuf or CBOR, can be employed to encode recursive data. Without particular treatment, these binary columns may not be easily queryable by the existing Arrow query engines. Therefore, it’s crucial to thoughtfully ascertain when and where to apply such a technique. While I’m not aware of any attempts to address this limitation within the Arrow system, it doesn’t seem insurmountable and would constitute a valuable extension. This would help reduce the complexity of integrating Arrow with other systems that rely on such recursive definitions.</p>

<h2 id="importance-of-sorting">Importance of sorting</h2>

<p>In our preceding <a href="https://arrow.apache.org/blog/2023/04/11/our-journey-at-f5-with-apache-arrow-part-1/">article</a>, we explored a variety of strategies to represent hierarchical data models, including nested structures based on struct/list/map/union, denormalization and flattening representations, as well as a multi-record approach. Each method presents its unique advantages and disadvantages. However, in this last section, we’ll delve deeper into the multi-record approach, focusing specifically on its ability to offer versatile sorting options and how these options contribute to an enhanced compression ratio.</p>

<p>In the OTel Arrow protocol, we leverage the multi-record approach to represent metrics, logs, and traces. The following entity-relationship diagram offers a simplified version of various record schemas and illustrates their relationships, specifically those used to represent gauges and sums. A comprehensive description of the Arrow data model employed in OpenTelemetry can be accessed <a href="https://github.com/f5/otel-arrow-adapter/blob/main/docs/data_model.md">here</a>.</p>

<p>These Arrow records, also referred to as tables, form a hierarchy with <code class="language-plaintext highlighter-rouge">METRICS</code> acting as the primary entry point. Each table can be independently sorted according to one or more columns. This sorting strategy facilitates the grouping of duplicated data, thereby improving the compression ratio.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/metric-dp-data-model.png" width="100%" class="img-responsive" alt="Fig 4: A simplified entity-relationship diagram representing OTel sum &amp; gauge metrics." />
  <figcaption>Fig 4: A simplified entity-relationship diagram representing OTel sum &amp; gauge metrics.</figcaption>
</figure>

<p>The relationship between the primary <code class="language-plaintext highlighter-rouge">METRICS</code> table and the secondary <code class="language-plaintext highlighter-rouge">RESOURCE_ATTRS</code>, <code class="language-plaintext highlighter-rouge">SCOPE_ATTRS</code>, and <code class="language-plaintext highlighter-rouge">NUMBER_DATA_POINTS</code> tables is established through a unique <code class="language-plaintext highlighter-rouge">id</code> in the main table and a <code class="language-plaintext highlighter-rouge">parent_id</code> column in each of the secondary tables. This {id,parent_id} pair represents an overhead that should be minimized to the greatest extent possible post-compression.</p>

<p>To achieve this, the ordering process for the different tables adheres to the hierarchy, starting from the main table down to the leaf. The main table is sorted (by one or multiple columns), and then an incremental id is assigned to each row. This numerical id is stored using delta-encoding, which is implemented on top of Arrow.</p>

<p>The secondary tables directly connected to the main table are sorted using the same principle, but the <code class="language-plaintext highlighter-rouge">parent_id</code> column is consistently utilized as the last column in the sort statement. Including the <code class="language-plaintext highlighter-rouge">parent_id</code> column in the sort statement enables the use of a variation of delta encoding. The efficiency of this approach is summarized in the chart below.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/compressed-message-size.png" width="100%" class="img-responsive" alt="Fig 5: Comparative analysis of compression ratios - OTLP Protocol vs. Two variations of the OTel Arrow Protocol with multivariate metrics stream. (lower percentage is better)" />
  <figcaption>Fig 5: Comparative analysis of compression ratios - OTLP Protocol vs. Two variations of the OTel Arrow Protocol with multivariate metrics stream. (lower percentage is better)</figcaption>
</figure>

<p>The second column presents the average size of the OTLP batch both pre- and post-ZSTD compression for batches of varying sizes. This column serves as a reference point for the ensuing two columns. The third column displays results for the OTel Arrow protocol without any sorting applied, while the final column showcases results for the OTel Arrow protocol with sorting enabled.</p>

<p>Before compression, the average batch sizes for the two OTel Arrow configurations are predictably similar. However, post-compression, the benefits of sorting each individual table on the compression ratio become immediately apparent. Without sorting, the OTel Arrow protocol exhibits a compression ratio that’s 1.40 to 1.67 times better than the reference. When sorting is enabled, the OTel Arrow protocol outperforms the reference by a factor ranging from 4.94 to 7.21 times!</p>

<p>The gains in terms of compression obviously depend on your data and the redundancy of information present in your data batches. According to our observations, the choice of a good sort generally improves the compression ratio by a factor of 1.5 to 8.</p>

<p>Decomposing a complex schema into multiple simpler schemas to enhance sorting capabilities, coupled with a targeted approach to efficiently encode the identifiers representing the relationships, emerges as an effective strategy for enhancing overall data compression. This method also eliminates complex Arrow data types, such as lists, maps, and unions. Consequently, it not only improves but also simplifies data query-ability. This simplification proves beneficial for existing query engines, which may struggle to operate on intricate schemas.</p>

<h2 id="conclusion-and-next-steps">Conclusion and next steps</h2>

<p>This article concludes our two-part series on Apache Arrow, wherein we have explored various strategies to maximize the utility of Apache Arrow within specific contexts. The adaptive schema architecture presented in the second part of this series paves the way for future optimization possibilities. We look forward to seeing what the community can add based on this contribution.</p>

<p>Apache Arrow is an exceptional project, continually enhanced by a thriving ecosystem. However, throughout our exploration, we have noticed certain gaps or points of friction that, if addressed, could significantly enrich the overall experience.</p>
<ul>
  <li>Designing an efficient Arrow schema can, in some cases, prove to be a challenging task. Having the <strong>ability to collect statistics</strong> at the record level could facilitate this design phase (data distribution per field, dictionary stats, Arrow array sizes before/after compression, and so on). These statistics would also assist in identifying the most effective columns on which to base the record sorting.</li>
  <li><strong>Native support for recursive schemas</strong> would also increase adoption by simplifying the use of Arrow in complex scenarios. While I’m not aware of any attempts to address this limitation within the Arrow system, it doesn’t seem insurmountable and would constitute a valuable extension. This would help reduce the complexity of integrating Arrow with other systems that rely on such recursive definitions.</li>
  <li><strong>Harmonizing the support for data types as well as IPC stream capabilities</strong> would also be a major benefit. Predominant client libraries support nested and hierarchical schemas, but their use is limited due to a lack of full support across the rest of the ecosystem. For example, list and/or union types are not well supported by query engines or Parquet bridges. Also, the advanced dictionary support within IPC streams is not consistent across different implementations (i.e. delta dictionaries and replacement dictionaries are not supported by all implementations).</li>
  <li><strong>Optimizing the support of complex schemas</strong> in terms of memory consumption and compression rate could be improved by natively integrating the concept of the dynamic schema presented in this article.</li>
  <li><strong>Detecting dictionary overflows</strong> (index level) is not something that is easy to test on the fly. The API could be improved to indicate this overflow as soon as an insertion occurs.</li>
</ul>

<p>Our effort to utilize Apache Arrow in conjunction with OpenTelemetry has produced encouraging results. While this has necessitated considerable investment in terms of development, exploration, and benchmarking, we hope that these articles will aid in accelerating your journey with Apache Arrow. Looking ahead, we envision an end-to-end integration with Apache Arrow and plan to significantly extend our use of the Arrow ecosystem. This extension involves providing a bridge with Parquet and integrating with a query engine such as DataFusion, with the goal of processing telemetry streams within the collector.</p>]]></content><author><name>Laurent Quérel</name></author><category term="application" /><summary type="html"><![CDATA[In the previous article, we discussed our use of Apache Arrow within the context of the OpenTelemetry project. We investigated various techniques to maximize the efficiency of Apache Arrow, aiming to find the optimal balance between data compression ratio and queryability. The compression results speak for themselves, boasting improvements ranging from 1.5x to 5x better than the original OTLP protocol. In this article, we will delve into three techniques that have enabled us to enhance both the compression ratio and memory usage of Apache Arrow buffers within the current version of the OTel Arrow protocol. The first technique we’ll discuss aims to optimize schemas in terms of memory usage. As you’ll see, the gains can be substantial, potentially halving memory usage in certain cases. The second section will delve more deeply into the various approaches that can be used to handle recursive schema definitions. Lastly, we’ll emphasize that the design of your schema(s), coupled with the sorts you can apply at the record level, play a pivotal role in maximizing the benefits of Apache Arrow and its columnar representation. Handling dynamic and unknown data distributions In certain contexts, the comprehensive definition of an Arrow schema can end up being overly broad and complex in order to cover all possible cases that you intend to represent in columnar form. However, as is often the case with complex schemas, only a subset of this schema will actually be utilized for a specific deployment. Similarly, it’s not always possible to determine the optimal dictionary encoding for one or more fields in advance. Employing a broad and very general schema that covers all cases is usually more memory-intensive. This is because, for most implementations, a column without value still continues to consume memory space. Likewise, a column with dictionary encoding that indexes a uint64 will occupy four times more memory than the same column with a dictionary encoding based on a uint8. To illustrate this more concretely, let’s consider an OTel collector positioned at the output of a production environment, receiving a telemetry data stream produced by a large and dynamic set of servers. Invariably, the content of this telemetry stream will change in volume and nature over time. It’s challenging to predict the optimal schema in such a scenario, and it’s equally difficult to know in advance the distribution of a particular attribute of the telemetry data passing through this point. To optimize such scenarios, we have adopted an intermediary approach that we have named dynamic Arrow schema, aiming to gradually adapt the schema based on the observed data. The general principle is relatively simple. We start with a general schema defining the maximum envelope of what should be represented. Some fields of this schema will be declared optional, while other fields will be encoded with multiple possible options depending on the observed distribution. In theory, this principle can be applied to other types of transformations (e.g., recursive column creation) but we will let your imagination explore these other options. So if you encounter data streams where certain fields are not utilized, some union variants remain unused, and/or the value distribution of a field cannot be determined a priori, it may be worthwhile to invest time in implementing this model. This can lead to improved efficiency in terms of compression ratio, memory usage, and processing speed. The following Go Arrow schema definition provides an example of such a schema, instrumented with a collection of annotations. These annotations will be processed by an enhanced Record Builder, equipped with the ability to dynamically adapt the schema. The structure of this system is illustrated in Figure 1. var ( // Arrow schema for the OTLP Arrow Traces record (without attributes, links, and events). TracesSchema = arrow.NewSchema([]arrow.Field{ // Nullabe:true means the field is optional, in this case of 16 bit unsigned integers {Name: constants.ID, Type: arrow.PrimitiveTypes.Uint16, Nullable: true}, {Name: constants.Resource, Type: arrow.StructOf([]arrow.Field{ {Name: constants.ID, Type: arrow.PrimitiveTypes.Uint16, Nullable: true}, // --- Use dictionary with 8 bit integers initially ---- {Name: constants.SchemaUrl,Type: arrow.BinaryTypes.String,Metadata: schema.Metadata(schema.Dictionary8), Nullable: true}, {Name: constants.DroppedAttributesCount,Type: arrow.PrimitiveTypes.Uint32,Nullable: true}, }...), Nullable: true}, {Name: constants.Scope, Type: arrow.StructOf([]arrow.Field{ {Name: constants.ID, Type: arrow.PrimitiveTypes.Uint16, Metadata: acommon.Metadata(acommon.DeltaEncoding), Nullable: true}, // --- Use dictionary with 8 bit integers initially ---- {Name: constants.Name, Type: arrow.BinaryTypes.String, Metadata: acommon.Metadata(acommon.Dictionary8), Nullable: true}, {Name: constants.Version, Type: arrow.BinaryTypes.String, Metadata: acommon.Metadata(acommon.Dictionary8), Nullable: true}, {Name: constants.DroppedAttributesCount, Type: arrow.PrimitiveTypes.Uint32, Nullable: true}, }...), Nullable: true}, {Name: constants.SchemaUrl, Type: arrow.BinaryTypes.String, Metadata: schema.Metadata(schema.Dictionary8), Nullable: true}, {Name: constants.StartTimeUnixNano, Type: arrow.FixedWidthTypes.Timestamp_ns}, {Name: constants.DurationTimeUnixNano, Type: arrow.FixedWidthTypes.Duration_ms, Metadata: schema.Metadata(schema.Dictionary8)}, {Name: constants.TraceId, Type: &amp;arrow.FixedSizeBinaryType{ByteWidth: 16}}, {Name: constants.SpanId, Type: &amp;arrow.FixedSizeBinaryType{ByteWidth: 8}}, {Name: constants.TraceState, Type: arrow.BinaryTypes.String, Metadata: schema.Metadata(schema.Dictionary8), Nullable: true}, {Name: constants.ParentSpanId, Type: &amp;arrow.FixedSizeBinaryType{ByteWidth: 8}, Nullable: true}, {Name: constants.Name, Type: arrow.BinaryTypes.String, Metadata: schema.Metadata(schema.Dictionary8)}, {Name: constants.KIND, Type: arrow.PrimitiveTypes.Int32, Metadata: schema.Metadata(schema.Dictionary8), Nullable: true}, {Name: constants.DroppedAttributesCount, Type: arrow.PrimitiveTypes.Uint32, Nullable: true}, {Name: constants.DroppedEventsCount, Type: arrow.PrimitiveTypes.Uint32, Nullable: true}, {Name: constants.DroppedLinksCount, Type: arrow.PrimitiveTypes.Uint32, Nullable: true}, {Name: constants.Status, Type: arrow.StructOf([]arrow.Field{ {Name: constants.StatusCode, Type: arrow.PrimitiveTypes.Int32, Metadata: schema.Metadata(schema.Dictionary8), Nullable: true}, {Name: constants.StatusMessage, Type: arrow.BinaryTypes.String, Metadata: schema.Metadata(schema.Dictionary8), Nullable: true}, }...), Nullable: true}, }, nil) ) In this example, Arrow field-level metadata are employed to designate when a field is optional (Nullable: true) or to specify the minimal dictionary encoding applicable to a particular field (Metadata Dictionary8/16/…). Now let’s imagine a scenario utilizing this schema in a straightforward scenario, wherein only a handful of fields are actually in use, and the cardinality of most dictionary-encoded fields is low (i.e., below 2^8). Ideally, we’d want a system capable of dynamically constructing the following simplified schema, which, in essence, is a strict subset of the original schema. var ( // Simplified schema definition generated by the Arrow Record encoder based on // the data observed. TracesSchema = arrow.NewSchema([]arrow.Field{ {Name: constants.ID, Type: arrow.PrimitiveTypes.Uint16, Nullable: true}, {Name: constants.StartTimeUnixNano, Type: arrow.FixedWidthTypes.Timestamp_ns}, {Name: constants.TraceId, Type: &amp;arrow.FixedSizeBinaryType{ByteWidth: 16}}, {Name: constants.SpanId, Type: &amp;arrow.FixedSizeBinaryType{ByteWidth: 8}}, {Name: constants.Name, Type: &amp;arrow.DictionaryType { IndexType: arrow.PrimitiveTypes.Uint8, ValueType: arrow.BinaryTypes.String}}, {Name: constants.KIND, Type: &amp;arrow.DictionaryType { IndexType: arrow.PrimitiveTypes.Uint8, ValueType: arrow.PrimitiveTypes.Int32, }, Nullable: true}, }, nil) ) Additionally, we desire a system capable of automatically adapting the aforementioned schema if it encounters new fields or existing fields with a cardinality exceeding the size of the current dictionary definition in future batches. In extreme scenarios, if the cardinality of a specific field surpasses a certain threshold, we would prefer the system to automatically revert to the non-dictionary representation (mechanism of dictionary overflow). That is precisely what we will elaborate on in the remainder of this section. An overview of the different components and events used to implement this approach is depicted in figure 1. Fig 1: Adaptive Arrow schema architecture overview. The overall Adaptive Arrow schema component takes a data stream segmented into batches and produces one or multiple streams of Arrow Records (one schema per stream). Each of these records is defined with an Arrow schema, which is based both on the annotated Arrow schema and the shape of fields observed in the incoming data. More specifically, the process of the Adaptive Arrow schema component consists of four main phases Initialization phase During the initialization phase, the Arrow Record Encoder reads the annotated Arrow schema (i.e. the reference schema) and generates a collection of transformations. When these transformations are applied to the reference schema, they yield the first minimal Arrow schema that adheres to the constraints depicted by these annotations. In this initial iteration, all optional fields are eliminated, and all dictionary-encoded fields are configured to utilize the smallest encoding as defined by the annotation (only Dictionary8 in the previous example). These transformations form a tree, reflecting the structure of the reference schema. Feeding phase Following the initialization is the feeding phase. Here, the Arrow Record Encoder scans the batch and attempts to store all the fields in an Arrow Record Builder, which is defined by the schema created in the prior step. If a field exists in the data but is not included in the schema, the encoder will trigger a missing field event. This process continues until the current batch is completely processed. An additional internal check is conducted on all dictionary-encoded fields in the Arrow Record builder to ensure there’s no dictionary overflow (i.e. more unique entries than the cardinality of the index permits). Dictionary overflow events are generated if such a situation is detected. Consequently, by the end, all unknown fields and dictionary overflow would have been detected, or alternatively, no discrepancies would have surfaced if the data aligns perfectly with the schema. Corrective phase If at least one event has been generated, a corrective phase will be initiated to fix the schema. This optional stage considers all the events generated in the previous stage and adjusts the transformation tree accordingly to align with the observed data. A missing field event will remove a NoField transformation for the corresponding field. A dictionary overflow event will modify the dictionary transformation to mirror the event (e.g. changing the index type from uint8 to uint16, or if the maximum index size has been reached, the transformation will remove the dictionary-encoding and revert to the original non-dictionary-encoded type). The updated transformation tree is subsequently used to create a new schema and a fresh Arrow Record Builder. This Record Builder is then utilized to replay the preceding feeding phase with the batch that wasn’t processed correctly. Routing phase Once a Record Builder has been properly fed, an Arrow Record is created, and the system transitions into the routing phase. The router component calculates a schema signature of the record and utilizes this signature to route the record to an existing Arrow stream compatible with the signature, or it initiates a new stream if there is no match. This four-phase process should gradually adapt and stabilize the schema to a structure and definition that is optimized for a specific data stream. Unused fields will never unnecessarily consume memory. Dictionary-encoded fields will be defined with the most optimal index size based on the observed data cardinality, and fields with a cardinality exceeding a certain threshold (defined by configuration) will automatically revert to their non-dictionary-encoded versions. To effectively execute this approach, you must ensure that there is a sufficient level of flexibility on the receiver side. It’s crucial that your downstream pipeline remains functional even when some fields are missing in the schema or when various dictionary index configurations are employed. While this may not always be feasible without implementing additional transformations upon reception, it proves worthwhile in certain scenarios. The following results highlight the significant memory usage reduction achieved through the application of various optimization techniques. These results were gathered using a schema akin to the one previously presented. The considerable memory efficiency underscores the effectiveness of this approach. Fig 2: Comparative analysis of memory usage for different schema optimizations. The concept of a transformation tree enables a generalized approach to perform various types of schema optimizations based on the knowledge acquired from the data. This architecture is highly flexible; the current implementation allows for the removal of unused fields, the application of the most specific dictionary encoding, and the optimization of union type variants. In the future, there is potential for introducing additional optimizations that can be expressed as transformations on the initial schema. An implementation of this approach is available here. Handling recursive schema definition Apache Arrow does not support recursive schema definitions, implying that data structures with variable depth cannot be directly represented. Figure 3 exemplifies such a recursive definition where the value of an attribute can either be a simple data type, a list of values, or a map of values. The depth of this definition cannot be predetermined. Fig 3: Recursive definition of OTel attributes. Several strategies can be employed to circumvent this limitation. Technically, the dynamic schema concept we’ve presented could be expanded to dynamically update the schema to include any missing level of recursion. However, for this use case, this method is complex and has the notable downside of not offering any assurance on the maximum size of the schema. This lack of constraint can pose security issues; hence, this approach isn’t elaborated upon. The second approach consists of breaking the recursion by employing a serialization format that supports the definition of a recursive schema. The result of this serialization can then be integrated into the Arrow record as a binary type column, effectively halting the recursion at a specific level. To fully leverage the advantages of columnar representation, it is crucial to apply this ad-hoc serialization as deeply within the data structure as feasible. In the context of OpenTelemetry, this is performed at the attribute level – more specifically, at the second level of attributes. A variety of serialization formats, such as protobuf or CBOR, can be employed to encode recursive data. Without particular treatment, these binary columns may not be easily queryable by the existing Arrow query engines. Therefore, it’s crucial to thoughtfully ascertain when and where to apply such a technique. While I’m not aware of any attempts to address this limitation within the Arrow system, it doesn’t seem insurmountable and would constitute a valuable extension. This would help reduce the complexity of integrating Arrow with other systems that rely on such recursive definitions. Importance of sorting In our preceding article, we explored a variety of strategies to represent hierarchical data models, including nested structures based on struct/list/map/union, denormalization and flattening representations, as well as a multi-record approach. Each method presents its unique advantages and disadvantages. However, in this last section, we’ll delve deeper into the multi-record approach, focusing specifically on its ability to offer versatile sorting options and how these options contribute to an enhanced compression ratio. In the OTel Arrow protocol, we leverage the multi-record approach to represent metrics, logs, and traces. The following entity-relationship diagram offers a simplified version of various record schemas and illustrates their relationships, specifically those used to represent gauges and sums. A comprehensive description of the Arrow data model employed in OpenTelemetry can be accessed here. These Arrow records, also referred to as tables, form a hierarchy with METRICS acting as the primary entry point. Each table can be independently sorted according to one or more columns. This sorting strategy facilitates the grouping of duplicated data, thereby improving the compression ratio. Fig 4: A simplified entity-relationship diagram representing OTel sum &amp; gauge metrics. The relationship between the primary METRICS table and the secondary RESOURCE_ATTRS, SCOPE_ATTRS, and NUMBER_DATA_POINTS tables is established through a unique id in the main table and a parent_id column in each of the secondary tables. This {id,parent_id} pair represents an overhead that should be minimized to the greatest extent possible post-compression. To achieve this, the ordering process for the different tables adheres to the hierarchy, starting from the main table down to the leaf. The main table is sorted (by one or multiple columns), and then an incremental id is assigned to each row. This numerical id is stored using delta-encoding, which is implemented on top of Arrow. The secondary tables directly connected to the main table are sorted using the same principle, but the parent_id column is consistently utilized as the last column in the sort statement. Including the parent_id column in the sort statement enables the use of a variation of delta encoding. The efficiency of this approach is summarized in the chart below. Fig 5: Comparative analysis of compression ratios - OTLP Protocol vs. Two variations of the OTel Arrow Protocol with multivariate metrics stream. (lower percentage is better) The second column presents the average size of the OTLP batch both pre- and post-ZSTD compression for batches of varying sizes. This column serves as a reference point for the ensuing two columns. The third column displays results for the OTel Arrow protocol without any sorting applied, while the final column showcases results for the OTel Arrow protocol with sorting enabled. Before compression, the average batch sizes for the two OTel Arrow configurations are predictably similar. However, post-compression, the benefits of sorting each individual table on the compression ratio become immediately apparent. Without sorting, the OTel Arrow protocol exhibits a compression ratio that’s 1.40 to 1.67 times better than the reference. When sorting is enabled, the OTel Arrow protocol outperforms the reference by a factor ranging from 4.94 to 7.21 times! The gains in terms of compression obviously depend on your data and the redundancy of information present in your data batches. According to our observations, the choice of a good sort generally improves the compression ratio by a factor of 1.5 to 8. Decomposing a complex schema into multiple simpler schemas to enhance sorting capabilities, coupled with a targeted approach to efficiently encode the identifiers representing the relationships, emerges as an effective strategy for enhancing overall data compression. This method also eliminates complex Arrow data types, such as lists, maps, and unions. Consequently, it not only improves but also simplifies data query-ability. This simplification proves beneficial for existing query engines, which may struggle to operate on intricate schemas. Conclusion and next steps This article concludes our two-part series on Apache Arrow, wherein we have explored various strategies to maximize the utility of Apache Arrow within specific contexts. The adaptive schema architecture presented in the second part of this series paves the way for future optimization possibilities. We look forward to seeing what the community can add based on this contribution. Apache Arrow is an exceptional project, continually enhanced by a thriving ecosystem. However, throughout our exploration, we have noticed certain gaps or points of friction that, if addressed, could significantly enrich the overall experience. Designing an efficient Arrow schema can, in some cases, prove to be a challenging task. Having the ability to collect statistics at the record level could facilitate this design phase (data distribution per field, dictionary stats, Arrow array sizes before/after compression, and so on). These statistics would also assist in identifying the most effective columns on which to base the record sorting. Native support for recursive schemas would also increase adoption by simplifying the use of Arrow in complex scenarios. While I’m not aware of any attempts to address this limitation within the Arrow system, it doesn’t seem insurmountable and would constitute a valuable extension. This would help reduce the complexity of integrating Arrow with other systems that rely on such recursive definitions. Harmonizing the support for data types as well as IPC stream capabilities would also be a major benefit. Predominant client libraries support nested and hierarchical schemas, but their use is limited due to a lack of full support across the rest of the ecosystem. For example, list and/or union types are not well supported by query engines or Parquet bridges. Also, the advanced dictionary support within IPC streams is not consistent across different implementations (i.e. delta dictionaries and replacement dictionaries are not supported by all implementations). Optimizing the support of complex schemas in terms of memory consumption and compression rate could be improved by natively integrating the concept of the dynamic schema presented in this article. Detecting dictionary overflows (index level) is not something that is easy to test on the fly. The API could be improved to indicate this overflow as soon as an insertion occurs. Our effort to utilize Apache Arrow in conjunction with OpenTelemetry has produced encouraging results. While this has necessitated considerable investment in terms of development, exploration, and benchmarking, we hope that these articles will aid in accelerating your journey with Apache Arrow. Looking ahead, we envision an end-to-end integration with Apache Arrow and plan to significantly extend our use of the Arrow ecosystem. This extension involves providing a bridge with Parquet and integrating with a query engine such as DataFusion, with the goal of processing telemetry streams within the collector.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow DataFusion 26.0.0</title><link href="https://arrow.apache.org/blog/2023/06/24/datafusion-25.0.0/" rel="alternate" type="text/html" title="Apache Arrow DataFusion 26.0.0" /><published>2023-06-24T00:00:00-04:00</published><updated>2023-06-24T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/06/24/datafusion-25.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/06/24/datafusion-25.0.0/"><![CDATA[<!--

-->

<p>It has been a whirlwind 6 months of DataFusion development since <a href="https://arrow.apache.org/blog/2023/01/19/datafusion-16.0.0">our
last update</a>: the community has grown, many features have been added,
performance improved and we are <a href="https://github.com/apache/arrow-datafusion/discussions/6475">discussing</a> branching out to our own
top level Apache Project.</p>

<h2 id="background">Background</h2>

<p><a href="https://arrow.apache.org/datafusion/">Apache Arrow DataFusion</a> is an extensible query engine and database
toolkit, written in <a href="https://www.rust-lang.org/">Rust</a>, that uses <a href="https://arrow.apache.org">Apache Arrow</a> as its in-memory
format.</p>

<p>DataFusion, along with <a href="https://calcite.apache.org">Apache Calcite</a>, Facebook’s <a href="https://github.com/facebookincubator/velox">Velox</a> and
similar technology are part of the next generation “<a href="https://www.usenix.org/publications/login/winter2018/khurana">Deconstructed
Database</a>” architectures, where new systems are built on a foundation
of fast, modular components, rather as a single tightly integrated
system.</p>

<p>While single tightly integrated systems such as <a href="https://spark.apache.org/">Spark</a>, <a href="https://duckdb.org">DuckDB</a> and
<a href="https://www.pola.rs/">Pola.rs</a> are great pieces of technology, our community believes that
anyone developing new data heavy application, such as those common in
machine learning in the next 5 years, will <strong>require</strong> a high
performance, vectorized, query engine to remain relevant. The only
practical way to gain access to such technology without investing many
millions of dollars to build a new tightly integrated engine, is
though open source projects like DataFusion and similar enabling
technologies such as <a href="https://arrow.apache.org">Apache Arrow</a> and <a href="https://www.rust-lang.org/">Rust</a>.</p>

<p>DataFusion is targeted primarily at developers creating other data
intensive analytics, and offers:</p>

<ul>
  <li>High performance, native, parallel streaming execution engine</li>
  <li>Mature <a href="https://arrow.apache.org/datafusion/user-guide/sql/index.html">SQL support</a>, featuring  subqueries, window functions, grouping sets, and more</li>
  <li>Built in support for Parquet, Avro, CSV, JSON and Arrow formats and easy extension for others</li>
  <li>Native DataFrame API and <a href="https://arrow.apache.org/datafusion-python/">python bindings</a></li>
  <li><a href="https://docs.rs/datafusion/latest/datafusion/index.html">Well documented</a> source code and architecture, designed to be customized to suit downstream project needs</li>
  <li>High quality, easy to use code <a href="https://crates.io/crates/datafusion/versions">released every 2 weeks to crates.io</a></li>
  <li>Welcoming, open community, governed by the highly regarded and well understood <a href="https://www.apache.org/">Apache Software Foundation</a></li>
</ul>

<p>The rest of this post highlights some of the improvements we have made
to DataFusion over the last 6 months and a preview of where we are
heading. You can see a list of all changes in the detailed
<a href="https://github.com/apache/arrow-datafusion/blob/main/datafusion/CHANGELOG.md">CHANGELOG</a>.</p>

<h2 id="even-better-performance">(Even) Better Performance</h2>

<p><a href="https://voltrondata.com/resources/speeds-and-feeds-hardware-and-software-matter">Various</a> benchmarks show DataFusion to be quite close or <a href="https://github.com/tustvold/access-log-bench">even
faster</a> to the state of the art in analytic performance (at the moment
this seems to be DuckDB). We continually work on improving performance
(see <a href="https://github.com/apache/arrow-datafusion/issues/5546">#5546</a> for a list) and would love additional help in this area.</p>

<p>DataFusion now reads single large Parquet files significantly faster by
<a href="https://github.com/apache/arrow-datafusion/pull/5057">parallelizing across multiple cores</a>. Native speeds for reading JSON
and CSV files are also up to 2.5x faster thanks to improvements
upstream in arrow-rs <a href="https://github.com/apache/arrow-rs/pull/3479#issuecomment-1384353159">JSON reader</a> and <a href="https://github.com/apache/arrow-rs/pull/3365">CSV reader</a>.</p>

<p>Also, we have integrated the <a href="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1/">arrow-rs Row Format</a> into DataFusion resulting in up to <a href="https://github.com/apache/arrow-datafusion/pull/6163">2-3x faster sorting and merging</a>.</p>

<h2 id="improved-documentation-and-website">Improved Documentation and Website</h2>

<p>Part of growing the DataFusion community is ensuring that DataFusion’s
features are understood and that it is easy to contribute and
participate. To that end the <a href="https://arrow.apache.org/datafusion/">website</a> has been cleaned up, <a href="https://docs.rs/datafusion/latest/datafusion/index.html#architecture">the
architecture guide</a> expanded, the <a href="https://arrow.apache.org/datafusion/contributor-guide/roadmap.html">roadmap</a> updated, and several
overview talks created:</p>

<ul>
  <li>Apr 2023 <em>Query Engine</em>: <a href="https://youtu.be/NVKujPxwSBA">recording</a> and <a href="https://docs.google.com/presentation/d/1D3GDVas-8y0sA4c8EOgdCvEjVND4s2E7I6zfs67Y4j8/edit#slide=id.p">slides</a></li>
  <li>April 2023 <em>Logical Plan and Expressions</em>: <a href="https://youtu.be/EzZTLiSJnhY">recording</a> and <a href="https://docs.google.com/presentation/d/1ypylM3-w60kVDW7Q6S99AHzvlBgciTdjsAfqNP85K30">slides</a></li>
  <li>April 2023 <em>Physical Plan and Execution</em>: <a href="https://youtu.be/2jkWU3_w6z0">recording</a> and <a href="https://docs.google.com/presentation/d/1cA2WQJ2qg6tx6y4Wf8FH2WVSm9JQ5UgmBWATHdik0hg">slides</a></li>
</ul>

<h2 id="new-features">New Features</h2>

<h3 id="more-streaming-less-memory">More Streaming, Less Memory</h3>

<p>We have made significant progress on the <a href="https://github.com/apache/arrow-datafusion/issues/4285">streaming execution roadmap</a>
such as <a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/trait.ExecutionPlan.html#method.unbounded_output">unbounded datasources</a>, <a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/aggregates/enum.GroupByOrderMode.html">streaming group by</a>, sophisticated
<a href="https://docs.rs/datafusion/latest/datafusion/physical_optimizer/global_sort_selection/index.html">sort</a> and <a href="https://docs.rs/datafusion/latest/datafusion/physical_optimizer/repartition/index.html">repartitioning</a> improvements in the optimizer, and support
for <a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/joins/struct.SymmetricHashJoinExec.html">symmetric hash join</a> (read more about that in the great <a href="https://www.synnada.ai/blog/general-purpose-stream-joins-via-pruning-symmetric-hash-joins">Synnada
Blog Post</a> on the topic). Together, these features both 1) make it
easier to build streaming systems using DataFusion that can
incrementally generate output before (or ever) seeing the end of the
input and 2) allow general queries to use less memory and generate their
results faster.</p>

<p>We have also improved the runtime <a href="https://docs.rs/datafusion/latest/datafusion/execution/memory_pool/index.html">memory management</a> system so that
DataFusion now stays within its declared memory budget <a href="https://github.com/apache/arrow-datafusion/issues/3941">generate
runtime errors</a>.</p>

<h3 id="dml-support-insert-delete-update-etc">DML Support (<code class="language-plaintext highlighter-rouge">INSERT</code>, <code class="language-plaintext highlighter-rouge">DELETE</code>, <code class="language-plaintext highlighter-rouge">UPDATE</code>, etc)</h3>

<p>Part of building high performance data systems includes writing data,
and DataFusion supports several features for creating new files:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">INSERT INTO</code> and <code class="language-plaintext highlighter-rouge">SELECT ... INTO </code> support for memory backed and CSV tables</li>
  <li>New <a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/insert/trait.DataSink.html">API for writing data into TableProviders</a></li>
</ul>

<p>We are working on easier to use <a href="https://github.com/apache/arrow-datafusion/issues/5654">COPY INTO</a> syntax, better support
for writing parquet, JSON, and AVRO, and more – see our <a href="https://github.com/apache/arrow-datafusion/issues/6569">tracking epic</a>
for more details.</p>

<h3 id="timestamp-and-intervals">Timestamp and Intervals</h3>

<p>One mark of the maturity of a SQL engine is how it handles the tricky
world of timestamp, date, times and interval arithmetic. DataFusion is
feature complete in this area and behaves as you would expect,
supporting queries such as</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="n">now</span><span class="p">()</span> <span class="o">+</span> <span class="s1">'1 month'</span> <span class="k">FROM</span> <span class="n">my_table</span><span class="p">;</span>
</code></pre></div></div>

<p>We still have a long tail of <a href="https://github.com/apache/arrow-datafusion/issues/3148">date and time improvements</a>, which we are working on as well.</p>

<h3 id="querying-structured-types-list-and-structs">Querying Structured Types (<code class="language-plaintext highlighter-rouge">List</code> and <code class="language-plaintext highlighter-rouge">Struct</code>s)</h3>

<p>Arrow and Parquet <a href="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/">support nested data</a> well and DataFusion lets you
easily query such <code class="language-plaintext highlighter-rouge">Struct</code> and <code class="language-plaintext highlighter-rouge">List</code>. For example, you can use
DataFusion to read and query the <a href="https://data.mendeley.com/datasets/ct8f9skv97">JSON Datasets for Exploratory OLAP -
Mendeley Data</a> like this:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">----------</span>
<span class="c1">-- Explore structured data using SQL</span>
<span class="c1">----------</span>
<span class="k">SELECT</span> <span class="k">delete</span> <span class="k">FROM</span> <span class="s1">'twitter-sample-head-100000.parquet'</span> <span class="k">WHERE</span> <span class="k">delete</span> <span class="k">IS</span> <span class="k">NOT</span> <span class="k">NULL</span> <span class="k">limit</span> <span class="mi">10</span><span class="p">;</span>
<span class="o">+</span><span class="c1">---------------------------------------------------------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="k">delete</span>                                                                                                                    <span class="o">|</span>
<span class="o">+</span><span class="c1">---------------------------------------------------------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">135037425050320896</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">135037425050320896</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">334902461</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">334902461</span><span class="p">}}</span> <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">134703982051463168</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">134703982051463168</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">405383453</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">405383453</span><span class="p">}}</span> <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">134773741740765184</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">134773741740765184</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">64823441</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">64823441</span><span class="p">}}</span>   <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">132543659655704576</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">132543659655704576</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">45917834</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">45917834</span><span class="p">}}</span>   <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">133786431926697984</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">133786431926697984</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">67229952</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">67229952</span><span class="p">}}</span>   <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">134619093570560002</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">134619093570560002</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">182430773</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">182430773</span><span class="p">}}</span> <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">134019857527214080</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">134019857527214080</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">257396311</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">257396311</span><span class="p">}}</span> <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">133931546469076993</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">133931546469076993</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">124539548</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">124539548</span><span class="p">}}</span> <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">134397743350296576</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">134397743350296576</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">139836391</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">139836391</span><span class="p">}}</span> <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">127833661767823360</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">127833661767823360</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">244442687</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">244442687</span><span class="p">}}</span> <span class="o">|</span>
<span class="o">+</span><span class="c1">---------------------------------------------------------------------------------------------------------------------------+</span>

<span class="c1">----------</span>
<span class="c1">-- Select some deeply nested fields</span>
<span class="c1">----------</span>
<span class="k">SELECT</span>
  <span class="k">delete</span><span class="p">[</span><span class="s1">'status'</span><span class="p">][</span><span class="s1">'id'</span><span class="p">][</span><span class="s1">'$numberLong'</span><span class="p">]</span> <span class="k">as</span> <span class="n">delete_id</span><span class="p">,</span>
  <span class="k">delete</span><span class="p">[</span><span class="s1">'status'</span><span class="p">][</span><span class="s1">'user_id'</span><span class="p">]</span> <span class="k">as</span> <span class="n">delete_user_id</span>
<span class="k">FROM</span> <span class="s1">'twitter-sample-head-100000.parquet'</span> <span class="k">WHERE</span> <span class="k">delete</span> <span class="k">IS</span> <span class="k">NOT</span> <span class="k">NULL</span> <span class="k">LIMIT</span> <span class="mi">10</span><span class="p">;</span>

<span class="o">+</span><span class="c1">--------------------+----------------+</span>
<span class="o">|</span> <span class="n">delete_id</span>          <span class="o">|</span> <span class="n">delete_user_id</span> <span class="o">|</span>
<span class="o">+</span><span class="c1">--------------------+----------------+</span>
<span class="o">|</span> <span class="mi">135037425050320896</span> <span class="o">|</span> <span class="mi">334902461</span>      <span class="o">|</span>
<span class="o">|</span> <span class="mi">134703982051463168</span> <span class="o">|</span> <span class="mi">405383453</span>      <span class="o">|</span>
<span class="o">|</span> <span class="mi">134773741740765184</span> <span class="o">|</span> <span class="mi">64823441</span>       <span class="o">|</span>
<span class="o">|</span> <span class="mi">132543659655704576</span> <span class="o">|</span> <span class="mi">45917834</span>       <span class="o">|</span>
<span class="o">|</span> <span class="mi">133786431926697984</span> <span class="o">|</span> <span class="mi">67229952</span>       <span class="o">|</span>
<span class="o">|</span> <span class="mi">134619093570560002</span> <span class="o">|</span> <span class="mi">182430773</span>      <span class="o">|</span>
<span class="o">|</span> <span class="mi">134019857527214080</span> <span class="o">|</span> <span class="mi">257396311</span>      <span class="o">|</span>
<span class="o">|</span> <span class="mi">133931546469076993</span> <span class="o">|</span> <span class="mi">124539548</span>      <span class="o">|</span>
<span class="o">|</span> <span class="mi">134397743350296576</span> <span class="o">|</span> <span class="mi">139836391</span>      <span class="o">|</span>
<span class="o">|</span> <span class="mi">127833661767823360</span> <span class="o">|</span> <span class="mi">244442687</span>      <span class="o">|</span>
<span class="o">+</span><span class="c1">--------------------+----------------+</span>
</code></pre></div></div>

<h3 id="subqueries-all-the-way-down">Subqueries All the Way Down</h3>

<p>DataFusion can run many different subqueries by rewriting them to
joins. It has been able to run the full suite of TPC-H queries for at
least the last year, but recently we have implemented significant
improvements to this logic, sufficient to run almost all queries in
the TPC-DS benchmark as well.</p>

<h2 id="community-and-project-growth">Community and Project Growth</h2>

<p>The six months since <a href="https://arrow.apache.org/blog/2023/01/19/datafusion-16.0.0">our last update</a> saw significant growth in
the DataFusion community. Between versions <code class="language-plaintext highlighter-rouge">17.0.0</code> and <code class="language-plaintext highlighter-rouge">26.0.0</code>,
DataFusion merged 711 PRs from 107 distinct contributors, not
including all the work that goes into our core dependencies such as
<a href="https://crates.io/crates/arrow">arrow</a>,
<a href="https://crates.io/crates/parquet">parquet</a>, and
<a href="https://crates.io/crates/object_store">object_store</a>, that much of
the same community helps support.</p>

<p>In addition, we have added 7 new committers and 1 new PMC member to
the Apache Arrow project, largely focused on DataFusion, and we
learned about some of the cool <a href="https://arrow.apache.org/datafusion/user-guide/introduction.html#known-users">new systems</a> which are using
DataFusion. Given the growth of the community and interest in the
project, we also clarified the <a href="https://github.com/apache/arrow-datafusion/discussions/6441">mission statement</a> and are
<a href="https://github.com/apache/arrow-datafusion/discussions/6475">discussing</a> “graduate”ing DataFusion to a new top level
Apache Software Foundation project.</p>

<!--
$ git log --pretty=oneline 17.0.0..26.0.0 . | wc -l
     711

$ git shortlog -sn 17.0.0..26.0.0 . | wc -l
      107
-->

<h1 id="how-to-get-involved">How to Get Involved</h1>

<p>Kudos to everyone in the community who has contributed ideas,
discussions, bug reports, documentation and code. It is exciting to be
innovating on the next generation of database architectures together!</p>

<p>If you are interested in contributing to DataFusion, we would love to
have you join us. You can try out DataFusion on some of your own
data and projects and let us know how it goes or contribute a PR with
documentation, tests or code. A list of open issues suitable for
beginners is <a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a>.</p>

<p>Check out our <a href="https://arrow.apache.org/datafusion/contributor-guide/communication.html">Communication Doc</a> for more ways to engage with the
community.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[It has been a whirlwind 6 months of DataFusion development since our last update: the community has grown, many features have been added, performance improved and we are discussing branching out to our own top level Apache Project. Background Apache Arrow DataFusion is an extensible query engine and database toolkit, written in Rust, that uses Apache Arrow as its in-memory format. DataFusion, along with Apache Calcite, Facebook’s Velox and similar technology are part of the next generation “Deconstructed Database” architectures, where new systems are built on a foundation of fast, modular components, rather as a single tightly integrated system. While single tightly integrated systems such as Spark, DuckDB and Pola.rs are great pieces of technology, our community believes that anyone developing new data heavy application, such as those common in machine learning in the next 5 years, will require a high performance, vectorized, query engine to remain relevant. The only practical way to gain access to such technology without investing many millions of dollars to build a new tightly integrated engine, is though open source projects like DataFusion and similar enabling technologies such as Apache Arrow and Rust. DataFusion is targeted primarily at developers creating other data intensive analytics, and offers: High performance, native, parallel streaming execution engine Mature SQL support, featuring subqueries, window functions, grouping sets, and more Built in support for Parquet, Avro, CSV, JSON and Arrow formats and easy extension for others Native DataFrame API and python bindings Well documented source code and architecture, designed to be customized to suit downstream project needs High quality, easy to use code released every 2 weeks to crates.io Welcoming, open community, governed by the highly regarded and well understood Apache Software Foundation The rest of this post highlights some of the improvements we have made to DataFusion over the last 6 months and a preview of where we are heading. You can see a list of all changes in the detailed CHANGELOG. (Even) Better Performance Various benchmarks show DataFusion to be quite close or even faster to the state of the art in analytic performance (at the moment this seems to be DuckDB). We continually work on improving performance (see #5546 for a list) and would love additional help in this area. DataFusion now reads single large Parquet files significantly faster by parallelizing across multiple cores. Native speeds for reading JSON and CSV files are also up to 2.5x faster thanks to improvements upstream in arrow-rs JSON reader and CSV reader. Also, we have integrated the arrow-rs Row Format into DataFusion resulting in up to 2-3x faster sorting and merging. Improved Documentation and Website Part of growing the DataFusion community is ensuring that DataFusion’s features are understood and that it is easy to contribute and participate. To that end the website has been cleaned up, the architecture guide expanded, the roadmap updated, and several overview talks created: Apr 2023 Query Engine: recording and slides April 2023 Logical Plan and Expressions: recording and slides April 2023 Physical Plan and Execution: recording and slides New Features More Streaming, Less Memory We have made significant progress on the streaming execution roadmap such as unbounded datasources, streaming group by, sophisticated sort and repartitioning improvements in the optimizer, and support for symmetric hash join (read more about that in the great Synnada Blog Post on the topic). Together, these features both 1) make it easier to build streaming systems using DataFusion that can incrementally generate output before (or ever) seeing the end of the input and 2) allow general queries to use less memory and generate their results faster. We have also improved the runtime memory management system so that DataFusion now stays within its declared memory budget generate runtime errors. DML Support (INSERT, DELETE, UPDATE, etc) Part of building high performance data systems includes writing data, and DataFusion supports several features for creating new files: INSERT INTO and SELECT ... INTO support for memory backed and CSV tables New API for writing data into TableProviders We are working on easier to use COPY INTO syntax, better support for writing parquet, JSON, and AVRO, and more – see our tracking epic for more details. Timestamp and Intervals One mark of the maturity of a SQL engine is how it handles the tricky world of timestamp, date, times and interval arithmetic. DataFusion is feature complete in this area and behaves as you would expect, supporting queries such as SELECT now() + '1 month' FROM my_table; We still have a long tail of date and time improvements, which we are working on as well. Querying Structured Types (List and Structs) Arrow and Parquet support nested data well and DataFusion lets you easily query such Struct and List. For example, you can use DataFusion to read and query the JSON Datasets for Exploratory OLAP - Mendeley Data like this: ---------- -- Explore structured data using SQL ---------- SELECT delete FROM 'twitter-sample-head-100000.parquet' WHERE delete IS NOT NULL limit 10; +---------------------------------------------------------------------------------------------------------------------------+ | delete | +---------------------------------------------------------------------------------------------------------------------------+ | {status: {id: {$numberLong: 135037425050320896}, id_str: 135037425050320896, user_id: 334902461, user_id_str: 334902461}} | | {status: {id: {$numberLong: 134703982051463168}, id_str: 134703982051463168, user_id: 405383453, user_id_str: 405383453}} | | {status: {id: {$numberLong: 134773741740765184}, id_str: 134773741740765184, user_id: 64823441, user_id_str: 64823441}} | | {status: {id: {$numberLong: 132543659655704576}, id_str: 132543659655704576, user_id: 45917834, user_id_str: 45917834}} | | {status: {id: {$numberLong: 133786431926697984}, id_str: 133786431926697984, user_id: 67229952, user_id_str: 67229952}} | | {status: {id: {$numberLong: 134619093570560002}, id_str: 134619093570560002, user_id: 182430773, user_id_str: 182430773}} | | {status: {id: {$numberLong: 134019857527214080}, id_str: 134019857527214080, user_id: 257396311, user_id_str: 257396311}} | | {status: {id: {$numberLong: 133931546469076993}, id_str: 133931546469076993, user_id: 124539548, user_id_str: 124539548}} | | {status: {id: {$numberLong: 134397743350296576}, id_str: 134397743350296576, user_id: 139836391, user_id_str: 139836391}} | | {status: {id: {$numberLong: 127833661767823360}, id_str: 127833661767823360, user_id: 244442687, user_id_str: 244442687}} | +---------------------------------------------------------------------------------------------------------------------------+ ---------- -- Select some deeply nested fields ---------- SELECT delete['status']['id']['$numberLong'] as delete_id, delete['status']['user_id'] as delete_user_id FROM 'twitter-sample-head-100000.parquet' WHERE delete IS NOT NULL LIMIT 10; +--------------------+----------------+ | delete_id | delete_user_id | +--------------------+----------------+ | 135037425050320896 | 334902461 | | 134703982051463168 | 405383453 | | 134773741740765184 | 64823441 | | 132543659655704576 | 45917834 | | 133786431926697984 | 67229952 | | 134619093570560002 | 182430773 | | 134019857527214080 | 257396311 | | 133931546469076993 | 124539548 | | 134397743350296576 | 139836391 | | 127833661767823360 | 244442687 | +--------------------+----------------+ Subqueries All the Way Down DataFusion can run many different subqueries by rewriting them to joins. It has been able to run the full suite of TPC-H queries for at least the last year, but recently we have implemented significant improvements to this logic, sufficient to run almost all queries in the TPC-DS benchmark as well. Community and Project Growth The six months since our last update saw significant growth in the DataFusion community. Between versions 17.0.0 and 26.0.0, DataFusion merged 711 PRs from 107 distinct contributors, not including all the work that goes into our core dependencies such as arrow, parquet, and object_store, that much of the same community helps support. In addition, we have added 7 new committers and 1 new PMC member to the Apache Arrow project, largely focused on DataFusion, and we learned about some of the cool new systems which are using DataFusion. Given the growth of the community and interest in the project, we also clarified the mission statement and are discussing “graduate”ing DataFusion to a new top level Apache Software Foundation project. How to Get Involved Kudos to everyone in the community who has contributed ideas, discussions, bug reports, documentation and code. It is exciting to be innovating on the next generation of database architectures together! If you are interested in contributing to DataFusion, we would love to have you join us. You can try out DataFusion on some of your own data and projects and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable for beginners is here. Check out our Communication Doc for more ways to engage with the community.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow nanoarrow 0.2 Release</title><link href="https://arrow.apache.org/blog/2023/06/22/nanoarrow-0.120-release/" rel="alternate" type="text/html" title="Apache Arrow nanoarrow 0.2 Release" /><published>2023-06-22T00:00:00-04:00</published><updated>2023-06-22T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/06/22/nanoarrow-0.120-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/06/22/nanoarrow-0.120-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.2.0 release of
Apache Arrow nanoarrow. This initial release covers 19 resolved issues from
6 contributors.</p>

<h2 id="release-highlights">Release Highlights</h2>

<ul>
  <li>Addition of the Arrow <a href="#ipc-stream-support">IPC stream reader extension</a></li>
  <li>Addition of the <a href="#getting-started-with-nanoarrow">Getting Started with nanoarrow</a>
tutorial</li>
  <li>Improvements in reliability and platform test coverage of the <a href="#c-library">C library</a></li>
  <li>Improvements in reliability and type support in the <a href="#r-bindings">R bindings</a></li>
</ul>

<p>See the
<a href="https://github.com/apache/arrow-nanoarrow/blob/apache-arrow-nanoarrow-0.2.0/CHANGELOG.md">Changelog</a>
for a detailed list of contributions to this release.</p>

<h2 id="ipc-stream-support">IPC stream support</h2>

<p>This release includes support for reading schemas and record batches serialized
using the
<a href="https://arrow.apache.org/docs/format/Columnar.html#serialization-and-interprocess-communication-ipc">Arrow IPC format</a>. Based on the
<a href="https://github.com/dvidelabs/flatcc">flatcc</a>
flatbuffers implementation, the nanoarrow IPC read support is implemented as
an optional extension to the core nanoarrow library. The easiest way to get
started is with the <code class="language-plaintext highlighter-rouge">ArrowArrayStream</code> provider using one of the built-in
<code class="language-plaintext highlighter-rouge">ArrowIpcInputStream</code> implementations:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;stdbool.h&gt;</span><span class="cp">
</span>
<span class="cp">#include</span> <span class="cpf">"nanoarrow_ipc.h"</span><span class="cp">
</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">*</span> <span class="n">argv</span><span class="p">[])</span> <span class="p">{</span>
  <span class="kt">FILE</span><span class="o">*</span> <span class="n">file_ptr</span> <span class="o">=</span> <span class="n">freopen</span><span class="p">(</span><span class="nb">NULL</span><span class="p">,</span> <span class="s">"rb"</span><span class="p">,</span> <span class="n">stdin</span><span class="p">);</span>

  <span class="k">struct</span> <span class="n">ArrowIpcInputStream</span> <span class="n">input</span><span class="p">;</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowIpcInputStreamInitFile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">input</span><span class="p">,</span> <span class="n">file_ptr</span><span class="p">,</span> <span class="nb">false</span><span class="p">));</span>

  <span class="k">struct</span> <span class="n">ArrowArrayStream</span> <span class="n">stream</span><span class="p">;</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowIpcArrayStreamReaderInit</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">input</span><span class="p">,</span> <span class="nb">NULL</span><span class="p">));</span>

  <span class="k">struct</span> <span class="n">ArrowSchema</span> <span class="n">schema</span><span class="p">;</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">stream</span><span class="p">.</span><span class="n">get_schema</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">schema</span><span class="p">));</span>

  <span class="k">struct</span> <span class="n">ArrowArray</span> <span class="n">array</span><span class="p">;</span>
  <span class="k">while</span> <span class="p">(</span><span class="nb">true</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">stream</span><span class="p">.</span><span class="n">get_next</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">array</span><span class="p">));</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">array</span><span class="p">.</span><span class="n">release</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">break</span><span class="p">;</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Facilities for advanced usage are also provided via the low-level <code class="language-plaintext highlighter-rouge">ArrowIpcDecoder</code>,
which takes care of the details of deserializing the flatbuffer headers and
assembling buffers into an <code class="language-plaintext highlighter-rouge">ArrowArray</code>. The current implementation can read
schema and record batch messages that contain any Arrow type that supported by
the C data interface. The initial version can read both big and little endian
streams originating from platforms of either endian. Dictionary encoding and
compression are not currently supported.</p>

<h2 id="getting-started-with-nanoarrow">Getting started with nanoarrow</h2>

<p>Early users of the nanoarrow C library respectfully noted that it was difficult
to know where to begin. This release includes improvements in reference documentation
but also includes a long-form tutorial for those just getting started with or
considering adopting the library. You can find the tutorial at the
<a href="https://apache.github.io/arrow-nanoarrow/dev/getting-started.html">nanoarrow documentation site</a>.</p>

<h2 id="c-library">C library</h2>

<p>The nanoarrow 0.2.0 release also includes a number of bugfixes and improvements
to the core C library, many of which were identified as a result of usage
connected with development of the IPC extension.</p>

<ul>
  <li>Helpers for extracting/appending <code class="language-plaintext highlighter-rouge">Decimal128</code> and <code class="language-plaintext highlighter-rouge">Decimal256</code> elements
from/to arrays were added.</li>
  <li>The C library can now perform “full” validation to validate untrusted input
(e.g., serialized IPC from the wire).</li>
  <li>The C library can now perform “minimal” validation that performs all checks
that do not access any buffer data. This feature was added to facilitate
future support for the <code class="language-plaintext highlighter-rouge">ArrowDeviceArray</code> that was recently added as the
Arrow C device interface.</li>
  <li>Release verification on Ubuntu (x86_64, arm64), Fedora (x86_64),
Archlinux (x86_64), Centos 7 (x86_64, arm64), Alpine (x86_64, arm64, s390x),
Windows (x86_64), and MacOS (x86_64) were added to the continuous
integration system. Linux verification is implemented using <code class="language-plaintext highlighter-rouge">docker compose</code>
to facilitate local checks when developing features that may affect
a specific platform.</li>
</ul>

<h2 id="r-bindings">R bindings</h2>

<p>The nanoarrow R bindings are distributed as the <code class="language-plaintext highlighter-rouge">nanoarrow</code> package on
<a href="https://cran.r-project.org/">CRAN</a>. The 0.2.0 release of the R bindings includes
improvements in type support, improvements in stability, and features required
for the forthcoming release of Arrow Database Connectivity (ADBC) R bindings.
Notably:</p>

<ul>
  <li>Support for conversion of union arrays to R objects was added to facilitate
support for an ADBC function that returns such an array.</li>
  <li>Support for adding an R-level finalizer to an <code class="language-plaintext highlighter-rouge">ArrowArrayStream</code> was added
to facilitate safely wrapping a stream resulting from an ADBC call at the
R level.</li>
</ul>

<h2 id="python-bindings">Python bindings?</h2>

<p>The nanoarrow 0.2.0 release does not include Python bindings, but improvements to the
<a href="https://github.com/apache/arrow-nanoarrow/tree/main/python">unreleased draft bindings</a>
were added to facilitate discussion among Python developers regarding the useful
scope of a potential future nanoarrow Python package. If one of those developers is
you, feel free to
<a href="https://github.com/apache/arrow-nanoarrow/issues/new/choose">open an issue</a>
or send a post to the
<a href="https://lists.apache.org/list.html?dev@arrow.apache.org">developer mailing list</a>
to engage in the discussion.</p>

<h2 id="contributors">Contributors</h2>

<p>This initial release consists of contributions from 6 contributors in addition
to the invaluable advice and support of the Apache Arrow developer mailing list.
Special thanks to David Li for reviewing nearly every PR in this release!</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog -sn d91c35b33c7b6ff94f5f929384879352e241ed71..apache-arrow-nanoarrow-0.2.0 | grep -v "GitHub Actions"
    61  Dewey Dunnington
     2  Dirk Eddelbuettel
     2  Joris Van den Bossche
     2  Kirill Müller
     2  William Ayd
     1  David Li
</code></pre></div></div>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.2.0 release of Apache Arrow nanoarrow. This initial release covers 19 resolved issues from 6 contributors. Release Highlights Addition of the Arrow IPC stream reader extension Addition of the Getting Started with nanoarrow tutorial Improvements in reliability and platform test coverage of the C library Improvements in reliability and type support in the R bindings See the Changelog for a detailed list of contributions to this release. IPC stream support This release includes support for reading schemas and record batches serialized using the Arrow IPC format. Based on the flatcc flatbuffers implementation, the nanoarrow IPC read support is implemented as an optional extension to the core nanoarrow library. The easiest way to get started is with the ArrowArrayStream provider using one of the built-in ArrowIpcInputStream implementations: #include &lt;stdio.h&gt; #include &lt;stdbool.h&gt; #include "nanoarrow_ipc.h" int main(int argc, char* argv[]) { FILE* file_ptr = freopen(NULL, "rb", stdin); struct ArrowIpcInputStream input; NANOARROW_RETURN_NOT_OK(ArrowIpcInputStreamInitFile(&amp;input, file_ptr, false)); struct ArrowArrayStream stream; NANOARROW_RETURN_NOT_OK(ArrowIpcArrayStreamReaderInit(&amp;stream, &amp;input, NULL)); struct ArrowSchema schema; NANOARROW_RETURN_NOT_OK(stream.get_schema(&amp;stream, &amp;schema)); struct ArrowArray array; while (true) { NANOARROW_RETURN_NOT_OK(stream.get_next(&amp;stream, &amp;array)); if (array.release == NULL) { break; } } return 0; } Facilities for advanced usage are also provided via the low-level ArrowIpcDecoder, which takes care of the details of deserializing the flatbuffer headers and assembling buffers into an ArrowArray. The current implementation can read schema and record batch messages that contain any Arrow type that supported by the C data interface. The initial version can read both big and little endian streams originating from platforms of either endian. Dictionary encoding and compression are not currently supported. Getting started with nanoarrow Early users of the nanoarrow C library respectfully noted that it was difficult to know where to begin. This release includes improvements in reference documentation but also includes a long-form tutorial for those just getting started with or considering adopting the library. You can find the tutorial at the nanoarrow documentation site. C library The nanoarrow 0.2.0 release also includes a number of bugfixes and improvements to the core C library, many of which were identified as a result of usage connected with development of the IPC extension. Helpers for extracting/appending Decimal128 and Decimal256 elements from/to arrays were added. The C library can now perform “full” validation to validate untrusted input (e.g., serialized IPC from the wire). The C library can now perform “minimal” validation that performs all checks that do not access any buffer data. This feature was added to facilitate future support for the ArrowDeviceArray that was recently added as the Arrow C device interface. Release verification on Ubuntu (x86_64, arm64), Fedora (x86_64), Archlinux (x86_64), Centos 7 (x86_64, arm64), Alpine (x86_64, arm64, s390x), Windows (x86_64), and MacOS (x86_64) were added to the continuous integration system. Linux verification is implemented using docker compose to facilitate local checks when developing features that may affect a specific platform. R bindings The nanoarrow R bindings are distributed as the nanoarrow package on CRAN. The 0.2.0 release of the R bindings includes improvements in type support, improvements in stability, and features required for the forthcoming release of Arrow Database Connectivity (ADBC) R bindings. Notably: Support for conversion of union arrays to R objects was added to facilitate support for an ADBC function that returns such an array. Support for adding an R-level finalizer to an ArrowArrayStream was added to facilitate safely wrapping a stream resulting from an ADBC call at the R level. Python bindings? The nanoarrow 0.2.0 release does not include Python bindings, but improvements to the unreleased draft bindings were added to facilitate discussion among Python developers regarding the useful scope of a potential future nanoarrow Python package. If one of those developers is you, feel free to open an issue or send a post to the developer mailing list to engage in the discussion. Contributors This initial release consists of contributions from 6 contributors in addition to the invaluable advice and support of the Apache Arrow developer mailing list. Special thanks to David Li for reviewing nearly every PR in this release! $ git shortlog -sn d91c35b33c7b6ff94f5f929384879352e241ed71..apache-arrow-nanoarrow-0.2.0 | grep -v "GitHub Actions" 61 Dewey Dunnington 2 Dirk Eddelbuettel 2 Joris Van den Bossche 2 Kirill Müller 2 William Ayd 1 David Li]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 0.5.0 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/06/21/adbc-0.5.0-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.5.0 (Libraries) Release" /><published>2023-06-21T00:00:00-04:00</published><updated>2023-06-21T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/06/21/adbc-0.5.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/06/21/adbc-0.5.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.5.0 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/6?closed=1"><strong>37
resolved issues</strong></a> from <a href="#contributors"><strong>12 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version
0.5.0.  The <strong>API specification</strong> is versioned separately and is
at version 1.0.0.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.5.0/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>Experimental C# and Rust codebases were added to the source tree.  No packages are released for them yet.</p>

<p>Go-based drivers, when using FFI, will now catch panics at the interface boundary and return them as errors.  Afterwards, all calls to the driver will immediately fail.  This lets applications handle errors and gracefully terminate instead of immediately aborting.</p>

<p>The JDBC adapter allows customizing the type mapping between JDBC types and Arrow types, using all the information provided by JDBC.  This allows proper mappings for certain databases (e.g. the PostgreSQL JDBC driver returns both tz-aware and tz-naive timestamps under the same type code, but differentiates them via the type name).</p>

<p>The PostgreSQL driver has better support for reading and writing different types, including datetime types in particular.  More metadata is now returned in GetObjects, including tables, columns, and constraints (primary/foreign keys).  The driver also now handles non-SELECT queries in <code class="language-plaintext highlighter-rouge">ExecuteQuery</code>.</p>

<p>The Python driver manager lets you choose whether to disable or enable autocommit.</p>

<p>The R driver manager now exposes easy convenience functions for reading/writing data.</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.4.0..apache-arrow-adbc-0.5.0
    36	David Li
    21	William Ayd
     6	Dewey Dunnington
     4	Matt Topol
     3	Kirill Müller
     2	Sutou Kouhei
     2	vipere
     1	Curt Hagenlocher
     1	Diego Fernández Giraldo
     1	Matthijs Brobbel
     1	Will Jones
     1	davidhcoe
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>Work for the proposed <a href="https://github.com/apache/arrow-adbc/milestone/3">1.1.0 API revision</a> is progressing on a <a href="https://github.com/apache/arrow-adbc/tree/spec-1.1.0">branch</a>, with several API proposals merged in and others awaiting review.  They will then receive prototype implementations before being submitted for review/voting.  This is not currently targeting any release of the ADBC libraries, although we hope to wrap up this effort in the next few months.</p>

<p>A milestone has been created to explore <a href="https://github.com/apache/arrow-adbc/milestone/9">asynchronous APIs</a>, which have been requested for a while now.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.5.0 release of the Apache Arrow ADBC libraries. This covers includes 37 resolved issues from 12 distinct contributors. This is a release of the libraries, which are at version 0.5.0. The API specification is versioned separately and is at version 1.0.0. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights Experimental C# and Rust codebases were added to the source tree. No packages are released for them yet. Go-based drivers, when using FFI, will now catch panics at the interface boundary and return them as errors. Afterwards, all calls to the driver will immediately fail. This lets applications handle errors and gracefully terminate instead of immediately aborting. The JDBC adapter allows customizing the type mapping between JDBC types and Arrow types, using all the information provided by JDBC. This allows proper mappings for certain databases (e.g. the PostgreSQL JDBC driver returns both tz-aware and tz-naive timestamps under the same type code, but differentiates them via the type name). The PostgreSQL driver has better support for reading and writing different types, including datetime types in particular. More metadata is now returned in GetObjects, including tables, columns, and constraints (primary/foreign keys). The driver also now handles non-SELECT queries in ExecuteQuery. The Python driver manager lets you choose whether to disable or enable autocommit. The R driver manager now exposes easy convenience functions for reading/writing data. Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.4.0..apache-arrow-adbc-0.5.0 36 David Li 21 William Ayd 6 Dewey Dunnington 4 Matt Topol 3 Kirill Müller 2 Sutou Kouhei 2 vipere 1 Curt Hagenlocher 1 Diego Fernández Giraldo 1 Matthijs Brobbel 1 Will Jones 1 davidhcoe Roadmap Work for the proposed 1.1.0 API revision is progressing on a branch, with several API proposals merged in and others awaiting review. They will then receive prototype implementations before being submitted for review/voting. This is not currently targeting any release of the ADBC libraries, although we hope to wrap up this effort in the next few months. A milestone has been created to explore asynchronous APIs, which have been requested for a while now. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 12.0.1 Release</title><link href="https://arrow.apache.org/blog/2023/06/13/12.0.1-release/" rel="alternate" type="text/html" title="Apache Arrow 12.0.1 Release" /><published>2023-06-13T00:00:00-04:00</published><updated>2023-06-13T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/06/13/12.0.1-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/06/13/12.0.1-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 12.0.1 release.
This is mostly a bugfix release that includes <a href="https://github.com/apache/arrow/milestone/54?closed=1"><strong>38 resolved issues</strong></a>
from <a href="/release/12.0.1.html#contributors"><strong>12 distinct contributors</strong></a>. See the Install Page to learn how to
get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/12.0.1.html#changelog">complete changelog</a>.</p>

<h2 id="c-notes">C++ notes</h2>

<ul>
  <li>Fixed a performance regression when writing data from non-arrow sources (e.g. pandas) (<a href="https://github.com/apache/arrow/pull/35565">GH-35498</a>)</li>
  <li>Fixed a “Data size too large” error that could occur when reading valid parquet files (<a href="https://github.com/apache/arrow/pull/35428">GH-35423</a>)</li>
  <li>It is now possible to specify field-level metadata in dataset writes (<a href="https://github.com/apache/arrow/pull/35860">GH-35730</a>)</li>
</ul>

<h2 id="go-notes">Go notes</h2>

<ul>
  <li>Fixed builds of the Go Arrow package on 32-bit systems (<a href="https://github.com/apache/arrow/pull/35767">GH-34784</a>)</li>
  <li>Added <code class="language-plaintext highlighter-rouge">ValueString(int) string</code> method to <code class="language-plaintext highlighter-rouge">arrow.Array</code> (<a href="https://github.com/apache/arrow/pull/34986">GH-34657</a>)</li>
  <li>Fixed ASAN failure when using go1.20+ by using <code class="language-plaintext highlighter-rouge">unsafe.StringData</code> (<a href="https://github.com/apache/arrow/pull/35338">GH-35337</a>)</li>
</ul>

<h2 id="java-notes">Java notes</h2>

<ul>
  <li>Bumped jackson-databind dependency version to avoid CVE-2022-42003. (<a href="https://github.com/apache/arrow/pull/35771">GH-35771</a>)</li>
</ul>

<h2 id="python-notes">Python notes</h2>

<ul>
  <li>Fix <code class="language-plaintext highlighter-rouge">Table.join</code> respecting the <code class="language-plaintext highlighter-rouge">coalesce_keys=False</code> option again (<a href="https://github.com/apache/arrow/issues/35389">GH-35389</a>)</li>
</ul>

<h2 id="r-notes">R notes</h2>

<ul>
  <li>Update the version of the date library vendored with Arrow C++ library for compatibility with tzdb 0.4.0 (<a href="https://github.com/apache/arrow/issues/35594">GH-35594</a>, <a href="https://github.com/apache/arrow/issues/35612">GH-35612</a>)</li>
</ul>

<h2 id="other-modules-and-languages">Other modules and languages</h2>

<p>No general changes were made to the other libraries or languages.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 12.0.1 release. This is mostly a bugfix release that includes 38 resolved issues from 12 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. C++ notes Fixed a performance regression when writing data from non-arrow sources (e.g. pandas) (GH-35498) Fixed a “Data size too large” error that could occur when reading valid parquet files (GH-35423) It is now possible to specify field-level metadata in dataset writes (GH-35730) Go notes Fixed builds of the Go Arrow package on 32-bit systems (GH-34784) Added ValueString(int) string method to arrow.Array (GH-34657) Fixed ASAN failure when using go1.20+ by using unsafe.StringData (GH-35337) Java notes Bumped jackson-databind dependency version to avoid CVE-2022-42003. (GH-35771) Python notes Fix Table.join respecting the coalesce_keys=False option again (GH-35389) R notes Update the version of the date library vendored with Arrow C++ library for compatibility with tzdb 0.4.0 (GH-35594, GH-35612) Other modules and languages No general changes were made to the other libraries or languages.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 0.4.0 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/05/15/adbc-0.4.0-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.4.0 (Libraries) Release" /><published>2023-05-15T00:00:00-04:00</published><updated>2023-05-15T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/05/15/adbc-0.4.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/05/15/adbc-0.4.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.4.0 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/5"><strong>47
resolved issues</strong></a> from <a href="#contributors"><strong>8 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version
0.4.0.  The <strong>API specification</strong> is versioned separately and is
at version 1.0.0.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.4.0/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>A Go-based driver for <a href="https://www.snowflake.com/en/">Snowflake</a> was added, along with bindings for Python and R.</p>

<p>The PostgreSQL driver now has much better support for different types, and properly handles NULL values.
It now also implements <code class="language-plaintext highlighter-rouge">AdbcConnectionGetTableSchema</code>.
All C/C++ components have been consolidated into a single build project, simplifying integration with IDEs.</p>

<p>Several <code class="language-plaintext highlighter-rouge">ArrowBuf</code> leaks in the Java libraries have been fixed, along with miscellaneous bugs in the JDBC adapter.
There was breaking change in the Java libraries: static driver instances were removed, as this was prone to the aformentioned leaks.
Instead, <code class="language-plaintext highlighter-rouge">AdbcDriver</code> instances can now be created with a <code class="language-plaintext highlighter-rouge">BufferAllocator</code>, giving the application control over allocations.</p>

<p>The Python, GLib, and Ruby bindings expose more of the API functions.
The Python bindings are now tested against the <a href="https://www.pola.rs/">polars</a> dataframe project, which has experimental integration with ADBC.
The release process was fixed to properly upload the Windows Python wheels.
The R bindings now include packages for the PostgreSQL driver.</p>

<p>There is a known issue with the Snowflake driver: it will not build on non-x86_64 (AMD64) platforms, including Linux/macOS arm64/aarch64.
This has already been fixed for the next release.</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.3.0..apache-arrow-adbc-0.4.0
    31	David Li
    15	Sutou Kouhei
     9	Dewey Dunnington
     7	William Ayd
     5	Matt Topol
     1	Jacob Marble
     1	Tornike Gurgenidze
     1	eitsupi
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>Work for the proposed <a href="https://github.com/apache/arrow-adbc/milestone/3">1.1.0 API revision</a> has begun on a new <a href="https://github.com/apache/arrow-adbc/tree/spec-1.1.0">branch</a>.
This is not currently targeting any release of the ADBC libraries.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.4.0 release of the Apache Arrow ADBC libraries. This covers includes 47 resolved issues from 8 distinct contributors. This is a release of the libraries, which are at version 0.4.0. The API specification is versioned separately and is at version 1.0.0. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights A Go-based driver for Snowflake was added, along with bindings for Python and R. The PostgreSQL driver now has much better support for different types, and properly handles NULL values. It now also implements AdbcConnectionGetTableSchema. All C/C++ components have been consolidated into a single build project, simplifying integration with IDEs. Several ArrowBuf leaks in the Java libraries have been fixed, along with miscellaneous bugs in the JDBC adapter. There was breaking change in the Java libraries: static driver instances were removed, as this was prone to the aformentioned leaks. Instead, AdbcDriver instances can now be created with a BufferAllocator, giving the application control over allocations. The Python, GLib, and Ruby bindings expose more of the API functions. The Python bindings are now tested against the polars dataframe project, which has experimental integration with ADBC. The release process was fixed to properly upload the Windows Python wheels. The R bindings now include packages for the PostgreSQL driver. There is a known issue with the Snowflake driver: it will not build on non-x86_64 (AMD64) platforms, including Linux/macOS arm64/aarch64. This has already been fixed for the next release. Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.3.0..apache-arrow-adbc-0.4.0 31 David Li 15 Sutou Kouhei 9 Dewey Dunnington 7 William Ayd 5 Matt Topol 1 Jacob Marble 1 Tornike Gurgenidze 1 eitsupi Roadmap Work for the proposed 1.1.0 API revision has begun on a new branch. This is not currently targeting any release of the ADBC libraries. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Adopting Apache Arrow at CloudQuery</title><link href="https://arrow.apache.org/blog/2023/05/04/adopting-apache-arrow-at-cloudquery/" rel="alternate" type="text/html" title="Adopting Apache Arrow at CloudQuery" /><published>2023-05-04T00:00:00-04:00</published><updated>2023-05-04T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/05/04/adopting-apache-arrow-at-cloudquery</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/05/04/adopting-apache-arrow-at-cloudquery/"><![CDATA[<!--

-->

<p>This post is a collaboration with CloudQuery and cross-posted on the CloudQuery <a href="https://cloudquery.io/blog/adopting-apache-arrow-at-cloudquery">blog</a>.</p>

<p><a href="https://github.com/cloudquery/cloudquery">CloudQuery</a> is an open source high performance ELT framework written in Go. We <a href="https://www.cloudquery.io/blog/building-cloudquery">previously</a> discussed some of the <a href="https://www.cloudquery.io/docs/developers/architecture">architecture</a> and design decisions that we took to build a performant ELT framework. A type system is a key component for creating a performant and scalable ELT framework where sources and destinations are decoupled. In this blog post we will go through why we decided to adopt Apache Arrow as our type system and replace our in-house implementation.</p>

<h1 id="what-is-a-type-system">What is a Type System?</h1>

<p>Let’s quickly <a href="https://www.cloudquery.io/blog/building-cloudquery#type-system">recap</a> what a type system is and why an ELT framework needs one. At a very high level, an ELT framework extracts data from some source and moves it to some destination with a specific schema.</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>API ---&gt; [Source Plugin]  -----&gt;    [Destination Plugin]
                          -----&gt;    [Destination Plugin]
                           gRPC
</code></pre></div></div>

<p>Sources and destinations are decoupled and communicate via gRPC. This is crucial to allowing the addition of new destinations and updating old destinations without requiring updates to source plugin code (which otherwise would introduce an unmaintainable architecture).</p>

<p>This is where a type system comes in. Source plugins extract information from APIs in the most performant way possible, defining a schema and then transforming the result from the API (JSON or any other format) to a well-defined type system. The destination plugin can then easily create the schema for its database and transform the incoming data to the destination types. So to recap, the source plugin sends mainly two things to a destination: 1) the schema 2) the records that fit the defined schema. In Arrow terminology, these are a schema and a record batch.</p>

<h1 id="why-arrow">Why Arrow?</h1>

<p>Before Arrow, we used our own type system that supported more than 14 types. This served us well, but we started to hit limitations in various use-cases. For example, in database to database replication, we needed to support many more types, including nested types. Also, performance-wise, lots of the time spent in an ELT process is around converting data from one format to another, so we wanted to take a step back and see if we can avoid this <a href="https://xkcd.com/927/">famous XKCD</a> (by building yet another format):</p>

<figure style="text-align: center;">
  <img src="https://imgs.xkcd.com/comics/standards.png" width="100%" class="img-responsive" alt="Yet another standard XKCD" />
</figure>

<p>This is where Arrow comes in. Apache Arrow defines a language-independent columnar format for flat and hierarchical data, and brings the following advantages:</p>

<ol>
  <li>Cross-language with extensive libraries for different languages - The <a href="https://arrow.apache.org/docs/format/Columnar.html">format</a> is defined via flatbuffers in such way that you can parse it in any language and already has extensive support in C/C++, C#, Go, Java, JavaScript, Julia, Matlab, Python, R, Ruby and Rust (at the time of writing). For CloudQuery this is important as it makes it much easier to develop source or destination plugins in different languages.</li>
  <li>Performance: Arrow adoption is rising especially in columnar based databases (<a href="https://duckdb.org/2021/12/03/duck-arrow.html">DuckDB</a>, <a href="https://clickhouse.com/docs/en/integrations/data-formats/arrow-avro-orc">ClickHouse</a>, <a href="https://cloud.google.com/bigquery/docs/samples/bigquerystorage-arrow-quickstart">BigQuery</a>) and file formats (<a href="https://arrow.apache.org/docs/python/parquet.html">Parquet</a>) which makes it easier to write CloudQuery destination or source plugins for databases that already support Arrow as well as much more efficient as we remove the need for additional serialization and transformation step. Moreover, just the performance of sending Arrow format from source plugin to destination is already more performant and memory efficient, given its “zero-copy” nature and not needing serialization/deserialization.</li>
  <li>Rich Data Types: Arrow supports more than <a href="https://arrow.apache.org/docs/python/api/datatypes.html">35 types</a> including composite types (i.e. lists, structs and maps of all the available types) and ability to extend the type system with custom types. Also, there is already built-in mapping from/to the arrow type system and the parquet type system (including nested types) which already supported in many of the arrow libraries as explained <a href="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/">here</a>.</li>
</ol>

<h1 id="summary">Summary</h1>

<p>Adopting Apache Arrow as the CloudQuery in-memory type system enables us to gain better performance, data interoperability and developer experience. Some plugins that are going to gain an immediate boost of rich type systems are our database-to-database replication plugins such as <a href="https://www.cloudquery.io/docs/plugins/sources/postgresql/overview">PostgreSQL CDC</a> source plugin (and all <a href="https://www.cloudquery.io/docs/plugins/destinations/overview">database destinations</a>) that are going to get support for all available types including nested ones.</p>

<p>We are excited about this step and joining the growing Arrow community. We already contributed more than <a href="https://github.com/search?q=is%3Apr+author%3Ayevgenypats+author%3Ahermanschaaf+author%3Acandiduslynx+author%3Adisq+label%3A%22Component%3A+Go%22++is%3Amerged+&amp;ref=simplesearch">30</a> upstream pull requests that were quickly reviewed by the Arrow maintainers, thank you!</p>]]></content><author><name>Yevgeny Pats</name></author><category term="application" /><summary type="html"><![CDATA[This post is a collaboration with CloudQuery and cross-posted on the CloudQuery blog. CloudQuery is an open source high performance ELT framework written in Go. We previously discussed some of the architecture and design decisions that we took to build a performant ELT framework. A type system is a key component for creating a performant and scalable ELT framework where sources and destinations are decoupled. In this blog post we will go through why we decided to adopt Apache Arrow as our type system and replace our in-house implementation. What is a Type System? Let’s quickly recap what a type system is and why an ELT framework needs one. At a very high level, an ELT framework extracts data from some source and moves it to some destination with a specific schema. API ---&gt; [Source Plugin]  -----&gt;    [Destination Plugin] -----&gt;    [Destination Plugin] gRPC Sources and destinations are decoupled and communicate via gRPC. This is crucial to allowing the addition of new destinations and updating old destinations without requiring updates to source plugin code (which otherwise would introduce an unmaintainable architecture). This is where a type system comes in. Source plugins extract information from APIs in the most performant way possible, defining a schema and then transforming the result from the API (JSON or any other format) to a well-defined type system. The destination plugin can then easily create the schema for its database and transform the incoming data to the destination types. So to recap, the source plugin sends mainly two things to a destination: 1) the schema 2) the records that fit the defined schema. In Arrow terminology, these are a schema and a record batch. Why Arrow? Before Arrow, we used our own type system that supported more than 14 types. This served us well, but we started to hit limitations in various use-cases. For example, in database to database replication, we needed to support many more types, including nested types. Also, performance-wise, lots of the time spent in an ELT process is around converting data from one format to another, so we wanted to take a step back and see if we can avoid this famous XKCD (by building yet another format): This is where Arrow comes in. Apache Arrow defines a language-independent columnar format for flat and hierarchical data, and brings the following advantages: Cross-language with extensive libraries for different languages - The format is defined via flatbuffers in such way that you can parse it in any language and already has extensive support in C/C++, C#, Go, Java, JavaScript, Julia, Matlab, Python, R, Ruby and Rust (at the time of writing). For CloudQuery this is important as it makes it much easier to develop source or destination plugins in different languages. Performance: Arrow adoption is rising especially in columnar based databases (DuckDB, ClickHouse, BigQuery) and file formats (Parquet) which makes it easier to write CloudQuery destination or source plugins for databases that already support Arrow as well as much more efficient as we remove the need for additional serialization and transformation step. Moreover, just the performance of sending Arrow format from source plugin to destination is already more performant and memory efficient, given its “zero-copy” nature and not needing serialization/deserialization. Rich Data Types: Arrow supports more than 35 types including composite types (i.e. lists, structs and maps of all the available types) and ability to extend the type system with custom types. Also, there is already built-in mapping from/to the arrow type system and the parquet type system (including nested types) which already supported in many of the arrow libraries as explained here. Summary Adopting Apache Arrow as the CloudQuery in-memory type system enables us to gain better performance, data interoperability and developer experience. Some plugins that are going to gain an immediate boost of rich type systems are our database-to-database replication plugins such as PostgreSQL CDC source plugin (and all database destinations) that are going to get support for all available types including nested ones. We are excited about this step and joining the growing Arrow community. We already contributed more than 30 upstream pull requests that were quickly reviewed by the Arrow maintainers, thank you!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 12.0.0 Release</title><link href="https://arrow.apache.org/blog/2023/05/02/12.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 12.0.0 Release" /><published>2023-05-02T00:00:00-04:00</published><updated>2023-05-02T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/05/02/12.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/05/02/12.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 12.0.0 release. This covers
over 3 months of development work and includes <a href="https://github.com/apache/arrow/milestone/51?closed=1"><strong>476 resolved issues</strong></a>
with <a href="/release/12.0.0.html#contributors"><strong>531 commits from 97 distinct contributors</strong></a>.
See the <a href="https://arrow.apache.org/install/">Install Page</a>
to learn how to get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/12.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 11.0.0 release, Wang Mingming, Mustafa Akur and Ruihang Xia
have been invited to be committers.
Will Jones have joined the Project Management Committee (PMC).</p>

<p>Thanks for your contributions and participation in the project!</p>

<h2 id="columnar-format-notes">Columnar Format Notes</h2>

<p>A first “canonical” extension type has been formalized: <code class="language-plaintext highlighter-rouge">arrow.fixed_shape_tensor</code> to
represent an Array where each slot contains a tensor, with all tensors having the same
dimension and shape, <a href="https://github.com/apache/arrow/issues/33923">GH-33923</a>.
This is based on a Fixed-Size List layout as storage array
(<a href="https://arrow.apache.org/docs/dev/format/CanonicalExtensions.html#fixed-shape-tensor-extension">https://arrow.apache.org/docs/dev/format/CanonicalExtensions.html#fixed-shape-tensor-extension</a>).</p>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>

<p>The JDBC driver for Arrow Flight SQL has had some bugfixes, and has been refactored into a core library (which is not distributed as an uberjar with shaded names) and a driver (which is distributed as an uberjar).</p>

<p>The Java server builder API now offers easier access to the underlying gRPC builder.</p>

<p>Go now implements the Flight SQL extensions for Substrait and transaction support.</p>

<h2 id="plasma-notes">Plasma notes</h2>

<p>Plasma was deprecated since 10.0.0. Plasma is removed in this
release. <a href="https://github.com/apache/arrow/issues/33243">GH-33243</a></p>

<h2 id="linux-packages-notes">Linux packages notes</h2>

<p>We dropped support for Ubuntu 18.04 because Ubuntu 18.04 reached EOL.</p>

<h2 id="c-notes">C++ notes</h2>

<ul>
  <li>Run-End Encoded Arrays have been implemented and are accessible (<a href="https://github.com/apache/arrow/issues/32104">GH-32104</a>)</li>
  <li>The FixedShapeTensor Logical value type has been implemented using ExtensionType (<a href="https://github.com/apache/arrow/issues/15483">GH-15483</a>, <a href="https://github.com/apache/arrow/issues/34796">GH-34796</a>)</li>
</ul>

<h3 id="compute">Compute</h3>

<ul>
  <li>New kernel to convert timestamp with timezone to wall time (<a href="https://github.com/apache/arrow/issues/33143">GH-33143</a>)</li>
  <li>Cast kernels are now built into libarrow by default (<a href="https://github.com/apache/arrow/issues/34388">GH-34388</a>)</li>
</ul>

<h3 id="acero">Acero</h3>

<ul>
  <li>Acero has been moved out of libarrow into it’s own shared library, allowing for smaller builds of the core libarrow (<a href="https://github.com/apache/arrow/issues/15280">GH-15280</a>)</li>
  <li>Exec nodes now can have a concept of “ordering” and will reject non-sensible plans (<a href="https://github.com/apache/arrow/issues/34136">GH-34136</a>)</li>
  <li>New exec nodes: “pivot_longer” (<a href="https://github.com/apache/arrow/issues/34266">GH-34266</a>), “order_by” (<a href="https://github.com/apache/arrow/issues/34248">GH-34248</a>) and “fetch” (<a href="https://github.com/apache/arrow/issues/34059">GH-34059</a>)</li>
  <li><em>Breaking Change:</em> Reorder output fields of “group_by” node so that keys/segment keys come before aggregates (<a href="https://github.com/apache/arrow/issues/33616">GH-33616</a>)</li>
</ul>

<h3 id="substrait">Substrait</h3>

<ul>
  <li>Add support for the <code class="language-plaintext highlighter-rouge">round</code> function <a href="https://github.com/apache/arrow/issues/33588">GH-33588</a></li>
  <li>Add support for the <code class="language-plaintext highlighter-rouge">cast</code> expression element <a href="https://github.com/apache/arrow/issues/31910">GH-31910</a></li>
  <li>Added API reference documentation <a href="https://github.com/apache/arrow/issues/34011">GH-34011</a></li>
  <li>Added an extension relation to support segmented aggregation <a href="https://github.com/apache/arrow/issues/34626">GH-34626</a></li>
  <li>The output of the aggregate relation now conforms to the spec <a href="https://github.com/apache/arrow/issues/34786">GH-34786</a></li>
</ul>

<h3 id="parquet">Parquet</h3>

<ul>
  <li>Added support for DeltaLengthByteArray encoding to the Parquet writer (<a href="https://github.com/apache/arrow/issues/33024">GH-33024</a>)</li>
  <li>NaNs are correctly handled now for Parquet predicate push-downs (<a href="https://github.com/apache/arrow/issues/18481">GH-18481</a>)</li>
  <li>Added support for reading Parquet page indexes (<a href="https://github.com/apache/arrow/issues/33596">GH-33596</a>) and writing page indexes (<a href="https://github.com/apache/arrow/issues/34053">GH-34053</a>)</li>
  <li>Parquet writer can write columns in parallel now (<a href="https://github.com/apache/arrow/issues/33655">GH-33655</a>)</li>
  <li>Fixed incorrect number of rows in Parquet V2 page headers (<a href="https://github.com/apache/arrow/issues/34086">GH-34086</a>)</li>
  <li>Fixed incorrect Parquet page null_count when stats are disabled (<a href="https://github.com/apache/arrow/issues/34326">GH-34326</a>)</li>
  <li>Added support for reading BloomFilters to the Parquet Reader (<a href="https://github.com/apache/arrow/issues/34665">GH-34665</a>)</li>
  <li>Parquet File-writer can now add additional key-value metadata after it has been opened (<a href="https://github.com/apache/arrow/issues/34888">GH-34888</a>)</li>
  <li><em>Breaking Change:</em> The default row group size for the Arrow writer changed from 64Mi rows to 1Mi rows. <a href="https://github.com/apache/arrow/issues/34280">GH-34280</a></li>
</ul>

<h3 id="orc">ORC</h3>

<ul>
  <li>Added support for the union type in ORC writer (<a href="https://github.com/apache/arrow/issues/34262">GH-34262</a>)</li>
  <li>Fixed ORC CHAR type mapping with Arrow (<a href="https://github.com/apache/arrow/issues/34823">GH-34823</a>)</li>
  <li>Fixed timestamp type mapping between ORC and arrow (<a href="https://github.com/apache/arrow/issues/34590">GH-34590</a>)</li>
</ul>

<h3 id="datasets">Datasets</h3>

<ul>
  <li>Added support for reading JSON datasets (<a href="https://github.com/apache/arrow/issues/33209">GH-33209</a>)</li>
  <li>Dataset writer now supports specifying a function callback to construct the file name in addition to the existing file name template (<a href="https://github.com/apache/arrow/issues/34565">GH-34565</a>)</li>
</ul>

<h3 id="filesystems">Filesystems</h3>

<ul>
  <li>GcsFileSystem::OpenInputFile avoids unnecessary downloads (<a href="https://github.com/apache/arrow/issues/34051">GH-34051</a>)</li>
</ul>

<h3 id="other-changes">Other changes</h3>

<ul>
  <li>Convenience Append(std::optional<T>...) methods have been added to array builders ([GH-14863](https://github.com/apache/arrow/issues/14863))</T></li>
  <li>A deprecated OpenTelemetry header was removed from the Flight library (<a href="https://github.com/apache/arrow/issues/34417">GH-34417</a>)</li>
  <li>Fixed crash in “take” kernels on ExtensionArrays with an underlying dictionary type (<a href="https://github.com/apache/arrow/issues/34619">GH-34619</a>)</li>
  <li>Fixed bug where the C-Data bridge did not preserve nullability of map values on import (<a href="https://github.com/apache/arrow/issues/34983">GH-34983</a>)</li>
  <li>Added support for EqualOptions to RecordBatch::Equals (<a href="https://github.com/apache/arrow/issues/34968">GH-34968</a>)</li>
  <li>zstd dependency upgraded to v1.5.5 (<a href="https://github.com/apache/arrow/issues/34899">GH-34899</a>)</li>
  <li>Improved handling of “logical” nulls such as with union and RunEndEncoded arrays (<a href="https://github.com/apache/arrow/issues/34361">GH-34361</a>)</li>
  <li>Fixed incorrect handling of uncompressed body buffers in IPC reader, added IpcWriteOptions::min_space_savings for optional compression optimizations (<a href="https://github.com/apache/arrow/issues/15102">GH-15102</a>)</li>
</ul>

<h2 id="c-notes-1">C# notes</h2>

<ul>
  <li>Support added for importing / exporting schemas and types via the
C data interface <a href="https://github.com/apache/arrow/issues/34737">GH-34737</a></li>
  <li>Support added for the half-float data type <a href="https://github.com/apache/arrow/issues/25163">GH-25163</a></li>
  <li>Schemas are now allowed to have multiple fields with the same name
<a href="https://github.com/apache/arrow/issues/34076">GH-34076</a></li>
  <li>Added support for reading compressed IPC files <a href="https://github.com/apache/arrow/issues/32240">GH-32240</a></li>
  <li>Add [] operator to Schema <a href="https://github.com/apache/arrow/issues/32240">GH-34119</a></li>
</ul>

<h2 id="go-notes">Go notes</h2>

<ul>
  <li>Run-End Encoded Arrays have been added to the Golang implementation (<a href="https://github.com/apache/arrow/issues/32104">GH-32104</a>, <a href="https://github.com/apache/arrow/issues/32946">GH-32946</a>, <a href="https://github.com/apache/arrow/issues/20407">GH-20407</a>, <a href="https://github.com/apache/arrow/issues/32949">GH-32949</a>)</li>
  <li>The SQLite Flight SQL Example has been improved and you can now <code class="language-plaintext highlighter-rouge">go get</code> a simple SQLite Flight SQL Server mainprog using <code class="language-plaintext highlighter-rouge">go get github.com/apache/arrow/go/v12/arrow/flight/flightsql/example/cmd/sqlite_flightsql_server</code> (<a href="https://github.com/apache/arrow/issues/33840">GH-33840</a>)</li>
  <li>Fixed bug causing builds to fail with the <code class="language-plaintext highlighter-rouge">noasm</code> build tag (<a href="https://github.com/apache/arrow/issues/34044">GH-34044</a>) and added a CI test run that uses the <code class="language-plaintext highlighter-rouge">noasm</code> tag (<a href="https://github.com/apache/arrow/issues/34055">GH-34055</a>)</li>
  <li>Fixed issue allowing building on riscv64-freebsd (<a href="https://github.com/apache/arrow/issues/34629">GH-34629</a>)</li>
  <li>Fixed issue preventing building on 32-bit platforms (<a href="https://github.com/apache/arrow/issues/35133">GH-35133</a>)</li>
</ul>

<h3 id="arrow">Arrow</h3>

<ul>
  <li>Fixed bug in C-Data handling of <code class="language-plaintext highlighter-rouge">ArrowArrayStream.get_next</code> when handling uninitialized <code class="language-plaintext highlighter-rouge">ArrowArrays</code> (<a href="https://github.com/apache/arrow/issues/33767">GH-33767</a>)</li>
  <li><em>Breaking Change:</em> Added <code class="language-plaintext highlighter-rouge">Err()</code> method to <code class="language-plaintext highlighter-rouge">RecordReader</code> interface so that it can propagate errors (<a href="https://github.com/apache/arrow/issues/33789">GH-33789</a>)</li>
  <li>Fixed potential panic in C-Data API for misusing an invalid handle (<a href="https://github.com/apache/arrow/issues/33864">GH-33864</a>)</li>
  <li>A new cgo-based Allocator that does not depend on libarrow has been added to the memory package (<a href="https://github.com/apache/arrow/issues/33901">GH-33901</a>)</li>
  <li>CSV Reader and Writer now support Extension type arrays (<a href="https://github.com/apache/arrow/issues/34334">GH-34334</a>)</li>
  <li>Fixed bug preventing the reading of IPC streams/files with compression enabled but uncompressed buffers (<a href="https://github.com/apache/arrow/issues/34385">GH-34385</a>)</li>
  <li>Added interface which can be added to an <code class="language-plaintext highlighter-rouge">ExtensionType</code> to allow Builders to be created via <code class="language-plaintext highlighter-rouge">NewBuilder</code>, enabling easy building of nested fields containing extension types (<a href="https://github.com/apache/arrow/issues/34453">GH-34453</a>)</li>
  <li>Added utilities to perform Array diffing (<a href="https://github.com/apache/arrow/issues/34790">GH-34790</a>)</li>
  <li>Added <code class="language-plaintext highlighter-rouge">SetColumn</code> method to <code class="language-plaintext highlighter-rouge">arrow.Record</code> (<a href="https://github.com/apache/arrow/issues/34832">GH-34832</a>)</li>
  <li>Added <code class="language-plaintext highlighter-rouge">GetValue</code> method to <code class="language-plaintext highlighter-rouge">arrow.Metadata</code> (<a href="https://github.com/apache/arrow/issues/34855">GH-34855</a>)</li>
  <li>Added <code class="language-plaintext highlighter-rouge">Pow</code> method to <code class="language-plaintext highlighter-rouge">decimal128.Num</code> and <code class="language-plaintext highlighter-rouge">decimal256.Num</code> (<a href="https://github.com/apache/arrow/issues/34863">GH-34863</a>)</li>
</ul>

<h4 id="flight">Flight</h4>

<ul>
  <li>Fixed bug in <code class="language-plaintext highlighter-rouge">StreamChunks</code> for Flight SQL to correctly propagate to the gRPC client (<a href="https://github.com/apache/arrow/issues/33717">GH-33717</a>)</li>
  <li>Fixed issue that prevented compatibility with gRPC &lt; v1.45 (<a href="https://github.com/apache/arrow/issues/33734">GH-33734</a>)</li>
  <li>Added support to bind a <code class="language-plaintext highlighter-rouge">RecordReader</code> for supplying parameters to a Flight SQL Prepared statement (<a href="https://github.com/apache/arrow/issues/33794">GH-33794</a>)</li>
  <li>Prepared Statement methods for FlightSQL client now allows gRPC Call Options (<a href="https://github.com/apache/arrow/issues/33867">GH-33867</a>)</li>
  <li>FlightSQL Extensions have been implemented (for transactions and Substrait support) (<a href="https://github.com/apache/arrow/issues/33935">GH-33935</a>)</li>
  <li>A driver compatible with <code class="language-plaintext highlighter-rouge">database/sql</code> for FlightSQL has been added (<a href="https://github.com/apache/arrow/issues/34332">GH-34332</a>)</li>
</ul>

<h4 id="compute-1">Compute</h4>

<ul>
  <li>“run_end_encode” and “run_end_decode” functions added to compute functions (<a href="https://github.com/apache/arrow/issues/20408">GH-20408</a>)</li>
  <li>“unique” function added (<a href="https://github.com/apache/arrow/issues/34171">GH-34171</a>)</li>
</ul>

<h3 id="parquet-1">Parquet</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">pqarrow</code> pkg now handles DICTIONARY fields natively (<a href="https://github.com/apache/arrow/issues/33466">GH-33466</a>)</li>
  <li>Fixed rare panic when writing list of 8 structs (<a href="https://github.com/apache/arrow/issues/33600">GH-33600</a>)</li>
  <li>Added support for <code class="language-plaintext highlighter-rouge">pqarrow</code> pkg to write LargeString and LargeBinary types (<a href="https://github.com/apache/arrow/issues/33875">GH-33875</a>)</li>
  <li>Fixed bug where <code class="language-plaintext highlighter-rouge">pqarrow.NewSchemaManifest</code> created the wrong field type for Array Object fields (<a href="https://github.com/apache/arrow/issues/34101">GH-34101</a>)</li>
  <li>Added support to Parquet Writer for Extension type Arrays (<a href="https://github.com/apache/arrow/issues/34330">GH-34330</a>)</li>
</ul>

<h2 id="java-notes">Java notes</h2>

<ul>
  <li>Update Java JNI modules to consider Arrow ACERO <a href="https://github.com/apache/arrow/issues/34862">GH-34862</a></li>
  <li>Ability to register additional GRPC services with FlightServer <a href="https://github.com/apache/arrow/issues/34778">GH-34778</a></li>
  <li>Allow sending custom headers/properties through Arrow Flight SQL JDBC <a href="https://github.com/apache/arrow/issues/33874">GH-33874</a></li>
</ul>

<h2 id="javascript-notes">JavaScript notes</h2>

<p>No changes.</p>

<h2 id="python-notes">Python notes</h2>

<p>Compatibility notes:</p>

<ul>
  <li>Plasma has been removed in this release (<a href="https://github.com/apache/arrow/issues/33243">GH-33243</a>). In addition, the deprecated serialization module in PyArrow was also removed (<a href="https://github.com/apache/arrow/issues/29705">GH-29705</a>). IPC (Inter-Process Communication) functionality of pyarrow or the standard library pickle should be used instead.</li>
  <li>The deprecated <code class="language-plaintext highlighter-rouge">use_async</code> keyword has been removed from the dataset module (<a href="https://github.com/apache/arrow/issues/30774">GH-30774</a>)</li>
  <li>Minimum Cython version to build PyArrow from source has been raised to 0.29.31 (<a href="https://github.com/apache/arrow/issues/34933">GH-34933</a>). In addition, PyArrow can now be compiled using Cython 3 (<a href="https://github.com/apache/arrow/issues/34564">GH-34564</a>).</li>
</ul>

<p>New features:</p>

<ul>
  <li>A new <code class="language-plaintext highlighter-rouge">pyarrow.acero</code> module with initial bindings for the Acero execution engine has been added (<a href="https://github.com/apache/arrow/issues/33976">GH-33976</a>)</li>
  <li>A new canonical extension type for fixed shaped tensor data has been defined. This is exposed in PyArrow as the FixedShapeTensorType (<a href="https://github.com/apache/arrow/issues/34882">GH-34882</a>, <a href="https://github.com/apache/arrow/issues/34956">GH-34956</a>)</li>
  <li>Run-End Encoded arrays binding has been implemented (<a href="https://github.com/apache/arrow/issues/34686">GH-34686</a>, <a href="https://github.com/apache/arrow/issues/34568">GH-34568</a>)</li>
  <li>Method <code class="language-plaintext highlighter-rouge">is_nan</code> has been added to Array, ChunkedArray and Expression (<a href="https://github.com/apache/arrow/issues/34154">GH-34154</a>)</li>
  <li>Dataframe interchange protocol has been implemented for RecordBatch (<a href="https://github.com/apache/arrow/issues/33926">GH-33926</a>)</li>
</ul>

<p>Other improvements:</p>

<ul>
  <li>Extension arrays can now be concatenated (<a href="https://github.com/apache/arrow/issues/31868">GH-31868</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">get_partition_keys</code> helper function is implemented in the <code class="language-plaintext highlighter-rouge">dataset</code> module to access the partitioning field’s key/value from the partition expression of a certain dataset fragment (<a href="https://github.com/apache/arrow/issues/33825">GH-33825</a>)</li>
  <li>PyArrow Array objects can now be accepted by the <code class="language-plaintext highlighter-rouge">pa.array()</code> constructor (<a href="https://github.com/apache/arrow/issues/34411">GH-34411</a>)</li>
  <li>The default row group size when writing parquet files has been changed (<a href="https://github.com/apache/arrow/issues/34280">GH-34280</a>)</li>
  <li>RecordBatch has the <code class="language-plaintext highlighter-rouge">select()</code> method implemented (<a href="https://github.com/apache/arrow/issues/34359">GH-34359</a>)</li>
  <li>New method <code class="language-plaintext highlighter-rouge">drop_column</code> on the <code class="language-plaintext highlighter-rouge">pyarrow.Table</code> supports passing a single column as a string (<a href="https://github.com/apache/arrow/issues/33377">GH-33377</a>)</li>
  <li>User-defined tabular functions, which are a user-functions implemented in Python that return a stateful stream of tabular data, are now also supported (<a href="https://github.com/apache/arrow/issues/32916">GH-32916</a>)</li>
  <li><a href="https://arrow.apache.org/docs/dev/developers/continuous_integration/archery.html">Arrow Archery tool</a> now includes linting of the Cython files (<a href="https://github.com/apache/arrow/issues/31905">GH-31905</a>)</li>
  <li><em>Breaking Change:</em> Reorder output fields of “group_by” node so that keys/segment keys come before aggregates (<a href="https://github.com/apache/arrow/issues/33616">GH-33616</a>)</li>
</ul>

<p>Relevant bug fixes:</p>

<ul>
  <li>Acero can now detect and raise an error in case a join operation needs too much bytes of key data (<a href="https://github.com/apache/arrow/issues/34474">GH-34474</a>)</li>
  <li>Fix for converting non-sequence object in <code class="language-plaintext highlighter-rouge">pa.array()</code> (<a href="https://github.com/apache/arrow/issues/34944">GH-34944</a>)</li>
  <li>Fix erroneous table conversion to pandas if table includes an extension array that does not implement <code class="language-plaintext highlighter-rouge">to_pandas_dtype</code> (<a href="https://github.com/apache/arrow/issues/34906">GH-34906</a>)</li>
  <li>Reading from a closed <code class="language-plaintext highlighter-rouge">ArrayStreamBatchReader</code> now returns invalid status instead of segfaulting (<a href="https://github.com/apache/arrow/issues/34165">GH-34165</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">array()</code> now returns <code class="language-plaintext highlighter-rouge">pyarrow.Array</code> and not <code class="language-plaintext highlighter-rouge">pyarrow.ChunkedArray</code> for columns with <code class="language-plaintext highlighter-rouge">__arrow_array__</code> method and only one chunk so that the conversion of pandas dataframe with categorical column of dtype <code class="language-plaintext highlighter-rouge">string[pyarrow]</code> does not fail (<a href="https://github.com/apache/arrow/issues/33727">GH-33727</a>)</li>
  <li>Custom type mapper in <code class="language-plaintext highlighter-rouge">to_pandas</code> now converts index dtypes together with column dtypes (<a href="https://github.com/apache/arrow/issues/34283">GH-34283</a>)</li>
</ul>

<h2 id="r-notes">R notes</h2>

<ul>
  <li>The <code class="language-plaintext highlighter-rouge">read_parquet()</code> and <code class="language-plaintext highlighter-rouge">read_feather()</code> functions can now accept URL
arguments.</li>
  <li>The <code class="language-plaintext highlighter-rouge">json_credentials</code> argument in <code class="language-plaintext highlighter-rouge">GcsFileSystem$create()</code> now accepts
a file path containing the appropriate authentication token.</li>
  <li>The <code class="language-plaintext highlighter-rouge">$options</code> member of <code class="language-plaintext highlighter-rouge">GcsFileSystem</code> objects can now be inspected.</li>
  <li>The <code class="language-plaintext highlighter-rouge">read_csv_arrow()</code> and <code class="language-plaintext highlighter-rouge">read_json_arrow()</code> functions now accept literal text input wrapped in
<code class="language-plaintext highlighter-rouge">I()</code> to improve compatability with <code class="language-plaintext highlighter-rouge">readr::read_csv()</code>.</li>
  <li>Nested fields can now be accessed using <code class="language-plaintext highlighter-rouge">$</code> and <code class="language-plaintext highlighter-rouge">[[</code> in dplyr expressions.</li>
</ul>

<p>For more on what’s in the 12.0.0 R package, see the <a href="/docs/r/news/">R changelog</a>.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<ul>
  <li>Flight SQL: Added support for authentication. <a href="https://github.com/apache/arrow/issues/34074">GH-34074</a></li>
  <li>Compute: Added <code class="language-plaintext highlighter-rouge">GArrowRankOptions</code>. <a href="https://github.com/apache/arrow/issues/34425">GH-34425</a></li>
  <li>Compute: Added <code class="language-plaintext highlighter-rouge">GArrowFilterOptions</code>. <a href="https://github.com/apache/arrow/issues/34650">GH-34650</a></li>
  <li>Compute: Added <code class="language-plaintext highlighter-rouge">GArrowIndexOptions</code>. <a href="https://github.com/apache/arrow/issues/15286">GH-15286</a></li>
  <li>Compute: Added <code class="language-plaintext highlighter-rouge">GArrowMatchSubstringOptions</code>. <a href="https://github.com/apache/arrow/issues/15285">GH-15285</a></li>
  <li>Added <code class="language-plaintext highlighter-rouge">GArrowDenseUnionArrayBuilder</code>. <a href="https://github.com/apache/arrow/issues/21429">GH-21429</a></li>
  <li>Added <code class="language-plaintext highlighter-rouge">GArrowSparseUnionArrayBuilder</code>. <a href="https://github.com/apache/arrow/issues/21430">GH-21430</a></li>
</ul>

<h3 id="ruby-notes">Ruby notes</h3>

<ul>
  <li>Improved <code class="language-plaintext highlighter-rouge">Arrow::Table#join</code> API. <a href="https://github.com/apache/arrow/issues/15287">GH-15287</a></li>
  <li>Flight SQL: Added <code class="language-plaintext highlighter-rouge">ArrowFlight::RecordBatchReader#each</code>. <a href="https://github.com/apache/arrow/issues/15287">GH-15287</a></li>
  <li>Added <code class="language-plaintext highlighter-rouge">Arrow::DenseUnionArray#get_value</code>. <a href="https://github.com/apache/arrow/issues/34837">GH-34837</a></li>
  <li>Added <code class="language-plaintext highlighter-rouge">Arrow::SparseUnionArray#get_value</code>. <a href="https://github.com/apache/arrow/issues/34837">GH-34837</a></li>
  <li>Expression: Added support for
<code class="language-plaintext highlighter-rouge">table.slice {|slicer| slicer.column.match_substring(string)</code> and
related shortcuts. <a href="https://github.com/apache/arrow/issues/34819">GH-34819</a> <a href="https://github.com/apache/arrow/issues/34951">GH-34951</a></li>
  <li><em>Breaking change:</em> <code class="language-plaintext highlighter-rouge">Arrow::Table#slice</code> with a filter removes null
records. <a href="https://github.com/apache/arrow/issues/34953">GH-34953</a></li>
</ul>

<h2 id="rust-notes">Rust notes</h2>

<p>The Rust projects have moved to separate repositories outside the
main Arrow monorepo. For notes on the latest release of the Rust
implementation, see the latest <a href="https://github.com/apache/arrow-rs/tags">Arrow Rust changelog</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 12.0.0 release. This covers over 3 months of development work and includes 476 resolved issues with 531 commits from 97 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 11.0.0 release, Wang Mingming, Mustafa Akur and Ruihang Xia have been invited to be committers. Will Jones have joined the Project Management Committee (PMC). Thanks for your contributions and participation in the project! Columnar Format Notes A first “canonical” extension type has been formalized: arrow.fixed_shape_tensor to represent an Array where each slot contains a tensor, with all tensors having the same dimension and shape, GH-33923. This is based on a Fixed-Size List layout as storage array (https://arrow.apache.org/docs/dev/format/CanonicalExtensions.html#fixed-shape-tensor-extension). Arrow Flight RPC notes The JDBC driver for Arrow Flight SQL has had some bugfixes, and has been refactored into a core library (which is not distributed as an uberjar with shaded names) and a driver (which is distributed as an uberjar). The Java server builder API now offers easier access to the underlying gRPC builder. Go now implements the Flight SQL extensions for Substrait and transaction support. Plasma notes Plasma was deprecated since 10.0.0. Plasma is removed in this release. GH-33243 Linux packages notes We dropped support for Ubuntu 18.04 because Ubuntu 18.04 reached EOL. C++ notes Run-End Encoded Arrays have been implemented and are accessible (GH-32104) The FixedShapeTensor Logical value type has been implemented using ExtensionType (GH-15483, GH-34796) Compute New kernel to convert timestamp with timezone to wall time (GH-33143) Cast kernels are now built into libarrow by default (GH-34388) Acero Acero has been moved out of libarrow into it’s own shared library, allowing for smaller builds of the core libarrow (GH-15280) Exec nodes now can have a concept of “ordering” and will reject non-sensible plans (GH-34136) New exec nodes: “pivot_longer” (GH-34266), “order_by” (GH-34248) and “fetch” (GH-34059) Breaking Change: Reorder output fields of “group_by” node so that keys/segment keys come before aggregates (GH-33616) Substrait Add support for the round function GH-33588 Add support for the cast expression element GH-31910 Added API reference documentation GH-34011 Added an extension relation to support segmented aggregation GH-34626 The output of the aggregate relation now conforms to the spec GH-34786 Parquet Added support for DeltaLengthByteArray encoding to the Parquet writer (GH-33024) NaNs are correctly handled now for Parquet predicate push-downs (GH-18481) Added support for reading Parquet page indexes (GH-33596) and writing page indexes (GH-34053) Parquet writer can write columns in parallel now (GH-33655) Fixed incorrect number of rows in Parquet V2 page headers (GH-34086) Fixed incorrect Parquet page null_count when stats are disabled (GH-34326) Added support for reading BloomFilters to the Parquet Reader (GH-34665) Parquet File-writer can now add additional key-value metadata after it has been opened (GH-34888) Breaking Change: The default row group size for the Arrow writer changed from 64Mi rows to 1Mi rows. GH-34280 ORC Added support for the union type in ORC writer (GH-34262) Fixed ORC CHAR type mapping with Arrow (GH-34823) Fixed timestamp type mapping between ORC and arrow (GH-34590) Datasets Added support for reading JSON datasets (GH-33209) Dataset writer now supports specifying a function callback to construct the file name in addition to the existing file name template (GH-34565) Filesystems GcsFileSystem::OpenInputFile avoids unnecessary downloads (GH-34051) Other changes Convenience Append(std::optional...) methods have been added to array builders ([GH-14863](https://github.com/apache/arrow/issues/14863)) A deprecated OpenTelemetry header was removed from the Flight library (GH-34417) Fixed crash in “take” kernels on ExtensionArrays with an underlying dictionary type (GH-34619) Fixed bug where the C-Data bridge did not preserve nullability of map values on import (GH-34983) Added support for EqualOptions to RecordBatch::Equals (GH-34968) zstd dependency upgraded to v1.5.5 (GH-34899) Improved handling of “logical” nulls such as with union and RunEndEncoded arrays (GH-34361) Fixed incorrect handling of uncompressed body buffers in IPC reader, added IpcWriteOptions::min_space_savings for optional compression optimizations (GH-15102) C# notes Support added for importing / exporting schemas and types via the C data interface GH-34737 Support added for the half-float data type GH-25163 Schemas are now allowed to have multiple fields with the same name GH-34076 Added support for reading compressed IPC files GH-32240 Add [] operator to Schema GH-34119 Go notes Run-End Encoded Arrays have been added to the Golang implementation (GH-32104, GH-32946, GH-20407, GH-32949) The SQLite Flight SQL Example has been improved and you can now go get a simple SQLite Flight SQL Server mainprog using go get github.com/apache/arrow/go/v12/arrow/flight/flightsql/example/cmd/sqlite_flightsql_server (GH-33840) Fixed bug causing builds to fail with the noasm build tag (GH-34044) and added a CI test run that uses the noasm tag (GH-34055) Fixed issue allowing building on riscv64-freebsd (GH-34629) Fixed issue preventing building on 32-bit platforms (GH-35133) Arrow Fixed bug in C-Data handling of ArrowArrayStream.get_next when handling uninitialized ArrowArrays (GH-33767) Breaking Change: Added Err() method to RecordReader interface so that it can propagate errors (GH-33789) Fixed potential panic in C-Data API for misusing an invalid handle (GH-33864) A new cgo-based Allocator that does not depend on libarrow has been added to the memory package (GH-33901) CSV Reader and Writer now support Extension type arrays (GH-34334) Fixed bug preventing the reading of IPC streams/files with compression enabled but uncompressed buffers (GH-34385) Added interface which can be added to an ExtensionType to allow Builders to be created via NewBuilder, enabling easy building of nested fields containing extension types (GH-34453) Added utilities to perform Array diffing (GH-34790) Added SetColumn method to arrow.Record (GH-34832) Added GetValue method to arrow.Metadata (GH-34855) Added Pow method to decimal128.Num and decimal256.Num (GH-34863) Flight Fixed bug in StreamChunks for Flight SQL to correctly propagate to the gRPC client (GH-33717) Fixed issue that prevented compatibility with gRPC &lt; v1.45 (GH-33734) Added support to bind a RecordReader for supplying parameters to a Flight SQL Prepared statement (GH-33794) Prepared Statement methods for FlightSQL client now allows gRPC Call Options (GH-33867) FlightSQL Extensions have been implemented (for transactions and Substrait support) (GH-33935) A driver compatible with database/sql for FlightSQL has been added (GH-34332) Compute “run_end_encode” and “run_end_decode” functions added to compute functions (GH-20408) “unique” function added (GH-34171) Parquet pqarrow pkg now handles DICTIONARY fields natively (GH-33466) Fixed rare panic when writing list of 8 structs (GH-33600) Added support for pqarrow pkg to write LargeString and LargeBinary types (GH-33875) Fixed bug where pqarrow.NewSchemaManifest created the wrong field type for Array Object fields (GH-34101) Added support to Parquet Writer for Extension type Arrays (GH-34330) Java notes Update Java JNI modules to consider Arrow ACERO GH-34862 Ability to register additional GRPC services with FlightServer GH-34778 Allow sending custom headers/properties through Arrow Flight SQL JDBC GH-33874 JavaScript notes No changes. Python notes Compatibility notes: Plasma has been removed in this release (GH-33243). In addition, the deprecated serialization module in PyArrow was also removed (GH-29705). IPC (Inter-Process Communication) functionality of pyarrow or the standard library pickle should be used instead. The deprecated use_async keyword has been removed from the dataset module (GH-30774) Minimum Cython version to build PyArrow from source has been raised to 0.29.31 (GH-34933). In addition, PyArrow can now be compiled using Cython 3 (GH-34564). New features: A new pyarrow.acero module with initial bindings for the Acero execution engine has been added (GH-33976) A new canonical extension type for fixed shaped tensor data has been defined. This is exposed in PyArrow as the FixedShapeTensorType (GH-34882, GH-34956) Run-End Encoded arrays binding has been implemented (GH-34686, GH-34568) Method is_nan has been added to Array, ChunkedArray and Expression (GH-34154) Dataframe interchange protocol has been implemented for RecordBatch (GH-33926) Other improvements: Extension arrays can now be concatenated (GH-31868) get_partition_keys helper function is implemented in the dataset module to access the partitioning field’s key/value from the partition expression of a certain dataset fragment (GH-33825) PyArrow Array objects can now be accepted by the pa.array() constructor (GH-34411) The default row group size when writing parquet files has been changed (GH-34280) RecordBatch has the select() method implemented (GH-34359) New method drop_column on the pyarrow.Table supports passing a single column as a string (GH-33377) User-defined tabular functions, which are a user-functions implemented in Python that return a stateful stream of tabular data, are now also supported (GH-32916) Arrow Archery tool now includes linting of the Cython files (GH-31905) Breaking Change: Reorder output fields of “group_by” node so that keys/segment keys come before aggregates (GH-33616) Relevant bug fixes: Acero can now detect and raise an error in case a join operation needs too much bytes of key data (GH-34474) Fix for converting non-sequence object in pa.array() (GH-34944) Fix erroneous table conversion to pandas if table includes an extension array that does not implement to_pandas_dtype (GH-34906) Reading from a closed ArrayStreamBatchReader now returns invalid status instead of segfaulting (GH-34165) array() now returns pyarrow.Array and not pyarrow.ChunkedArray for columns with __arrow_array__ method and only one chunk so that the conversion of pandas dataframe with categorical column of dtype string[pyarrow] does not fail (GH-33727) Custom type mapper in to_pandas now converts index dtypes together with column dtypes (GH-34283) R notes The read_parquet() and read_feather() functions can now accept URL arguments. The json_credentials argument in GcsFileSystem$create() now accepts a file path containing the appropriate authentication token. The $options member of GcsFileSystem objects can now be inspected. The read_csv_arrow() and read_json_arrow() functions now accept literal text input wrapped in I() to improve compatability with readr::read_csv(). Nested fields can now be accessed using $ and [[ in dplyr expressions. For more on what’s in the 12.0.0 R package, see the R changelog. Ruby and C GLib notes Flight SQL: Added support for authentication. GH-34074 Compute: Added GArrowRankOptions. GH-34425 Compute: Added GArrowFilterOptions. GH-34650 Compute: Added GArrowIndexOptions. GH-15286 Compute: Added GArrowMatchSubstringOptions. GH-15285 Added GArrowDenseUnionArrayBuilder. GH-21429 Added GArrowSparseUnionArrayBuilder. GH-21430 Ruby notes Improved Arrow::Table#join API. GH-15287 Flight SQL: Added ArrowFlight::RecordBatchReader#each. GH-15287 Added Arrow::DenseUnionArray#get_value. GH-34837 Added Arrow::SparseUnionArray#get_value. GH-34837 Expression: Added support for table.slice {|slicer| slicer.column.match_substring(string) and related shortcuts. GH-34819 GH-34951 Breaking change: Arrow::Table#slice with a filter removes null records. GH-34953 Rust notes The Rust projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Rust implementation, see the latest Arrow Rust changelog.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>