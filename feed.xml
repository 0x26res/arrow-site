<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://arrow.apache.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arrow.apache.org/" rel="alternate" type="text/html" /><updated>2023-12-20T17:01:11-05:00</updated><id>https://arrow.apache.org/feed.xml</id><title type="html">Apache Arrow</title><subtitle>Apache Arrow is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, C++, C#, Go, Java, JavaScript, MATLAB, Python, R, Ruby, and Rust.</subtitle><entry><title type="html">Apache Arrow 14.0.2 Release</title><link href="https://arrow.apache.org/blog/2023/12/18/14.0.2-release/" rel="alternate" type="text/html" title="Apache Arrow 14.0.2 Release" /><published>2023-12-18T00:00:00-05:00</published><updated>2023-12-18T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/12/18/14.0.2-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/12/18/14.0.2-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 14.0.2 release.
This is mostly a bugfix release that includes <a href="https://github.com/apache/arrow/milestone/58?closed=1"><strong>33 resolved issues</strong></a>
from <a href="/release/14.0.2.html#contributors"><strong>11 distinct contributors</strong></a>. See the Install Page to learn how to
get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Other bugfixes and improvements have been made: we refer
you to the <a href="/release/14.0.2.html#changelog">complete changelog</a>.</p>

<h2 id="c-notes">C++ notes</h2>

<ul>
  <li>S3FileSystem: fix regression in deleting explicitly created sub-directories (<a href="https://github.com/apache/arrow/issues/38618">GH-38618</a>)</li>
</ul>

<h3 id="parquet">Parquet</h3>

<ul>
  <li>Fix performance regression in reading binary/string columns (<a href="https://github.com/apache/arrow/issues/38432">GH-38432</a>)</li>
  <li>Fix regression in reading Parquet files in parallel (<a href="https://github.com/apache/arrow/issues/38432">GH-38432</a>)</li>
  <li>Fix bug in reading valid Parquet files written with older versions (<a href="https://github.com/apache/arrow/issues/38577">GH-38577</a>)</li>
</ul>

<h2 id="python-notes">Python notes</h2>

<ul>
  <li>Delay initializing the S3 filesystem until first usage (instead on import) (<a href="https://github.com/apache/arrow/issues/38364">GH-38364</a>)</li>
  <li>Fix segfault when PyArrow is imported at shutdown (<a href="https://github.com/apache/arrow/issues/38626">GH-38626</a>)</li>
  <li>Fix potential deadlock when CSV reading errors out (<a href="https://github.com/apache/arrow/issues/38676">GH-38676</a>)</li>
</ul>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 14.0.2 release. This is mostly a bugfix release that includes 33 resolved issues from 11 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Other bugfixes and improvements have been made: we refer you to the complete changelog. C++ notes S3FileSystem: fix regression in deleting explicitly created sub-directories (GH-38618) Parquet Fix performance regression in reading binary/string columns (GH-38432) Fix regression in reading Parquet files in parallel (GH-38432) Fix bug in reading valid Parquet files written with older versions (GH-38577) Python notes Delay initializing the S3 filesystem until first usage (instead on import) (GH-38364) Fix segfault when PyArrow is imported at shutdown (GH-38626) Fix potential deadlock when CSV reading errors out (GH-38676)]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 14.0.1 Release</title><link href="https://arrow.apache.org/blog/2023/11/09/14.0.1-release/" rel="alternate" type="text/html" title="Apache Arrow 14.0.1 Release" /><published>2023-11-09T00:00:00-05:00</published><updated>2023-11-09T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/11/09/14.0.1-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/11/09/14.0.1-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 14.0.1 release of Apache Arrow.</p>

<p>This release contains a single security fix for PyArrow. Other implementations
are unchanged.</p>

<p>It is recommended that users of PyArrow upgrade to 14.0.1. Similarly, it is
recommended that downstream libraries upgrade their dependency requirements
to PyArrow 14.0.1 or later. Binary packages are available on PyPI and conda-forge.</p>

<p>If it is not possible to upgrade, we provide a separate package <code class="language-plaintext highlighter-rouge">pyarrow-hotfix</code>
that disables the vulnerability on older PyArrow versions.
See <a href="https://pypi.org/project/pyarrow-hotfix/">usage instructions</a>.</p>

<h2 id="vulnerability-description">Vulnerability description</h2>

<p>Deserialization of untrusted data in IPC and Parquet readers in PyArrow versions
0.14.0 to 14.0.0 allows arbitrary code execution. An application is vulnerable
if it reads Arrow IPC, Feather or Parquet data from untrusted sources
(for example user-supplied input files).</p>

<p>This vulnerability only affects PyArrow, not other Apache Arrow implementations or bindings.</p>

<p>Reference: <a href="https://www.cve.org/CVERecord?id=CVE-2023-47248">CVE-2023-47248</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 14.0.1 release of Apache Arrow. This release contains a single security fix for PyArrow. Other implementations are unchanged. It is recommended that users of PyArrow upgrade to 14.0.1. Similarly, it is recommended that downstream libraries upgrade their dependency requirements to PyArrow 14.0.1 or later. Binary packages are available on PyPI and conda-forge. If it is not possible to upgrade, we provide a separate package pyarrow-hotfix that disables the vulnerability on older PyArrow versions. See usage instructions. Vulnerability description Deserialization of untrusted data in IPC and Parquet readers in PyArrow versions 0.14.0 to 14.0.0 allows arbitrary code execution. An application is vulnerable if it reads Arrow IPC, Feather or Parquet data from untrusted sources (for example user-supplied input files). This vulnerability only affects PyArrow, not other Apache Arrow implementations or bindings. Reference: CVE-2023-47248.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 0.8.0 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/11/09/adbc-0.8.0-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.8.0 (Libraries) Release" /><published>2023-11-09T00:00:00-05:00</published><updated>2023-11-09T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2023/11/09/adbc-0.8.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/11/09/adbc-0.8.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.8.0 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/12"><strong>43
resolved issues</strong></a> from <a href="#contributors"><strong>12 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version
0.8.0.  The <strong>API specification</strong> is versioned separately and is
at version 1.1.0.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.8.0/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>C#/.NET has added a BigQuery driver. Also, it now can be used through ADO.NET.</p>

<p>The R bindings now use ADBC 1.1.0, and more packages are now available on
CRAN.</p>

<p>The Snowflake driver now has an option to control whether to return decimal
types or attempt to convert to integers/floats.  The PostgreSQL driver now
uses COPY for bulk ingestion, which Pandas has found is <a href="https://github.com/pandas-dev/pandas/pull/53869#issuecomment-1771657930">approximately 35x
faster</a> than their previous method.  The SQLite driver can now
load <a href="https://www.sqlite.org/loadext.html">extensions</a> and supports more data types, including
Arrow binary (SQL BLOB), and supports binding some dictionary-encoded types
(which will be unpacked).</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.7.0..apache-arrow-adbc-0.8.0
    27	David Li
    23	William Ayd
    19	Dewey Dunnington
     4	Matt Topol
     4	Solomon Choe
     4	davidhcoe
     2	vleslief-ms
     1	Aaron Ross
     1	Fredrik Hoem Grelland
     1	Joel Lubinitsky
     1	OleMussmann
     1	Ruoxuan Wang
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>Work continues on improving the existing drivers in terms of supported
Arrow/database types, performance, and so on.</p>

<p>For Java, the minimum JDK version will stay on JDK 8 as long as the Arrow Java
libraries remain on that version.</p>

<p>There are currently no plans for a second API revision. As work progresses
on asynchronous and device-aware APIs in the Arrow ecosystem, ADBC will
eventually be updated to support any new APIs.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.8.0 release of the Apache Arrow ADBC libraries. This covers includes 43 resolved issues from 12 distinct contributors. This is a release of the libraries, which are at version 0.8.0. The API specification is versioned separately and is at version 1.1.0. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights C#/.NET has added a BigQuery driver. Also, it now can be used through ADO.NET. The R bindings now use ADBC 1.1.0, and more packages are now available on CRAN. The Snowflake driver now has an option to control whether to return decimal types or attempt to convert to integers/floats. The PostgreSQL driver now uses COPY for bulk ingestion, which Pandas has found is approximately 35x faster than their previous method. The SQLite driver can now load extensions and supports more data types, including Arrow binary (SQL BLOB), and supports binding some dictionary-encoded types (which will be unpacked). Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.7.0..apache-arrow-adbc-0.8.0 27 David Li 23 William Ayd 19 Dewey Dunnington 4 Matt Topol 4 Solomon Choe 4 davidhcoe 2 vleslief-ms 1 Aaron Ross 1 Fredrik Hoem Grelland 1 Joel Lubinitsky 1 OleMussmann 1 Ruoxuan Wang Roadmap Work continues on improving the existing drivers in terms of supported Arrow/database types, performance, and so on. For Java, the minimum JDK version will stay on JDK 8 as long as the Arrow Java libraries remain on that version. There are currently no plans for a second API revision. As work progresses on asynchronous and device-aware APIs in the Arrow ecosystem, ADBC will eventually be updated to support any new APIs. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 14.0.0 Release</title><link href="https://arrow.apache.org/blog/2023/11/01/14.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 14.0.0 Release" /><published>2023-11-01T00:00:00-04:00</published><updated>2023-11-01T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/11/01/14.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/11/01/14.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 14.0.0 release. This covers
over 3 months of development work and includes <a href="https://github.com/apache/arrow/milestone/55?closed=1"><strong>483 resolved issues</strong></a>
from <a href="/release/14.0.0.html#contributors"><strong>116 distinct contributors</strong></a>. See the <a href="https://arrow.apache.org/install/">Install Page</a>
to learn how to get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/14.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 13.0.0 release, Metehan Yildirim and Oleks V. have been invited to be committers.</p>

<p>Thanks for your contributions and participation in the project!</p>

<h2 id="columnar-format-notes">Columnar Format Notes</h2>

<p>Motivated by recent innovations in DuckDB and Meta’s Velox engine, new “view” data types were added to the Arrow columnar format spec:</p>

<ul>
  <li>16-byte StringView and BinaryView data types which enable better buffer reuse, faster “false” string comparisons (due to maintaining a prefix) and short string inlining (<a href="https://github.com/apache/arrow/issues/35627">GH-35627</a>).</li>
  <li>ListView and LargeListView types for more performant “out-of-order” building and processing of lists and better buffer reuse (<a href="https://github.com/apache/arrow/issues/37876">GH-37876</a>).</li>
</ul>

<p>A <code class="language-plaintext highlighter-rouge">VariableShapeTensorType</code> was added to the Arrow specification as a canonical extension type (<a href="https://github.com/apache/arrow/issues/24868">GH-24868</a>).</p>

<h2 id="c-data-interface-notes">C Data Interface notes</h2>

<p>Integration testing has been added for the C Data Interface accross Arrow implementations,
ensuring mutual compatibility. (<a href="https://github.com/apache/arrow/issues/37537">GH-37537</a>).
The C++, C# and Go implementations are covered, with Arrow Java soon to come.</p>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>

<p>A new RPC method was added to allow polling for completion in long-running queries as an alternative to the blocking GetFlightInfo call (<a href="https://github.com/apache/arrow/issues/36155">GH-36155</a>). Also, <code class="language-plaintext highlighter-rouge">app_metadata</code> was added to <code class="language-plaintext highlighter-rouge">FlightInfo</code> and <code class="language-plaintext highlighter-rouge">FlightEndpoint</code> (<a href="https://github.com/apache/arrow/issues/37635">GH-37635</a>).</p>

<p>In C++ and Python, an experimental asynchronous GetFlightInfo call was added to the client-side API (<a href="https://github.com/apache/arrow/issues/36512">GH-36512</a>). <code class="language-plaintext highlighter-rouge">ServerCallContext</code> now exposes conveniences to send headers/trailers without having to use middleware (<a href="https://github.com/apache/arrow/issues/36952">GH-36952</a>). The implementation was fixed to not reject unknown field tags to enable interoperability with future versions of Flight that could add new fields (<a href="https://github.com/apache/arrow/issues/36975">GH-36975</a>). The CMake configuration was fixed to correctly require linking to Arrow Flight RPC when using Arrow Flight SQL (<a href="https://github.com/apache/arrow/issues/37406">GH-37406</a>).</p>

<p>In Go, the underlying generated Protobuf code is now exposed for easier low-level integrations with Flight (<a href="https://github.com/apache/arrow/issues/36893">GH-36893</a>).</p>

<p>In Java, the stateful “login” authentication APIs using the Handshake RPC are deprecated; it will not be removed, but it should not be used unless you specifically want the old behavior (<a href="https://github.com/apache/arrow/issues/37722">GH-37722</a>). Utilities were added to help implement basic Flight SQL services for unit testing (<a href="https://github.com/apache/arrow/issues/37795">GH-37795</a>).</p>

<h2 id="c-notes">C++ notes</h2>

<p>Experimental APIs for exporting and importing non-CPU arrays using the C Device Data Interface
have been added (<a href="https://github.com/apache/arrow/issues/36488">GH-36488</a>), together with an experimental API for device synchronization
(<a href="https://github.com/apache/arrow/issues/36103">GH-36103</a>).</p>

<p>Initial compatibility with Emscripten without threading support has been added (<a href="https://github.com/apache/arrow/issues/35176">GH-35176</a>).</p>

<h3 id="compute-layer">Compute layer</h3>

<p>New compute functions:</p>
<ul>
  <li>a <code class="language-plaintext highlighter-rouge">cumulative_mean</code> function on numeric data (<a href="https://github.com/apache/arrow/issues/36931">GH-36931</a>);</li>
</ul>

<p>Improved compute functions:</p>
<ul>
  <li>rounding functions now work natively on integer inputs instead of casting them to floats (<a href="https://github.com/apache/arrow/issues/35273">GH-35273</a>);</li>
  <li>the <code class="language-plaintext highlighter-rouge">divide</code> function now supports duration inputs (<a href="https://github.com/apache/arrow/issues/36789">GH-36789</a>);</li>
  <li><code class="language-plaintext highlighter-rouge">take</code> and <code class="language-plaintext highlighter-rouge">filter</code> now support sparse unions in addition to dense unions (<a href="https://github.com/apache/arrow/issues/36905">GH-36905</a>);</li>
  <li><code class="language-plaintext highlighter-rouge">if_else</code>, <code class="language-plaintext highlighter-rouge">coalesce</code>, <code class="language-plaintext highlighter-rouge">choose</code> and <code class="language-plaintext highlighter-rouge">case_when</code> now support duration inputs (<a href="https://github.com/apache/arrow/issues/37028">GH-37028</a>);</li>
  <li>casting between fixed-size lists and variable-size lists is now supported (<a href="https://github.com/apache/arrow/issues/20086">GH-20086</a>);</li>
  <li>casting from strings to dates is now supported (<a href="https://github.com/apache/arrow/issues/37411">GH-37411</a>);</li>
  <li><code class="language-plaintext highlighter-rouge">mean</code> on integer inputs now uses a floating-point representation for its intermediate sum,
avoiding integer overflow on large inputs (<a href="https://github.com/apache/arrow/issues/34909">GH-34909</a>);</li>
</ul>

<h3 id="datasets">Datasets</h3>

<p>Support for writing encrypted Parquet datasets has been added (<a href="https://github.com/apache/arrow/issues/29238">GH-29238</a>).</p>

<h3 id="gandiva">Gandiva</h3>

<p>Gandiva now supports linking dynamically to LLVM on non-Windows platforms (<a href="https://github.com/apache/arrow/issues/37410">GH-37410</a>).
Previously, Gandiva would always link LLVM statically into <code class="language-plaintext highlighter-rouge">libgandiva</code>.</p>

<h3 id="parquet">Parquet</h3>

<p>RLE is used by default when encoding boolean values if v2 data pages are enabled
(<a href="https://github.com/apache/arrow/issues/36882">GH-36882</a>).</p>

<p>Page indexes can now be encrypted as per the specification (<a href="https://github.com/apache/arrow/issues/34950">GH-34950</a>).</p>

<p>A bug in the DELTA_BINARY_PACKED encoder leading to suboptimal column sizes was fixed (<a href="https://github.com/apache/arrow/issues/37939">GH-37939</a>).</p>

<h3 id="substrait">Substrait</h3>

<p>It is now possible to serialize and deserialize individual expressions using Substrait,
not only full query plans (<a href="https://github.com/apache/arrow/issues/33985">GH-33985</a>).</p>

<h3 id="miscellaneous">Miscellaneous</h3>

<p>A new <code class="language-plaintext highlighter-rouge">CodecOptions</code> class allows customizing compression parameters per-codec (<a href="https://github.com/apache/arrow/issues/35287">GH-35287</a>).</p>

<p>The environment variable <code class="language-plaintext highlighter-rouge">AWS_ENDPOINT_URL</code> is now respected when resolving S3 URIs (<a href="https://github.com/apache/arrow/issues/36770">GH-36770</a>).</p>

<p>Recursively listing S3 filesystem trees should now issue less requests,
leading to improved performance (<a href="https://github.com/apache/arrow/issues/34213">GH-34213</a>).</p>

<p>Comparing a <code class="language-plaintext highlighter-rouge">ChunkedArray</code> to itself now behaves correctly with NaN values (<a href="https://github.com/apache/arrow/issues/37515">GH-37515</a>).</p>

<p>The use of BMI2 instructions on x86 was incorrectly guarded. Those instructions
could be executed on platforms without BMI2 support, leading to crashes (<a href="https://github.com/apache/arrow/issues/37017">GH-37017</a>).</p>

<h2 id="c-notes-1">C# notes</h2>

<p>The following features have been added to the C# implementation apart from other minor ones and some fixes.</p>

<ul>
  <li>Support fixed-size lists (<a href="https://github.com/apache/arrow/issues/33032">GH-33032</a>)</li>
  <li>Support DateOnly and TimeOnly on .NET 6.0+ (<a href="https://github.com/apache/arrow/issues/34620">GH-34620</a>)</li>
  <li>Implement MapType (<a href="https://github.com/apache/arrow/issues/35243">GH-35243</a>)</li>
  <li>Flight SQL implementation for C# (<a href="https://github.com/apache/arrow/issues/36078">GH-36078</a>)</li>
  <li>Implement support for dense and sparse unions (<a href="https://github.com/apache/arrow/issues/36795">GH-36795</a>)</li>
</ul>

<h2 id="go-notes">Go Notes</h2>

<ul>
  <li>The minimum version of Go officially supported is now <code class="language-plaintext highlighter-rouge">go1.19</code> instead of <code class="language-plaintext highlighter-rouge">go1.17</code> (<a href="https://github.com/apache/arrow/issues/37636">GH-37636</a>)</li>
</ul>

<h3 id="bug-fixes">Bug Fixes</h3>

<h4 id="arrow">Arrow</h4>

<ul>
  <li>Documentation fixed to correctly state that the default unit for <code class="language-plaintext highlighter-rouge">TimestampType</code> is seconds (<a href="https://github.com/apache/arrow/issues/35770">GH-35770</a>)</li>
  <li>Fixed leak in the <code class="language-plaintext highlighter-rouge">Concatenate</code> function if there is a panic that is recovered (<a href="https://github.com/apache/arrow/issues/36850">GH-36850</a>)</li>
  <li>Ensure Binary dictionary indices are released on panic (<a href="https://github.com/apache/arrow/issues/36858">GH-36858</a>)</li>
  <li>Fix overflow value causing invalid dates for <code class="language-plaintext highlighter-rouge">MarshalJSON</code> on some timestamps (<a href="https://github.com/apache/arrow/issues/36935">GH-36935</a>)</li>
  <li>Fix leaking dictionary allocations in IPC reader (<a href="https://github.com/apache/arrow/issues/36981">GH-36981</a>)</li>
</ul>

<h4 id="parquet-1">Parquet</h4>

<ul>
  <li>Fixed an issue where DeltaLengthByteArray encoding fails on certain null value scenarios (<a href="https://github.com/apache/arrow/issues/36318">GH-36318</a>)</li>
  <li>Correctly propagate internal <code class="language-plaintext highlighter-rouge">writer.sink.Close()</code> errors from <code class="language-plaintext highlighter-rouge">writer.Close()</code> when writing a Parquet file (<a href="https://github.com/apache/arrow/issues/36645">GH-36645</a>)</li>
  <li>Fixed a panic when writing some specific DeltaBitPacked datasets (<a href="https://github.com/apache/arrow/issues/37102">GH-37102</a>)</li>
  <li>Proper support for Decimal256 data type in Parquet lib (<a href="https://github.com/apache/arrow/issues/37419">GH-37419</a>)</li>
  <li>Corrected inconsistent behavior in <code class="language-plaintext highlighter-rouge">pqarrow</code> column chunk reader (<a href="https://github.com/apache/arrow/issues/37845">GH-37845</a>)</li>
  <li>Rewrote and Fixed ARM64 assembly for bitmap bit extractions and integer packing (<a href="https://github.com/apache/arrow/issues/37712">GH-37712</a>)</li>
</ul>

<h3 id="enhancements">Enhancements</h3>

<ul>
  <li>C Data Interface integration testing has been added and implemented (<a href="https://github.com/apache/arrow/issues/37789">GH-37789</a>)</li>
  <li>pkg.go.dev link is fixed in the Readme (<a href="https://github.com/apache/arrow/issues/37779">GH-37779</a>)</li>
</ul>

<h4 id="arrow-1">Arrow</h4>

<ul>
  <li>Added <code class="language-plaintext highlighter-rouge">String()</code> method to <code class="language-plaintext highlighter-rouge">arrow.Table</code> (<a href="https://github.com/apache/arrow/issues/35296">GH-35296</a>)</li>
  <li>Add proper <code class="language-plaintext highlighter-rouge">array.Null</code> type support handling for arrow/csv writing (<a href="https://github.com/apache/arrow/issues/36623">GH-36623</a>)</li>
  <li>Optimized <code class="language-plaintext highlighter-rouge">GetOrInsert</code> function for memo table handling of dictionary builders (<a href="https://github.com/apache/arrow/issues/36671">GH-36671</a>)</li>
  <li>Made it possible to add custom functions in the <code class="language-plaintext highlighter-rouge">compute</code> package (<a href="https://github.com/apache/arrow/issues/36936">GH-36936</a>)</li>
  <li>Improved performance of dictionary unifier (<a href="https://github.com/apache/arrow/issues/37306">GH-37306</a>)</li>
  <li>Added direct access to dictionary builder indices (<a href="https://github.com/apache/arrow/issues/37416">GH-37416</a>)</li>
  <li>Added ability to read back values from Boolean builders (<a href="https://github.com/apache/arrow/issues/37465">GH-37465</a>)</li>
  <li>Add <code class="language-plaintext highlighter-rouge">ValueLen</code> function to string array (<a href="https://github.com/apache/arrow/issues/37584">GH-37584</a>)</li>
  <li>Avoid unnecessary copying in the default go allocator (<a href="https://github.com/apache/arrow/issues/37687">GH-37687</a>)</li>
  <li>Add <code class="language-plaintext highlighter-rouge">SetNull(i int)</code> to array builders (<a href="https://github.com/apache/arrow/issues/37694">GH-37694</a>)</li>
</ul>

<h4 id="parquet-2">Parquet</h4>

<ul>
  <li>Parquet metadata is allowed to write metadata after writing rowgroups using <code class="language-plaintext highlighter-rouge">pqarrow.FileWriter</code> (<a href="https://github.com/apache/arrow/issues/35775">GH-35775</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">MapOf</code> and <code class="language-plaintext highlighter-rouge">ListOf</code> helper functions have been improved to provide clearer error messages and have better documentation (<a href="https://github.com/apache/arrow/issues/36696">GH-36696</a>)</li>
  <li>Struct tag of <code class="language-plaintext highlighter-rouge">parquet:"-"</code> will be allowed to skip fields when converting a struct to a parquet schema (<a href="https://github.com/apache/arrow/issues/36793">GH-36793</a>)</li>
</ul>

<h2 id="java-notes">Java notes</h2>

<p>Java 21 is enabled and validated in CI (<a href="https://github.com/apache/arrow/issues/37914">GH-37914</a>).</p>

<p>The Gandiva module implemented a breaking change by moving <code class="language-plaintext highlighter-rouge">Types.proto</code> into a subfolder (<a href="https://github.com/apache/arrow/issues/37893">GH-37893</a>).</p>

<p><code class="language-plaintext highlighter-rouge">DefaultVectorComparators</code> added support for <code class="language-plaintext highlighter-rouge">LargeVarCharVector</code>, <code class="language-plaintext highlighter-rouge">LargeVarBinaryVector</code> (<a href="https://github.com/apache/arrow/issues/25659">GH-25659</a>) and for <code class="language-plaintext highlighter-rouge">BitVector</code>, <code class="language-plaintext highlighter-rouge">DateDayVector</code>, <code class="language-plaintext highlighter-rouge">DateMilliVector</code>
<code class="language-plaintext highlighter-rouge">Decimal256Vector</code>, <code class="language-plaintext highlighter-rouge">DecimalVector</code>, <code class="language-plaintext highlighter-rouge">DurationVector</code>, <code class="language-plaintext highlighter-rouge">IntervalDayVector</code>, <code class="language-plaintext highlighter-rouge">TimeMicroVector</code>, <code class="language-plaintext highlighter-rouge">TimeMilliVector</code>, <code class="language-plaintext highlighter-rouge">TimeNanoVector</code>, <code class="language-plaintext highlighter-rouge">TimeSecVector</code>, <code class="language-plaintext highlighter-rouge">TimeStampVector</code> (<a href="https://github.com/apache/arrow/issues/37701">GH-37701</a>).</p>

<p>A bug was fixed in <code class="language-plaintext highlighter-rouge">VectorAppender</code> to prevent resizing the data buffer twice when appending variable-length vectors (<a href="https://github.com/apache/arrow/issues/37829">GH-37829</a>).</p>

<p><code class="language-plaintext highlighter-rouge">VarCharWriter</code> added support for writing from <code class="language-plaintext highlighter-rouge">Text</code> and <code class="language-plaintext highlighter-rouge">String</code> (<a href="https://github.com/apache/arrow/issues/37706">GH-37706</a>). <code class="language-plaintext highlighter-rouge">VarBinaryWriter</code> added support for writing from <code class="language-plaintext highlighter-rouge">byte[]</code> and <code class="language-plaintext highlighter-rouge">ByteBuffer</code> (<a href="https://github.com/apache/arrow/issues/37705">GH-37705</a>).</p>

<p>The JDBC driver will now ignore username and password authentication if a token is provided (<a href="https://github.com/apache/arrow/issues/37073">GH-37073</a>).</p>

<p>A bug was fixed in the Java C-Data interface when importing a vector with an empty array (<a href="https://github.com/apache/arrow/issues/37056">GH-37056</a>).</p>

<p>A bug was fixed in the S3 file system implementation when closing the connection (<a href="https://github.com/apache/arrow/issues/36069">GH-36069</a>).</p>

<p>Arrow datasets now support Substrait <code class="language-plaintext highlighter-rouge">ExtendedExpression</code>s as inputs to filter and project operations (<a href="https://github.com/apache/arrow/issues/34252">GH-34252</a>).</p>

<h2 id="javascript-notes">JavaScript notes</h2>

<ul>
  <li>GH-21815: [JS] Add support for Duration type #37341</li>
  <li>GH-31621: [JS] Fix Union null bitmaps #37122</li>
</ul>

<h2 id="python-notes">Python notes</h2>

<p>Compatibility notes:</p>
<ul>
  <li>Support for Python 3.12 was added (<a href="https://github.com/apache/arrow/issues/37880">GH-37880</a>)</li>
  <li>Support for Cython 3 was added (<a href="https://github.com/apache/arrow/issues/37742">GH-37742</a>)</li>
  <li>PyArrow is now compatible with numpy 2.0 (<a href="https://github.com/apache/arrow/issues/37574">GH-37574</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">pyarrow.compute.CumulativeSumOptions</code> has been deprecated, use <code class="language-plaintext highlighter-rouge">pyarrow.compute.CumulativeOptions</code> instead (<a href="https://github.com/apache/arrow/issues/36240">GH-36240</a>)</li>
</ul>

<p>New features:</p>
<ul>
  <li>Allowing type promotion is now possible in <code class="language-plaintext highlighter-rouge">pyarrow.concat_tables</code> (<a href="https://github.com/apache/arrow/issues/36845">GH-36845</a>)</li>
  <li>Support for vector function UDF was added (<a href="https://github.com/apache/arrow/issues/36672">GH-36672</a>)</li>
</ul>

<p>Other improvements:</p>
<ul>
  <li>The default of <code class="language-plaintext highlighter-rouge">pre_buffer</code> is now set to <code class="language-plaintext highlighter-rouge">True</code> for reading Parquet when using <code class="language-plaintext highlighter-rouge">pyarrow.dataset</code> directly. This can give significant speed-up on filesystems like S3 and is now aligned to <code class="language-plaintext highlighter-rouge">pyarrow.parquet.read_table</code> interface (<a href="https://github.com/apache/arrow/issues/36765">GH-36765</a>)</li>
  <li>Path to timezone database can now be set through python API (<a href="https://github.com/apache/arrow/issues/35600">GH-35600</a>, [GH-38145] (https://github.com/apache/arrow/issues/38145))</li>
  <li><code class="language-plaintext highlighter-rouge">pyarrow.MapScalar.as_py</code>can now be called with custom field name (<a href="https://github.com/apache/arrow/issues/36809">GH-36809</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">FixedShapeTensorType</code> string representation now prints the type parameters (<a href="https://github.com/apache/arrow/issues/35623">GH-35623</a>)</li>
</ul>

<p>Relevant bug fixes:</p>
<ul>
  <li>String to date cast kernel was added to fix python scalar cast regression (<a href="https://github.com/apache/arrow/issues/37411">GH-37411</a>)</li>
  <li>Fix conversion from Python to Arrow when chunking large nested structs (<a href="https://github.com/apache/arrow/issues/32439">GH-32439</a>)</li>
  <li>Fix segfault when passing table as argument to <code class="language-plaintext highlighter-rouge">pyarrow.Table.filter</code> (<a href="https://github.com/apache/arrow/issues/37650">GH-37650</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">use_threads</code> keyword was added to the <code class="language-plaintext highlighter-rouge">group_by</code> method on <code class="language-plaintext highlighter-rouge">pyarrow.Table</code> which gets passed through to the <code class="language-plaintext highlighter-rouge">pyarrow.acero.Declaration.to_table</code> call. Specifing <code class="language-plaintext highlighter-rouge">use_threads=False</code>allows to get stable ordering of the output (<a href="https://github.com/apache/arrow/issues/36709">GH-36709</a>)</li>
  <li>Fix printable representation for <code class="language-plaintext highlighter-rouge">pyarrow.TimestampScalar</code> when values are outside datetime range (<a href="https://github.com/apache/arrow/issues/36323">GH-36323</a>)</li>
  <li>Empty dataframes with zero chunks can now be consumed by the Dataframe Interchange Protocol implementation (<a href="https://github.com/apache/arrow/issues/37050">GH-37050</a>)</li>
  <li>Fix dtype information for categorical columns in the Dataframe Interchange Protocol implementation (<a href="https://github.com/apache/arrow/issues/38034">GH-38034</a>)</li>
  <li>Boolean columns with bitsize 1 are now supported in <code class="language-plaintext highlighter-rouge">from_dataframe</code>of the Dataframe Interchange Protocol (<a href="https://github.com/apache/arrow/issues/37145">GH-37145</a>)</li>
</ul>

<p>Further, the Python bindings benefit from improvements in the C++ library
(e.g. new compute functions); see the C++ notes above for additional details.</p>

<p>The Arrow documentation is now built with an updated Pydata Sphinx Theme which includes light/dark theme, new colors from <a href="https://github.com/Quansight-Labs/accessible-pygments">Accessible pygments themes</a>, version switcher dropdown, search button, etc. (<a href="https://github.com/apache/arrow/issues/36590">GH-36590</a>, <a href="https://github.com/apache/arrow/issues/32451">GH-32451</a>)</p>

<h2 id="r-notes">R notes</h2>

<p>This release of the R package features a substantial refactor of the package configuration, build, and installation. This change should be transparent to most users; however, package contributors can take advantage of a substantially simplified development setup: in most cases, package contributors should be able to use a pre-built nightly version of Arrow C++ in place of a local Arrow development setup. Special thanks to Jacob Wujciak-Jens for taking on this incredible refactor!</p>

<p>In addition to a number of bugfixes and improvements, this release includes several new features related to CSV input/output:</p>

<ul>
  <li>Added support for <code class="language-plaintext highlighter-rouge">,</code> or other characters as a decimal point.</li>
  <li>Added <code class="language-plaintext highlighter-rouge">write_csv_dataset()</code> to better document CSV-specific dataset writing options.</li>
  <li>Ensured that the <code class="language-plaintext highlighter-rouge">schema</code> argument can be specified when reading a CSV dataset with partitions.</li>
</ul>

<p>For more on what’s in the 14.0.0 R package, see the <a href="/docs/r/news/">R changelog</a>.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<h3 id="ruby">Ruby</h3>

<ul>
  <li>Add support for prepared INSERT queries (<a href="https://github.com/apache/arrow/issues/37143">GH-37143</a>)</li>
  <li>When a prepared statement is automatically closed upon exiting a block, use the same options as when the statement was prepared (<a href="https://github.com/apache/arrow/issues/37257">GH-37257</a>)</li>
</ul>

<h3 id="c-glib">C GLib</h3>

<ul>
  <li>Support more properties of <code class="language-plaintext highlighter-rouge">ArrowFlight::ClientOptions</code> (<a href="https://github.com/apache/arrow/issues/37141">GH-37141</a>)</li>
  <li>Add support for prepared INSERT queries (<a href="https://github.com/apache/arrow/issues/37143">GH-37143</a>)</li>
</ul>

<h2 id="rust-notes">Rust notes</h2>

<p>The Rust projects have moved to separate repositories outside the
main Arrow monorepo. For notes on the latest release of the Rust
implementation, see the latest <a href="https://github.com/apache/arrow-rs/tags">Arrow Rust changelog</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 14.0.0 release. This covers over 3 months of development work and includes 483 resolved issues from 116 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 13.0.0 release, Metehan Yildirim and Oleks V. have been invited to be committers. Thanks for your contributions and participation in the project! Columnar Format Notes Motivated by recent innovations in DuckDB and Meta’s Velox engine, new “view” data types were added to the Arrow columnar format spec: 16-byte StringView and BinaryView data types which enable better buffer reuse, faster “false” string comparisons (due to maintaining a prefix) and short string inlining (GH-35627). ListView and LargeListView types for more performant “out-of-order” building and processing of lists and better buffer reuse (GH-37876). A VariableShapeTensorType was added to the Arrow specification as a canonical extension type (GH-24868). C Data Interface notes Integration testing has been added for the C Data Interface accross Arrow implementations, ensuring mutual compatibility. (GH-37537). The C++, C# and Go implementations are covered, with Arrow Java soon to come. Arrow Flight RPC notes A new RPC method was added to allow polling for completion in long-running queries as an alternative to the blocking GetFlightInfo call (GH-36155). Also, app_metadata was added to FlightInfo and FlightEndpoint (GH-37635). In C++ and Python, an experimental asynchronous GetFlightInfo call was added to the client-side API (GH-36512). ServerCallContext now exposes conveniences to send headers/trailers without having to use middleware (GH-36952). The implementation was fixed to not reject unknown field tags to enable interoperability with future versions of Flight that could add new fields (GH-36975). The CMake configuration was fixed to correctly require linking to Arrow Flight RPC when using Arrow Flight SQL (GH-37406). In Go, the underlying generated Protobuf code is now exposed for easier low-level integrations with Flight (GH-36893). In Java, the stateful “login” authentication APIs using the Handshake RPC are deprecated; it will not be removed, but it should not be used unless you specifically want the old behavior (GH-37722). Utilities were added to help implement basic Flight SQL services for unit testing (GH-37795). C++ notes Experimental APIs for exporting and importing non-CPU arrays using the C Device Data Interface have been added (GH-36488), together with an experimental API for device synchronization (GH-36103). Initial compatibility with Emscripten without threading support has been added (GH-35176). Compute layer New compute functions: a cumulative_mean function on numeric data (GH-36931); Improved compute functions: rounding functions now work natively on integer inputs instead of casting them to floats (GH-35273); the divide function now supports duration inputs (GH-36789); take and filter now support sparse unions in addition to dense unions (GH-36905); if_else, coalesce, choose and case_when now support duration inputs (GH-37028); casting between fixed-size lists and variable-size lists is now supported (GH-20086); casting from strings to dates is now supported (GH-37411); mean on integer inputs now uses a floating-point representation for its intermediate sum, avoiding integer overflow on large inputs (GH-34909); Datasets Support for writing encrypted Parquet datasets has been added (GH-29238). Gandiva Gandiva now supports linking dynamically to LLVM on non-Windows platforms (GH-37410). Previously, Gandiva would always link LLVM statically into libgandiva. Parquet RLE is used by default when encoding boolean values if v2 data pages are enabled (GH-36882). Page indexes can now be encrypted as per the specification (GH-34950). A bug in the DELTA_BINARY_PACKED encoder leading to suboptimal column sizes was fixed (GH-37939). Substrait It is now possible to serialize and deserialize individual expressions using Substrait, not only full query plans (GH-33985). Miscellaneous A new CodecOptions class allows customizing compression parameters per-codec (GH-35287). The environment variable AWS_ENDPOINT_URL is now respected when resolving S3 URIs (GH-36770). Recursively listing S3 filesystem trees should now issue less requests, leading to improved performance (GH-34213). Comparing a ChunkedArray to itself now behaves correctly with NaN values (GH-37515). The use of BMI2 instructions on x86 was incorrectly guarded. Those instructions could be executed on platforms without BMI2 support, leading to crashes (GH-37017). C# notes The following features have been added to the C# implementation apart from other minor ones and some fixes. Support fixed-size lists (GH-33032) Support DateOnly and TimeOnly on .NET 6.0+ (GH-34620) Implement MapType (GH-35243) Flight SQL implementation for C# (GH-36078) Implement support for dense and sparse unions (GH-36795) Go Notes The minimum version of Go officially supported is now go1.19 instead of go1.17 (GH-37636) Bug Fixes Arrow Documentation fixed to correctly state that the default unit for TimestampType is seconds (GH-35770) Fixed leak in the Concatenate function if there is a panic that is recovered (GH-36850) Ensure Binary dictionary indices are released on panic (GH-36858) Fix overflow value causing invalid dates for MarshalJSON on some timestamps (GH-36935) Fix leaking dictionary allocations in IPC reader (GH-36981) Parquet Fixed an issue where DeltaLengthByteArray encoding fails on certain null value scenarios (GH-36318) Correctly propagate internal writer.sink.Close() errors from writer.Close() when writing a Parquet file (GH-36645) Fixed a panic when writing some specific DeltaBitPacked datasets (GH-37102) Proper support for Decimal256 data type in Parquet lib (GH-37419) Corrected inconsistent behavior in pqarrow column chunk reader (GH-37845) Rewrote and Fixed ARM64 assembly for bitmap bit extractions and integer packing (GH-37712) Enhancements C Data Interface integration testing has been added and implemented (GH-37789) pkg.go.dev link is fixed in the Readme (GH-37779) Arrow Added String() method to arrow.Table (GH-35296) Add proper array.Null type support handling for arrow/csv writing (GH-36623) Optimized GetOrInsert function for memo table handling of dictionary builders (GH-36671) Made it possible to add custom functions in the compute package (GH-36936) Improved performance of dictionary unifier (GH-37306) Added direct access to dictionary builder indices (GH-37416) Added ability to read back values from Boolean builders (GH-37465) Add ValueLen function to string array (GH-37584) Avoid unnecessary copying in the default go allocator (GH-37687) Add SetNull(i int) to array builders (GH-37694) Parquet Parquet metadata is allowed to write metadata after writing rowgroups using pqarrow.FileWriter (GH-35775) MapOf and ListOf helper functions have been improved to provide clearer error messages and have better documentation (GH-36696) Struct tag of parquet:"-" will be allowed to skip fields when converting a struct to a parquet schema (GH-36793) Java notes Java 21 is enabled and validated in CI (GH-37914). The Gandiva module implemented a breaking change by moving Types.proto into a subfolder (GH-37893). DefaultVectorComparators added support for LargeVarCharVector, LargeVarBinaryVector (GH-25659) and for BitVector, DateDayVector, DateMilliVector Decimal256Vector, DecimalVector, DurationVector, IntervalDayVector, TimeMicroVector, TimeMilliVector, TimeNanoVector, TimeSecVector, TimeStampVector (GH-37701). A bug was fixed in VectorAppender to prevent resizing the data buffer twice when appending variable-length vectors (GH-37829). VarCharWriter added support for writing from Text and String (GH-37706). VarBinaryWriter added support for writing from byte[] and ByteBuffer (GH-37705). The JDBC driver will now ignore username and password authentication if a token is provided (GH-37073). A bug was fixed in the Java C-Data interface when importing a vector with an empty array (GH-37056). A bug was fixed in the S3 file system implementation when closing the connection (GH-36069). Arrow datasets now support Substrait ExtendedExpressions as inputs to filter and project operations (GH-34252). JavaScript notes GH-21815: [JS] Add support for Duration type #37341 GH-31621: [JS] Fix Union null bitmaps #37122 Python notes Compatibility notes: Support for Python 3.12 was added (GH-37880) Support for Cython 3 was added (GH-37742) PyArrow is now compatible with numpy 2.0 (GH-37574) pyarrow.compute.CumulativeSumOptions has been deprecated, use pyarrow.compute.CumulativeOptions instead (GH-36240) New features: Allowing type promotion is now possible in pyarrow.concat_tables (GH-36845) Support for vector function UDF was added (GH-36672) Other improvements: The default of pre_buffer is now set to True for reading Parquet when using pyarrow.dataset directly. This can give significant speed-up on filesystems like S3 and is now aligned to pyarrow.parquet.read_table interface (GH-36765) Path to timezone database can now be set through python API (GH-35600, [GH-38145] (https://github.com/apache/arrow/issues/38145)) pyarrow.MapScalar.as_pycan now be called with custom field name (GH-36809) FixedShapeTensorType string representation now prints the type parameters (GH-35623) Relevant bug fixes: String to date cast kernel was added to fix python scalar cast regression (GH-37411) Fix conversion from Python to Arrow when chunking large nested structs (GH-32439) Fix segfault when passing table as argument to pyarrow.Table.filter (GH-37650) use_threads keyword was added to the group_by method on pyarrow.Table which gets passed through to the pyarrow.acero.Declaration.to_table call. Specifing use_threads=Falseallows to get stable ordering of the output (GH-36709) Fix printable representation for pyarrow.TimestampScalar when values are outside datetime range (GH-36323) Empty dataframes with zero chunks can now be consumed by the Dataframe Interchange Protocol implementation (GH-37050) Fix dtype information for categorical columns in the Dataframe Interchange Protocol implementation (GH-38034) Boolean columns with bitsize 1 are now supported in from_dataframeof the Dataframe Interchange Protocol (GH-37145) Further, the Python bindings benefit from improvements in the C++ library (e.g. new compute functions); see the C++ notes above for additional details. The Arrow documentation is now built with an updated Pydata Sphinx Theme which includes light/dark theme, new colors from Accessible pygments themes, version switcher dropdown, search button, etc. (GH-36590, GH-32451) R notes This release of the R package features a substantial refactor of the package configuration, build, and installation. This change should be transparent to most users; however, package contributors can take advantage of a substantially simplified development setup: in most cases, package contributors should be able to use a pre-built nightly version of Arrow C++ in place of a local Arrow development setup. Special thanks to Jacob Wujciak-Jens for taking on this incredible refactor! In addition to a number of bugfixes and improvements, this release includes several new features related to CSV input/output: Added support for , or other characters as a decimal point. Added write_csv_dataset() to better document CSV-specific dataset writing options. Ensured that the schema argument can be specified when reading a CSV dataset with partitions. For more on what’s in the 14.0.0 R package, see the R changelog. Ruby and C GLib notes Ruby Add support for prepared INSERT queries (GH-37143) When a prepared statement is automatically closed upon exiting a block, use the same options as when the statement was prepared (GH-37257) C GLib Support more properties of ArrowFlight::ClientOptions (GH-37141) Add support for prepared INSERT queries (GH-37143) Rust notes The Rust projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Rust implementation, see the latest Arrow Rust changelog.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow nanoarrow 0.3.0 Release</title><link href="https://arrow.apache.org/blog/2023/10/03/nanoarrow-0.3.0-release/" rel="alternate" type="text/html" title="Apache Arrow nanoarrow 0.3.0 Release" /><published>2023-10-03T00:00:00-04:00</published><updated>2023-10-03T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/10/03/nanoarrow-0.3.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/10/03/nanoarrow-0.3.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.3.0 release of
Apache Arrow nanoarrow. This release covers 42 resolved issues from
4 contributors.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>See the
<a href="https://github.com/apache/arrow-nanoarrow/blob/apache-arrow-nanoarrow-0.3.0/CHANGELOG.md">Changelog</a>
for a detailed list of contributions to this release.</p>

<h3 id="c-library">C library</h3>

<p>The nanoarrow 0.3.0 release includes a number of bugfixes and improvements
to the core C library and IPC C extension:</p>

<ul>
  <li>Improved bit packing/unpacking utilities and performance in tandem with an
<a href="https://github.com/pandas-dev/pandas/pull/54506">experiment to use bitmasks in pandas</a></li>
  <li>Added support for building and consuming interval arrays</li>
  <li>Fixed full validation of offset buffers for specific types of corrupted data
that caused overflow</li>
  <li>Fixed crashes that occurred for certain types of corrupted data and improved
error messages that were misleading for other types of corrupted data.</li>
</ul>

<h3 id="r-bindings">R bindings</h3>

<p>The nanoarrow R bindings are distributed as the <code class="language-plaintext highlighter-rouge">nanoarrow</code> package on
<a href="https://cran.r-project.org/">CRAN</a>. The 0.3.0 release of the R bindings includes
improvements in type support and stability. Notably:</p>

<ul>
  <li>Conversion to/from <code class="language-plaintext highlighter-rouge">bit64::integer64()</code> is now supported</li>
  <li>Warnings and errors for conversions that are invalid or
may loose accuracy were improved</li>
  <li>Extension types and extension type registration are now supported</li>
  <li>Conversion of dictionary-encoded arrays to R vectors is now supported</li>
  <li>Conversion of map arrays to R vectors is now supported</li>
  <li>Improved performance of stream conversion to R vectors for streams of
indeterminate length (e.g., database results from ADBC).</li>
</ul>

<h3 id="experimental-non-cpu-support">Experimental non-CPU support</h3>

<p>With the addition of the
<a href="https://arrow.apache.org/docs/format/CDeviceDataInterface.html">Arrow C Device data interface</a>
comes an opportunity for nanoarrow to handle non-CPU (e.g., GPU) data. The
<a href="https://github.com/apache/arrow-nanoarrow/tree/main/extensions/nanoarrow_device">nanoarrow_device</a>
extension was drafted in tandem with the specification and Arrow C++ implementation.
While it is not an official part of the 0.3.0 release, it is available in an
experimental state for those interested in its use and/or development.</p>

<h3 id="contributor-experience">Contributor experience</h3>

<p>The 0.3.0 release features adoption of <code class="language-plaintext highlighter-rouge">pre-commit</code> hooks to improve consistency
and contributor experience with a complex repository. Special thanks to
<a href="https://github.com/WillAyd">@WillAyd</a> for setting this up!</p>

<h2 id="contributors">Contributors</h2>

<p>This release consists of contributions from 4 contributors in addition
to the invaluable advice and support of the Apache Arrow developer mailing list.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>git shortlog <span class="nt">-sn</span> d4f038ce58655ba6e996cdae165f1b33c3919d51..apache-arrow-nanoarrow-0.3.0 | <span class="nb">grep</span> <span class="nt">-v</span> <span class="s2">"GitHub Actions"</span>
<span class="go">    40  Dewey Dunnington
     7  William Ayd
     2  Bryce Mecum
     1  Dane Pitkin
</span></code></pre></div></div>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.3.0 release of Apache Arrow nanoarrow. This release covers 42 resolved issues from 4 contributors. Release Highlights See the Changelog for a detailed list of contributions to this release. C library The nanoarrow 0.3.0 release includes a number of bugfixes and improvements to the core C library and IPC C extension: Improved bit packing/unpacking utilities and performance in tandem with an experiment to use bitmasks in pandas Added support for building and consuming interval arrays Fixed full validation of offset buffers for specific types of corrupted data that caused overflow Fixed crashes that occurred for certain types of corrupted data and improved error messages that were misleading for other types of corrupted data. R bindings The nanoarrow R bindings are distributed as the nanoarrow package on CRAN. The 0.3.0 release of the R bindings includes improvements in type support and stability. Notably: Conversion to/from bit64::integer64() is now supported Warnings and errors for conversions that are invalid or may loose accuracy were improved Extension types and extension type registration are now supported Conversion of dictionary-encoded arrays to R vectors is now supported Conversion of map arrays to R vectors is now supported Improved performance of stream conversion to R vectors for streams of indeterminate length (e.g., database results from ADBC). Experimental non-CPU support With the addition of the Arrow C Device data interface comes an opportunity for nanoarrow to handle non-CPU (e.g., GPU) data. The nanoarrow_device extension was drafted in tandem with the specification and Arrow C++ implementation. While it is not an official part of the 0.3.0 release, it is available in an experimental state for those interested in its use and/or development. Contributor experience The 0.3.0 release features adoption of pre-commit hooks to improve consistency and contributor experience with a complex repository. Special thanks to @WillAyd for setting this up! Contributors This release consists of contributions from 4 contributors in addition to the invaluable advice and support of the Apache Arrow developer mailing list. $ git shortlog -sn d4f038ce58655ba6e996cdae165f1b33c3919d51..apache-arrow-nanoarrow-0.3.0 | grep -v "GitHub Actions" 40 Dewey Dunnington 7 William Ayd 2 Bryce Mecum 1 Dane Pitkin]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 0.7.0 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/09/23/adbc-0.7.0-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.7.0 (Libraries) Release" /><published>2023-09-23T00:00:00-04:00</published><updated>2023-09-23T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/09/23/adbc-0.7.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/09/23/adbc-0.7.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.7.0 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/11"><strong>50
resolved issues</strong></a> from <a href="#contributors"><strong>8 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version
0.7.0.  The <strong>API specification</strong> is versioned separately and is
at version 1.1.0.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.7.0/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>This is the first release to implement version 1.1.0 of the ADBC API
specification.  The <a href="https://arrow.apache.org/adbc/current/format/specification.html#version-1-1-0">documentation</a> has a fuller
overview of new features, but in brief:</p>

<ul>
  <li>More common options and info keys were added, and clients can now
retrieve option values instead of only being able to set them.</li>
  <li>Connections/statements can be cancelled.</li>
  <li>Drivers can add arbitrary metadata to errors.  For example, the
Flight SQL driver exposes gRPC status details, and the PostgreSQL
driver provides PostgreSQL-specific error fields.</li>
  <li>Miscellaneous other APIs to cover specific feature gaps (e.g.
getting/setting the ‘active’ catalog and schema).</li>
</ul>

<p>The PostgreSQL and SQLite drivers support ingesting Arrow data into
temporary tables, and targeting a table in a particular namespace.
Also, they handle quoting of table/column names in bulk ingestion
better.  Both drivers also support more Arrow types when executing
queries and ingesting data.</p>

<p>The FlightSQL driver supports basic logging via configuration in
Go code or via an environment variable when built as a shared library
(for Python/R).</p>

<p>The Snowflake driver properly handles <code class="language-plaintext highlighter-rouge">TIME</code> fields.</p>

<p>The minimum Go version was bumped to 1.19 (in line with the next
release of the Arrow Go libraries).</p>

<p>R drivers expose functions to quote/escape names and strings for the
specific backend.</p>

<p>The Python DBAPI layer wraps Arrow record batch readers to raise
ADBC exceptions if the driver provides error metadata (see above).</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.6.0..apache-arrow-adbc-0.7.0
    26	David Li
     7	Dewey Dunnington
     7	William Ayd
     2	ElenaHenderson
     1	Matt Topol
     1	Solomon Choe
     1	Sutou Kouhei
     1	davidhcoe
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>Work continues on improving the existing drivers in terms of supported
Arrow/database types, performance, and so on.</p>

<p>For Java, the minimum JDK version may be raised from JDK 8 to 11 or 17,
a move that the Arrow Java libraries are also considering.</p>

<p>There are currently no plans for a second API revision. As work progresses
on asynchronous and device-aware APIs in the Arrow ecosystem, ADBC will
eventually be updated to support any new APIs.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.7.0 release of the Apache Arrow ADBC libraries. This covers includes 50 resolved issues from 8 distinct contributors. This is a release of the libraries, which are at version 0.7.0. The API specification is versioned separately and is at version 1.1.0. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights This is the first release to implement version 1.1.0 of the ADBC API specification. The documentation has a fuller overview of new features, but in brief: More common options and info keys were added, and clients can now retrieve option values instead of only being able to set them. Connections/statements can be cancelled. Drivers can add arbitrary metadata to errors. For example, the Flight SQL driver exposes gRPC status details, and the PostgreSQL driver provides PostgreSQL-specific error fields. Miscellaneous other APIs to cover specific feature gaps (e.g. getting/setting the ‘active’ catalog and schema). The PostgreSQL and SQLite drivers support ingesting Arrow data into temporary tables, and targeting a table in a particular namespace. Also, they handle quoting of table/column names in bulk ingestion better. Both drivers also support more Arrow types when executing queries and ingesting data. The FlightSQL driver supports basic logging via configuration in Go code or via an environment variable when built as a shared library (for Python/R). The Snowflake driver properly handles TIME fields. The minimum Go version was bumped to 1.19 (in line with the next release of the Arrow Go libraries). R drivers expose functions to quote/escape names and strings for the specific backend. The Python DBAPI layer wraps Arrow record batch readers to raise ADBC exceptions if the driver provides error metadata (see above). Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.6.0..apache-arrow-adbc-0.7.0 26 David Li 7 Dewey Dunnington 7 William Ayd 2 ElenaHenderson 1 Matt Topol 1 Solomon Choe 1 Sutou Kouhei 1 davidhcoe Roadmap Work continues on improving the existing drivers in terms of supported Arrow/database types, performance, and so on. For Java, the minimum JDK version may be raised from JDK 8 to 11 or 17, a move that the Arrow Java libraries are also considering. There are currently no plans for a second API revision. As work progresses on asynchronous and device-aware APIs in the Arrow ecosystem, ADBC will eventually be updated to support any new APIs. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow Flight SQL adapter for PostgreSQL 0.1.0 Release</title><link href="https://arrow.apache.org/blog/2023/09/13/flight-sql-postgresql-0.1.0-release/" rel="alternate" type="text/html" title="Apache Arrow Flight SQL adapter for PostgreSQL 0.1.0 Release" /><published>2023-09-13T00:00:00-04:00</published><updated>2023-09-13T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/09/13/flight-sql-postgresql-0.1.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/09/13/flight-sql-postgresql-0.1.0-release/"><![CDATA[<p>The Apache Arrow team is pleased to announce the 0.1.0 release of
the Apache Arrow Flight SQL adapter for PostgreSQL. This includes
<a href="https://github.com/apache/arrow-flight-sql-postgresql/compare/dc7f34e2636732acd0d015a7cd8259334f1acb16...0.1.0"><strong>60 commits</strong></a> from <a href="#contributors"><strong>1 distinct
contributors</strong></a>.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bug fixes and improvements have
been made: we refer you to <a href="https://arrow.apache.org/flight-sql-postgresql/0.1.0/release-notes.html#version-0-1-0">the complete release notes</a>.</p>

<h2 id="what-is-apache-arrow-flight-sql-adapter-for-postgresql">What is Apache Arrow Flight SQL adapter for PostgreSQL?</h2>

<p>Apache Arrow Flight SQL adapter for PostgreSQL is a PostgreSQL
extension that adds an <a href="https://arrow.apache.org/docs/format/FlightSql.html">Apache Arrow Flight
SQL</a> endpoint to
PostgreSQL.</p>

<p>Apache Arrow Flight SQL is a protocol to use <a href="https://arrow.apache.org/docs/format/Columnar.html">Apache Arrow
format</a> to
interact with SQL databases. You can use Apache Arrow Flight SQL
instead of <a href="https://www.postgresql.org/docs/current/protocol.html">the PostgreSQL wire
protocol</a> to
interact with PostgreSQL by Apache Arrow Flight SQL adapter for
PostgreSQL.</p>

<p>Apache Arrow format is designed for fast typed table data exchange. If
you want to get large data by <code class="language-plaintext highlighter-rouge">SELECT</code> or <code class="language-plaintext highlighter-rouge">INSERT</code>/<code class="language-plaintext highlighter-rouge">UPDATE</code> large
data, Apache Arrow Flight SQL will be faster than the PostgreSQL wire
protocol.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>This is the initial release!</p>

<p>This includes the following basic features. Other features will be
implemented in the future releases.</p>

<ul>
  <li>Literal <code class="language-plaintext highlighter-rouge">SELECT</code>/<code class="language-plaintext highlighter-rouge">INSERT</code>/<code class="language-plaintext highlighter-rouge">UPDATE</code>/<code class="language-plaintext highlighter-rouge">DELETE</code></li>
  <li>Prepared <code class="language-plaintext highlighter-rouge">SELECT</code>/<code class="language-plaintext highlighter-rouge">INSERT</code>/<code class="language-plaintext highlighter-rouge">UPDATE</code>/<code class="language-plaintext highlighter-rouge">DELETE</code></li>
  <li><code class="language-plaintext highlighter-rouge">password</code>/<code class="language-plaintext highlighter-rouge">trust</code> authentications</li>
  <li>TLS connection</li>
  <li>Integer family types</li>
  <li>Floating point family types</li>
  <li>Text family types</li>
  <li>Binary family types</li>
  <li>Timestamp with time zone type</li>
</ul>

<p>Packages for Debian GNU/Linux bookworm and Ubuntu 22.04 are available.</p>

<h2 id="install">Install</h2>

<p>See <a href="https://arrow.apache.org/flight-sql-postgresql/0.1.0/install.html">the install document</a> for details.</p>

<h2 id="contributors">Contributors</h2>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>git shortlog <span class="nt">--perl-regexp</span> <span class="nt">--author</span><span class="o">=</span>^<span class="o">((</span>?!dependabot<span class="se">\[</span>bot<span class="se">\]</span><span class="o">)</span>.<span class="k">*</span><span class="o">)</span><span class="nv">$ </span><span class="nt">-sn</span> dc7f34e2636732acd0d015a7cd8259334f1acb16...0.1.0
<span class="go">    59	Sutou Kouhei
</span></code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<ul>
  <li>Add support for more data types</li>
  <li>Add support for mTLS</li>
  <li>Add support for <a href="https://arrow.apache.org/docs/format/FlightSql.html#sql-metadata">Apache Arrow Flight SQL commands to fetch
metadata</a></li>
  <li>Add support for Apache Arrow Flight SQL transaction related APIs</li>
  <li>Add support for canceling a query</li>
  <li>Add more benchmark results</li>
  <li>Document architecture</li>
</ul>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested. Issues can
be filed on <a href="https://github.com/apache/arrow-flight-sql-postgresql/issues">GitHub</a>, and questions can be directed to GitHub or
<a href="/community/">the Apache Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.1.0 release of the Apache Arrow Flight SQL adapter for PostgreSQL. This includes 60 commits from 1 distinct contributors. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bug fixes and improvements have been made: we refer you to the complete release notes. What is Apache Arrow Flight SQL adapter for PostgreSQL? Apache Arrow Flight SQL adapter for PostgreSQL is a PostgreSQL extension that adds an Apache Arrow Flight SQL endpoint to PostgreSQL. Apache Arrow Flight SQL is a protocol to use Apache Arrow format to interact with SQL databases. You can use Apache Arrow Flight SQL instead of the PostgreSQL wire protocol to interact with PostgreSQL by Apache Arrow Flight SQL adapter for PostgreSQL. Apache Arrow format is designed for fast typed table data exchange. If you want to get large data by SELECT or INSERT/UPDATE large data, Apache Arrow Flight SQL will be faster than the PostgreSQL wire protocol. Release Highlights This is the initial release! This includes the following basic features. Other features will be implemented in the future releases. Literal SELECT/INSERT/UPDATE/DELETE Prepared SELECT/INSERT/UPDATE/DELETE password/trust authentications TLS connection Integer family types Floating point family types Text family types Binary family types Timestamp with time zone type Packages for Debian GNU/Linux bookworm and Ubuntu 22.04 are available. Install See the install document for details. Contributors $ git shortlog --perl-regexp --author=^((?!dependabot\[bot\]).*)$ -sn dc7f34e2636732acd0d015a7cd8259334f1acb16...0.1.0 59 Sutou Kouhei Roadmap Add support for more data types Add support for mTLS Add support for Apache Arrow Flight SQL commands to fetch metadata Add support for Apache Arrow Flight SQL transaction related APIs Add support for canceling a query Add more benchmark results Document architecture Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Apache Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 0.6.0 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/08/28/adbc-0.6.0-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.6.0 (Libraries) Release" /><published>2023-08-28T00:00:00-04:00</published><updated>2023-08-28T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/08/28/adbc-0.6.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/08/28/adbc-0.6.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.6.0 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/10"><strong>46
resolved issues</strong></a> from <a href="#contributors"><strong>9 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version 0.6.0.
The <strong>API specification</strong> is versioned separately and is at version
1.0.0.  (The API version will be updated to 1.1.0 in the coming
release.)</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.6.0/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>The PostgreSQL and SQLite drivers have better type support.  The
PostgreSQL driver will also chunk its output now instead of always
delivering a single record batch.  The Snowflake driver no longer
requires a URI to connect.  The Python driver manager and Flight SQL
driver deliver more concise error messages, with less fluff around the
actual error message delivered by the database.</p>

<p>A critical bugfix for all Go-based drivers (Flight SQL and Snowflake)
is included.  This fixes panics caused by uninitialized memory in C
Data Interface structures appearing to the garbage collector as
(invalid) pointers to Go memory.  For details, see <a href="https://github.com/apache/arrow-adbc/issues/729">issue
729</a>.</p>

<p>While C# is not yet included in this release, the library is now more
compatible with .NET 4.</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.5.1..apache-arrow-adbc-0.6.0
    30	David Li
    15	William Ayd
     9	Dewey Dunnington
     5	Matt Topol
     5	Solomon Choe
     2	davidhcoe
     1	Alexandre Crayssac
     1	Curt Hagenlocher
     1	Diego Fernández Giraldo
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>The 1.1.0 revision of the ADBC API was accepted, but is not included
in this release.  It will be released as ADBC 0.7.0.</p>

<p>There are currently no plans for a second API revision.  As work
progresses on asynchronous and device-aware APIs in the Arrow
ecosystem, ADBC will eventually be updated to support any new APIs.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.6.0 release of the Apache Arrow ADBC libraries. This covers includes 46 resolved issues from 9 distinct contributors. This is a release of the libraries, which are at version 0.6.0. The API specification is versioned separately and is at version 1.0.0. (The API version will be updated to 1.1.0 in the coming release.) The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights The PostgreSQL and SQLite drivers have better type support. The PostgreSQL driver will also chunk its output now instead of always delivering a single record batch. The Snowflake driver no longer requires a URI to connect. The Python driver manager and Flight SQL driver deliver more concise error messages, with less fluff around the actual error message delivered by the database. A critical bugfix for all Go-based drivers (Flight SQL and Snowflake) is included. This fixes panics caused by uninitialized memory in C Data Interface structures appearing to the garbage collector as (invalid) pointers to Go memory. For details, see issue 729. While C# is not yet included in this release, the library is now more compatible with .NET 4. Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.5.1..apache-arrow-adbc-0.6.0 30 David Li 15 William Ayd 9 Dewey Dunnington 5 Matt Topol 5 Solomon Choe 2 davidhcoe 1 Alexandre Crayssac 1 Curt Hagenlocher 1 Diego Fernández Giraldo Roadmap The 1.1.0 revision of the ADBC API was accepted, but is not included in this release. It will be released as ADBC 0.7.0. There are currently no plans for a second API revision. As work progresses on asynchronous and device-aware APIs in the Arrow ecosystem, ADBC will eventually be updated to support any new APIs. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 13.0.0 Release</title><link href="https://arrow.apache.org/blog/2023/08/24/13.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 13.0.0 Release" /><published>2023-08-24T00:00:00-04:00</published><updated>2023-08-24T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/08/24/13.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/08/24/13.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 13.0.0 release. This covers
over 3 months of development work and includes <a href="https://github.com/apache/arrow/milestone/53?closed=1"><strong>456 resolved issues</strong></a>
from <a href="/release/13.0.0.html#contributors"><strong>108 distinct contributors</strong></a>. See the <a href="https://arrow.apache.org/install/">Install Page</a>
to learn how to get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/13.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 12.0.0 release, Marco Neumann, Gang Wu, Mehmet Ozan Kabak and Kevin Gurney
have been invited to be committers.
Matt Topol, Jie Wen, Ben Baumgold and Dewey Dunnington have joined the
Project Management Committee (PMC).</p>

<p>Thanks for your contributions and participation in the project!</p>

<h2 id="columnar-format-notes">Columnar Format Notes</h2>

<p>The <a href="https://arrow.apache.org/docs/dev/format/Columnar.html#run-end-encoded-layout">run-end encoded layout</a>
has been added.  This layout can allow data with long runs of duplicate values to be encoded and processed
efficiently.  Initial support has been added for <a href="https://github.com/apache/arrow/pull/14179">C++</a> and
<a href="https://github.com/apache/arrow/pull/14223">Go</a>.</p>

<h3 id="c-device-data-interface">C Device Data Interface</h3>

<p>An <strong>experimental</strong> new specification, the
<a href="https://arrow.apache.org/docs/dev/format/CDeviceDataInterface.html">C Device Data Interface</a>,
has been accepted for inclusion <a href="https://github.com/apache/arrow/issues/34971">GH-34971</a>. It builds on the existing
C Data Interface to provide a runtime-agnostic zero-copy sharing mechanism
for Arrow data residing on non-CPU devices.</p>

<p>Reference implementations of the C Device Data Interface will progressively
be added to the standard Arrow libraries after the 13.0.0 release.</p>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>

<p>Support for flagging ordered result sets to clients is now added. (<a href="https://github.com/apache/arrow/issues/34852">GH-34852</a>)</p>

<p>gRPC 1.30 is now the minimum supported version in C++/Python/R/etc. (<a href="https://github.com/apache/arrow/issues/36479">GH-34679</a>)</p>

<p>In C++, various methods now receive a full <code class="language-plaintext highlighter-rouge">ServerCallContext</code> (<a href="https://github.com/apache/arrow/issues/35442">GH-35442</a>, <a href="https://github.com/apache/arrow/issues/35377">GH-35377</a>) and the context now exposes headers sent by the client (<a href="https://github.com/apache/arrow/issues/35375">GH-35375</a>).</p>

<h2 id="c-notes">C++ notes</h2>

<h3 id="building">Building</h3>

<p>CMake 3.16 or later is now required for building Arrow C++ <a href="https://github.com/apache/arrow/issues/34921">GH-34921</a>.</p>

<p>Optimizations are not disabled anymore when the <code class="language-plaintext highlighter-rouge">RelWithDebInfo</code> build type
is selected <a href="https://github.com/apache/arrow/issues/35850">GH-35850</a>. Furthermore, compiler flags can now properly be
customized per-build type using <code class="language-plaintext highlighter-rouge">ARROW_C_FLAGS_DEBUG</code>, <code class="language-plaintext highlighter-rouge">ARROW_CXX_FLAGS_DEBUG</code>
and related variables <a href="https://github.com/apache/arrow/issues/35870">GH-35870</a>.</p>

<h3 id="acero">Acero</h3>

<p>Handling of unaligned buffers is input nodes can be configured programmatically
or by setting the environment variable <code class="language-plaintext highlighter-rouge">ACERO_ALIGNMENT_HANDLING</code>. The default
behavior is to warn when an unaligned buffer is detected <a href="https://github.com/apache/arrow/issues/35498">GH-35498</a>.</p>

<h3 id="compute">Compute</h3>

<p>Several new functions have been added:</p>
<ul>
  <li>aggregate functions “first”, “last”, “first_last” <a href="https://github.com/apache/arrow/issues/34911">GH-34911</a>;</li>
  <li>vector functions “cumulative_prod”, “cumulative_min”, “cumulative_max” <a href="https://github.com/apache/arrow/issues/32190">GH-32190</a>;</li>
  <li>vector function “pairwise_diff” <a href="https://github.com/apache/arrow/issues/35786">GH-35786</a>.</li>
</ul>

<p>Sorting now works on dictionary arrays, with a much better performance than
the naive approach of sorting the decoded dictionary <a href="https://github.com/apache/arrow/issues/29887">GH-29887</a>. Sorting also
works on struct arrays, and nested sort keys are supported using <code class="language-plaintext highlighter-rouge">FieldRed</code> <a href="https://github.com/apache/arrow/issues/33206">GH-33206</a>.</p>

<p>The <code class="language-plaintext highlighter-rouge">check_overflow</code> option has been removed from <code class="language-plaintext highlighter-rouge">CumulativeSumOptions</code> as
it was redundant with the availability of two different functions:
“cumulative_sum” and “cumulative_sum_checked” <a href="https://github.com/apache/arrow/issues/35789">GH-35789</a>.</p>

<p>Run-end encoded filters are efficiently supported <a href="https://github.com/apache/arrow/issues/35749">GH-35749</a>.</p>

<p>Duration types are supported with the “is_in” and “index_in” functions <a href="https://github.com/apache/arrow/issues/36047">GH-36047</a>.
They can be multiplied with all integer types <a href="https://github.com/apache/arrow/issues/36128">GH-36128</a>.</p>

<p>“is_in” and “index_in” now cast their inputs more flexibly: they first attempt
to cast the value set to the input type, then in the other direction if the
former fails <a href="https://github.com/apache/arrow/issues/36203">GH-36203</a>.</p>

<p>Multiple bugs have been fixed in “utf8_slice_codeunits” when the <code class="language-plaintext highlighter-rouge">stop</code> option
is omitted <a href="https://github.com/apache/arrow/issues/36311">GH-36311</a>.</p>

<h3 id="dataset">Dataset</h3>

<p>A custom schema can now be passed when writing a dataset <a href="https://github.com/apache/arrow/issues/35730">GH-35730</a>. The custom
schema can alter nullability or metadata information, but is not allowed to
change the datatypes written.</p>

<h3 id="filesystems">Filesystems</h3>

<p>The S3 filesystem now writes files in equal-sized chunks, for compatibility with
Cloudflare’s “R2” Storage <a href="https://github.com/apache/arrow/issues/34363">GH-34363</a>.</p>

<p>A long-standing issue where S3 support could crash at shutdown because of resources
still being alive after S3 finalization has been fixed <a href="https://github.com/apache/arrow/issues/36346">GH-36346</a>. Now, attempts
to use S3 resources (such as making filesystem calls) after S3 finalization should
result in a clean error.</p>

<p>The GCS filesystem accepts a new option to set the project id <a href="https://github.com/apache/arrow/issues/36227">GH-36227</a>.</p>

<h3 id="ipc">IPC</h3>

<p>Nullability and metadata information for sub-fields of map types is now preserved
when deserializing Arrow IPC <a href="https://github.com/apache/arrow/issues/35297">GH-35297</a>.</p>

<h3 id="orc">Orc</h3>

<p>The Orc adapter now maps Arrow field metadata to Orc type attributes when writing,
and vice-versa when reading <a href="https://github.com/apache/arrow/issues/35304">GH-35304</a>.</p>

<h3 id="parquet">Parquet</h3>

<p>It is now possible to write additional metadata while a <code class="language-plaintext highlighter-rouge">ParquetFileWriter</code> is
open <a href="https://github.com/apache/arrow/issues/34888">GH-34888</a>.</p>

<p>Writing a page index can be enabled selectively per-column <a href="https://github.com/apache/arrow/issues/34949">GH-34949</a>.
In addition, page header statistics are not written anymore if the page
index is enabled for the given column <a href="https://github.com/apache/arrow/issues/34375">GH-34375</a>, as the information would
be redundant and less efficiently accessed.</p>

<p>Parquet writer properties allow specifying the sorting columns <a href="https://github.com/apache/arrow/issues/35331">GH-35331</a>.
The user is responsible for ensuring that the data written to the file
actually complies with the given sorting.</p>

<p>CRC computation has been implemented for v2 data pages <a href="https://github.com/apache/arrow/issues/35171">GH-35171</a>.
It was already implemented for v1 data pages.</p>

<p>Writing compliant nested types is now enabled by default <a href="https://github.com/apache/arrow/issues/29781">GH-29781</a>. This
should not have any negative implication.</p>

<p>Attempting to load a subset of an Arrow extension type is now forbidden
<a href="https://github.com/apache/arrow/issues/20385">GH-20385</a>. Previously, if an extension type’s storage is nested (for example
a “Point” extension type backed by a <code class="language-plaintext highlighter-rouge">struct&lt;x: float64, y: float64&gt;</code>),
it was possible to load selectively some of the columns of the storage type.</p>

<h3 id="substrait">Substrait</h3>

<p>Support for various functions has been added: “stddev”, “variance”, “first”,
“last” (GH-35247, GH-35506).</p>

<p>Deserializing sorts is now supported <a href="https://github.com/apache/arrow/issues/32763">GH-32763</a>. However, some features,
such as clustered sort direction or custom sort functions, are not
implemented.</p>

<h3 id="miscellaneous">Miscellaneous</h3>

<p><code class="language-plaintext highlighter-rouge">FieldRef</code> sports additional methods to get a flattened version of nested
fields <a href="https://github.com/apache/arrow/issues/14946">GH-14946</a>. Compared to their non-flattened counterparts,
the methods <code class="language-plaintext highlighter-rouge">GetFlattened</code>, <code class="language-plaintext highlighter-rouge">GetAllFlattened</code>, <code class="language-plaintext highlighter-rouge">GetOneFlattened</code> and
<code class="language-plaintext highlighter-rouge">GetOneOrNoneFlattened</code> combine a child’s null bitmap with its ancestors’
null bitmaps such as to compute the field’s overall logical validity bitmap.</p>

<p>In other words, given the struct array <code class="language-plaintext highlighter-rouge">[null, {'x': null}, {'x': 5}]</code>,
<code class="language-plaintext highlighter-rouge">FieldRef("x")::Get</code> might return <code class="language-plaintext highlighter-rouge">[0, null, 5]</code>
while <code class="language-plaintext highlighter-rouge">FieldRef("y")::GetFlattened</code> will <em>always</em> return <code class="language-plaintext highlighter-rouge">[null, null, 5]</code>.</p>

<p><code class="language-plaintext highlighter-rouge">Scalar::hash()</code> has been fixed for sliced nested arrays <a href="https://github.com/apache/arrow/issues/35360">GH-35360</a>.</p>

<p>A new floating-point to decimal conversion algorithm exhibits much better
precision <a href="https://github.com/apache/arrow/issues/35576">GH-35576</a>.</p>

<p>It is now possible to cast between scalars of different list-like types
<a href="https://github.com/apache/arrow/issues/36309">GH-36309</a>.</p>

<h2 id="c-notes-1">C# notes</h2>

<h3 id="enhancements">Enhancements</h3>

<ul>
  <li>
    <p>The <a href="https://arrow.apache.org/docs/format/CDataInterface.html">C Data Interface</a> is now supported in the .NET Apache.Arrow library. The main entry points are <code class="language-plaintext highlighter-rouge">CArrowArrayImporter.ImportArray</code>, <code class="language-plaintext highlighter-rouge">CArrowArrayExporter.ExportArray</code>, <code class="language-plaintext highlighter-rouge">CArrowArrayStreamImporter.ImportArrayStream</code>, and <code class="language-plaintext highlighter-rouge">CArrowArrayStreamExporter.ExportArrayStream</code> in the <code class="language-plaintext highlighter-rouge">Apache.Arrow.C</code> namespace. (<a href="https://github.com/apache/arrow/issues/33856">GH-33856</a>, <a href="https://github.com/apache/arrow/issues/33857">GH-33857</a>, <a href="https://github.com/apache/arrow/issues/36120">GH-36120</a>, and <a href="https://github.com/apache/arrow/issues/35809">GH-35809</a>).</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">ArrowBuffer.BitmapBuilder</code> adds <code class="language-plaintext highlighter-rouge">Append(ReadOnlySpan&lt;byte&gt; source, int validBits)</code> and <code class="language-plaintext highlighter-rouge">AppendRange(bool value, int length)</code> to improve performance of array concatenation (<a href="https://github.com/apache/arrow/issues/32605">GH-32605</a>)</p>
  </li>
</ul>

<h3 id="bug-fixes">Bug Fixes</h3>

<ul>
  <li>TotalBytes and TotalRecords are now being serialized in FlightInfo (<a href="https://github.com/apache/arrow/issues/35267">GH-35267</a>)
    <h2 id="go-notes">Go notes</h2>
  </li>
</ul>

<h3 id="enhancements-1">Enhancements</h3>

<h4 id="arrow">Arrow</h4>

<ul>
  <li>Compute arithmetic functions are now available for Float16 (<a href="https://github.com/apache/arrow/issues/35162">GH-35162</a>)</li>
  <li>Float16, Large* and Fixed types are all now supported by the CSV reader/writer (<a href="https://github.com/apache/arrow/issues/36105">GH-36105</a> and <a href="https://github.com/apache/arrow/issues/36141">GH-36141</a>)</li>
  <li>CSV Reader uses <code class="language-plaintext highlighter-rouge">AppendValueFromString</code> for extension types and properly reads empty values as null (<a href="https://github.com/apache/arrow/issues/35188">GH-35188</a> and <a href="https://github.com/apache/arrow/issues/35190">GH-35190</a>)</li>
  <li><a href="https://github.com/substrait-io/substrait-go">Substrait</a> expressions can now be executed using the Compute library (<a href="https://github.com/apache/arrow/issues/35652">GH-35652</a>)</li>
  <li>You can now read back values from Dictionary Builders before finishing the array (<a href="https://github.com/apache/arrow/issues/35711">GH-35711</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">MapType.ValueField</code> and <code class="language-plaintext highlighter-rouge">MapType.ValueType</code> are now deprecated in favor of <code class="language-plaintext highlighter-rouge">MapType.Elem().(*StructType)</code> (<a href="https://github.com/apache/arrow/issues/35909">GH-35909</a>)</li>
  <li>Multiple equality functions which have been deprecated since v9 have  now been removed (Such as <code class="language-plaintext highlighter-rouge">array.ArraySliceEqual</code> in favor of <code class="language-plaintext highlighter-rouge">array.SliceEqual</code>) (<a href="https://github.com/apache/arrow/issues/36198">GH-36198</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">ValueStr</code> method on Timestamp arrays now includes the zone in the output (<a href="https://github.com/apache/arrow/issues/36568">GH-36568</a>)</li>
  <li><em>BREAKING CHANGE</em> <code class="language-plaintext highlighter-rouge">FixedSizeListBuilder.AppendNull</code> no longer requires manually appending nulls to the underlying list  (<a href="https://github.com/apache/arrow/issues/35482">GH-35482</a>)</li>
</ul>

<h4 id="flight">Flight</h4>

<ul>
  <li>FlightSQL driver supports non-prepared queries now (<a href="https://github.com/apache/arrow/issues/35136">GH-35136</a>)</li>
</ul>

<h4 id="parquet-1">Parquet</h4>

<ul>
  <li>Error messages in row group writer have been improved (<a href="https://github.com/apache/arrow/issues/36319">GH-36319</a>)</li>
</ul>

<h3 id="bug-fixes-1">Bug Fixes</h3>

<ul>
  <li>Cross architecture build failures with v12.0.1 have been fixed (<a href="https://github.com/apache/arrow/issues/36052">GH-36052</a>)</li>
</ul>

<h4 id="arrow-1">Arrow</h4>

<ul>
  <li>It is now possible to build the Arrow Go lib using tinygo for building smaller WASM binaries (<a href="https://github.com/apache/arrow/issues/32832">GH-32832</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">Fields</code> method for Schema and StructType now returns a copy of the slice to ensure immutability (<a href="https://github.com/apache/arrow/issues/35306">GH-35306</a> and <a href="https://github.com/apache/arrow/issues/35866">GH-35866</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">array.ApproxEqual</code> for Maps now allows entries for a given element to be presented in any order (<a href="https://github.com/apache/arrow/issues/35828">GH-35828</a>)</li>
  <li>Fix issues with decimal256 arrays (<a href="https://github.com/apache/arrow/issues/35911">GH-35911</a>, <a href="https://github.com/apache/arrow/issues/35965">GH-35965</a>, and <a href="https://github.com/apache/arrow/issues/35975">GH-35975</a>)</li>
  <li>StructType now allows duplicate field names correctly (<a href="https://github.com/apache/arrow/issues/36014">GH-36014</a>)</li>
</ul>

<h4 id="flight-1">Flight</h4>

<ul>
  <li>Fix crash in client middleware (<a href="https://github.com/apache/arrow/issues/35240">GH-35240</a>)</li>
</ul>

<h4 id="parquet-2">Parquet</h4>

<ul>
  <li>Various memory leaks addressed in pqarrow package (<a href="https://github.com/apache/arrow/issues/35015">GH-35015</a>)</li>
  <li>Fixed panic for <code class="language-plaintext highlighter-rouge">ListOf(types)</code> if null (<a href="https://github.com/apache/arrow/issues/35684">GH-35684</a>)</li>
</ul>

<h2 id="java-notes">Java notes</h2>

<p>The JNI bindings for Arrow Dataset now support execute <a href="https://substrait.io/">Substrait</a> plans via the <a href="https://arrow.apache.org/docs/dev/cpp/streaming_execution.html">Acero</a> query engine. (<a href="https://github.com/apache/arrow/issues/34223">GH-34223</a>)</p>

<p>Arrow packages that depend on Netty (most notably, <code class="language-plaintext highlighter-rouge">arrow-memory-netty</code>, but also Arrow Flight) now require either Netty &lt; 4.1.94.Final or Netty &gt;= 4.1.96.Final. In Netty versions 4.1.94.Final and 4.1.95.Final, there was a breaking change in an internal API that affected Arrow; this was reverted in 4.1.96.Final (<a href="https://github.com/apache/arrow/issues/36209">GH-36209</a>, <a href="https://github.com/apache/arrow/issues/36928">GH-36928</a>)</p>

<p><code class="language-plaintext highlighter-rouge">VectorSchemaRoot#slice</code> now always makes a copy, including when the slice covers all rows (previously it did not make a copy in this case). This is a potentially-breaking change if your application depended on the old behavior. (<a href="https://github.com/apache/arrow/issues/35275">GH-35275</a>)</p>

<p>Debug info for allocations is no longer automatically enabled when assertions are enabled (e.g. when running unit tests). Instead, support must be explicitly enabled. This is not quite a breaking change, but may be surprising if you are used to using this information while debugging tests. However, performance should be greatly improved while running tests. (<a href="https://github.com/apache/arrow/issues/34338">GH-34338</a>)</p>

<p>Support for the upcoming Java 21 was added, though we do not yet test this in CI (<a href="https://github.com/apache/arrow/issues/35053">GH-5053</a>). The JNI bindings for Arrow Dataset now expose JSON support (<a href="https://github.com/apache/arrow/issues/36421">GH-36421</a>). Dictionary replacement is now supported when writing the IPC stream format (<a href="https://github.com/apache/arrow/issues/18547">GH-18547</a>).</p>

<h2 id="javascript-notes">JavaScript notes</h2>

<ul>
  <li>Updated dependencies: https://github.com/apache/arrow/pull/36032</li>
</ul>

<h2 id="python-notes">Python notes</h2>

<p>Compatibility notes:</p>

<ul>
  <li>The default format version for Parquet has been bumped from 2.4 to 2.6 <a href="https://github.com/apache/arrow/issues/35746">GH-35746</a>. In practice, this means that nanosecond timestamps now preserve its resolution instead of being converted to microseconds.</li>
  <li>Support for Python 3.7 is dropped <a href="https://github.com/apache/arrow/issues/34788">GH-34788</a></li>
</ul>

<p>New features:</p>

<ul>
  <li>Conversion to non-nano datetime64 for pandas &gt;= 2.0 is now supported <a href="https://github.com/apache/arrow/issues/33321">GH-33321</a></li>
  <li>Write page index is now supported <a href="https://github.com/apache/arrow/issues/36284">GH-36284</a></li>
  <li>Bindings for reading JSON format in Dataset are added <a href="https://github.com/apache/arrow/issues/34216">GH-34216</a></li>
  <li><code class="language-plaintext highlighter-rouge">keys_sorted</code> property of MapType is now exposed <a href="https://github.com/apache/arrow/issues/35112">GH-35112</a></li>
</ul>

<p>Other improvements:</p>

<ul>
  <li>Common python functionality between <code class="language-plaintext highlighter-rouge">Table</code> and <code class="language-plaintext highlighter-rouge">RecordBatch</code> classes has been consolidated ( <a href="https://github.com/apache/arrow/issues/36129">GH-36129</a>, <a href="https://github.com/apache/arrow/issues/35415">GH-35415</a>, <a href="https://github.com/apache/arrow/issues/35390">GH-35390</a>, <a href="https://github.com/apache/arrow/issues/34979">GH-34979</a>, <a href="https://github.com/apache/arrow/issues/34868">GH-34868</a>, <a href="https://github.com/apache/arrow/issues/31868">GH-31868</a>)</li>
  <li>Some functionality for <code class="language-plaintext highlighter-rouge">FixedShapeTensorType</code> has been improved (<code class="language-plaintext highlighter-rouge">__reduce__</code> <a href="https://github.com/apache/arrow/issues/36038">GH-36038</a>, picklability <a href="https://github.com/apache/arrow/issues/35599">GH-35599</a>)</li>
  <li>Pyarrow scalars can now be accepted in the <code class="language-plaintext highlighter-rouge">array</code> constructor <a href="https://github.com/apache/arrow/issues/21761">GH-21761</a></li>
  <li>DataFrame Interchange Protocol implementation and usage is now documented <a href="https://github.com/apache/arrow/issues/33980">GH-33980</a></li>
  <li>Conversion between Arrow and Pandas for map/pydict now has enhanced support <a href="https://github.com/apache/arrow/issues/34729">GH-34729</a></li>
  <li>Usability of <code class="language-plaintext highlighter-rouge">pc.map_lookup</code> / <code class="language-plaintext highlighter-rouge">MapLookupOptions</code> is improved <a href="https://github.com/apache/arrow/issues/36045">GH-36045</a></li>
  <li><code class="language-plaintext highlighter-rouge">zero_copy_only</code> keyword can now also be accepted in <code class="language-plaintext highlighter-rouge">ChunkedArray.to_numpy()</code> <a href="https://github.com/apache/arrow/issues/34787">GH-34787</a></li>
  <li>Python C++ codebase now has linter support in Archery and the CI <a href="https://github.com/apache/arrow/issues/35485">GH-35485</a></li>
</ul>

<p>Relevant bug fixes:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">__array__</code> numpy conversion for Table and RecordBatch is now corrected so that <code class="language-plaintext highlighter-rouge">np.asarray(pa.Table)</code> doesn’t return a transposed result <a href="https://github.com/apache/arrow/issues/34886">GH-34886</a></li>
  <li><code class="language-plaintext highlighter-rouge">parquet.write_to_dataset</code> doesn’t create empty files for non-observed dictionary (category) values anymore <a href="https://github.com/apache/arrow/issues/23870">GH-23870</a></li>
  <li>Dataset writer now also correctly follows default Parquet version of 2.6 <a href="https://github.com/apache/arrow/issues/36537">GH-36537</a></li>
  <li>Comparing <code class="language-plaintext highlighter-rouge">pyarrow.dataset.Partitioning</code> with other type is now correctly handled <a href="https://github.com/apache/arrow/issues/36659">GH-36659</a></li>
  <li>Pickling of pyarrow.dataset PartitioningFactory objects is now supported <a href="https://github.com/apache/arrow/issues/34884">GH-34884</a></li>
  <li>None schema is now disallowed in parquet writer <a href="https://github.com/apache/arrow/issues/35858">GH-35858</a></li>
  <li><code class="language-plaintext highlighter-rouge">pa.FixedShapeTensorArray.to_numpy_ndarray</code> is not failing on sliced arrays <a href="https://github.com/apache/arrow/issues/35573">GH-35573</a></li>
  <li>Halffloat type is now supported in the conversion from Arrow list to pandas <a href="https://github.com/apache/arrow/issues/36168">GH-36168</a></li>
  <li><code class="language-plaintext highlighter-rouge">__from_arrow__</code> is now also implemented for <code class="language-plaintext highlighter-rouge">Array.to_pandas</code> for pandas extension data types <a href="https://github.com/apache/arrow/issues/36096">GH-36096</a></li>
</ul>

<h2 id="r-notes">R notes</h2>

<p>New features:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">open_dataset()</code> now works with ND-JSON files <a href="https://github.com/apache/arrow/issues/35055">GH-35055</a></li>
  <li>Calling <code class="language-plaintext highlighter-rouge">schema()</code> on multiple Arrow objects now returns the object’s schema <a href="https://github.com/apache/arrow/issues/35543">GH-35543</a></li>
  <li>dplyr <code class="language-plaintext highlighter-rouge">.by</code>/<code class="language-plaintext highlighter-rouge">by</code> argument now supported in arrow implementation of dplyr verbs  <a href="https://github.com/apache/arrow/issues/35667">GH-35667</a></li>
</ul>

<p>Other improvements:</p>
<ul>
  <li>Convenience function <code class="language-plaintext highlighter-rouge">arrow_array()</code> can be used to create Arrow Arrays <a href="https://github.com/apache/arrow/issues/36381">GH-36381</a></li>
  <li>Convenience function <code class="language-plaintext highlighter-rouge">scalar()</code> can be used to create Arrow Scalars  <a href="https://github.com/apache/arrow/issues/36265">GH-36265</a></li>
  <li>Prevent crashed when passing data between arrow and duckdb by always calling <code class="language-plaintext highlighter-rouge">RecordBatchReader::ReadNext()</code> from DuckDB from the main R thread <a href="https://github.com/apache/arrow/issues/36307">GH-36307</a></li>
  <li>Issue a warning for <code class="language-plaintext highlighter-rouge">set_io_thread_count()</code> with <code class="language-plaintext highlighter-rouge">num_threads</code> &lt; 2 <a href="https://github.com/apache/arrow/issues/36304">GH-36304</a></li>
  <li>Ensure missing grouping variables are added to the beginning of the variable list <a href="https://github.com/apache/arrow/issues/36305">GH-36305</a></li>
  <li>CSV File reader options class objects can print the selected values <a href="https://github.com/apache/arrow/issues/35955">GH-35955</a></li>
  <li>Schema metadata can be set as a named character vector <a href="https://github.com/apache/arrow/issues/35954">GH-35954</a></li>
  <li>Ensure that the RStringViewer helper class does not own any Array references <a href="https://github.com/apache/arrow/issues/35812">GH-35812</a></li>
  <li><code class="language-plaintext highlighter-rouge">strptime()</code> in arrow will return a timezone-aware timestamp if <code class="language-plaintext highlighter-rouge">%z</code> is part of the format string <a href="https://github.com/apache/arrow/issues/35671">GH-35671</a></li>
  <li>Column ordering when combining <code class="language-plaintext highlighter-rouge">group_by()</code> and <code class="language-plaintext highlighter-rouge">across()</code> now matches dplyr <a href="https://github.com/apache/arrow/issues/35473">GH-35473</a></li>
  <li>Link to correct version of OpenSSL when using autobrew <a href="https://github.com/apache/arrow/issues/36551">GH-36551</a></li>
  <li>Require cmake 3.16 in bundled build script <a href="https://github.com/apache/arrow/issues/36321">GH-36321</a></li>
</ul>

<p>For more on what’s in the 13.0.0 R package, see the <a href="/docs/r/news/">R changelog</a>.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<h3 id="ruby">Ruby</h3>

<h4 id="bug-fixes-2">Bug fixes</h4>

<ul>
  <li>Fixed GC-related issue against random segfault in hash join <a href="https://github.com/apache/arrow/issues/35819">GH-35819</a></li>
  <li>Fixed segfault in <code class="language-plaintext highlighter-rouge">CallExpression.new</code> <a href="https://github.com/apache/arrow/issues/35915">GH-35915</a></li>
</ul>

<h4 id="improvements">Improvements</h4>

<ul>
  <li>FlightRPC: Added a convenient wrapper for the authentication method <a href="https://github.com/apache/arrow/issues/35435">GH-35435</a></li>
  <li>Added empty table support in <code class="language-plaintext highlighter-rouge">#select_columns</code> <a href="https://github.com/apache/arrow/issues/35681">GH-35681</a></li>
  <li>Added optional hash support in <code class="language-plaintext highlighter-rouge">Expression.try_convert</code> <a href="https://github.com/apache/arrow/issues/35915">GH-35915</a></li>
  <li>Parquet: Added <code class="language-plaintext highlighter-rouge">Parquet::ArrowFileReader#each_row_group</code> <a href="https://github.com/apache/arrow/issues/36008">GH-36008</a></li>
  <li>Added support of the automatic installation of arrow-c-glib on Conda environment <a href="https://github.com/apache/arrow/issues/36287">GH-36287</a></li>
</ul>

<h3 id="c-glib">C GLib</h3>

<h4 id="bug-fixes-3">Bug fixes</h4>

<ul>
  <li>Parquet: Fixed GC-related bug in metadata dependencies <a href="https://github.com/apache/arrow/issues/35266">GH-35266</a></li>
  <li>Fixed potentially GC-related issue against random segfault in hash join <a href="https://github.com/apache/arrow/issues/35819">GH-35819</a></li>
</ul>

<h4 id="improvements-1">Improvements</h4>

<ul>
  <li>FlightRPC: Added support to pass <code class="language-plaintext highlighter-rouge">GAFlightServerCallContext</code> object in several methods of <code class="language-plaintext highlighter-rouge">GAFlightServerCustomAuthHandler</code> <a href="https://github.com/apache/arrow/issues/35377">GH-35377</a></li>
  <li>FlightSQL: Added support for INSERT/UPDATE/DELETE <a href="https://github.com/apache/arrow/issues/36408">GH-36408</a></li>
  <li>Added <code class="language-plaintext highlighter-rouge">GArrowRunEndEncodedDataType</code> <a href="https://github.com/apache/arrow/issues/35417">GH-35417</a></li>
  <li>Added <code class="language-plaintext highlighter-rouge">GArrowRunEndEncodedArray</code> <a href="https://github.com/apache/arrow/issues/35418">GH-35418</a></li>
</ul>

<h2 id="rust-notes">Rust notes</h2>

<p>The Rust projects have moved to separate repositories outside the
main Arrow monorepo. For notes on the latest release of the Rust
implementation, see the latest <a href="https://github.com/apache/arrow-rs/tags">Arrow Rust changelog</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 13.0.0 release. This covers over 3 months of development work and includes 456 resolved issues from 108 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 12.0.0 release, Marco Neumann, Gang Wu, Mehmet Ozan Kabak and Kevin Gurney have been invited to be committers. Matt Topol, Jie Wen, Ben Baumgold and Dewey Dunnington have joined the Project Management Committee (PMC). Thanks for your contributions and participation in the project! Columnar Format Notes The run-end encoded layout has been added. This layout can allow data with long runs of duplicate values to be encoded and processed efficiently. Initial support has been added for C++ and Go. C Device Data Interface An experimental new specification, the C Device Data Interface, has been accepted for inclusion GH-34971. It builds on the existing C Data Interface to provide a runtime-agnostic zero-copy sharing mechanism for Arrow data residing on non-CPU devices. Reference implementations of the C Device Data Interface will progressively be added to the standard Arrow libraries after the 13.0.0 release. Arrow Flight RPC notes Support for flagging ordered result sets to clients is now added. (GH-34852) gRPC 1.30 is now the minimum supported version in C++/Python/R/etc. (GH-34679) In C++, various methods now receive a full ServerCallContext (GH-35442, GH-35377) and the context now exposes headers sent by the client (GH-35375). C++ notes Building CMake 3.16 or later is now required for building Arrow C++ GH-34921. Optimizations are not disabled anymore when the RelWithDebInfo build type is selected GH-35850. Furthermore, compiler flags can now properly be customized per-build type using ARROW_C_FLAGS_DEBUG, ARROW_CXX_FLAGS_DEBUG and related variables GH-35870. Acero Handling of unaligned buffers is input nodes can be configured programmatically or by setting the environment variable ACERO_ALIGNMENT_HANDLING. The default behavior is to warn when an unaligned buffer is detected GH-35498. Compute Several new functions have been added: aggregate functions “first”, “last”, “first_last” GH-34911; vector functions “cumulative_prod”, “cumulative_min”, “cumulative_max” GH-32190; vector function “pairwise_diff” GH-35786. Sorting now works on dictionary arrays, with a much better performance than the naive approach of sorting the decoded dictionary GH-29887. Sorting also works on struct arrays, and nested sort keys are supported using FieldRed GH-33206. The check_overflow option has been removed from CumulativeSumOptions as it was redundant with the availability of two different functions: “cumulative_sum” and “cumulative_sum_checked” GH-35789. Run-end encoded filters are efficiently supported GH-35749. Duration types are supported with the “is_in” and “index_in” functions GH-36047. They can be multiplied with all integer types GH-36128. “is_in” and “index_in” now cast their inputs more flexibly: they first attempt to cast the value set to the input type, then in the other direction if the former fails GH-36203. Multiple bugs have been fixed in “utf8_slice_codeunits” when the stop option is omitted GH-36311. Dataset A custom schema can now be passed when writing a dataset GH-35730. The custom schema can alter nullability or metadata information, but is not allowed to change the datatypes written. Filesystems The S3 filesystem now writes files in equal-sized chunks, for compatibility with Cloudflare’s “R2” Storage GH-34363. A long-standing issue where S3 support could crash at shutdown because of resources still being alive after S3 finalization has been fixed GH-36346. Now, attempts to use S3 resources (such as making filesystem calls) after S3 finalization should result in a clean error. The GCS filesystem accepts a new option to set the project id GH-36227. IPC Nullability and metadata information for sub-fields of map types is now preserved when deserializing Arrow IPC GH-35297. Orc The Orc adapter now maps Arrow field metadata to Orc type attributes when writing, and vice-versa when reading GH-35304. Parquet It is now possible to write additional metadata while a ParquetFileWriter is open GH-34888. Writing a page index can be enabled selectively per-column GH-34949. In addition, page header statistics are not written anymore if the page index is enabled for the given column GH-34375, as the information would be redundant and less efficiently accessed. Parquet writer properties allow specifying the sorting columns GH-35331. The user is responsible for ensuring that the data written to the file actually complies with the given sorting. CRC computation has been implemented for v2 data pages GH-35171. It was already implemented for v1 data pages. Writing compliant nested types is now enabled by default GH-29781. This should not have any negative implication. Attempting to load a subset of an Arrow extension type is now forbidden GH-20385. Previously, if an extension type’s storage is nested (for example a “Point” extension type backed by a struct&lt;x: float64, y: float64&gt;), it was possible to load selectively some of the columns of the storage type. Substrait Support for various functions has been added: “stddev”, “variance”, “first”, “last” (GH-35247, GH-35506). Deserializing sorts is now supported GH-32763. However, some features, such as clustered sort direction or custom sort functions, are not implemented. Miscellaneous FieldRef sports additional methods to get a flattened version of nested fields GH-14946. Compared to their non-flattened counterparts, the methods GetFlattened, GetAllFlattened, GetOneFlattened and GetOneOrNoneFlattened combine a child’s null bitmap with its ancestors’ null bitmaps such as to compute the field’s overall logical validity bitmap. In other words, given the struct array [null, {'x': null}, {'x': 5}], FieldRef("x")::Get might return [0, null, 5] while FieldRef("y")::GetFlattened will always return [null, null, 5]. Scalar::hash() has been fixed for sliced nested arrays GH-35360. A new floating-point to decimal conversion algorithm exhibits much better precision GH-35576. It is now possible to cast between scalars of different list-like types GH-36309. C# notes Enhancements The C Data Interface is now supported in the .NET Apache.Arrow library. The main entry points are CArrowArrayImporter.ImportArray, CArrowArrayExporter.ExportArray, CArrowArrayStreamImporter.ImportArrayStream, and CArrowArrayStreamExporter.ExportArrayStream in the Apache.Arrow.C namespace. (GH-33856, GH-33857, GH-36120, and GH-35809). ArrowBuffer.BitmapBuilder adds Append(ReadOnlySpan&lt;byte&gt; source, int validBits) and AppendRange(bool value, int length) to improve performance of array concatenation (GH-32605) Bug Fixes TotalBytes and TotalRecords are now being serialized in FlightInfo (GH-35267) Go notes Enhancements Arrow Compute arithmetic functions are now available for Float16 (GH-35162) Float16, Large* and Fixed types are all now supported by the CSV reader/writer (GH-36105 and GH-36141) CSV Reader uses AppendValueFromString for extension types and properly reads empty values as null (GH-35188 and GH-35190) Substrait expressions can now be executed using the Compute library (GH-35652) You can now read back values from Dictionary Builders before finishing the array (GH-35711) MapType.ValueField and MapType.ValueType are now deprecated in favor of MapType.Elem().(*StructType) (GH-35909) Multiple equality functions which have been deprecated since v9 have now been removed (Such as array.ArraySliceEqual in favor of array.SliceEqual) (GH-36198) ValueStr method on Timestamp arrays now includes the zone in the output (GH-36568) BREAKING CHANGE FixedSizeListBuilder.AppendNull no longer requires manually appending nulls to the underlying list (GH-35482) Flight FlightSQL driver supports non-prepared queries now (GH-35136) Parquet Error messages in row group writer have been improved (GH-36319) Bug Fixes Cross architecture build failures with v12.0.1 have been fixed (GH-36052) Arrow It is now possible to build the Arrow Go lib using tinygo for building smaller WASM binaries (GH-32832) Fields method for Schema and StructType now returns a copy of the slice to ensure immutability (GH-35306 and GH-35866) array.ApproxEqual for Maps now allows entries for a given element to be presented in any order (GH-35828) Fix issues with decimal256 arrays (GH-35911, GH-35965, and GH-35975) StructType now allows duplicate field names correctly (GH-36014) Flight Fix crash in client middleware (GH-35240) Parquet Various memory leaks addressed in pqarrow package (GH-35015) Fixed panic for ListOf(types) if null (GH-35684) Java notes The JNI bindings for Arrow Dataset now support execute Substrait plans via the Acero query engine. (GH-34223) Arrow packages that depend on Netty (most notably, arrow-memory-netty, but also Arrow Flight) now require either Netty &lt; 4.1.94.Final or Netty &gt;= 4.1.96.Final. In Netty versions 4.1.94.Final and 4.1.95.Final, there was a breaking change in an internal API that affected Arrow; this was reverted in 4.1.96.Final (GH-36209, GH-36928) VectorSchemaRoot#slice now always makes a copy, including when the slice covers all rows (previously it did not make a copy in this case). This is a potentially-breaking change if your application depended on the old behavior. (GH-35275) Debug info for allocations is no longer automatically enabled when assertions are enabled (e.g. when running unit tests). Instead, support must be explicitly enabled. This is not quite a breaking change, but may be surprising if you are used to using this information while debugging tests. However, performance should be greatly improved while running tests. (GH-34338) Support for the upcoming Java 21 was added, though we do not yet test this in CI (GH-5053). The JNI bindings for Arrow Dataset now expose JSON support (GH-36421). Dictionary replacement is now supported when writing the IPC stream format (GH-18547). JavaScript notes Updated dependencies: https://github.com/apache/arrow/pull/36032 Python notes Compatibility notes: The default format version for Parquet has been bumped from 2.4 to 2.6 GH-35746. In practice, this means that nanosecond timestamps now preserve its resolution instead of being converted to microseconds. Support for Python 3.7 is dropped GH-34788 New features: Conversion to non-nano datetime64 for pandas &gt;= 2.0 is now supported GH-33321 Write page index is now supported GH-36284 Bindings for reading JSON format in Dataset are added GH-34216 keys_sorted property of MapType is now exposed GH-35112 Other improvements: Common python functionality between Table and RecordBatch classes has been consolidated ( GH-36129, GH-35415, GH-35390, GH-34979, GH-34868, GH-31868) Some functionality for FixedShapeTensorType has been improved (__reduce__ GH-36038, picklability GH-35599) Pyarrow scalars can now be accepted in the array constructor GH-21761 DataFrame Interchange Protocol implementation and usage is now documented GH-33980 Conversion between Arrow and Pandas for map/pydict now has enhanced support GH-34729 Usability of pc.map_lookup / MapLookupOptions is improved GH-36045 zero_copy_only keyword can now also be accepted in ChunkedArray.to_numpy() GH-34787 Python C++ codebase now has linter support in Archery and the CI GH-35485 Relevant bug fixes: __array__ numpy conversion for Table and RecordBatch is now corrected so that np.asarray(pa.Table) doesn’t return a transposed result GH-34886 parquet.write_to_dataset doesn’t create empty files for non-observed dictionary (category) values anymore GH-23870 Dataset writer now also correctly follows default Parquet version of 2.6 GH-36537 Comparing pyarrow.dataset.Partitioning with other type is now correctly handled GH-36659 Pickling of pyarrow.dataset PartitioningFactory objects is now supported GH-34884 None schema is now disallowed in parquet writer GH-35858 pa.FixedShapeTensorArray.to_numpy_ndarray is not failing on sliced arrays GH-35573 Halffloat type is now supported in the conversion from Arrow list to pandas GH-36168 __from_arrow__ is now also implemented for Array.to_pandas for pandas extension data types GH-36096 R notes New features: open_dataset() now works with ND-JSON files GH-35055 Calling schema() on multiple Arrow objects now returns the object’s schema GH-35543 dplyr .by/by argument now supported in arrow implementation of dplyr verbs GH-35667 Other improvements: Convenience function arrow_array() can be used to create Arrow Arrays GH-36381 Convenience function scalar() can be used to create Arrow Scalars GH-36265 Prevent crashed when passing data between arrow and duckdb by always calling RecordBatchReader::ReadNext() from DuckDB from the main R thread GH-36307 Issue a warning for set_io_thread_count() with num_threads &lt; 2 GH-36304 Ensure missing grouping variables are added to the beginning of the variable list GH-36305 CSV File reader options class objects can print the selected values GH-35955 Schema metadata can be set as a named character vector GH-35954 Ensure that the RStringViewer helper class does not own any Array references GH-35812 strptime() in arrow will return a timezone-aware timestamp if %z is part of the format string GH-35671 Column ordering when combining group_by() and across() now matches dplyr GH-35473 Link to correct version of OpenSSL when using autobrew GH-36551 Require cmake 3.16 in bundled build script GH-36321 For more on what’s in the 13.0.0 R package, see the R changelog. Ruby and C GLib notes Ruby Bug fixes Fixed GC-related issue against random segfault in hash join GH-35819 Fixed segfault in CallExpression.new GH-35915 Improvements FlightRPC: Added a convenient wrapper for the authentication method GH-35435 Added empty table support in #select_columns GH-35681 Added optional hash support in Expression.try_convert GH-35915 Parquet: Added Parquet::ArrowFileReader#each_row_group GH-36008 Added support of the automatic installation of arrow-c-glib on Conda environment GH-36287 C GLib Bug fixes Parquet: Fixed GC-related bug in metadata dependencies GH-35266 Fixed potentially GC-related issue against random segfault in hash join GH-35819 Improvements FlightRPC: Added support to pass GAFlightServerCallContext object in several methods of GAFlightServerCustomAuthHandler GH-35377 FlightSQL: Added support for INSERT/UPDATE/DELETE GH-36408 Added GArrowRunEndEncodedDataType GH-35417 Added GArrowRunEndEncodedArray GH-35418 Rust notes The Rust projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Rust implementation, see the latest Arrow Rust changelog.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Aggregating Millions of Groups Fast in Apache Arrow DataFusion 28.0.0</title><link href="https://arrow.apache.org/blog/2023/08/05/datafusion_fast_grouping/" rel="alternate" type="text/html" title="Aggregating Millions of Groups Fast in Apache Arrow DataFusion 28.0.0" /><published>2023-08-05T00:00:00-04:00</published><updated>2023-08-05T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/08/05/datafusion_fast_grouping</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/08/05/datafusion_fast_grouping/"><![CDATA[<!--

-->

<!--- Converted from Google Docs using https://www.buymeacoffee.com/docstomarkdown --->

<h2 id="aggregating-millions-of-groups-fast-in-apache-arrow-datafusion">Aggregating Millions of Groups Fast in Apache Arrow DataFusion</h2>

<p>Andrew Lamb, Daniël Heres, Raphael Taylor-Davies,</p>

<p><em>Note: this article was originally published on the <a href="https://www.influxdata.com/blog/aggregating-millions-groups-fast-apache-arrow-datafusion">InfluxData Blog</a></em></p>

<h2 id="tldr">TLDR</h2>

<p>Grouped aggregations are a core part of any analytic tool, creating understandable summaries of huge data volumes. <a href="https://arrow.apache.org/datafusion/">Apache Arrow DataFusion</a>’s parallel aggregation capability is 2-3x faster in the <a href="https://crates.io/crates/datafusion/28.0.0">newly released version <code class="language-plaintext highlighter-rouge">28.0.0</code></a> for queries with a large number (10,000 or more) of groups.</p>

<p>Improving aggregation performance matters to all users of DataFusion. For example, both InfluxDB, a <a href="https://github.com/influxdata/influxdb">time series data platform</a> and Coralogix, a <a href="https://coralogix.com/?utm_source=InfluxDB&amp;utm_medium=Blog&amp;utm_campaign=organic">full-stack observability</a> platform, aggregate vast amounts of raw data to monitor and create insights for our customers. Improving DataFusion’s performance lets us provide better user experiences by generating insights faster with fewer resources. Because DataFusion is open source and released under the permissive <a href="https://github.com/apache/arrow-datafusion/blob/main/LICENSE.txt">Apache 2.0</a> license, the whole DataFusion community benefits as well.</p>

<p>With the new optimizations, DataFusion’s grouping speed is now close to DuckDB, a system that regularly reports <a href="https://duckdblabs.github.io/db-benchmark/">great</a> <a href="https://duckdb.org/2022/03/07/aggregate-hashtable.html#experiments">grouping</a> benchmark performance numbers. Figure 1 contains a representative sample of <a href="https://github.com/ClickHouse/ClickBench/tree/main">ClickBench</a> on a single Parquet file, and the full results are at the end of this article.</p>

<p><img src="/assets/datafusion_fast_grouping/summary.png" width="700" /></p>

<p><strong>Figure 1</strong>: Query performance for ClickBench queries on queries 16, 17, 18 and 19 on a single Parquet file for DataFusion <code class="language-plaintext highlighter-rouge">27.0.0</code>, DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code> and DuckDB <code class="language-plaintext highlighter-rouge">0.8.1</code>.</p>

<h2 id="introduction-to-high-cardinality-grouping">Introduction to high cardinality grouping</h2>

<p>Aggregation is a fancy word for computing summary statistics across many rows that have the same value in one or more columns. We call the rows with the same values <em>groups</em> and “high cardinality” means there are a large number of distinct groups in the dataset. At the time of writing, a “large” number of groups in analytic engines is around 10,000.</p>

<p>For example the <a href="https://github.com/ClickHouse/ClickBench">ClickBench</a> <em>hits</em> dataset contains 100 million anonymized user clicks across a set of websites. ClickBench Query 17 is:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="nv">"UserID"</span><span class="p">,</span> <span class="nv">"SearchPhrase"</span><span class="p">,</span> <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span>
<span class="k">FROM</span> <span class="n">hits</span>
<span class="k">GROUP</span> <span class="k">BY</span> <span class="nv">"UserID"</span><span class="p">,</span> <span class="nv">"SearchPhrase"</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span>
<span class="k">DESC</span> <span class="k">LIMIT</span> <span class="mi">10</span><span class="p">;</span>
</code></pre></div></div>

<p>In English, this query finds “the top ten (user, search phrase) combinations, across all clicks” and produces the following results (there are no search phrases for the top ten users):</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+---------------------+--------------+-----------------+
| UserID              | SearchPhrase | COUNT(UInt8(1)) |
+---------------------+--------------+-----------------+
| 1313338681122956954 |              | 29097           |
| 1907779576417363396 |              | 25333           |
| 2305303682471783379 |              | 10597           |
| 7982623143712728547 |              | 6669            |
| 7280399273658728997 |              | 6408            |
| 1090981537032625727 |              | 6196            |
| 5730251990344211405 |              | 6019            |
| 6018350421959114808 |              | 5990            |
| 835157184735512989  |              | 5209            |
| 770542365400669095  |              | 4906            |
+---------------------+--------------+-----------------+
</code></pre></div></div>

<p>The ClickBench dataset contains</p>

<ul>
  <li>99,997,497 total rows<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></li>
  <li>17,630,976 different users (distinct UserIDs)<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></li>
  <li>6,019,103 different search phrases<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup></li>
  <li>24,070,560 distinct combinations<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup> of (UserID, SearchPhrase)
Thus, to answer the query, DataFusion must map each of the 100M different input rows into one of the <strong>24 million different groups</strong>, and keep count of how many such rows there are in each group.</li>
</ul>

<h2 id="the-solution">The solution</h2>

<p>Like most concepts in databases and other analytic systems, the basic ideas of this algorithm are straightforward and taught in introductory computer science courses. You could compute the query with a program such as this<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>

<span class="c1"># read file
</span><span class="n">hits</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="s">'hits.parquet'</span><span class="p">,</span> <span class="n">engine</span><span class="o">=</span><span class="s">'pyarrow'</span><span class="p">)</span>

<span class="c1"># build groups
</span><span class="n">counts</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">hits</span><span class="p">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">group</span> <span class="o">=</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s">'UserID'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s">'SearchPhrase'</span><span class="p">]);</span>
    <span class="c1"># update the dict entry for the corresponding key
</span>    <span class="n">counts</span><span class="p">[</span><span class="n">group</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># Print the top 10 values
</span><span class="k">print</span> <span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">counts</span><span class="p">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]))</span>
</code></pre></div></div>

<p>This approach, while simple, is both slow and very memory inefficient. It requires over 40 seconds to compute the results for less than 1% of the dataset<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup>. Both DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code> and DuckDB <code class="language-plaintext highlighter-rouge">0.8.1</code> compute results in under 10 seconds for the <em>entire</em> dataset.</p>

<p>To answer this query quickly and efficiently, you have to write your code such that it:</p>

<ol>
  <li>Keeps all cores busy aggregating via parallelized computation</li>
  <li>Updates aggregate values quickly, using vectorizable loops that are easy for compilers to translate into the high performance <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data">SIMD</a> instructions available in modern CPUs.</li>
</ol>

<p>The rest of this article explains how grouping works in DataFusion and the improvements we made in <code class="language-plaintext highlighter-rouge">28.0.0</code>.</p>

<h3 id="two-phase-parallel-partitioned-grouping">Two phase parallel partitioned grouping</h3>

<p>Both DataFusion <code class="language-plaintext highlighter-rouge">27.0.</code> and <code class="language-plaintext highlighter-rouge">28.0.0</code> use state-of-the-art, two phase parallel hash partitioned grouping, similar to other high-performance vectorized engines like <a href="https://duckdb.org/2022/03/07/aggregate-hashtable.html">DuckDB’s Parallel Grouped Aggregates</a>. In pictures this looks like:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            ▲                        ▲
            │                        │
            │                        │
            │                        │
┌───────────────────────┐  ┌───────────────────┐
│        GroupBy        │  │      GroupBy      │      Step 4
│        (Final)        │  │      (Final)      │
└───────────────────────┘  └───────────────────┘
            ▲                        ▲
            │                        │
            └────────────┬───────────┘
                         │
                         │
            ┌─────────────────────────┐
            │       Repartition       │               Step 3
            │         HASH(x)         │
            └─────────────────────────┘
                         ▲
                         │
            ┌────────────┴──────────┐
            │                       │
            │                       │
 ┌────────────────────┐  ┌─────────────────────┐
 │      GroupyBy      │  │       GroupBy       │      Step 2
 │     (Partial)      │  │      (Partial)      │
 └────────────────────┘  └─────────────────────┘
            ▲                       ▲
         ┌──┘                       └─┐
         │                            │
    .─────────.                  .─────────.
 ,─'           '─.            ,─'           '─.
;      Input      :          ;      Input      :      Step 1
:    Stream 1     ;          :    Stream 2     ;
 ╲               ╱            ╲               ╱
  '─.         ,─'              '─.         ,─'
     `───────'                    `───────'
</code></pre></div></div>

<p><strong>Figure 2</strong>: Two phase repartitioned grouping: data flows from bottom (source) to top (results) in two phases. First (Steps 1 and 2), each core reads the data into a core-specific hash table, computing intermediate aggregates without any cross-core coordination. Then (Steps 3 and 4) DataFusion divides the data (“repartitions”) into distinct subsets by group value, and each subset is sent to a specific core which computes the final aggregate.</p>

<p>The two phases are critical for keeping cores busy in a multi-core system. Both phases use the same hash table approach (explained in the next section), but differ in how the groups are distributed and the partial results emitted from the accumulators. The first phase aggregates data as soon as possible after it is produced. However, as shown in Figure 2, the groups can be anywhere in any input, so the same group is often found on many different cores. The second phase uses a hash function to redistribute data evenly across the cores, so each group value is processed by exactly one core which emits the final results for that group.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ┌─────┐    ┌─────┐
    │  1  │    │  3  │
    │  2  │    │  4  │   2. After Repartitioning: each
    └─────┘    └─────┘   group key  appears in exactly
    ┌─────┐    ┌─────┐   one partition
    │  1  │    │  3  │
    │  2  │    │  4  │
    └─────┘    └─────┘

─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─

    ┌─────┐    ┌─────┐
    │  2  │    │  2  │
    │  1  │    │  2  │
    │  3  │    │  3  │
    │  4  │    │  1  │
    └─────┘    └─────┘    1. Input Stream: groups
      ...        ...      values are spread
    ┌─────┐    ┌─────┐    arbitrarily over each input
    │  1  │    │  4  │
    │  4  │    │  3  │
    │  1  │    │  1  │
    │  4  │    │  3  │
    │  3  │    │  2  │
    │  2  │    │  2  │
    │  2  │    └─────┘
    └─────┘

    Core A      Core B

</code></pre></div></div>

<p><strong>Figure 3</strong>: Group value distribution across 2 cores during aggregation phases. In the first phase, every group value <code class="language-plaintext highlighter-rouge">1</code>, <code class="language-plaintext highlighter-rouge">2</code>, <code class="language-plaintext highlighter-rouge">3</code>, <code class="language-plaintext highlighter-rouge">4</code>, is present in the input stream processed by each core. In the second phase, after repartitioning, the group values <code class="language-plaintext highlighter-rouge">1</code> and <code class="language-plaintext highlighter-rouge">2</code> are processed by core A, and values <code class="language-plaintext highlighter-rouge">3</code> and <code class="language-plaintext highlighter-rouge">4</code> are processed only by core B.</p>

<p>There are some additional subtleties in the <a href="https://github.com/apache/arrow-datafusion/blob/main/datafusion/core/src/physical_plan/aggregates/row_hash.rs">DataFusion implementation</a> not mentioned above due to space constraints, such as:</p>

<ol>
  <li>The policy of when to emit data from the first phase’s hash table (e.g. because the data is partially sorted)</li>
  <li>Handling specific filters per aggregate (due to the <code class="language-plaintext highlighter-rouge">FILTER</code> SQL clause)</li>
  <li>Data types of intermediate values (which may not be the same as the final output for some aggregates such as <code class="language-plaintext highlighter-rouge">AVG</code>).</li>
  <li>Action taken when memory use exceeds its budget.</li>
</ol>

<h3 id="hash-grouping">Hash grouping</h3>

<p>DataFusion queries can compute many different aggregate functions for each group, both <a href="https://arrow.apache.org/datafusion/user-guide/sql/aggregate_functions.html">built in</a> and/or user defined <a href="https://docs.rs/datafusion/latest/datafusion/logical_expr/struct.AggregateUDF.html"><code class="language-plaintext highlighter-rouge">AggregateUDFs</code></a>. The state for each aggregate function, called an <em>accumulator</em>, is tracked with a hash table (DataFusion uses the excellent <a href="https://docs.rs/hashbrown/latest/hashbrown/index.html">HashBrown</a> <a href="https://docs.rs/hashbrown/latest/hashbrown/raw/struct.RawTable.html">RawTable API</a>), which logically stores the “index”  identifying the specific group value.</p>

<h3 id="hash-grouping-in-2700">Hash grouping in <code class="language-plaintext highlighter-rouge">27.0.0</code></h3>

<p>As shown in Figure 3, DataFusion <code class="language-plaintext highlighter-rouge">27.0.0</code> stores the data in a <a href="https://github.com/apache/arrow-datafusion/blob/4d93b6a3802151865b68967bdc4c7d7ef425b49a/datafusion/core/src/physical_plan/aggregates/utils.rs#L38-L50"><code class="language-plaintext highlighter-rouge">GroupState</code></a> structure which, unsurprisingly, tracks the state for each group. The state for each group consists of:</p>

<ol>
  <li>The actual value of the group columns, in <a href="https://docs.rs/arrow-row/latest/arrow_row/index.html">Arrow Row</a> format.</li>
  <li>In-progress accumulations (e.g. the running counts for the <code class="language-plaintext highlighter-rouge">COUNT</code> aggregate) for each group, in one of two possible formats (<a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/expr/src/accumulator.rs#L24-L49"><code class="language-plaintext highlighter-rouge">Accumulator</code></a>  or <a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/physical-expr/src/aggregate/row_accumulator.rs#L26-L46"><code class="language-plaintext highlighter-rouge">RowAccumulator</code></a>).</li>
  <li>Scratch space for tracking which rows match each aggregate in each batch.</li>
</ol>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                           ┌──────────────────────────────────────┐
                           │                                      │
                           │                  ...                 │
                           │ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ │
                           │ ┃                                  ┃ │
    ┌─────────┐            │ ┃ ┌──────────────────────────────┐ ┃ │
    │         │            │ ┃ │group values: OwnedRow        │ ┃ │
    │ ┌─────┐ │            │ ┃ └──────────────────────────────┘ ┃ │
    │ │  5  │ │            │ ┃ ┌──────────────────────────────┐ ┃ │
    │ ├─────┤ │            │ ┃ │Row accumulator:              │ ┃ │
    │ │  9  │─┼────┐       │ ┃ │Vec&lt;u8&gt;                       │ ┃ │
    │ ├─────┤ │    │       │ ┃ └──────────────────────────────┘ ┃ │
    │ │ ... │ │    │       │ ┃ ┌──────────────────────┐         ┃ │
    │ ├─────┤ │    │       │ ┃ │┌──────────────┐      │         ┃ │
    │ │  1  │ │    │       │ ┃ ││Accumulator 1 │      │         ┃ │
    │ ├─────┤ │    │       │ ┃ │└──────────────┘      │         ┃ │
    │ │ ... │ │    │       │ ┃ │┌──────────────┐      │         ┃ │
    │ └─────┘ │    │       │ ┃ ││Accumulator 2 │      │         ┃ │
    │         │    │       │ ┃ │└──────────────┘      │         ┃ │
    └─────────┘    │       │ ┃ │ Box&lt;dyn Accumulator&gt; │         ┃ │
    Hash Table     │       │ ┃ └──────────────────────┘         ┃ │
                   │       │ ┃ ┌─────────────────────────┐      ┃ │
                   │       │ ┃ │scratch indices: Vec&lt;u32&gt;│      ┃ │
                   │       │ ┃ └─────────────────────────┘      ┃ │
                   │       │ ┃ GroupState                       ┃ │
                   └─────▶ │ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │
                           │                                      │
  Hash table tracks an     │                 ...                  │
  index into group_states  │                                      │
                           └──────────────────────────────────────┘
                           group_states: Vec&lt;GroupState&gt;

                           There is one GroupState PER GROUP

</code></pre></div></div>

<p><strong>Figure 4</strong>: Hash group operator structure in DataFusion <code class="language-plaintext highlighter-rouge">27.0.0</code>. A hash table maps each group to a GroupState which contains all the per-group states.</p>

<p>To compute the aggregate, DataFusion performs the following steps for each input batch:</p>

<ol>
  <li>Calculate hash using <a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/physical-expr/src/hash_utils.rs#L264-L307">efficient vectorized code</a>, specialized for each data type.</li>
  <li>Determine group indexes for each input row using the hash table (creating new entries for newly seen groups).</li>
  <li><a href="https://github.com/apache/arrow-datafusion/blob/4ab8be57dee3bfa72dd105fbd7b8901b873a4878/datafusion/core/src/physical_plan/aggregates/row_hash.rs#L562-L602">Update Accumulators for each group that had input rows,</a> assembling the rows into a contiguous range for vectorized accumulator if there are a sufficient number of them.</li>
</ol>

<p>DataFusion also stores the hash values in the table to avoid potentially costly hash recomputation when resizing the hash table.</p>

<p>This scheme works very well for a relatively small number of distinct groups: all accumulators are efficiently updated with large contiguous batches of rows.</p>

<p>However, this scheme is not ideal for high cardinality grouping due to:</p>

<ol>
  <li><strong>Multiple allocations per group</strong> for the group value row format, as well as for the <code class="language-plaintext highlighter-rouge">RowAccumulator</code>s and each  <code class="language-plaintext highlighter-rouge">Accumulator</code>. The <code class="language-plaintext highlighter-rouge">Accumulator</code> may have additional allocations within it as well.</li>
  <li><strong>Non-vectorized updates:</strong> Accumulator updates often fall back to a slower non-vectorized form because the number of distinct groups is large (and thus number of values per group is small) in each input batch.</li>
</ol>

<h3 id="hash-grouping-in-2800">Hash grouping in <code class="language-plaintext highlighter-rouge">28.0.0</code></h3>

<p>For <code class="language-plaintext highlighter-rouge">28.0.0</code>, we rewrote the core group by implementation following traditional system optimization principles: fewer allocations, type specialization, and aggressive vectorization.</p>

<p>DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code> uses the same RawTable and still stores group indexes. The major differences, as shown in Figure 4, are:</p>

<ol>
  <li>Group values are stored either
    <ol>
      <li>Inline in the <code class="language-plaintext highlighter-rouge">RawTable</code> (for single columns of primitive types), where the conversion to Row format costs more than its benefit</li>
      <li>In a separate <a href="https://docs.rs/arrow-row/latest/arrow_row/struct.Row.html">Rows</a> structure with a single contiguous allocation for all groups values, rather than an allocation per group. Accumulators manage the state for all the groups internally, so the code to update intermediate values is a tight type specialized loop. The new <a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/physical-expr/src/aggregate/groups_accumulator/mod.rs#L66-L75"><code class="language-plaintext highlighter-rouge">GroupsAccumulator</code></a> interface results in highly efficient type accumulator update loops.</li>
    </ol>
  </li>
</ol>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌───────────────────────────────────┐     ┌───────────────────────┐
│ ┌ ─ ─ ─ ─ ─ ┐  ┌─────────────────┐│     │ ┏━━━━━━━━━━━━━━━━━━━┓ │
│                │                 ││     │ ┃  ┌──────────────┐ ┃ │
│ │           │  │ ┌ ─ ─ ┐┌─────┐  ││     │ ┃  │┌───────────┐ │ ┃ │
│                │    X   │  5  │  ││     │ ┃  ││  value1   │ │ ┃ │
│ │           │  │ ├ ─ ─ ┤├─────┤  ││     │ ┃  │└───────────┘ │ ┃ │
│                │    Q   │  9  │──┼┼──┐  │ ┃  │     ...      │ ┃ │
│ │           │  │ ├ ─ ─ ┤├─────┤  ││  └──┼─╋─▶│              │ ┃ │
│                │   ...  │ ... │  ││     │ ┃  │┌───────────┐ │ ┃ │
│ │           │  │ ├ ─ ─ ┤├─────┤  ││     │ ┃  ││  valueN   │ │ ┃ │
│                │    H   │  1  │  ││     │ ┃  │└───────────┘ │ ┃ │
│ │           │  │ ├ ─ ─ ┤├─────┤  ││     │ ┃  │values: Vec&lt;T&gt;│ ┃ │
│     Rows       │   ...  │ ... │  ││     │ ┃  └──────────────┘ ┃ │
│ │           │  │ └ ─ ─ ┘└─────┘  ││     │ ┃                   ┃ │
│  ─ ─ ─ ─ ─ ─   │                 ││     │ ┃ GroupsAccumulator ┃ │
│                └─────────────────┘│     │ ┗━━━━━━━━━━━━━━━━━━━┛ │
│                  Hash Table       │     │                       │
│                                   │     │          ...          │
└───────────────────────────────────┘     └───────────────────────┘
  GroupState                               Accumulators


Hash table value stores group_indexes     One  GroupsAccumulator
and group values.                         per aggregate. Each
                                          stores the state for
Group values are stored either inline     *ALL* groups, typically
in the hash table or in a single          using a native Vec&lt;T&gt;
allocation using the arrow Row format
</code></pre></div></div>

<p><strong>Figure 5</strong>: Hash group operator structure in DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code>. Group values are stored either directly in the hash table, or in a single allocation using the arrow Row format. The hash table contains group indexes. A single <code class="language-plaintext highlighter-rouge">GroupsAccumulator</code> stores the per-aggregate state for <em>all</em> groups.</p>

<p>This new structure improves performance significantly for high cardinality groups due to:</p>

<ol>
  <li><strong>Reduced allocations</strong>: There are no longer any individual allocations per group.</li>
  <li><strong>Contiguous native accumulator states</strong>: Type-specialized accumulators store the values for all groups in a single contiguous allocation using a <a href="https://doc.rust-lang.org/std/vec/struct.Vec.html">Rust Vec&lt;T&gt;</a> of some native type.</li>
  <li><strong>Vectorized state update</strong>: The inner aggregate update loops, which are type-specialized and in terms of native <code class="language-plaintext highlighter-rouge">Vec</code>s, are well-vectorized by the Rust compiler (thanks <a href="https://llvm.org/">LLVM</a>!).</li>
</ol>

<h3 id="notes">Notes</h3>

<p>Some vectorized grouping implementations store the accumulator state row-wise directly in the hash table, which often uses modern CPU caches efficiently. Managing accumulator state in columnar fashion may sacrifice some cache locality, however it ensures the size of the hash table remains small, even when there are large numbers of groups and aggregates, making it easier for the compiler to vectorize the accumulator update.</p>

<p>Depending on the cost of recomputing hash values, DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code> may or may not store the hash values in the table. This optimizes the tradeoff between the cost of computing the hash value (which is expensive for strings, for example) vs. the cost of storing it in the hash table.</p>

<p>One subtlety that arises from pushing state updates into GroupsAccumulators is that each accumulator must handle similar variations with/without filtering and with/without nulls in the input. DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code> uses a templated <a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/physical-expr/src/aggregate/groups_accumulator/accumulate.rs#L28-L54"><code class="language-plaintext highlighter-rouge">NullState</code></a> which encapsulates these common patterns across accumulators.</p>

<p>The code structure is heavily influenced by the fact DataFusion is implemented using <a href="https://www.rust-lang.org/">Rust</a>, a new(ish) systems programming language focused on speed and safety. Rust heavily discourages many of the traditional pointer casting “tricks” used in C/C++ hash grouping implementations. The DataFusion aggregation code is almost entirely <a href="https://doc.rust-lang.org/nomicon/meet-safe-and-unsafe.html#:~:text=Safe%20Rust%20is%20the%20true,Undefined%20Behavior%20(a.k.a.%20UB)."><code class="language-plaintext highlighter-rouge">safe</code></a>, deviating into <code class="language-plaintext highlighter-rouge">unsafe</code> only when necessary. (Rust is a great choice because it makes DataFusion fast, easy to embed, and prevents many crashes and security issues often associated with multi-threaded C/C++ code).</p>

<h2 id="clickbench-results">ClickBench results</h2>

<p>The full results of running the <a href="https://github.com/ClickHouse/ClickBench/tree/main">ClickBench</a> queries against the single Parquet file with DataFusion <code class="language-plaintext highlighter-rouge">27.0.0</code>, DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code>, and DuckDB <code class="language-plaintext highlighter-rouge">0.8.1</code> are below. These numbers were run on a GCP <code class="language-plaintext highlighter-rouge">e2-standard-8 machine</code> with 8 cores and 32 GB of RAM, using the scripts <a href="https://github.com/alamb/datafusion-duckdb-benchmark">here</a>.</p>

<p>As the industry moves towards data systems assembled from components, it is increasingly important that they exchange data using open standards such as <a href="https://arrow.apache.org/">Apache Arrow</a> and <a href="https://parquet.apache.org/">Parquet</a> rather than custom storage and in-memory formats. Thus, this benchmark uses a single input Parquet file representative of many DataFusion users and aligned with the current trend in analytics of avoiding a costly load/transformation into a custom storage format prior to query.</p>

<p>DataFusion now reaches near-DuckDB-speeds querying Parquet data. While we don’t plan to engage in a benchmarking shootout with a team that literally wrote <a href="https://dl.acm.org/doi/abs/10.1145/3209950.3209955">Fair Benchmarking Considered Difficult</a>, hopefully everyone can agree that DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code> is a significant improvement.</p>

<p><img src="/assets/datafusion_fast_grouping/full.png" width="700" /></p>

<p><strong>Figure 6</strong>: Performance of DataFusion <code class="language-plaintext highlighter-rouge">27.0.0</code>, DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code>, and DuckDB <code class="language-plaintext highlighter-rouge">0.8.1</code> on all 43 ClickBench queries against a single <code class="language-plaintext highlighter-rouge">hits.parquet</code> file. Lower is better.</p>

<h3 id="notes-1">Notes</h3>

<p>DataFusion <code class="language-plaintext highlighter-rouge">27.0.0</code> was not able to run several queries due to either planner bugs (Q9, Q11, Q12, 14) or running out of memory (Q33). DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code> solves those issues.</p>

<p>DataFusion is faster than DuckDB for query 21 and 22, likely due to optimized implementations of string pattern matching.</p>

<h2 id="conclusion-performance-matters">Conclusion: performance matters</h2>

<p>Improving aggregation performance by more than a factor of two allows developers building products and projects with DataFusion to spend more time on value-added domain specific features. We believe building systems with DataFusion is much faster than trying to build something similar from scratch. DataFusion increases productivity because it eliminates the need to rebuild well-understood, but costly to implement, analytic database technology. While we’re pleased with the improvements in DataFusion <code class="language-plaintext highlighter-rouge">28.0.0</code>, we are by no means done and are pursuing <a href="https://github.com/apache/arrow-datafusion/issues/7000">(Even More) Aggregation Performance</a>. The future for performance is bright.</p>

<h2 id="acknowledgments">Acknowledgments</h2>

<p>DataFusion is a <a href="https://arrow.apache.org/datafusion/contributor-guide/communication.html">community effort</a> and this work was not possible without contributions from many in the community. A special shout out to <a href="https://github.com/sunchao">sunchao</a>, <a href="https://github.com/jyshen">yjshen</a>, <a href="https://github.com/yahoNanJing">yahoNanJing</a>, <a href="https://github.com/mingmwang">mingmwang</a>, <a href="https://github.com/ozankabak">ozankabak</a>, <a href="https://github.com/mustafasrepo">mustafasrepo</a>, and everyone else who contributed ideas, reviews, and encouragement <a href="https://github.com/apache/arrow-datafusion/pull/6800">during</a> this <a href="https://github.com/apache/arrow-datafusion/pull/6904">work</a>.</p>

<h2 id="about-datafusion">About DataFusion</h2>

<p><a href="https://arrow.apache.org/datafusion/">Apache Arrow DataFusion</a> is an extensible query engine and database toolkit, written in <a href="https://www.rust-lang.org/">Rust</a>, that uses <a href="https://arrow.apache.org/">Apache Arrow</a> as its in-memory format. DataFusion, along with <a href="https://calcite.apache.org/">Apache Calcite</a>, Facebook’s <a href="https://github.com/facebookincubator/velox">Velox</a>, and similar technology are part of the next generation “<a href="https://www.usenix.org/publications/login/winter2018/khurana">Deconstructed Database</a>” architectures, where new systems are built on a foundation of fast, modular components, rather than as a single tightly integrated system.</p>

<!-- Footnotes themselves at the bottom. -->
<h2 id="notes-2">Notes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p><code class="language-plaintext highlighter-rouge">SELECT COUNT(*) FROM 'hits.parquet';</code> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p><code class="language-plaintext highlighter-rouge">SELECT COUNT(DISTINCT "UserID") as num_users FROM 'hits.parquet';</code> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p><code class="language-plaintext highlighter-rouge">SELECT COUNT(DISTINCT "SearchPhrase") as num_phrases FROM 'hits.parquet';</code> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p><code class="language-plaintext highlighter-rouge">SELECT COUNT(*) FROM (SELECT DISTINCT "UserID", "SearchPhrase" FROM 'hits.parquet')</code> <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Full script at <a href="https://github.com/alamb/datafusion-duckdb-benchmark/blob/main/hash.py">hash.py</a> <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p><a href="https://datasets.clickhouse.com/hits_compatible/athena_partitioned/hits_%7B%7D.parquet">hits_0.parquet</a>, one of the files from the partitioned ClickBench dataset, which has <code class="language-plaintext highlighter-rouge">100,000</code> rows and is 117 MB in size. The entire dataset has <code class="language-plaintext highlighter-rouge">100,000,000</code> rows in a single 14 GB Parquet file. The script did not complete on the entire dataset after 40 minutes, and used 212 GB RAM at peak. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>alamb, Dandandan, tustvold</name></author><category term="release" /><summary type="html"><![CDATA[Aggregating Millions of Groups Fast in Apache Arrow DataFusion Andrew Lamb, Daniël Heres, Raphael Taylor-Davies, Note: this article was originally published on the InfluxData Blog TLDR Grouped aggregations are a core part of any analytic tool, creating understandable summaries of huge data volumes. Apache Arrow DataFusion’s parallel aggregation capability is 2-3x faster in the newly released version 28.0.0 for queries with a large number (10,000 or more) of groups. Improving aggregation performance matters to all users of DataFusion. For example, both InfluxDB, a time series data platform and Coralogix, a full-stack observability platform, aggregate vast amounts of raw data to monitor and create insights for our customers. Improving DataFusion’s performance lets us provide better user experiences by generating insights faster with fewer resources. Because DataFusion is open source and released under the permissive Apache 2.0 license, the whole DataFusion community benefits as well. With the new optimizations, DataFusion’s grouping speed is now close to DuckDB, a system that regularly reports great grouping benchmark performance numbers. Figure 1 contains a representative sample of ClickBench on a single Parquet file, and the full results are at the end of this article. Figure 1: Query performance for ClickBench queries on queries 16, 17, 18 and 19 on a single Parquet file for DataFusion 27.0.0, DataFusion 28.0.0 and DuckDB 0.8.1. Introduction to high cardinality grouping Aggregation is a fancy word for computing summary statistics across many rows that have the same value in one or more columns. We call the rows with the same values groups and “high cardinality” means there are a large number of distinct groups in the dataset. At the time of writing, a “large” number of groups in analytic engines is around 10,000. For example the ClickBench hits dataset contains 100 million anonymized user clicks across a set of websites. ClickBench Query 17 is: SELECT "UserID", "SearchPhrase", COUNT(*) FROM hits GROUP BY "UserID", "SearchPhrase" ORDER BY COUNT(*) DESC LIMIT 10; In English, this query finds “the top ten (user, search phrase) combinations, across all clicks” and produces the following results (there are no search phrases for the top ten users): +---------------------+--------------+-----------------+ | UserID | SearchPhrase | COUNT(UInt8(1)) | +---------------------+--------------+-----------------+ | 1313338681122956954 | | 29097 | | 1907779576417363396 | | 25333 | | 2305303682471783379 | | 10597 | | 7982623143712728547 | | 6669 | | 7280399273658728997 | | 6408 | | 1090981537032625727 | | 6196 | | 5730251990344211405 | | 6019 | | 6018350421959114808 | | 5990 | | 835157184735512989 | | 5209 | | 770542365400669095 | | 4906 | +---------------------+--------------+-----------------+ The ClickBench dataset contains 99,997,497 total rows1 17,630,976 different users (distinct UserIDs)2 6,019,103 different search phrases3 24,070,560 distinct combinations4 of (UserID, SearchPhrase) Thus, to answer the query, DataFusion must map each of the 100M different input rows into one of the 24 million different groups, and keep count of how many such rows there are in each group. The solution Like most concepts in databases and other analytic systems, the basic ideas of this algorithm are straightforward and taught in introductory computer science courses. You could compute the query with a program such as this5: import pandas as pd from collections import defaultdict from operator import itemgetter # read file hits = pd.read_parquet('hits.parquet', engine='pyarrow') # build groups counts = defaultdict(int) for index, row in hits.iterrows(): group = (row['UserID'], row['SearchPhrase']); # update the dict entry for the corresponding key counts[group] += 1 # Print the top 10 values print (dict(sorted(counts.items(), key=itemgetter(1), reverse=True)[:10])) This approach, while simple, is both slow and very memory inefficient. It requires over 40 seconds to compute the results for less than 1% of the dataset6. Both DataFusion 28.0.0 and DuckDB 0.8.1 compute results in under 10 seconds for the entire dataset. To answer this query quickly and efficiently, you have to write your code such that it: Keeps all cores busy aggregating via parallelized computation Updates aggregate values quickly, using vectorizable loops that are easy for compilers to translate into the high performance SIMD instructions available in modern CPUs. The rest of this article explains how grouping works in DataFusion and the improvements we made in 28.0.0. Two phase parallel partitioned grouping Both DataFusion 27.0. and 28.0.0 use state-of-the-art, two phase parallel hash partitioned grouping, similar to other high-performance vectorized engines like DuckDB’s Parallel Grouped Aggregates. In pictures this looks like: ▲ ▲ │ │ │ │ │ │ ┌───────────────────────┐ ┌───────────────────┐ │ GroupBy │ │ GroupBy │ Step 4 │ (Final) │ │ (Final) │ └───────────────────────┘ └───────────────────┘ ▲ ▲ │ │ └────────────┬───────────┘ │ │ ┌─────────────────────────┐ │ Repartition │ Step 3 │ HASH(x) │ └─────────────────────────┘ ▲ │ ┌────────────┴──────────┐ │ │ │ │ ┌────────────────────┐ ┌─────────────────────┐ │ GroupyBy │ │ GroupBy │ Step 2 │ (Partial) │ │ (Partial) │ └────────────────────┘ └─────────────────────┘ ▲ ▲ ┌──┘ └─┐ │ │ .─────────. .─────────. ,─' '─. ,─' '─. ; Input : ; Input : Step 1 : Stream 1 ; : Stream 2 ; ╲ ╱ ╲ ╱ '─. ,─' '─. ,─' `───────' `───────' Figure 2: Two phase repartitioned grouping: data flows from bottom (source) to top (results) in two phases. First (Steps 1 and 2), each core reads the data into a core-specific hash table, computing intermediate aggregates without any cross-core coordination. Then (Steps 3 and 4) DataFusion divides the data (“repartitions”) into distinct subsets by group value, and each subset is sent to a specific core which computes the final aggregate. The two phases are critical for keeping cores busy in a multi-core system. Both phases use the same hash table approach (explained in the next section), but differ in how the groups are distributed and the partial results emitted from the accumulators. The first phase aggregates data as soon as possible after it is produced. However, as shown in Figure 2, the groups can be anywhere in any input, so the same group is often found on many different cores. The second phase uses a hash function to redistribute data evenly across the cores, so each group value is processed by exactly one core which emits the final results for that group. ┌─────┐ ┌─────┐ │ 1 │ │ 3 │ │ 2 │ │ 4 │ 2. After Repartitioning: each └─────┘ └─────┘ group key appears in exactly ┌─────┐ ┌─────┐ one partition │ 1 │ │ 3 │ │ 2 │ │ 4 │ └─────┘ └─────┘ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌─────┐ ┌─────┐ │ 2 │ │ 2 │ │ 1 │ │ 2 │ │ 3 │ │ 3 │ │ 4 │ │ 1 │ └─────┘ └─────┘ 1. Input Stream: groups ... ... values are spread ┌─────┐ ┌─────┐ arbitrarily over each input │ 1 │ │ 4 │ │ 4 │ │ 3 │ │ 1 │ │ 1 │ │ 4 │ │ 3 │ │ 3 │ │ 2 │ │ 2 │ │ 2 │ │ 2 │ └─────┘ └─────┘ Core A Core B Figure 3: Group value distribution across 2 cores during aggregation phases. In the first phase, every group value 1, 2, 3, 4, is present in the input stream processed by each core. In the second phase, after repartitioning, the group values 1 and 2 are processed by core A, and values 3 and 4 are processed only by core B. There are some additional subtleties in the DataFusion implementation not mentioned above due to space constraints, such as: The policy of when to emit data from the first phase’s hash table (e.g. because the data is partially sorted) Handling specific filters per aggregate (due to the FILTER SQL clause) Data types of intermediate values (which may not be the same as the final output for some aggregates such as AVG). Action taken when memory use exceeds its budget. Hash grouping DataFusion queries can compute many different aggregate functions for each group, both built in and/or user defined AggregateUDFs. The state for each aggregate function, called an accumulator, is tracked with a hash table (DataFusion uses the excellent HashBrown RawTable API), which logically stores the “index” identifying the specific group value. Hash grouping in 27.0.0 As shown in Figure 3, DataFusion 27.0.0 stores the data in a GroupState structure which, unsurprisingly, tracks the state for each group. The state for each group consists of: The actual value of the group columns, in Arrow Row format. In-progress accumulations (e.g. the running counts for the COUNT aggregate) for each group, in one of two possible formats (Accumulator or RowAccumulator). Scratch space for tracking which rows match each aggregate in each batch. ┌──────────────────────────────────────┐ │ │ │ ... │ │ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ │ │ ┃ ┃ │ ┌─────────┐ │ ┃ ┌──────────────────────────────┐ ┃ │ │ │ │ ┃ │group values: OwnedRow │ ┃ │ │ ┌─────┐ │ │ ┃ └──────────────────────────────┘ ┃ │ │ │ 5 │ │ │ ┃ ┌──────────────────────────────┐ ┃ │ │ ├─────┤ │ │ ┃ │Row accumulator: │ ┃ │ │ │ 9 │─┼────┐ │ ┃ │Vec&lt;u8&gt; │ ┃ │ │ ├─────┤ │ │ │ ┃ └──────────────────────────────┘ ┃ │ │ │ ... │ │ │ │ ┃ ┌──────────────────────┐ ┃ │ │ ├─────┤ │ │ │ ┃ │┌──────────────┐ │ ┃ │ │ │ 1 │ │ │ │ ┃ ││Accumulator 1 │ │ ┃ │ │ ├─────┤ │ │ │ ┃ │└──────────────┘ │ ┃ │ │ │ ... │ │ │ │ ┃ │┌──────────────┐ │ ┃ │ │ └─────┘ │ │ │ ┃ ││Accumulator 2 │ │ ┃ │ │ │ │ │ ┃ │└──────────────┘ │ ┃ │ └─────────┘ │ │ ┃ │ Box&lt;dyn Accumulator&gt; │ ┃ │ Hash Table │ │ ┃ └──────────────────────┘ ┃ │ │ │ ┃ ┌─────────────────────────┐ ┃ │ │ │ ┃ │scratch indices: Vec&lt;u32&gt;│ ┃ │ │ │ ┃ └─────────────────────────┘ ┃ │ │ │ ┃ GroupState ┃ │ └─────▶ │ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │ │ │ Hash table tracks an │ ... │ index into group_states │ │ └──────────────────────────────────────┘ group_states: Vec&lt;GroupState&gt; There is one GroupState PER GROUP Figure 4: Hash group operator structure in DataFusion 27.0.0. A hash table maps each group to a GroupState which contains all the per-group states. To compute the aggregate, DataFusion performs the following steps for each input batch: Calculate hash using efficient vectorized code, specialized for each data type. Determine group indexes for each input row using the hash table (creating new entries for newly seen groups). Update Accumulators for each group that had input rows, assembling the rows into a contiguous range for vectorized accumulator if there are a sufficient number of them. DataFusion also stores the hash values in the table to avoid potentially costly hash recomputation when resizing the hash table. This scheme works very well for a relatively small number of distinct groups: all accumulators are efficiently updated with large contiguous batches of rows. However, this scheme is not ideal for high cardinality grouping due to: Multiple allocations per group for the group value row format, as well as for the RowAccumulators and each Accumulator. The Accumulator may have additional allocations within it as well. Non-vectorized updates: Accumulator updates often fall back to a slower non-vectorized form because the number of distinct groups is large (and thus number of values per group is small) in each input batch. Hash grouping in 28.0.0 For 28.0.0, we rewrote the core group by implementation following traditional system optimization principles: fewer allocations, type specialization, and aggressive vectorization. DataFusion 28.0.0 uses the same RawTable and still stores group indexes. The major differences, as shown in Figure 4, are: Group values are stored either Inline in the RawTable (for single columns of primitive types), where the conversion to Row format costs more than its benefit In a separate Rows structure with a single contiguous allocation for all groups values, rather than an allocation per group. Accumulators manage the state for all the groups internally, so the code to update intermediate values is a tight type specialized loop. The new GroupsAccumulator interface results in highly efficient type accumulator update loops. ┌───────────────────────────────────┐ ┌───────────────────────┐ │ ┌ ─ ─ ─ ─ ─ ┐ ┌─────────────────┐│ │ ┏━━━━━━━━━━━━━━━━━━━┓ │ │ │ ││ │ ┃ ┌──────────────┐ ┃ │ │ │ │ │ ┌ ─ ─ ┐┌─────┐ ││ │ ┃ │┌───────────┐ │ ┃ │ │ │ X │ 5 │ ││ │ ┃ ││ value1 │ │ ┃ │ │ │ │ │ ├ ─ ─ ┤├─────┤ ││ │ ┃ │└───────────┘ │ ┃ │ │ │ Q │ 9 │──┼┼──┐ │ ┃ │ ... │ ┃ │ │ │ │ │ ├ ─ ─ ┤├─────┤ ││ └──┼─╋─▶│ │ ┃ │ │ │ ... │ ... │ ││ │ ┃ │┌───────────┐ │ ┃ │ │ │ │ │ ├ ─ ─ ┤├─────┤ ││ │ ┃ ││ valueN │ │ ┃ │ │ │ H │ 1 │ ││ │ ┃ │└───────────┘ │ ┃ │ │ │ │ │ ├ ─ ─ ┤├─────┤ ││ │ ┃ │values: Vec&lt;T&gt;│ ┃ │ │ Rows │ ... │ ... │ ││ │ ┃ └──────────────┘ ┃ │ │ │ │ │ └ ─ ─ ┘└─────┘ ││ │ ┃ ┃ │ │ ─ ─ ─ ─ ─ ─ │ ││ │ ┃ GroupsAccumulator ┃ │ │ └─────────────────┘│ │ ┗━━━━━━━━━━━━━━━━━━━┛ │ │ Hash Table │ │ │ │ │ │ ... │ └───────────────────────────────────┘ └───────────────────────┘ GroupState Accumulators Hash table value stores group_indexes One GroupsAccumulator and group values. per aggregate. Each stores the state for Group values are stored either inline *ALL* groups, typically in the hash table or in a single using a native Vec&lt;T&gt; allocation using the arrow Row format Figure 5: Hash group operator structure in DataFusion 28.0.0. Group values are stored either directly in the hash table, or in a single allocation using the arrow Row format. The hash table contains group indexes. A single GroupsAccumulator stores the per-aggregate state for all groups. This new structure improves performance significantly for high cardinality groups due to: Reduced allocations: There are no longer any individual allocations per group. Contiguous native accumulator states: Type-specialized accumulators store the values for all groups in a single contiguous allocation using a Rust Vec&lt;T&gt; of some native type. Vectorized state update: The inner aggregate update loops, which are type-specialized and in terms of native Vecs, are well-vectorized by the Rust compiler (thanks LLVM!). Notes Some vectorized grouping implementations store the accumulator state row-wise directly in the hash table, which often uses modern CPU caches efficiently. Managing accumulator state in columnar fashion may sacrifice some cache locality, however it ensures the size of the hash table remains small, even when there are large numbers of groups and aggregates, making it easier for the compiler to vectorize the accumulator update. Depending on the cost of recomputing hash values, DataFusion 28.0.0 may or may not store the hash values in the table. This optimizes the tradeoff between the cost of computing the hash value (which is expensive for strings, for example) vs. the cost of storing it in the hash table. One subtlety that arises from pushing state updates into GroupsAccumulators is that each accumulator must handle similar variations with/without filtering and with/without nulls in the input. DataFusion 28.0.0 uses a templated NullState which encapsulates these common patterns across accumulators. The code structure is heavily influenced by the fact DataFusion is implemented using Rust, a new(ish) systems programming language focused on speed and safety. Rust heavily discourages many of the traditional pointer casting “tricks” used in C/C++ hash grouping implementations. The DataFusion aggregation code is almost entirely safe, deviating into unsafe only when necessary. (Rust is a great choice because it makes DataFusion fast, easy to embed, and prevents many crashes and security issues often associated with multi-threaded C/C++ code). ClickBench results The full results of running the ClickBench queries against the single Parquet file with DataFusion 27.0.0, DataFusion 28.0.0, and DuckDB 0.8.1 are below. These numbers were run on a GCP e2-standard-8 machine with 8 cores and 32 GB of RAM, using the scripts here. As the industry moves towards data systems assembled from components, it is increasingly important that they exchange data using open standards such as Apache Arrow and Parquet rather than custom storage and in-memory formats. Thus, this benchmark uses a single input Parquet file representative of many DataFusion users and aligned with the current trend in analytics of avoiding a costly load/transformation into a custom storage format prior to query. DataFusion now reaches near-DuckDB-speeds querying Parquet data. While we don’t plan to engage in a benchmarking shootout with a team that literally wrote Fair Benchmarking Considered Difficult, hopefully everyone can agree that DataFusion 28.0.0 is a significant improvement. Figure 6: Performance of DataFusion 27.0.0, DataFusion 28.0.0, and DuckDB 0.8.1 on all 43 ClickBench queries against a single hits.parquet file. Lower is better. Notes DataFusion 27.0.0 was not able to run several queries due to either planner bugs (Q9, Q11, Q12, 14) or running out of memory (Q33). DataFusion 28.0.0 solves those issues. DataFusion is faster than DuckDB for query 21 and 22, likely due to optimized implementations of string pattern matching. Conclusion: performance matters Improving aggregation performance by more than a factor of two allows developers building products and projects with DataFusion to spend more time on value-added domain specific features. We believe building systems with DataFusion is much faster than trying to build something similar from scratch. DataFusion increases productivity because it eliminates the need to rebuild well-understood, but costly to implement, analytic database technology. While we’re pleased with the improvements in DataFusion 28.0.0, we are by no means done and are pursuing (Even More) Aggregation Performance. The future for performance is bright. Acknowledgments DataFusion is a community effort and this work was not possible without contributions from many in the community. A special shout out to sunchao, yjshen, yahoNanJing, mingmwang, ozankabak, mustafasrepo, and everyone else who contributed ideas, reviews, and encouragement during this work. About DataFusion Apache Arrow DataFusion is an extensible query engine and database toolkit, written in Rust, that uses Apache Arrow as its in-memory format. DataFusion, along with Apache Calcite, Facebook’s Velox, and similar technology are part of the next generation “Deconstructed Database” architectures, where new systems are built on a foundation of fast, modular components, rather than as a single tightly integrated system. Notes SELECT COUNT(*) FROM 'hits.parquet'; &#8617; SELECT COUNT(DISTINCT "UserID") as num_users FROM 'hits.parquet'; &#8617; SELECT COUNT(DISTINCT "SearchPhrase") as num_phrases FROM 'hits.parquet'; &#8617; SELECT COUNT(*) FROM (SELECT DISTINCT "UserID", "SearchPhrase" FROM 'hits.parquet') &#8617; Full script at hash.py &#8617; hits_0.parquet, one of the files from the partitioned ClickBench dataset, which has 100,000 rows and is 117 MB in size. The entire dataset has 100,000,000 rows in a single 14 GB Parquet file. The script did not complete on the entire dataset after 40 minutes, and used 212 GB RAM at peak. &#8617;]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>