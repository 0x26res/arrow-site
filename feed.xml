<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://arrow.apache.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arrow.apache.org/" rel="alternate" type="text/html" /><updated>2022-03-15T08:27:40-04:00</updated><id>https://arrow.apache.org/feed.xml</id><title type="html">Apache Arrow</title><subtitle>Apache Arrow is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, C++, C#, Go, Java, JavaScript, MATLAB, Python, R, Ruby, and Rust.</subtitle><entry><title type="html">Apache Arrow DataFusion 7.0.0 Release</title><link href="https://arrow.apache.org/blog/2022/02/28/datafusion-7.0.0/" rel="alternate" type="text/html" title="Apache Arrow DataFusion 7.0.0 Release" /><published>2022-02-28T00:00:00-05:00</published><updated>2022-02-28T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2022/02/28/datafusion-7.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/02/28/datafusion-7.0.0/"><![CDATA[<!--

-->

<h1 id="introduction">Introduction</h1>

<p><a href="https://arrow.apache.org/datafusion/">DataFusion</a> is an extensible query execution framework, written in Rust, that uses Apache Arrow as its in-memory format.</p>

<p>When you want to extend your Rust project with <a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html">SQL support</a>, a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is definitely worth checking out.</p>

<p>DataFusion’s  SQL, <code class="language-plaintext highlighter-rouge">DataFrame</code>, and manual <code class="language-plaintext highlighter-rouge">PlanBuilder</code> API let users access a sophisticated query optimizer and execution engine capable of fast, resource efficient, and parallel execution that takes optimal advantage of todays multicore hardware. Being written in Rust means DataFusion can offer <em>both</em> the safety of dynamic languages as well as the resource efficiency of a compiled language.</p>

<p>The Apache Arrow team is pleased to announce the DataFusion 7.0.0 release. This covers 4 months of development work
and includes 195 commits from the following 37 distinct contributors.</p>

<!--
git log --pretty=oneline 5.0.0..6.0.0 datafusion datafusion-cli datafusion-examples | wc -l
     134

git shortlog -sn 5.0.0..6.0.0 datafusion datafusion-cli datafusion-examples | wc -l
      29

      Carlos and xudong963 are same individual
-->

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    44  Andrew Lamb
    24  Kun Liu
    23  Jiayu Liu
    17  xudong.w
    11  Yijie Shen
     9  Matthew Turner
     7  Liang-Chi Hsieh
     5  Lin Ma
     4  Stephen Carman
     4  James Katz
     4  Dmitry Patsura
     4  QP Hou
     3  dependabot[bot]
     3  Remzi Yang
     3  Yang
     3  ic4y
     3  Daniël Heres
     2  Andy Grove
     2  Raphael Taylor-Davies
     2  Jason Tianyi Wang
     2  Dan Harris
     2  Sergey Melnychuk
     1  Nitish Tiwari
     1  Dom
     1  Eduard Karacharov
     1  Javier Goday
     1  Boaz
     1  Marko Mikulicic
     1  Max Burke
     1  Carol (Nichols || Goulding)
     1  Phillip Cloud
     1  Rich
     1  Toby Hede
     1  Will Jones
     1  r.4ntix
     1  rdettai
</code></pre></div></div>

<p>The following section highlights some of the improvements in this release. Of course, many other bug fixes and improvements have also been made and we refer you to the complete <a href="https://github.com/apache/arrow-datafusion/blob/7.0.0/datafusion/CHANGELOG.md">changelog</a> for the full detail.</p>

<h1 id="summary">Summary</h1>

<ul>
  <li>DataFusion Crate
    <ul>
      <li>The DataFusion crate is being split into multiple crates to decrease compilation times and improve the development experience. Initially, <code class="language-plaintext highlighter-rouge">datafusion-common</code> (the core DataFusion components) and <code class="language-plaintext highlighter-rouge">datafusion-expr</code> (DataFusion expressions, functions, and operators) have been split out. There will be additional splits after the 7.0 release.</li>
    </ul>
  </li>
  <li>Performance Improvements and Optimizations
    <ul>
      <li>Arrow’s dyn scalar kernels are now used to enable efficient operations on <code class="language-plaintext highlighter-rouge">DictionaryArray</code>s <a href="https://github.com/apache/arrow-datafusion/pull/1685">#1685</a></li>
      <li>Switch from <code class="language-plaintext highlighter-rouge">std::sync::Mutex</code> to <code class="language-plaintext highlighter-rouge">parking_lot::Mutex</code> <a href="https://github.com/apache/arrow-datafusion/pull/1720">#1720</a></li>
    </ul>
  </li>
  <li>New Features
    <ul>
      <li>Support for memory tracking and spilling to disk
        <ul>
          <li>MemoryMananger and DiskManager <a href="https://github.com/apache/arrow-datafusion/pull/1526">#1526</a></li>
          <li>Out of core sort <a href="https://github.com/apache/arrow-datafusion/pull/1526">#1526</a></li>
          <li>New metrics
            <ul>
              <li><code class="language-plaintext highlighter-rouge">Gauge</code> and <code class="language-plaintext highlighter-rouge">CurrentMemoryUsage</code> <a href="https://github.com/apache/arrow-datafusion/pull/1682">#1682</a></li>
              <li><code class="language-plaintext highlighter-rouge">Spill_count</code> and <code class="language-plaintext highlighter-rouge">spilled_bytes</code> <a href="https://github.com/apache/arrow-datafusion/pull/1641">#1641</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li>New math functions
        <ul>
          <li><code class="language-plaintext highlighter-rouge">Approx_quantile</code> <a href="https://github.com/apache/arrow-datafusion/pull/1539">#1529</a></li>
          <li><code class="language-plaintext highlighter-rouge">stddev</code> and <code class="language-plaintext highlighter-rouge">variance</code> (sample and population) <a href="https://github.com/apache/arrow-datafusion/pull/1525">#1525</a></li>
          <li><code class="language-plaintext highlighter-rouge">corr</code> <a href="https://github.com/apache/arrow-datafusion/pull/1561">#1561</a></li>
        </ul>
      </li>
      <li>Support decimal type <a href="https://github.com/apache/arrow-datafusion/pull/1394">#1394</a><a href="https://github.com/apache/arrow-datafusion/pull/1407">#1407</a><a href="https://github.com/apache/arrow-datafusion/pull/1408">#1408</a><a href="https://github.com/apache/arrow-datafusion/pull/1431">#1431</a><a href="https://github.com/apache/arrow-datafusion/pull/1483">#1483</a><a href="https://github.com/apache/arrow-datafusion/pull/1554">#1554</a><a href="https://github.com/apache/arrow-datafusion/pull/1640">#1640</a></li>
      <li>Support for reading Parquet files with evolved schemas <a href="https://github.com/apache/arrow-datafusion/pull/1622">#1622</a><a href="https://github.com/apache/arrow-datafusion/pull/1709">#1709</a></li>
      <li>Support for registering <code class="language-plaintext highlighter-rouge">DataFrame</code> as table <a href="https://github.com/apache/arrow-datafusion/pull/1699">#1699</a></li>
      <li>Support for the <code class="language-plaintext highlighter-rouge">substring</code> function <a href="https://github.com/apache/arrow-datafusion/pull/1621">#1621</a></li>
      <li>Support <code class="language-plaintext highlighter-rouge">array_agg(distinct ...)</code> <a href="https://github.com/apache/arrow-datafusion/pull/1579">#1579</a></li>
      <li>Support <code class="language-plaintext highlighter-rouge">sort</code> on unprojected columns <a href="https://github.com/apache/arrow-datafusion/pull/1415">#1415</a></li>
    </ul>
  </li>
  <li>Additional Integration Points
    <ul>
      <li>A new public Expression simplification API <a href="https://github.com/apache/arrow-datafusion/pull/1717">#1717</a></li>
    </ul>
  </li>
  <li><a href="https://github.com/datafusion-contrib">DataFusion-Contrib</a>
    <ul>
      <li>A new GitHub organization created as a home for both <code class="language-plaintext highlighter-rouge">DataFusion</code> extensions and as a testing ground for new features.
        <ul>
          <li>Extensions
            <ul>
              <li><a href="https://github.com/datafusion-contrib/datafusion-python">DataFusion-Python</a></li>
              <li><a href="https://github.com/datafusion-contrib/datafusion-java">DataFusion-Java</a></li>
              <li><a href="https://github.com/datafusion-contrib/datafusion-hdfs-native">DataFusion-hdsfs-native</a></li>
              <li><a href="https://github.com/datafusion-contrib/datafusion-objectstore-s3">DataFusion-ObjectStore-s3</a></li>
            </ul>
          </li>
          <li>New Features
            <ul>
              <li><a href="https://github.com/datafusion-contrib/datafusion-streams">DataFusion-Streams</a></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="https://github.com/jorgecarleitao/arrow2">Arrow2</a>
    <ul>
      <li>An <a href="https://github.com/apache/arrow-datafusion/tree/arrow2">Arrow2 Branch</a> has been created.  There are ongoing discussions in <a href="https://github.com/apache/arrow-datafusion/issues/1532">DataFusion</a> and <a href="https://github.com/apache/arrow-rs/issues/1176">arrow-rs</a> about migrating <code class="language-plaintext highlighter-rouge">DataFusion</code> to <code class="language-plaintext highlighter-rouge">Arrow2</code></li>
    </ul>
  </li>
</ul>

<h1 id="documentation-and-roadmap">Documentation and Roadmap</h1>

<p>We are working to consolidate the documentation into the <a href="https://arrow.apache.org/datafusion">official site</a>.  You can find more details there on topics such as the <a href="https://arrow.apache.org/datafusion/user-guide/sql/index.html">SQL status</a>  and a <a href="https://arrow.apache.org/datafusion/user-guide/introduction.html#introduction">user guide</a>. This is also an area we would love to get help from the broader community <a href="https://github.com/apache/arrow-datafusion/issues/1821">#1821</a>.</p>

<p>To provide transparency on DataFusion’s priorities to users and developers a three month roadmap will be published at the beginning of each quarter.  This can be found here <a href="https://arrow.apache.org/datafusion/specification/roadmap.html">here</a>.</p>

<h1 id="upcoming-attractions">Upcoming Attractions</h1>

<ul>
  <li>Ballista is gaining momentum, and several groups are now evaluating and contributing to the project.
    <ul>
      <li>Some of the proposed improvements
        <ul>
          <li><a href="https://github.com/apache/arrow-datafusion/issues/1701">Improvements Overview</a></li>
          <li><a href="https://github.com/apache/arrow-datafusion/issues/1675">Extensibility</a></li>
          <li><a href="https://github.com/apache/arrow-datafusion/issues/1702">File system access</a></li>
          <li><a href="https://github.com/apache/arrow-datafusion/issues/1704">Cluster state</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Continued improvements for working with limited resources and large datasets
    <ul>
      <li>Memory limited joins<a href="https://github.com/apache/arrow-datafusion/issues/1599">#1599</a></li>
      <li>Sort-merge join<a href="https://github.com/apache/arrow-datafusion/issues/141">#141</a><a href="https://github.com/apache/arrow-datafusion/pull/1776">#1776</a></li>
      <li>Introduce row based bytes representation <a href="https://github.com/apache/arrow-datafusion/pull/1708">#1708</a></li>
    </ul>
  </li>
</ul>

<h1 id="how-to-get-involved">How to Get Involved</h1>

<p>If you are interested in contributing to DataFusion, and learning about state of
the art query processing, we would love to have you join us on the journey! You
can help by trying out DataFusion on some of your own data and projects and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable for beginners is <a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a></p>

<p>Check out our new <a href="https://arrow.apache.org/datafusion/community/communication.html">Communication Doc</a> on more
ways to engage with the community.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[Introduction DataFusion is an extensible query execution framework, written in Rust, that uses Apache Arrow as its in-memory format. When you want to extend your Rust project with SQL support, a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is definitely worth checking out. DataFusion’s SQL, DataFrame, and manual PlanBuilder API let users access a sophisticated query optimizer and execution engine capable of fast, resource efficient, and parallel execution that takes optimal advantage of todays multicore hardware. Being written in Rust means DataFusion can offer both the safety of dynamic languages as well as the resource efficiency of a compiled language. The Apache Arrow team is pleased to announce the DataFusion 7.0.0 release. This covers 4 months of development work and includes 195 commits from the following 37 distinct contributors. 44 Andrew Lamb 24 Kun Liu 23 Jiayu Liu 17 xudong.w 11 Yijie Shen 9 Matthew Turner 7 Liang-Chi Hsieh 5 Lin Ma 4 Stephen Carman 4 James Katz 4 Dmitry Patsura 4 QP Hou 3 dependabot[bot] 3 Remzi Yang 3 Yang 3 ic4y 3 Daniël Heres 2 Andy Grove 2 Raphael Taylor-Davies 2 Jason Tianyi Wang 2 Dan Harris 2 Sergey Melnychuk 1 Nitish Tiwari 1 Dom 1 Eduard Karacharov 1 Javier Goday 1 Boaz 1 Marko Mikulicic 1 Max Burke 1 Carol (Nichols || Goulding) 1 Phillip Cloud 1 Rich 1 Toby Hede 1 Will Jones 1 r.4ntix 1 rdettai The following section highlights some of the improvements in this release. Of course, many other bug fixes and improvements have also been made and we refer you to the complete changelog for the full detail. Summary DataFusion Crate The DataFusion crate is being split into multiple crates to decrease compilation times and improve the development experience. Initially, datafusion-common (the core DataFusion components) and datafusion-expr (DataFusion expressions, functions, and operators) have been split out. There will be additional splits after the 7.0 release. Performance Improvements and Optimizations Arrow’s dyn scalar kernels are now used to enable efficient operations on DictionaryArrays #1685 Switch from std::sync::Mutex to parking_lot::Mutex #1720 New Features Support for memory tracking and spilling to disk MemoryMananger and DiskManager #1526 Out of core sort #1526 New metrics Gauge and CurrentMemoryUsage #1682 Spill_count and spilled_bytes #1641 New math functions Approx_quantile #1529 stddev and variance (sample and population) #1525 corr #1561 Support decimal type #1394#1407#1408#1431#1483#1554#1640 Support for reading Parquet files with evolved schemas #1622#1709 Support for registering DataFrame as table #1699 Support for the substring function #1621 Support array_agg(distinct ...) #1579 Support sort on unprojected columns #1415 Additional Integration Points A new public Expression simplification API #1717 DataFusion-Contrib A new GitHub organization created as a home for both DataFusion extensions and as a testing ground for new features. Extensions DataFusion-Python DataFusion-Java DataFusion-hdsfs-native DataFusion-ObjectStore-s3 New Features DataFusion-Streams Arrow2 An Arrow2 Branch has been created. There are ongoing discussions in DataFusion and arrow-rs about migrating DataFusion to Arrow2 Documentation and Roadmap We are working to consolidate the documentation into the official site. You can find more details there on topics such as the SQL status and a user guide. This is also an area we would love to get help from the broader community #1821. To provide transparency on DataFusion’s priorities to users and developers a three month roadmap will be published at the beginning of each quarter. This can be found here here. Upcoming Attractions Ballista is gaining momentum, and several groups are now evaluating and contributing to the project. Some of the proposed improvements Improvements Overview Extensibility File system access Cluster state Continued improvements for working with limited resources and large datasets Memory limited joins#1599 Sort-merge join#141#1776 Introduce row based bytes representation #1708 How to Get Involved If you are interested in contributing to DataFusion, and learning about state of the art query processing, we would love to have you join us on the journey! You can help by trying out DataFusion on some of your own data and projects and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable for beginners is here Check out our new Communication Doc on more ways to engage with the community.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Introducing Apache Arrow Flight SQL: Accelerating Database Access</title><link href="https://arrow.apache.org/blog/2022/02/16/introducing-arrow-flight-sql/" rel="alternate" type="text/html" title="Introducing Apache Arrow Flight SQL: Accelerating Database Access" /><published>2022-02-16T00:00:00-05:00</published><updated>2022-02-16T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2022/02/16/introducing-arrow-flight-sql</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/02/16/introducing-arrow-flight-sql/"><![CDATA[<!--

-->

<p>We would like to introduce Flight SQL, a new client-server protocol developed by the Apache Arrow community for interacting with SQL databases that makes use of the Arrow in-memory columnar format and the Flight RPC framework.</p>

<p>Flight SQL aims to provide broadly similar functionality to existing APIs like JDBC and ODBC, including executing queries; creating prepared statements; and fetching metadata about the supported SQL dialect, available types, defined tables, and so on.
By building on Apache Arrow, however, Flight SQL makes it easy for clients to talk to Arrow-native databases without converting data.
And by using <a href="/blog/2019/10/13/introducing-arrow-flight/">Flight</a>, it provides an efficient implementation of a wire format that supports features like encryption and authentication out of the box, while allowing for further optimizations like parallel data access.</p>

<p>While it can be directly used for database access, it is not a direct replacement for JDBC/ODBC. Instead, Flight SQL serves as a concrete wire protocol/driver implementation that can support a JDBC/ODBC driver and reduces implementation burden on databases.</p>

<!-- mermaidjs:

graph LR
    JDBC[JDBC]
    ODBC
    FlightSQL[Flight SQL<br>libraries]
    ANA[Arrow-native app]
    DB[(Database with<br>Flight SQL endpoint)]

    JDBC --&gt; FlightSQL
    ODBC --&gt; FlightSQL
    ANA --&gt; FlightSQL

    FlightSQL --&gt;|Flight RPC| DB

-->

<div align="center">
<img src="/img/20220216-flight-sql-jdbc-odbc.svg" alt="Illustration of where Flight SQL sits in the stack. JDBC and ODBC drivers can wrap Flight SQL, or an Arrow-native application can directly use the Flight SQL libraries. Flight SQL in turn talks over Arrow Flight to a database exposing a Flight SQL endpoint." width="90%" class="img-responsive" />
</div>

<h2 id="motivation">Motivation</h2>

<p>While standards like <a href="https://docs.oracle.com/javase/8/docs/technotes/guides/jdbc/">JDBC</a> and <a href="https://docs.microsoft.com/en-us/sql/odbc/reference/odbc-overview?view=sql-server-ver15">ODBC</a> have served users well for decades, they fall short for databases and clients which wish to use Apache Arrow or columnar data in general.
Row-based APIs like JDBC or <a href="https://www.python.org/dev/peps/pep-0249/">PEP 249</a> require transposing data in this case, and for a database which is itself columnar, this means that data has to be transposed twice—once to present it in rows for the API, and once to get it back into columns for the consumer.
Meanwhile, while APIs like ODBC do provide bulk access to result buffers, this data must still be copied into Arrow arrays for use with the broader Arrow ecosystem, as implemented by projects like <a href="https://turbodbc.readthedocs.io/en/latest/">Turbodbc</a>.
Flight SQL aims to get rid of these intermediate steps.</p>

<p>Flight SQL means database servers can implement a standard interface that is designed around Apache Arrow and columnar data from the start.
Just like how Arrow provides a standard in-memory format, Flight SQL saves developers from having to design and implement an entirely new wire protocol.
As mentioned, Flight already implements features like encryption on the wire and authentication of requests, which databases do not need to re-implement.</p>

<p>For clients, Flight SQL provides bulk access to query results without having to convert data from another API or format.
Additionally, by pushing the work of implementing the wire protocol into the Flight and Flight SQL libraries, less code has to be written for each client language or driver.
And by using Flight underneath, clients and servers can cooperate to implement optimizations like parallel data access, <a href="/blog/2019/10/13/introducing-arrow-flight/#horizontal-scalability-parallel-and-partitioned-data-access">one of the original goals of Flight itself</a>.
Databases can return multiple “endpoints” to a Flight SQL client, which can then pull data from all of them in parallel, enabling the database backend to horizontally scale.</p>

<h2 id="flight-sql-basics">Flight SQL Basics</h2>

<p>Flight SQL makes full use of the Flight RPC framework and its extensibility, defining additional request/response messages via <a href="https://developers.google.com/protocol-buffers/">Protobuf</a>.
We’ll go over the Flight SQL protocol briefly, but C++ and Java already implement clients that manage much of this work.
The full <a href="https://github.com/apache/arrow/blob/release-7.0.0/format/FlightSql.proto">protocol</a> can be found on GitHub.</p>

<p>Most requests follow this pattern:</p>
<ol>
  <li>The client constructs a request using one of the defined Protobuf messages.</li>
  <li>The client sends the request via the GetSchema RPC method (to get the schema of the response) or the GetFlightInfo RPC method (to execute the request).</li>
  <li>The client makes request(s) to the endpoints returned from GetFlightInfo to get the response.</li>
</ol>

<p>Flight SQL defines methods to query database metadata, execute queries, or manipulate prepared statements.</p>

<p>Metadata requests:</p>
<ul>
  <li>CommandGetCatalogs: list catalogs in a database.</li>
  <li>CommandGetCrossReference: list foreign key columns that reference a particular other table.</li>
  <li>CommandGetDbSchemas: list schemas in a catalog.</li>
  <li>CommandGetExportedKeys: list foreign keys referencing a table.</li>
  <li>CommandGetImportedKeys: list foreign keys of a table.</li>
  <li>CommandGetPrimaryKeys: list primary keys of a table.</li>
  <li>CommandGetSqlInfo: get information about the database itself and its supported SQL dialect.</li>
  <li>CommandGetTables: list tables in a catalog/schema.</li>
  <li>CommandGetTableTypes: list table types supported (e.g. table, view, system table).</li>
</ul>

<p>Queries:</p>
<ul>
  <li>CommandStatementQuery: execute a one-off SQL query.</li>
  <li>CommandStatementUpdate: execute a one-off SQL update query.</li>
</ul>

<p>Prepared statements:</p>
<ul>
  <li>ActionClosePreparedStatementRequest: close a prepared statement.</li>
  <li>ActionCreatePreparedStatementRequest: create a new prepared statement.</li>
  <li>CommandPreparedStatementQuery: execute a prepared statement.</li>
  <li>CommandPreparedStatementUpdate: execute a prepared statement that updates data.</li>
</ul>

<p>For example, to list all tables:</p>

<!-- mermaidjs:

sequenceDiagram
    Client->>Server: GetFlightInfo(CommandGetTables)
    Server->>Client: FlightInfo{..., Ticket, ...}
    Client->>Server: DoGet(Ticket)
    Server->>Client: list of tables as Arrow data

-->

<div align="center">
<img src="/img/20220216-flight-sql-gettables.svg" alt="Sequence diagram showing how to use CommandGetTables. First, the client calls the GetFlightInfo RPC method with a serialized CommandGetTables message as the argument. The server returns a FlightInfo message containing a Ticket message. The client then calls the DoGet RPC method with the Ticket as the argument, and gets back a stream of Arrow record batches containing the tables in the database." height="363" class="img-responsive" />
</div>

<p>To execute a query:</p>

<!-- mermaidjs:

sequenceDiagram
    Client->>Server: GetFlightInfo(CommandStatementQuery)
    Server->>Client: FlightInfo{..., Ticket, ...}
    Client->>Server: DoGet(Ticket)
    Server->>Client: query results as Arrow data

-->

<div align="center">
<img src="/img/20220216-flight-sql-query.svg" alt="Sequence diagram showing how to use CommandStatementQuery. First, the client calls the GetFlightInfo RPC method with a serialized CommandStatementQuery message as the argument. This message contains the SQL query. The server returns a FlightInfo message containing a Ticket message. The client then calls the DoGet RPC method with the Ticket as the argument, and gets back a stream of Arrow record batches containing the query results." height="363" class="img-responsive" />
</div>

<p>To create and execute a prepared statement to insert rows:</p>

<!-- mermaidjs:

sequenceDiagram
    Client->>Server: DoAction(ActionCreatePreparedStatementRequest)
    Server->>Client: ActionCreatePreparedStatementResult
    Client->>Server: DoPut(CommandPreparedStatementUpdate)
    Client--&gt;>Server: Arrow data representing parameter values
    Server->>Client: DoPutUpdateResult
    Client->>Server: DoAction(ActionClosePreparedStatementRequest)
-->

<div align="center">
<img src="/img/20220216-flight-sql-prepared.svg" alt="Sequence diagram showing how to use ActionCreatePreparedStatementResult. First, the client calls the DoAction RPC method with a serialized ActionCreatePreparedStatementResult message as the argument. This message contains the SQL query. The server returns a serialized ActionCreatePreparedStatementResult message containing an opaque handle for the prepared statement. The client then calls the DoPut RPC method with a CommandPreparedStatementUpdate message, containing the opaque handle, as the argument, and uploads a stream of Arrow record batches containing query parameters. The server responds with a serialized DoPutUpdateResult message containing the number of affected rows. Finally, the client calls DoAction again with ActionClosePreparedStatementRequest to clean up the prepared statement." height="459" class="img-responsive" />
</div>

<h2 id="getting-started">Getting Started</h2>

<p>Note that while Flight SQL is shipping as part of Apache Arrow 7.0.0, it is still under development, and detailed documentation is forthcoming.
However, implementations are already available in C++ and Java, which provide a low-level client that can be used as well as a server skeleton that can be implemented.</p>

<p>For those interested, a <a href="https://github.com/apache/arrow/blob/release-7.0.0/java/flight/flight-sql/src/test/java/org/apache/arrow/flight/sql/example/FlightSqlExample.java">server implementation wrapping Apache Derby</a> and <a href="https://github.com/apache/arrow/blob/release-7.0.0/cpp/src/arrow/flight/sql/example/sqlite_server.h">one wrapping SQLite</a> are available in the source.
A <a href="https://github.com/apache/arrow/blob/release-7.0.0/cpp/src/arrow/flight/sql/test_app_cli.cc">simple CLI demonstrating the client</a> is also available. Finally, we can look at a brief example of executing a query and fetching results:</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">flight</span><span class="o">::</span><span class="n">FlightCallOptions</span> <span class="n">call_options</span><span class="p">;</span>

<span class="c1">// Execute the query, getting a FlightInfo describing how to fetch the results</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Executing query: '"</span> <span class="o">&lt;&lt;</span> <span class="n">FLAGS_query</span> <span class="o">&lt;&lt;</span> <span class="s">"'"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="n">ARROW_ASSIGN_OR_RAISE</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">flight</span><span class="o">::</span><span class="n">FlightInfo</span><span class="o">&gt;</span> <span class="n">flight_info</span><span class="p">,</span>
                      <span class="n">client</span><span class="o">-&gt;</span><span class="n">Execute</span><span class="p">(</span><span class="n">call_options</span><span class="p">,</span> <span class="n">FLAGS_query</span><span class="p">));</span>

<span class="c1">// Fetch each partition sequentially (though this can be done in parallel)</span>
<span class="k">for</span> <span class="p">(</span><span class="k">const</span> <span class="n">flight</span><span class="o">::</span><span class="n">FlightEndpoint</span><span class="o">&amp;</span> <span class="n">endpoint</span> <span class="o">:</span> <span class="n">flight_info</span><span class="o">-&gt;</span><span class="n">endpoints</span><span class="p">())</span> <span class="p">{</span>
  <span class="c1">// Here we assume each partition is on the same server we originally queried, but this</span>
  <span class="c1">// isn't true in general: the server may split the query results between multiple</span>
  <span class="c1">// other servers, which we would have to connect to.</span>

  <span class="c1">// The "ticket" in the endpoint is opaque to the client. The server uses it to</span>
  <span class="c1">// identify which part of the query results to return.</span>
  <span class="n">ARROW_ASSIGN_OR_RAISE</span><span class="p">(</span><span class="k">auto</span> <span class="n">stream</span><span class="p">,</span> <span class="n">client</span><span class="o">-&gt;</span><span class="n">DoGet</span><span class="p">(</span><span class="n">call_options</span><span class="p">,</span> <span class="n">endpoint</span><span class="p">.</span><span class="n">ticket</span><span class="p">));</span>
  <span class="c1">// Read all results into an Arrow Table, though we can iteratively process record</span>
  <span class="c1">// batches as they arrive as well</span>
  <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">arrow</span><span class="o">::</span><span class="n">Table</span><span class="o">&gt;</span> <span class="n">table</span><span class="p">;</span>
  <span class="n">ARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">stream</span><span class="o">-&gt;</span><span class="n">ReadAll</span><span class="p">(</span><span class="o">&amp;</span><span class="n">table</span><span class="p">));</span>
  <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Read one partition:"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
  <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">table</span><span class="o">-&gt;</span><span class="n">ToString</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The full source is <a href="https://github.com/apache/arrow/blob/master/cpp/examples/arrow/flight_sql_example.cc">available on GitHub</a>.</p>

<h2 id="whats-next--getting-involved">What’s Next &amp; Getting Involved</h2>

<p>Compared to existing libraries like PyODBC, <a href="https://www.dremio.com/subsurface/arrow-flight-and-flight-sql-accelerating-data-movement/">Arrow Flight is already as much as 20x faster</a> (~00:21:00).
Flight SQL will package these performance advantages into a standard interface, ready for clients and databases to implement.</p>

<p>Further protocol refinements and extensions are expected.
Some of this work is to make it possible to implement APIs like JDBC on top of Flight SQL; a JDBC driver is being actively worked on.
While this again introduces the overhead of data conversion, it means a database can make itself accessible to both Arrow-native clients and traditional clients by implementing Flight SQL.
Other improvements in the future may include Python bindings, an ODBC driver, and more.</p>

<p>For anyone interested in getting involved, either as a contributor or adopter, please reach out on the <a href="/community/#mailing-lists">mailing list</a> or join the discussion on <a href="https://github.com/apache/arrow">GitHub</a>.</p>]]></content><author><name>José Almeida, James Duong, Vinicius Fraga, Juscelino Junior, David Li, Kyle Porter, Rafael Telles</name></author><category term="application" /><summary type="html"><![CDATA[This post introduces Arrow Flight SQL, a protocol for interacting with SQL databases over Arrow Flight. We have been working on this protocol over the last six months, and are looking for feedback, interested contributors, and early adopters.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">February 2022 Rust Apache Arrow and Parquet Highlights</title><link href="https://arrow.apache.org/blog/2022/02/13/rust-9.0/" rel="alternate" type="text/html" title="February 2022 Rust Apache Arrow and Parquet Highlights" /><published>2022-02-13T01:00:00-05:00</published><updated>2022-02-13T01:00:00-05:00</updated><id>https://arrow.apache.org/blog/2022/02/13/rust-9.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/02/13/rust-9.0/"><![CDATA[<!--

-->

<p>The Rust implementation of <a href="https://arrow.apache.org/">Apache Arrow</a> has just released version <code class="language-plaintext highlighter-rouge">9.0.2</code>.</p>

<p>While a major version of this magnitude may shock some in the Rust
community to whom it implies a slow moving 20 year old piece of
software, nothing could be further from the truth!</p>

<p>With regular and predictable bi-weekly releases, the library continues
to evolve rapidly, and <code class="language-plaintext highlighter-rouge">9.0.2</code> is no exception. Some recent highlights:</p>

<h1 id="parquet-async-performance-safety-and-nested-types"><code class="language-plaintext highlighter-rouge">parquet</code>: async, performance, safety and nested types</h1>

<p>The <a href="https://crates.io/crates/arrow/9.0.2">parquet <code class="language-plaintext highlighter-rouge">9.0.2</code></a> release includes an <a href="https://github.com/apache/arrow-rs/blob/9.0.2/parquet/src/arrow/async_reader.rs#L21-L75"><code class="language-plaintext highlighter-rouge">async</code> reader</a>, a long time requested feature. Using the <code class="language-plaintext highlighter-rouge">async</code>
reader it is now possible to read only the relevant parts of a parquet
file from a networked source such as object storage. Previously the
entire file had to be buffered locally. We are hoping to add an <code class="language-plaintext highlighter-rouge">async</code>
writer in a future release and would love some
<a href="https://github.com/apache/arrow-rs/issues/1269">help</a>.</p>

<p>It is also significantly faster to read parquet data (up to
<a href="https://github.com/apache/arrow-rs/pull/1180#issuecomment-1018518863">60x</a>
in some cases) than with previous versions of the <code class="language-plaintext highlighter-rouge">parquet</code>
crate. Kudos to <a href="https://github.com/tustvold">tustvold</a> and
<a href="https://github.com/yordan-pavlov">yordan-pavlov</a> for their
contributions in these areas.</p>

<p>With <code class="language-plaintext highlighter-rouge">8.0.0</code> and later, the code that reads and writes <code class="language-plaintext highlighter-rouge">RecordBatch</code>es
to and from Parquet now supports all types, including deeply nested
structs and lists. Thanks <a href="https://github.com/helgikrs">helgikrs</a> for
cleaning up the last corner cases!</p>

<p>Other notable recent additions to parquet are <code class="language-plaintext highlighter-rouge">UTF-8</code> validation on
string data for improved security against malicious inputs.</p>

<p>Planned upcoming work includes <a href="https://github.com/apache/arrow-rs/issues/1191">pushing more
filtering</a> directly
into the parquet scan as well as an <code class="language-plaintext highlighter-rouge">async</code> writer.</p>

<h1 id="arrow-performance-dyn-kernels-and-decimalarray"><code class="language-plaintext highlighter-rouge">arrow</code>: performance, dyn kernels, and DecimalArray</h1>

<p>The <a href="https://docs.rs/arrow/latest/arrow/compute/index.html">compute</a>
kernels have been improved significantly in <a href="https://crates.io/crates/parquet/9.0.2">arrow <code class="language-plaintext highlighter-rouge">9.0.2</code></a>. Some <a href="https://github.com/apache/arrow-rs/pull/1228#issue-1111889246">filter
benchmarks</a>
are twice as fast and the SIMD kernels are also <a href="https://github.com/apache/arrow-rs/pull/1221">significantly
faster</a>. Many thanks to
<a href="https://github.com/tustvold">tustvold</a> and
<a href="https://github.com/jhorstmann">jhorstmann</a>.
<a href="https://github.com/apache/arrow-rs/pull/1248">Additional substantial</a>
improvements are likely to land in arrow <code class="language-plaintext highlighter-rouge">10.0.0</code>.</p>

<p>We are working on new set of “dynamic” <code class="language-plaintext highlighter-rouge">dyn_</code> kernels (for example,
<a href="https://docs.rs/arrow/8.0.0/arrow/compute/kernels/comparison/fn.eq_dyn.html"><code class="language-plaintext highlighter-rouge">eq_dyn</code></a>)
that make it easier to invoke the heavily optimized kernels provided
by the <code class="language-plaintext highlighter-rouge">arrow</code> crate. Work is underway to expand the breadth of types
supported by these new kernels to make them even more useful. Thanks
to <a href="https://github.com/matthewmturner">matthewmturner</a> and
<a href="https://github.com/viirya">viirya</a> for their help in this
effort.</p>

<p>While <code class="language-plaintext highlighter-rouge">arrow</code> has had basic support for <code class="language-plaintext highlighter-rouge">DecimalArray</code> since version
<code class="language-plaintext highlighter-rouge">3.0.0</code>, support has been expanded for <code class="language-plaintext highlighter-rouge">Decimal</code> type in calculation
kernels such as <code class="language-plaintext highlighter-rouge">sort</code>, <code class="language-plaintext highlighter-rouge">take</code> and <code class="language-plaintext highlighter-rouge">filter</code> thanks to some great
contributions from <a href="https://github.com/liukun4515">liukun4515</a>. There
is <a href="https://github.com/apache/arrow-rs/pull/1223">ongoing work</a> to
improve the API ergonomics and performance of <code class="language-plaintext highlighter-rouge">DecimalArray</code> as well.</p>

<h1 id="security">Security</h1>

<p>The <code class="language-plaintext highlighter-rouge">6.4.0</code> release resolved the last outstanding
<a href="https://rustsec.org/">RUSTSEC</a>
<a href="https://github.com/rustsec/advisory-db/pull/1131">advisory</a> on the
arrow crate and the <code class="language-plaintext highlighter-rouge">8.0.0</code> release resolved the last outstanding
known security issues. While these security issues were mostly limited
misuse of the low level “power user” APIs which most users do not (and
should not) be using, it was good to tighten up that area.</p>

<p>Now that <code class="language-plaintext highlighter-rouge">arrow-rs</code> is releasing major versions every other week, we
are also able to update dependencies at the same pace, helping to
ensure that security fixes upstream can flow more quickly to
downstream projects.</p>

<h1 id="final-shoutout">Final shoutout</h1>
<p>It takes a community to build great software, and we would like to
thank everyone who has contributed to the arrow-rs repository since
the <code class="language-plaintext highlighter-rouge">7.0.0</code> release:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">git shortlog -sn 7.0.0..9.0.0
    22  Raphael Taylor-Davies
    18  Andrew Lamb
     6  Helgi Kristvin Sigurbjarnarson
     6  Remzi Yang
     5  Jörn Horstmann
     4  Liang-Chi Hsieh
     3  Jiayu Liu
     2  dependabot[bot]
     2  Yijie Shen
     1  Matthew Turner
     1  Kun Liu
     1  Yang
     1  Edd Robinson
     1  Patrick More
</span></code></pre></div></div>

<h1 id="how-to-get-involved">How to Get Involved</h1>

<p>If you are interested in contributing to the Rust subproject in Apache Arrow, you can find a list of open issues
suitable for beginners <a href="https://github.com/apache/arrow-rs/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a>
and the full list <a href="https://github.com/apache/arrow-rs/issues">here</a>.</p>

<p>Other ways to get involved include trying out Arrow on some of your data and filing bug reports, and helping to
improve the documentation.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Rust implementation of Apache Arrow has just released version 9.0.2. While a major version of this magnitude may shock some in the Rust community to whom it implies a slow moving 20 year old piece of software, nothing could be further from the truth! With regular and predictable bi-weekly releases, the library continues to evolve rapidly, and 9.0.2 is no exception. Some recent highlights: parquet: async, performance, safety and nested types The parquet 9.0.2 release includes an async reader, a long time requested feature. Using the async reader it is now possible to read only the relevant parts of a parquet file from a networked source such as object storage. Previously the entire file had to be buffered locally. We are hoping to add an async writer in a future release and would love some help. It is also significantly faster to read parquet data (up to 60x in some cases) than with previous versions of the parquet crate. Kudos to tustvold and yordan-pavlov for their contributions in these areas. With 8.0.0 and later, the code that reads and writes RecordBatches to and from Parquet now supports all types, including deeply nested structs and lists. Thanks helgikrs for cleaning up the last corner cases! Other notable recent additions to parquet are UTF-8 validation on string data for improved security against malicious inputs. Planned upcoming work includes pushing more filtering directly into the parquet scan as well as an async writer. arrow: performance, dyn kernels, and DecimalArray The compute kernels have been improved significantly in arrow 9.0.2. Some filter benchmarks are twice as fast and the SIMD kernels are also significantly faster. Many thanks to tustvold and jhorstmann. Additional substantial improvements are likely to land in arrow 10.0.0. We are working on new set of “dynamic” dyn_ kernels (for example, eq_dyn) that make it easier to invoke the heavily optimized kernels provided by the arrow crate. Work is underway to expand the breadth of types supported by these new kernels to make them even more useful. Thanks to matthewmturner and viirya for their help in this effort. While arrow has had basic support for DecimalArray since version 3.0.0, support has been expanded for Decimal type in calculation kernels such as sort, take and filter thanks to some great contributions from liukun4515. There is ongoing work to improve the API ergonomics and performance of DecimalArray as well. Security The 6.4.0 release resolved the last outstanding RUSTSEC advisory on the arrow crate and the 8.0.0 release resolved the last outstanding known security issues. While these security issues were mostly limited misuse of the low level “power user” APIs which most users do not (and should not) be using, it was good to tighten up that area. Now that arrow-rs is releasing major versions every other week, we are also able to update dependencies at the same pace, helping to ensure that security fixes upstream can flow more quickly to downstream projects. Final shoutout It takes a community to build great software, and we would like to thank everyone who has contributed to the arrow-rs repository since the 7.0.0 release: git shortlog -sn 7.0.0..9.0.0 22 Raphael Taylor-Davies 18 Andrew Lamb 6 Helgi Kristvin Sigurbjarnarson 6 Remzi Yang 5 Jörn Horstmann 4 Liang-Chi Hsieh 3 Jiayu Liu 2 dependabot[bot] 2 Yijie Shen 1 Matthew Turner 1 Kun Liu 1 Yang 1 Edd Robinson 1 Patrick More How to Get Involved If you are interested in contributing to the Rust subproject in Apache Arrow, you can find a list of open issues suitable for beginners here and the full list here. Other ways to get involved include trying out Arrow on some of your data and filing bug reports, and helping to improve the documentation.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 7.0.0 Release</title><link href="https://arrow.apache.org/blog/2022/02/08/7.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 7.0.0 Release" /><published>2022-02-08T01:00:00-05:00</published><updated>2022-02-08T01:00:00-05:00</updated><id>https://arrow.apache.org/blog/2022/02/08/7.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/02/08/7.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 7.0.0 release. This covers
over 3 months of development work and includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%207.0.0"><strong>617 resolved issues</strong></a>
from <a href="/release/7.0.0.html#contributors"><strong>105 distinct contributors</strong></a>. See the Install Page to learn how to
get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/7.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 6.0.1 release, Rémi Dattai and Alessandro Molina have been invited to be committers.
Daniël Heres and Yibo Cai have joined the Project Management Committee (PMC).
Thanks for your contributions and participation in the project!</p>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>

<p>The Flight specification has been clarified to note that schemas are expected to be IPC-encapsulated on the wire.</p>

<p>Documentation has been generally improved; see the <a href="https://arrow.apache.org/cookbook/">Arrow Cookbook</a> for recipes on how to use Flight in Python and R, and a new <a href="https://github.com/apache/arrow/blob/master/cpp/examples/arrow/flight_grpc_example.cc">example</a> on how to use Flight and gRPC services on the same port.</p>

<p>This release includes Arrow Flight SQL, a protocol for using Arrow Flight to execute queries against and fetch metadata from SQL databases. Support is included for C++ and Java (but <em>not</em> languages that bind to C++, like Python or R). A more detailed blog post is forthcoming (<em>EDIT</em> 2022/02/16: see the <a href="/blog/2022/02/16/introducing-arrow-flight-sql/">Flight SQL announcement</a>). Note that development is ongoing and the specification is currently experimental.</p>

<h2 id="c-notes">C++ notes</h2>

<p>A set of CMake presets has been added to ease building Arrow in a number
of cases (ARROW-14678, ARROW-14714).</p>

<p>The <code class="language-plaintext highlighter-rouge">arrow::BitUtil</code> namespace has been renamed to <code class="language-plaintext highlighter-rouge">arrow::bit_util</code>
(ARROW-13494).</p>

<p>Concatenation of union arrays is now supported (ARROW-4975).</p>

<p><code class="language-plaintext highlighter-rouge">StructType</code> gained three convenience methods to add, change and remove
a given field (ARROW-11424).</p>

<p>The <code class="language-plaintext highlighter-rouge">Datum</code> kind <code class="language-plaintext highlighter-rouge">COLLECTION</code> has been removed as it was entirely unused
in the codebase (ARROW-13598).</p>

<h3 id="compute-layer">Compute Layer</h3>

<p>A number of compute functions have been added:</p>

<ul>
  <li>functions operating on strings: “binary_reverse” (ARROW-14306),
“string_repeat” (ARROW-12712), “utf8_normalize” (ARROW-14205);</li>
  <li>“fill_null_forward”, “fill_null_backward” (ARROW-1699);</li>
  <li>“ceil_temporal”, “floor_temporal”, “round_temporal” to adjust temporal input
to an integral multiple of a given unit (ARROW-14822);</li>
  <li>“year_month_day” to extract the calendar components of the input (ARROW-15032);</li>
  <li>“random” to general random floating-point values between 0 and 1 (ARROW-12404);</li>
  <li>“indices_nonzero” to return the indices in the input where there are
non-zero, non-null values (ARROW-13035).</li>
</ul>

<p>Decimal data is now supported as input of the arithmetic kernels
(ARROW-13130).</p>

<p>Dictionary data is now supported as input of the hash join execution node
(ARROW-14181).</p>

<p>Residual predicates have been implemented in the hash join node
(ARROW-13643).</p>

<p>The “list_parent_indices” function now always returns int64 data
regardless of the input type (ARROW-14592).</p>

<p>Month-day-nano interval data is now supported as input of the same functions
as other interval types (ARROW-13989).</p>

<h3 id="csv">CSV</h3>

<p>The CSV writer got additional configuration options:</p>
<ul>
  <li>the string representation of null values (ARROW-14905);</li>
  <li>the quoting strategy: always / never / as needed (ARROW-14905);</li>
  <li>the end of line character(s) (ARROW-14907)</li>
</ul>

<h3 id="dataset-layer">Dataset Layer</h3>

<p><a href="/blog/2022/01/31/skyhook-bringing-computation-to-storage-with-apache-arrow/">Skyhook</a>,
a dataset addition that offloads fragment scan operations to a
Ceph distributed storage cluster, was contributed (ARROW-13607).</p>

<p>The dataset writer now exposes options <code class="language-plaintext highlighter-rouge">min_rows_per_group</code> and
<code class="language-plaintext highlighter-rouge">max_rows_per_group</code> to control the size of row groups created (ARROW-14426).</p>

<h3 id="io-and-filesystem-layer">IO and Filesystem Layer</h3>

<p>A critical bug in the AWS SDK for C++ that risks losing data in S3 multipart
uploads has been circumvented (ARROW-14523).</p>

<p>The Google Cloud Storage filesystem is now featureful enough to pass all
generic filesystem tests (ARROW-14924).</p>

<p>The OpenAppendStream method of filesystems has been un-deprecated; however,
it still cannot be implemented for all filesystem backends (ARROW-14969).</p>

<p>A new function <code class="language-plaintext highlighter-rouge">arrow::fs::ResolveS3BucketRegion</code> allows resolving the
region where a particular S3 bucket resides (ARROW-15165).</p>

<p>The S3 filesystem now sets the Content-Type of output files to
“application/octet-stream” (instead of “application/xml” previously)
if not explicitly specified by the caller (ARROW-15306).</p>

<h3 id="ipc">IPC</h3>

<p>Fine-grained I/O (coalescing) is now enabled in the synchronous (ARROW-12683)
and asynchronous (ARROW-14577) IPC reader.</p>

<p>It is now possible to set the compression level when using LZ4 compression
(ARROW-9648).</p>

<h3 id="orc">ORC</h3>

<p>The ORC adapters have been significantly improved. A lot more properties of the ORC reader as well as ORC writer options are now available. Moreover API docs for both the ORC reader and the ORC writer have been generated.  (ARROW-11297)</p>
<h3 id="parquet">Parquet</h3>

<p>DELTA_BYTE_ARRAY-encoded data can now be read from (but not written to)
bytearray columns in Parquet files (PARQUET-492).</p>

<h2 id="go-notes">Go notes</h2>

<h3 id="arrow">Arrow</h3>

<h4 id="bug-fixes">Bug Fixes</h4>

<ul>
  <li>License lifted up a level so that it is properly detected for the github.com/apache/arrow/go/v7 module for pkg.go.dev <a href="https://github.com/apache/arrow/pull/11715">ARROW-14728</a>. Documentation on pkg.go.dev will look correct with complete major version handling as of the v7.0.0 release.</li>
  <li>Errors from <code class="language-plaintext highlighter-rouge">MessageReader.Message</code> get properly surfaced by <code class="language-plaintext highlighter-rouge">Reader.Read</code> <a href="https://github.com/apache/arrow/pull/11739">ARROW-14769</a></li>
  <li><code class="language-plaintext highlighter-rouge">ipc.Reader</code> properly uses the allocator it is initialized with instead of making native byte slices <a href="https://github.com/apache/arrow/pull/11712">ARROW-14717</a></li>
  <li>Fixed a CI issue where the CGO tests were crashing on windows <a href="https://github.com/apache/arrow/pull/11611">ARROW-14589</a></li>
  <li>Various fixes for internal usages of <code class="language-plaintext highlighter-rouge">Release</code> and <code class="language-plaintext highlighter-rouge">Retain</code> to maintain proper management of reference counting.</li>
</ul>

<h4 id="enhancements">Enhancements</h4>

<ul>
  <li>Continuous Integration for Go library now uses Go1.16 as the version being tested <a href="https://github.com/apache/arrow/pull/11860">ARROW-14985</a></li>
  <li><code class="language-plaintext highlighter-rouge">ValueOffsets</code> function added to <code class="language-plaintext highlighter-rouge">array.String</code> to return the entire slice of offsets <a href="https://github.com/apache/arrow/pull/11653">ARROW-14645</a></li>
  <li><code class="language-plaintext highlighter-rouge">array.Interface</code> has been lifted to <code class="language-plaintext highlighter-rouge">arrow.Array</code>, <code class="language-plaintext highlighter-rouge">array.{Record,Column,Chunked,Table}</code> have been lifted to <code class="language-plaintext highlighter-rouge">arrow.{Record,Column,Chunked,Table}</code>. Interface <code class="language-plaintext highlighter-rouge">arrow.ArrayData</code> has been created to be used instead of <code class="language-plaintext highlighter-rouge">array.Data</code>. Aliases have been provided for the <code class="language-plaintext highlighter-rouge">array</code> package so existing code that doesn’t directly use <code class="language-plaintext highlighter-rouge">array.Data</code> shouldn’t be affected. The aliases will be removed in v8. <a href="https://github.com/apache/arrow/pull/11832">ARROW-5599</a>. The <code class="language-plaintext highlighter-rouge">Chunked.NewSlice</code> method has been removed and is replaced by the <code class="language-plaintext highlighter-rouge">array.NewChunkedSlice</code> function.</li>
  <li>Arrays and Records now support marshalling to JSON via the <code class="language-plaintext highlighter-rouge">json.Marshaller</code> interface. Builders support adding values to them by unmarshalling from JSON via the <code class="language-plaintext highlighter-rouge">json.Unmarshaller</code> interface. <code class="language-plaintext highlighter-rouge">array.FromJSON</code> function added to create Arrays from JSON directly. <a href="https://github.com/apache/arrow/pull/11359">ARROW-9630</a></li>
  <li>Basic handling of field referencing and expression building similar to the C++ Compute APIs added through the new <code class="language-plaintext highlighter-rouge">compute</code> package in preparation for adding compute interfaces. Does not yet allow <em>executing</em> expressions. <a href="https://github.com/apache/arrow/pull/11514">ARROW-14430</a></li>
</ul>

<h3 id="parquet-1">Parquet</h3>

<h4 id="enhancements-1">Enhancements</h4>

<ul>
  <li>Updated dependency versions <a href="https://github.com/apache/arrow/pull/11537">ARROW-14462</a></li>
  <li><code class="language-plaintext highlighter-rouge">file</code> module added, Go Parquet library now supports full file reading and writing. <a href="https://github.com/apache/arrow/pull/11146">ARROW-13984</a> <a href="https://github.com/apache/arrow/pull/11538">ARROW-13986</a>. Does not yet provide direct Parquet &lt;–&gt; Arrow conversions.</li>
  <li>Internal min_max utility functions given Arm64 NEON SIMD optimized assembly, gaining a 4x - 6x performance improvement. <a href="https://github.com/apache/arrow/pull/12163">ARROW-15536</a></li>
</ul>

<h2 id="java-notes">Java notes</h2>

<ul>
  <li>Flight SQL support is now available in the Java library, with integration tests to verify it against the C++ reference implementation.</li>
  <li><code class="language-plaintext highlighter-rouge">GeneralOutOfPlaceVectorSorter</code> is now available for sorting any kind of vector. In general if dedicated sorters can be used (like <code class="language-plaintext highlighter-rouge">FixedWidthInPlaceVectorSorter</code>) they should preferred as they will generally perform better.</li>
  <li><code class="language-plaintext highlighter-rouge">log4j2</code> dependency was removed as it was unused and a possible vector for attacks</li>
  <li><code class="language-plaintext highlighter-rouge">VectorSchemaRootAppender</code> now works with <code class="language-plaintext highlighter-rouge">BitVector</code></li>
</ul>

<h2 id="javascript-notes">JavaScript notes</h2>

<ul>
  <li>Major simplifications to the API. There is only a single Vector class now. See the (also much improved) docs for details.</li>
  <li>Dictionary vectors created with <code class="language-plaintext highlighter-rouge">vectorFromArray</code> are automatically cached for better performance.</li>
  <li>Better tree shaking support. Some bundles can now be only a few kb.</li>
</ul>

<h2 id="python-notes">Python notes</h2>

<ul>
  <li>Official support for Python 3.6 has been dropped.</li>
  <li><code class="language-plaintext highlighter-rouge">random</code> and <code class="language-plaintext highlighter-rouge">indices_nonzero</code> compute functions are now supported in Python</li>
  <li><code class="language-plaintext highlighter-rouge">pyarrow.orc.read_table</code> is now provided to easily read the content of ORC files to a Table.</li>
  <li><code class="language-plaintext highlighter-rouge">pyarrow.orc.ORCFile</code> now has a lot more properties exposed.</li>
  <li><code class="language-plaintext highlighter-rouge">pyarrow.orc.ORCWriter</code> and <code class="language-plaintext highlighter-rouge">pyarrow.orc.write_table</code> now have the writer options available.</li>
  <li><code class="language-plaintext highlighter-rouge">pyarrow.orc</code> now has much better API documentation.</li>
  <li>Support for compute functions arguments and options has been improved in general,  arguments are not position only, while options can be provided as keyword args or not, and error reporting for wrong arguments has been improved.</li>
  <li><code class="language-plaintext highlighter-rouge">Table</code> now has a <code class="language-plaintext highlighter-rouge">group_by</code> method that allows to perform aggregations on table data. The compute functions documentation has also been improved to better distinguish between standard compute functions and <code class="language-plaintext highlighter-rouge">HASH_AGGREGATE</code> compute functions that can only be using for aggregations.</li>
  <li>Python documentation now provides interlinking for references to parameter types and return values, thus making far easier to navigate the documentation.</li>
</ul>

<h2 id="r-notes">R notes</h2>

<p>This release adds additional improvements to the <code class="language-plaintext highlighter-rouge">dplyr</code> interface, to CSV support, and to the C-Data interface to exchange data with other languages. For more details, see the <a href="/docs/r/news/">complete R changelog</a>.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<h3 id="ruby">Ruby</h3>

<p>There are two new contributors @okadakk and @simpl1g .</p>

<p>The updates of Red Arrow consists of the following improvements:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Arrow::Function#execute</code> now accepts an instance of an <code class="language-plaintext highlighter-rouge">Arrow::Column</code> as its argument <a href="https://issues.apache.org/jira/browse/ARROW-14551">(ARROW-14551)</a></li>
  <li><code class="language-plaintext highlighter-rouge">Arrow::Table.load</code> now supports <code class="language-plaintext highlighter-rouge">.arrows</code> files to load <a href="https://issues.apache.org/jira/browse/ARROW-15356">(ARROW-15356)</a></li>
  <li>Add support loading <code class="language-plaintext highlighter-rouge">Arrow::Table</code> by a <code class="language-plaintext highlighter-rouge">URI</code> in <code class="language-plaintext highlighter-rouge">Arrow::Table.load</code> <a href="https://issues.apache.org/jira/browse/ARROW-14562">(ARROW-14562)</a></li>
  <li><code class="language-plaintext highlighter-rouge">Arrow::Table</code> now supports to join two tables <a href="https://issues.apache.org/jira/browse/ARROW-14531">(ARROW-14531)</a></li>
  <li><code class="language-plaintext highlighter-rouge">Arrow::Function#execute</code> gets more easier to use than before <a href="https://issues.apache.org/jira/browse/ARROW-15274">(ARROW-15274)</a></li>
  <li><code class="language-plaintext highlighter-rouge">Arrow::SortKey#name</code> has been renamed to <code class="language-plaintext highlighter-rouge">Arrow::SortKey#target</code> <a href="https://issues.apache.org/jira/browse/ARROW-14784">(ARROW-14784)</a></li>
  <li>Add Cookbook section to documentation <a href="https://issues.apache.org/jira/browse/ARROW-14636">(ARROW-14636)</a></li>
  <li>Support the explicit initialization of S3 API by the <code class="language-plaintext highlighter-rouge">Arrow.s3_initialize</code> method <a href="https://issues.apache.org/jira/browse/ARROW-14637">(ARROW-14637)</a></li>
  <li>On macOS, stop specifying the version of openssl package explicitly when building the extension library <a href="https://issues.apache.org/jira/browse/ARROW-14619">(ARROW-14619)</a></li>
</ul>

<h3 id="c-glib">C GLib</h3>

<p>The updates of Arrow GLib etc. consists of the following improvements:</p>

<ul>
  <li>Add <code class="language-plaintext highlighter-rouge">garrow_execute_plan_build_hash_join_node</code> function, <code class="language-plaintext highlighter-rouge">GArrowHashJoinNodeOption</code>, and <code class="language-plaintext highlighter-rouge">GArrowJoinType</code> <a href="https://issues.apache.org/jira/browse/ARROW-15288">(ARROW-15288)</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">garrow_function_get_options_type</code> function <a href="https://issues.apache.org/jira/browse/ARROW-15273">(ARROW-15273)</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">garrow_function_get_default_options</code> function <a href="https://issues.apache.org/jira/browse/ARROW-15267">(ARROW-15267)</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">GArrowRoundToMultipleOptions</code> to customize the <code class="language-plaintext highlighter-rouge">round_to_multiple</code> function <a href="https://issues.apache.org/jira/browse/ARROW-15216">(ARROW-15216)</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">garrow_function_all</code> function to list up all the functions <a href="https://issues.apache.org/jira/browse/ARROW-15205">(ARROW-15205)</a>
    <ul>
      <li>In addition, add <code class="language-plaintext highlighter-rouge">garrow_function_get_name</code>, <code class="language-plaintext highlighter-rouge">garrow_function_equal</code>, and <code class="language-plaintext highlighter-rouge">garrow_function_to_string</code> functions for convenience</li>
    </ul>
  </li>
  <li>Add <code class="language-plaintext highlighter-rouge">GArrowRoundOptions</code> <a href="https://issues.apache.org/jira/browse/ARROW-15204">(ARROW-15204)</a></li>
  <li>Add <code class="language-plaintext highlighter-rouge">garrow_struct_scalar_get_value</code> function for converting a C++ scalar value to a GLib value <a href="https://issues.apache.org/jira/browse/ARROW-15203">(ARROW-15203)</a></li>
  <li>Add the following three interval data types <a href="https://issues.apache.org/jira/browse/ARROW-15134">(ARROW-15134)</a>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">GArrowMonthIntervalDataType</code> for the interval with the month component</li>
      <li><code class="language-plaintext highlighter-rouge">GArrowDayTimeIntervalDataType</code> for the interval with the days and the milliseconds components</li>
      <li><code class="language-plaintext highlighter-rouge">GArrowMonthDayNanoIntervalDataType</code> for the interval with the months, the days, and the nanoseconds components</li>
    </ul>
  </li>
  <li>Rename <code class="language-plaintext highlighter-rouge">GArrowSortKey::name</code> to <code class="language-plaintext highlighter-rouge">::target</code> <a href="https://issues.apache.org/jira/browse/ARROW-14784">(ARROW-14784)</a></li>
  <li>Support the explicit initialization of S3 API by the <code class="language-plaintext highlighter-rouge">garrow_s3_initialize</code> function <a href="https://issues.apache.org/jira/browse/ARROW-14637">(ARROW-14637)</a></li>
  <li><code class="language-plaintext highlighter-rouge">garrow_decimal128_new_string</code> and <code class="language-plaintext highlighter-rouge">garrow_decimal256_new_string</code> now returns errors when they gets a invalid decimal string <a href="https://issues.apache.org/jira/browse/ARROW-14530">(ARROW-14530)</a></li>
  <li><code class="language-plaintext highlighter-rouge">garrow_decimal128_data_type_new</code> and <code class="language-plaintext highlighter-rouge">garrow_decimal256_data_type_new</code> functions now validates the given precision <a href="https://issues.apache.org/jira/browse/ARROW-14529">(ARROW-14529)</a></li>
</ul>

<h2 id="rust-notes">Rust notes</h2>

<p>Rust releases minor versions every 2 weeks in addition to a major
version with the rest of the Arrow language implementations. Thus most
enhancements have been incrementally released over the last 3 months
as part of the 6.x.</p>

<p>Going forward, the Rust implementation version will start deviating
from the rest of the Arrow implementations, incrementing a major
version if the changes to the crate require it. We still plan a
release every other week. Please see issue <a href="https://github.com/apache/arrow-rs/issues/1120">#1120</a>
for more details</p>

<p>Major changes in the 7.0.0 release include:</p>
<ol>
  <li>Additional support for <code class="language-plaintext highlighter-rouge">Decimal</code></li>
  <li>More ergonomic compute kernels that take <code class="language-plaintext highlighter-rouge">dyn Array</code></li>
  <li><code class="language-plaintext highlighter-rouge">Union</code> type now follows the latest Arrow standard</li>
  <li>Support for custom datetime format for inference and parsing CSV files</li>
</ol>

<p>Another highlight is that the community continues to improve the
safety of the arrow crate. The 6.4.0 release included complete data
validation and has resolved all outstanding RUSTSEC issues against the
crate.</p>

<p>For additional details on the 7.0.0
Rust implementation, please see the <a href="https://github.com/apache/arrow-rs/blob/7.0.0/CHANGELOG.md">Arrow Rust CHANGELOG</a></p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 7.0.0 release. This covers over 3 months of development work and includes 617 resolved issues from 105 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 6.0.1 release, Rémi Dattai and Alessandro Molina have been invited to be committers. Daniël Heres and Yibo Cai have joined the Project Management Committee (PMC). Thanks for your contributions and participation in the project! Arrow Flight RPC notes The Flight specification has been clarified to note that schemas are expected to be IPC-encapsulated on the wire. Documentation has been generally improved; see the Arrow Cookbook for recipes on how to use Flight in Python and R, and a new example on how to use Flight and gRPC services on the same port. This release includes Arrow Flight SQL, a protocol for using Arrow Flight to execute queries against and fetch metadata from SQL databases. Support is included for C++ and Java (but not languages that bind to C++, like Python or R). A more detailed blog post is forthcoming (EDIT 2022/02/16: see the Flight SQL announcement). Note that development is ongoing and the specification is currently experimental. C++ notes A set of CMake presets has been added to ease building Arrow in a number of cases (ARROW-14678, ARROW-14714). The arrow::BitUtil namespace has been renamed to arrow::bit_util (ARROW-13494). Concatenation of union arrays is now supported (ARROW-4975). StructType gained three convenience methods to add, change and remove a given field (ARROW-11424). The Datum kind COLLECTION has been removed as it was entirely unused in the codebase (ARROW-13598). Compute Layer A number of compute functions have been added: functions operating on strings: “binary_reverse” (ARROW-14306), “string_repeat” (ARROW-12712), “utf8_normalize” (ARROW-14205); “fill_null_forward”, “fill_null_backward” (ARROW-1699); “ceil_temporal”, “floor_temporal”, “round_temporal” to adjust temporal input to an integral multiple of a given unit (ARROW-14822); “year_month_day” to extract the calendar components of the input (ARROW-15032); “random” to general random floating-point values between 0 and 1 (ARROW-12404); “indices_nonzero” to return the indices in the input where there are non-zero, non-null values (ARROW-13035). Decimal data is now supported as input of the arithmetic kernels (ARROW-13130). Dictionary data is now supported as input of the hash join execution node (ARROW-14181). Residual predicates have been implemented in the hash join node (ARROW-13643). The “list_parent_indices” function now always returns int64 data regardless of the input type (ARROW-14592). Month-day-nano interval data is now supported as input of the same functions as other interval types (ARROW-13989). CSV The CSV writer got additional configuration options: the string representation of null values (ARROW-14905); the quoting strategy: always / never / as needed (ARROW-14905); the end of line character(s) (ARROW-14907) Dataset Layer Skyhook, a dataset addition that offloads fragment scan operations to a Ceph distributed storage cluster, was contributed (ARROW-13607). The dataset writer now exposes options min_rows_per_group and max_rows_per_group to control the size of row groups created (ARROW-14426). IO and Filesystem Layer A critical bug in the AWS SDK for C++ that risks losing data in S3 multipart uploads has been circumvented (ARROW-14523). The Google Cloud Storage filesystem is now featureful enough to pass all generic filesystem tests (ARROW-14924). The OpenAppendStream method of filesystems has been un-deprecated; however, it still cannot be implemented for all filesystem backends (ARROW-14969). A new function arrow::fs::ResolveS3BucketRegion allows resolving the region where a particular S3 bucket resides (ARROW-15165). The S3 filesystem now sets the Content-Type of output files to “application/octet-stream” (instead of “application/xml” previously) if not explicitly specified by the caller (ARROW-15306). IPC Fine-grained I/O (coalescing) is now enabled in the synchronous (ARROW-12683) and asynchronous (ARROW-14577) IPC reader. It is now possible to set the compression level when using LZ4 compression (ARROW-9648). ORC The ORC adapters have been significantly improved. A lot more properties of the ORC reader as well as ORC writer options are now available. Moreover API docs for both the ORC reader and the ORC writer have been generated. (ARROW-11297) Parquet DELTA_BYTE_ARRAY-encoded data can now be read from (but not written to) bytearray columns in Parquet files (PARQUET-492). Go notes Arrow Bug Fixes License lifted up a level so that it is properly detected for the github.com/apache/arrow/go/v7 module for pkg.go.dev ARROW-14728. Documentation on pkg.go.dev will look correct with complete major version handling as of the v7.0.0 release. Errors from MessageReader.Message get properly surfaced by Reader.Read ARROW-14769 ipc.Reader properly uses the allocator it is initialized with instead of making native byte slices ARROW-14717 Fixed a CI issue where the CGO tests were crashing on windows ARROW-14589 Various fixes for internal usages of Release and Retain to maintain proper management of reference counting. Enhancements Continuous Integration for Go library now uses Go1.16 as the version being tested ARROW-14985 ValueOffsets function added to array.String to return the entire slice of offsets ARROW-14645 array.Interface has been lifted to arrow.Array, array.{Record,Column,Chunked,Table} have been lifted to arrow.{Record,Column,Chunked,Table}. Interface arrow.ArrayData has been created to be used instead of array.Data. Aliases have been provided for the array package so existing code that doesn’t directly use array.Data shouldn’t be affected. The aliases will be removed in v8. ARROW-5599. The Chunked.NewSlice method has been removed and is replaced by the array.NewChunkedSlice function. Arrays and Records now support marshalling to JSON via the json.Marshaller interface. Builders support adding values to them by unmarshalling from JSON via the json.Unmarshaller interface. array.FromJSON function added to create Arrays from JSON directly. ARROW-9630 Basic handling of field referencing and expression building similar to the C++ Compute APIs added through the new compute package in preparation for adding compute interfaces. Does not yet allow executing expressions. ARROW-14430 Parquet Enhancements Updated dependency versions ARROW-14462 file module added, Go Parquet library now supports full file reading and writing. ARROW-13984 ARROW-13986. Does not yet provide direct Parquet &lt;–&gt; Arrow conversions. Internal min_max utility functions given Arm64 NEON SIMD optimized assembly, gaining a 4x - 6x performance improvement. ARROW-15536 Java notes Flight SQL support is now available in the Java library, with integration tests to verify it against the C++ reference implementation. GeneralOutOfPlaceVectorSorter is now available for sorting any kind of vector. In general if dedicated sorters can be used (like FixedWidthInPlaceVectorSorter) they should preferred as they will generally perform better. log4j2 dependency was removed as it was unused and a possible vector for attacks VectorSchemaRootAppender now works with BitVector JavaScript notes Major simplifications to the API. There is only a single Vector class now. See the (also much improved) docs for details. Dictionary vectors created with vectorFromArray are automatically cached for better performance. Better tree shaking support. Some bundles can now be only a few kb. Python notes Official support for Python 3.6 has been dropped. random and indices_nonzero compute functions are now supported in Python pyarrow.orc.read_table is now provided to easily read the content of ORC files to a Table. pyarrow.orc.ORCFile now has a lot more properties exposed. pyarrow.orc.ORCWriter and pyarrow.orc.write_table now have the writer options available. pyarrow.orc now has much better API documentation. Support for compute functions arguments and options has been improved in general, arguments are not position only, while options can be provided as keyword args or not, and error reporting for wrong arguments has been improved. Table now has a group_by method that allows to perform aggregations on table data. The compute functions documentation has also been improved to better distinguish between standard compute functions and HASH_AGGREGATE compute functions that can only be using for aggregations. Python documentation now provides interlinking for references to parameter types and return values, thus making far easier to navigate the documentation. R notes This release adds additional improvements to the dplyr interface, to CSV support, and to the C-Data interface to exchange data with other languages. For more details, see the complete R changelog. Ruby and C GLib notes Ruby There are two new contributors @okadakk and @simpl1g . The updates of Red Arrow consists of the following improvements: Arrow::Function#execute now accepts an instance of an Arrow::Column as its argument (ARROW-14551) Arrow::Table.load now supports .arrows files to load (ARROW-15356) Add support loading Arrow::Table by a URI in Arrow::Table.load (ARROW-14562) Arrow::Table now supports to join two tables (ARROW-14531) Arrow::Function#execute gets more easier to use than before (ARROW-15274) Arrow::SortKey#name has been renamed to Arrow::SortKey#target (ARROW-14784) Add Cookbook section to documentation (ARROW-14636) Support the explicit initialization of S3 API by the Arrow.s3_initialize method (ARROW-14637) On macOS, stop specifying the version of openssl package explicitly when building the extension library (ARROW-14619) C GLib The updates of Arrow GLib etc. consists of the following improvements: Add garrow_execute_plan_build_hash_join_node function, GArrowHashJoinNodeOption, and GArrowJoinType (ARROW-15288) Add garrow_function_get_options_type function (ARROW-15273) Add garrow_function_get_default_options function (ARROW-15267) Add GArrowRoundToMultipleOptions to customize the round_to_multiple function (ARROW-15216) Add garrow_function_all function to list up all the functions (ARROW-15205) In addition, add garrow_function_get_name, garrow_function_equal, and garrow_function_to_string functions for convenience Add GArrowRoundOptions (ARROW-15204) Add garrow_struct_scalar_get_value function for converting a C++ scalar value to a GLib value (ARROW-15203) Add the following three interval data types (ARROW-15134) GArrowMonthIntervalDataType for the interval with the month component GArrowDayTimeIntervalDataType for the interval with the days and the milliseconds components GArrowMonthDayNanoIntervalDataType for the interval with the months, the days, and the nanoseconds components Rename GArrowSortKey::name to ::target (ARROW-14784) Support the explicit initialization of S3 API by the garrow_s3_initialize function (ARROW-14637) garrow_decimal128_new_string and garrow_decimal256_new_string now returns errors when they gets a invalid decimal string (ARROW-14530) garrow_decimal128_data_type_new and garrow_decimal256_data_type_new functions now validates the given precision (ARROW-14529) Rust notes Rust releases minor versions every 2 weeks in addition to a major version with the rest of the Arrow language implementations. Thus most enhancements have been incrementally released over the last 3 months as part of the 6.x. Going forward, the Rust implementation version will start deviating from the rest of the Arrow implementations, incrementing a major version if the changes to the crate require it. We still plan a release every other week. Please see issue #1120 for more details Major changes in the 7.0.0 release include: Additional support for Decimal More ergonomic compute kernels that take dyn Array Union type now follows the latest Arrow standard Support for custom datetime format for inference and parsing CSV files Another highlight is that the community continues to improve the safety of the arrow crate. The 6.4.0 release included complete data validation and has resolved all outstanding RUSTSEC issues against the crate. For additional details on the 7.0.0 Rust implementation, please see the Arrow Rust CHANGELOG]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Skyhook: Bringing Computation to Storage with Apache Arrow</title><link href="https://arrow.apache.org/blog/2022/01/31/skyhook-bringing-computation-to-storage-with-apache-arrow/" rel="alternate" type="text/html" title="Skyhook: Bringing Computation to Storage with Apache Arrow" /><published>2022-01-31T00:00:00-05:00</published><updated>2022-01-31T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2022/01/31/skyhook-bringing-computation-to-storage-with-apache-arrow</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/01/31/skyhook-bringing-computation-to-storage-with-apache-arrow/"><![CDATA[<!--

-->

<p>CPUs, memory, storage, and network bandwidth get better every year, but increasingly, they’re improving in different dimensions.
Processors are faster, but their memory bandwidth hasn’t kept up; meanwhile, cloud computing has led to storage being separated from applications across a network link.
This divergent evolution means we need to rethink where and when we perform computation to best make use of the resources available to us.</p>

<p>For example, when querying a dataset on a storage system like Ceph or Amazon S3, all the work of filtering data gets done by the client.
Data has to be transferred over the network, and then the client has to spend precious CPU cycles decoding it, only to throw it away in the end due to a filter.
While formats like Apache Parquet enable some optimizations, fundamentally, the responsibility is all on the client.
Meanwhile, even though the storage system has its own compute capabilities, it’s relegated to just serving “dumb bytes”.</p>

<p>Thanks to the <a href="https://cross.ucsc.edu/">Center for Research in Open Source Software</a> (CROSS) at the University of California, Santa Cruz, Apache Arrow 7.0.0 includes Skyhook, an <a href="https://arrow.apache.org/docs/cpp/dataset.html">Arrow Datasets</a> extension that solves this problem by using the storage layer to reduce client resource utilization.
We’ll examine the developments surrounding Skyhook as well as how Skyhook works.</p>

<h2 id="introducing-programmable-storage">Introducing Programmable Storage</h2>

<p>Skyhook is an example of programmable storage: exposing higher-level functionality from storage systems for clients to build upon.
This allows us to make better use of existing resources (both hardware and development effort) in such systems, reduces the implementation burden of common operations for each client, and enables such operations to scale with the storage layer.</p>

<p>Historically, big data systems like Apache Hadoop have tried to colocate computation and storage for efficiency.
More recently, cloud and distributed computing have disaggregated computation and storage for flexibility and scalability, but at a performance cost.
Programmable storage strikes a balance between these goals, allowing some operations to be run right next to the data while still keeping data and compute separate at a higher level.</p>

<p>In particular, Skyhook builds on <a href="https://ceph.io/en/">Ceph</a>, a distributed storage system that scales to exabytes of data while being reliable and flexible.
With its Object Class SDK, Ceph enables programmable storage by allowing extensions that define new object types with custom functionality.</p>

<h2 id="skyhook-architecture">Skyhook Architecture</h2>

<p>Let’s look at how Skyhook applies these ideas.
Overall, the idea is simple: the client should be able to ask Ceph to perform basic operations like decoding files, filtering the data, and selecting columns.
That way, the work gets done using existing storage cluster resources, which means it’s both adjacent to the data and can scale with the cluster size.
Also, this reduces the data transferred over the network, and of course reduces the client workload.</p>

<p>On the storage system side, Skyhook uses the Ceph Object Class SDK to define scan operations on data stored in Parquet or Feather format.
To implement these operations, Skyhook first implements a file system shim in Ceph’s object storage layer, then uses the existing filtering and projection capabilities of the Arrow Datasets library on top of that shim.</p>

<p>Then, Skyhook defines a custom “file format” in the Arrow Datasets layer.
Queries against such files get translated to direct requests to Ceph using those new operations, bypassing the traditional POSIX file system layer.
After decoding, filtering, and projecting, Ceph sends the Arrow record batches directly to the client, minimizing CPU overhead for encoding/decoding—another optimization Arrow makes possible.
The record batches use Arrow’s compression support to further save bandwidth.</p>

<figure>
  <img src="/img/20220131-skyhook-architecture.png" alt="Skyhook Architecture" width="100%" class="img-responsive" />
  <figcaption>
    <p>Skyhook extends Ceph and Arrow Datasets to push queries down to Ceph, reducing the client workload and network traffic.
(Figure sourced from <a href="https://jayjeetc.medium.com/skyhookdm-is-now-a-part-of-apache-arrow-e5d7b9a810ba">“SkyhookDM is now a part of Apache Arrow!”</a>.)</p>
  </figcaption>
</figure>

<p>Skyhook also optimizes how Parquet files in particular are stored.
Parquet files consist of a series of row groups, which each contain a chunk of the rows in a file.
When storing such files, Skyhook either pads or splits them so that each row group is stored as its own Ceph object.
By striping or splitting the file in this way, we can parallelize scanning at sub-file granularity across the Ceph nodes for further performance improvements.</p>

<h2 id="applications">Applications</h2>

<p>In benchmarks, Skyhook has minimal storage-side CPU overhead and virtually eliminates client-side CPU usage.
Scaling the storage cluster decreases query latency commensurately.
For systems like Dask that use the Arrow Datasets API, this means that just by switching to the Skyhook file format, we can speed up dataset scans, reduce the amount of data that needs to be transferred, and free up CPU resources for computations.</p>

<figure>
  <img src="/img/20220131-skyhook-cpu.png" alt="In benchmarks, Skyhook reduces client CPU usage while minimally impacting storage cluster CPU usage." width="100%" class="img-responsive" />
  <figcaption>
    Skyhook frees the client CPU to do useful work, while minimally impacting the work done by the storage machines.
    The client still does some work in decompressing the LZ4-compressed record batches sent by Skyhook.
    (Note that the storage cluster plot is cumulative.)
  </figcaption>
</figure>

<p>Of course, the ideas behind Skyhook apply to other systems adjacent to and beyond Apache Arrow.
For example, “lakehouse” systems like Apache Iceberg and Delta Lake also build on distributed storage systems, and can naturally benefit from Skyhook to offload computation.
Additionally, in-memory SQL-based query engines like <a href="/blog/2021/12/03/arrow-duckdb/">DuckDB</a>, which integrate seamlessly with Apache Arrow, can benefit from Skyhook by offloading portions of SQL queries.</p>

<h2 id="summary-and-acknowledgements">Summary and Acknowledgements</h2>

<p>Skyhook, available in Arrow 7.0.0, builds on research into programmable storage systems.
By pushing filters and projections to the storage layer, we can speed up dataset scans by freeing precious CPU resources on the client, reducing the amount of data sent across the network, and better utilizing the scalability of systems like Ceph.
To get started, just <a href="https://arrow.apache.org/docs/developers/cpp/building.html">build Arrow</a> with Skyhook enabled, deploy the Skyhook object class extensions to Ceph (see “Usage” in the <a href="https://jayjeetc.medium.com/skyhookdm-is-now-a-part-of-apache-arrow-e5d7b9a810ba">announcement post</a>), and then use the <code class="language-plaintext highlighter-rouge">SkyhookFileFormat</code> to construct an Arrow dataset.
A small code example is shown here.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Licensed to the Apache Software Foundation (ASF) under one</span>
<span class="c1">// or more contributor license agreements. See the NOTICE file</span>
<span class="c1">// distributed with this work for additional information</span>
<span class="c1">// regarding copyright ownership. The ASF licenses this file</span>
<span class="c1">// to you under the Apache License, Version 2.0 (the</span>
<span class="c1">// "License"); you may not use this file except in compliance</span>
<span class="c1">// with the License. You may obtain a copy of the License at</span>
<span class="c1">//</span>
<span class="c1">// http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">//</span>
<span class="c1">// Unless required by applicable law or agreed to in writing,</span>
<span class="c1">// software distributed under the License is distributed on an</span>
<span class="c1">// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY</span>
<span class="c1">// KIND, either express or implied. See the License for the</span>
<span class="c1">// specific language governing permissions and limitations</span>
<span class="c1">// under the License.</span>

<span class="cp">#include</span> <span class="cpf">&lt;arrow/compute/api.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;arrow/dataset/api.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;arrow/filesystem/api.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;arrow/table.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;skyhook/client/file_skyhook.h&gt;</span><span class="cp">
</span>
<span class="cp">#include</span> <span class="cpf">&lt;cstdlib&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;memory&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;string&gt;</span><span class="cp">
</span>
<span class="k">namespace</span> <span class="n">cp</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">::</span><span class="n">compute</span><span class="p">;</span>
<span class="k">namespace</span> <span class="n">ds</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">::</span><span class="n">dataset</span><span class="p">;</span>
<span class="k">namespace</span> <span class="n">fs</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">::</span><span class="n">fs</span><span class="p">;</span>

<span class="c1">// Demonstrate reading a dataset via Skyhook.</span>
<span class="n">arrow</span><span class="o">::</span><span class="n">Status</span> <span class="nf">ScanDataset</span><span class="p">()</span> <span class="p">{</span>
  <span class="c1">// Configure SkyhookFileFormat to connect to our Ceph cluster.</span>
  <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">ceph_config_path</span> <span class="o">=</span> <span class="s">"/etc/ceph/ceph.conf"</span><span class="p">;</span>
  <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">ceph_data_pool</span> <span class="o">=</span> <span class="s">"cephfs_data"</span><span class="p">;</span>
  <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">ceph_user_name</span> <span class="o">=</span> <span class="s">"client.admin"</span><span class="p">;</span>
  <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">ceph_cluster_name</span> <span class="o">=</span> <span class="s">"ceph"</span><span class="p">;</span>
  <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">ceph_cls_name</span> <span class="o">=</span> <span class="s">"skyhook"</span><span class="p">;</span>
  <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">skyhook</span><span class="o">::</span><span class="n">RadosConnCtx</span><span class="o">&gt;</span> <span class="n">rados_ctx</span> <span class="o">=</span>
      <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">skyhook</span><span class="o">::</span><span class="n">RadosConnCtx</span><span class="o">&gt;</span><span class="p">(</span><span class="n">ceph_config_path</span><span class="p">,</span> <span class="n">ceph_data_pool</span><span class="p">,</span>
                                              <span class="n">ceph_user_name</span><span class="p">,</span> <span class="n">ceph_cluster_name</span><span class="p">,</span>
                                              <span class="n">ceph_cls_name</span><span class="p">);</span>
  <span class="n">ARROW_ASSIGN_OR_RAISE</span><span class="p">(</span><span class="k">auto</span> <span class="n">format</span><span class="p">,</span>
                        <span class="n">skyhook</span><span class="o">::</span><span class="n">SkyhookFileFormat</span><span class="o">::</span><span class="n">Make</span><span class="p">(</span><span class="n">rados_ctx</span><span class="p">,</span> <span class="s">"parquet"</span><span class="p">));</span>

  <span class="c1">// Create the filesystem.</span>
  <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">root</span><span class="p">;</span>
  <span class="n">ARROW_ASSIGN_OR_RAISE</span><span class="p">(</span><span class="k">auto</span> <span class="n">fs</span><span class="p">,</span> <span class="n">fs</span><span class="o">::</span><span class="n">FileSystemFromUri</span><span class="p">(</span><span class="s">"file:///mnt/cephfs/nyc"</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">root</span><span class="p">));</span>

  <span class="c1">// Create our dataset.</span>
  <span class="n">fs</span><span class="o">::</span><span class="n">FileSelector</span> <span class="n">selector</span><span class="p">;</span>
  <span class="n">selector</span><span class="p">.</span><span class="n">base_dir</span> <span class="o">=</span> <span class="n">root</span><span class="p">;</span>
  <span class="n">selector</span><span class="p">.</span><span class="n">recursive</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>

  <span class="n">ds</span><span class="o">::</span><span class="n">FileSystemFactoryOptions</span> <span class="n">options</span><span class="p">;</span>
  <span class="n">options</span><span class="p">.</span><span class="n">partitioning</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">ds</span><span class="o">::</span><span class="n">HivePartitioning</span><span class="o">&gt;</span><span class="p">(</span>
      <span class="n">arrow</span><span class="o">::</span><span class="n">schema</span><span class="p">({</span><span class="n">arrow</span><span class="o">::</span><span class="n">field</span><span class="p">(</span><span class="s">"payment_type"</span><span class="p">,</span> <span class="n">arrow</span><span class="o">::</span><span class="n">int32</span><span class="p">()),</span>
                     <span class="n">arrow</span><span class="o">::</span><span class="n">field</span><span class="p">(</span><span class="s">"VendorID"</span><span class="p">,</span> <span class="n">arrow</span><span class="o">::</span><span class="n">int32</span><span class="p">())}));</span>
  <span class="n">ARROW_ASSIGN_OR_RAISE</span><span class="p">(</span><span class="k">auto</span> <span class="n">factory</span><span class="p">,</span>
                        <span class="n">ds</span><span class="o">::</span><span class="n">FileSystemDatasetFactory</span><span class="o">::</span><span class="n">Make</span><span class="p">(</span><span class="n">fs</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">selector</span><span class="p">),</span>
                                                           <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">format</span><span class="p">),</span> <span class="n">options</span><span class="p">));</span>

  <span class="n">ds</span><span class="o">::</span><span class="n">InspectOptions</span> <span class="n">inspect_options</span><span class="p">;</span>
  <span class="n">ds</span><span class="o">::</span><span class="n">FinishOptions</span> <span class="n">finish_options</span><span class="p">;</span>
  <span class="n">ARROW_ASSIGN_OR_RAISE</span><span class="p">(</span><span class="k">auto</span> <span class="n">schema</span><span class="p">,</span> <span class="n">factory</span><span class="o">-&gt;</span><span class="n">Inspect</span><span class="p">(</span><span class="n">inspect_options</span><span class="p">));</span>
  <span class="n">ARROW_ASSIGN_OR_RAISE</span><span class="p">(</span><span class="k">auto</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">factory</span><span class="o">-&gt;</span><span class="n">Finish</span><span class="p">(</span><span class="n">finish_options</span><span class="p">));</span>

  <span class="c1">// Scan the dataset.</span>
  <span class="k">auto</span> <span class="n">filter</span> <span class="o">=</span> <span class="n">cp</span><span class="o">::</span><span class="n">greater</span><span class="p">(</span><span class="n">cp</span><span class="o">::</span><span class="n">field_ref</span><span class="p">(</span><span class="s">"payment_type"</span><span class="p">),</span> <span class="n">cp</span><span class="o">::</span><span class="n">literal</span><span class="p">(</span><span class="mi">2</span><span class="p">));</span>
  <span class="n">ARROW_ASSIGN_OR_RAISE</span><span class="p">(</span><span class="k">auto</span> <span class="n">scanner_builder</span><span class="p">,</span> <span class="n">dataset</span><span class="o">-&gt;</span><span class="n">NewScan</span><span class="p">());</span>
  <span class="n">ARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">scanner_builder</span><span class="o">-&gt;</span><span class="n">Filter</span><span class="p">(</span><span class="n">filter</span><span class="p">));</span>
  <span class="n">ARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">scanner_builder</span><span class="o">-&gt;</span><span class="n">UseThreads</span><span class="p">(</span><span class="nb">true</span><span class="p">));</span>
  <span class="n">ARROW_ASSIGN_OR_RAISE</span><span class="p">(</span><span class="k">auto</span> <span class="n">scanner</span><span class="p">,</span> <span class="n">scanner_builder</span><span class="o">-&gt;</span><span class="n">Finish</span><span class="p">());</span>

  <span class="n">ARROW_ASSIGN_OR_RAISE</span><span class="p">(</span><span class="k">auto</span> <span class="n">table</span><span class="p">,</span> <span class="n">scanner</span><span class="o">-&gt;</span><span class="n">ToTable</span><span class="p">());</span>

  <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"Got "</span> <span class="o">&lt;&lt;</span> <span class="n">table</span><span class="o">-&gt;</span><span class="n">num_rows</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="s">" rows"</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
  <span class="k">return</span> <span class="n">arrow</span><span class="o">::</span><span class="n">Status</span><span class="o">::</span><span class="n">OK</span><span class="p">();</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">auto</span> <span class="n">status</span> <span class="o">=</span> <span class="n">ScanDataset</span><span class="p">();</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">status</span><span class="p">.</span><span class="n">ok</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="n">status</span><span class="p">.</span><span class="n">message</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="k">return</span> <span class="n">EXIT_FAILURE</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">EXIT_SUCCESS</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>We would like to acknowledge Ivo Jimenez, Jeff LeFevre, Michael Sevilla, and Noah Watkins for their contributions to this project.</p>

<p>This work was supported in part by the National Science Foundation under Cooperative Agreement OAC-1836650, the US Department of Energy ASCR DE-NA0003525 (FWP 20-023266), and the Center for Research in Open Source Software (<a href="https://cross.ucsc.edu/">cross.ucsc.edu</a>).</p>

<p>For more information, see these papers and articles:</p>

<ul>
  <li><a href="https://www.usenix.org/publications/login/summer2020/lefevre">SkyhookDM: Data Processing in Ceph with Programmable Storage.</a> (USENIX <em>;login:</em> issue Summer 2020, Vol. 45, No. 2)</li>
  <li><a href="https://jayjeetc.medium.com/skyhookdm-is-now-a-part-of-apache-arrow-e5d7b9a810ba">SkyhookDM is now a part of Apache Arrow!</a> (Medium)</li>
  <li><a href="https://arxiv.org/abs/2105.09894">Towards an Arrow-native Storage System.</a> (arXiv.org)</li>
</ul>]]></content><author><name>Jayjeet Chakraborty, Carlos Maltzahn, David Li, Tom Drabas</name></author><category term="application" /><summary type="html"><![CDATA[CPUs, memory, storage, and network bandwidth get better every year, but increasingly, they’re improving in different dimensions. Processors are faster, but their memory bandwidth hasn’t kept up; meanwhile, cloud computing has led to storage being separated from applications across a network link. This divergent evolution means we need to rethink where and when we perform computation to best make use of the resources available to us. For example, when querying a dataset on a storage system like Ceph or Amazon S3, all the work of filtering data gets done by the client. Data has to be transferred over the network, and then the client has to spend precious CPU cycles decoding it, only to throw it away in the end due to a filter. While formats like Apache Parquet enable some optimizations, fundamentally, the responsibility is all on the client. Meanwhile, even though the storage system has its own compute capabilities, it’s relegated to just serving “dumb bytes”. Thanks to the Center for Research in Open Source Software (CROSS) at the University of California, Santa Cruz, Apache Arrow 7.0.0 includes Skyhook, an Arrow Datasets extension that solves this problem by using the storage layer to reduce client resource utilization. We’ll examine the developments surrounding Skyhook as well as how Skyhook works. Introducing Programmable Storage Skyhook is an example of programmable storage: exposing higher-level functionality from storage systems for clients to build upon. This allows us to make better use of existing resources (both hardware and development effort) in such systems, reduces the implementation burden of common operations for each client, and enables such operations to scale with the storage layer. Historically, big data systems like Apache Hadoop have tried to colocate computation and storage for efficiency. More recently, cloud and distributed computing have disaggregated computation and storage for flexibility and scalability, but at a performance cost. Programmable storage strikes a balance between these goals, allowing some operations to be run right next to the data while still keeping data and compute separate at a higher level. In particular, Skyhook builds on Ceph, a distributed storage system that scales to exabytes of data while being reliable and flexible. With its Object Class SDK, Ceph enables programmable storage by allowing extensions that define new object types with custom functionality. Skyhook Architecture Let’s look at how Skyhook applies these ideas. Overall, the idea is simple: the client should be able to ask Ceph to perform basic operations like decoding files, filtering the data, and selecting columns. That way, the work gets done using existing storage cluster resources, which means it’s both adjacent to the data and can scale with the cluster size. Also, this reduces the data transferred over the network, and of course reduces the client workload. On the storage system side, Skyhook uses the Ceph Object Class SDK to define scan operations on data stored in Parquet or Feather format. To implement these operations, Skyhook first implements a file system shim in Ceph’s object storage layer, then uses the existing filtering and projection capabilities of the Arrow Datasets library on top of that shim. Then, Skyhook defines a custom “file format” in the Arrow Datasets layer. Queries against such files get translated to direct requests to Ceph using those new operations, bypassing the traditional POSIX file system layer. After decoding, filtering, and projecting, Ceph sends the Arrow record batches directly to the client, minimizing CPU overhead for encoding/decoding—another optimization Arrow makes possible. The record batches use Arrow’s compression support to further save bandwidth. Skyhook extends Ceph and Arrow Datasets to push queries down to Ceph, reducing the client workload and network traffic. (Figure sourced from “SkyhookDM is now a part of Apache Arrow!”.) Skyhook also optimizes how Parquet files in particular are stored. Parquet files consist of a series of row groups, which each contain a chunk of the rows in a file. When storing such files, Skyhook either pads or splits them so that each row group is stored as its own Ceph object. By striping or splitting the file in this way, we can parallelize scanning at sub-file granularity across the Ceph nodes for further performance improvements. Applications In benchmarks, Skyhook has minimal storage-side CPU overhead and virtually eliminates client-side CPU usage. Scaling the storage cluster decreases query latency commensurately. For systems like Dask that use the Arrow Datasets API, this means that just by switching to the Skyhook file format, we can speed up dataset scans, reduce the amount of data that needs to be transferred, and free up CPU resources for computations. Skyhook frees the client CPU to do useful work, while minimally impacting the work done by the storage machines. The client still does some work in decompressing the LZ4-compressed record batches sent by Skyhook. (Note that the storage cluster plot is cumulative.) Of course, the ideas behind Skyhook apply to other systems adjacent to and beyond Apache Arrow. For example, “lakehouse” systems like Apache Iceberg and Delta Lake also build on distributed storage systems, and can naturally benefit from Skyhook to offload computation. Additionally, in-memory SQL-based query engines like DuckDB, which integrate seamlessly with Apache Arrow, can benefit from Skyhook by offloading portions of SQL queries. Summary and Acknowledgements Skyhook, available in Arrow 7.0.0, builds on research into programmable storage systems. By pushing filters and projections to the storage layer, we can speed up dataset scans by freeing precious CPU resources on the client, reducing the amount of data sent across the network, and better utilizing the scalability of systems like Ceph. To get started, just build Arrow with Skyhook enabled, deploy the Skyhook object class extensions to Ceph (see “Usage” in the announcement post), and then use the SkyhookFileFormat to construct an Arrow dataset. A small code example is shown here. // Licensed to the Apache Software Foundation (ASF) under one // or more contributor license agreements. See the NOTICE file // distributed with this work for additional information // regarding copyright ownership. The ASF licenses this file // to you under the Apache License, Version 2.0 (the // "License"); you may not use this file except in compliance // with the License. You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, // software distributed under the License is distributed on an // "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY // KIND, either express or implied. See the License for the // specific language governing permissions and limitations // under the License. #include &lt;arrow/compute/api.h&gt; #include &lt;arrow/dataset/api.h&gt; #include &lt;arrow/filesystem/api.h&gt; #include &lt;arrow/table.h&gt; #include &lt;skyhook/client/file_skyhook.h&gt; #include &lt;cstdlib&gt; #include &lt;iostream&gt; #include &lt;memory&gt; #include &lt;string&gt; namespace cp = arrow::compute; namespace ds = arrow::dataset; namespace fs = arrow::fs; // Demonstrate reading a dataset via Skyhook. arrow::Status ScanDataset() { // Configure SkyhookFileFormat to connect to our Ceph cluster. std::string ceph_config_path = "/etc/ceph/ceph.conf"; std::string ceph_data_pool = "cephfs_data"; std::string ceph_user_name = "client.admin"; std::string ceph_cluster_name = "ceph"; std::string ceph_cls_name = "skyhook"; std::shared_ptr&lt;skyhook::RadosConnCtx&gt; rados_ctx = std::make_shared&lt;skyhook::RadosConnCtx&gt;(ceph_config_path, ceph_data_pool, ceph_user_name, ceph_cluster_name, ceph_cls_name); ARROW_ASSIGN_OR_RAISE(auto format, skyhook::SkyhookFileFormat::Make(rados_ctx, "parquet")); // Create the filesystem. std::string root; ARROW_ASSIGN_OR_RAISE(auto fs, fs::FileSystemFromUri("file:///mnt/cephfs/nyc", &amp;root)); // Create our dataset. fs::FileSelector selector; selector.base_dir = root; selector.recursive = true; ds::FileSystemFactoryOptions options; options.partitioning = std::make_shared&lt;ds::HivePartitioning&gt;( arrow::schema({arrow::field("payment_type", arrow::int32()), arrow::field("VendorID", arrow::int32())})); ARROW_ASSIGN_OR_RAISE(auto factory, ds::FileSystemDatasetFactory::Make(fs, std::move(selector), std::move(format), options)); ds::InspectOptions inspect_options; ds::FinishOptions finish_options; ARROW_ASSIGN_OR_RAISE(auto schema, factory-&gt;Inspect(inspect_options)); ARROW_ASSIGN_OR_RAISE(auto dataset, factory-&gt;Finish(finish_options)); // Scan the dataset. auto filter = cp::greater(cp::field_ref("payment_type"), cp::literal(2)); ARROW_ASSIGN_OR_RAISE(auto scanner_builder, dataset-&gt;NewScan()); ARROW_RETURN_NOT_OK(scanner_builder-&gt;Filter(filter)); ARROW_RETURN_NOT_OK(scanner_builder-&gt;UseThreads(true)); ARROW_ASSIGN_OR_RAISE(auto scanner, scanner_builder-&gt;Finish()); ARROW_ASSIGN_OR_RAISE(auto table, scanner-&gt;ToTable()); std::cout &lt;&lt; "Got " &lt;&lt; table-&gt;num_rows() &lt;&lt; " rows" &lt;&lt; std::endl; return arrow::Status::OK(); } int main(int, char**) { auto status = ScanDataset(); if (!status.ok()) { std::cerr &lt;&lt; status.message() &lt;&lt; std::endl; return EXIT_FAILURE; } return EXIT_SUCCESS; } We would like to acknowledge Ivo Jimenez, Jeff LeFevre, Michael Sevilla, and Noah Watkins for their contributions to this project. This work was supported in part by the National Science Foundation under Cooperative Agreement OAC-1836650, the US Department of Energy ASCR DE-NA0003525 (FWP 20-023266), and the Center for Research in Open Source Software (cross.ucsc.edu). For more information, see these papers and articles: SkyhookDM: Data Processing in Ceph with Programmable Storage. (USENIX ;login: issue Summer 2020, Vol. 45, No. 2) SkyhookDM is now a part of Apache Arrow! (Medium) Towards an Arrow-native Storage System. (arXiv.org)]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">DuckDB quacks Arrow: A zero-copy data integration between Apache Arrow and DuckDB</title><link href="https://arrow.apache.org/blog/2021/12/03/arrow-duckdb/" rel="alternate" type="text/html" title="DuckDB quacks Arrow: A zero-copy data integration between Apache Arrow and DuckDB" /><published>2021-12-03T00:00:00-05:00</published><updated>2021-12-03T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2021/12/03/arrow-duckdb</id><content type="html" xml:base="https://arrow.apache.org/blog/2021/12/03/arrow-duckdb/"><![CDATA[<!--

-->

<p><em>TLDR: The zero-copy integration between DuckDB and Apache Arrow allows for rapid analysis of larger than memory datasets in Python and R using either SQL or relational APIs.</em></p>

<p>This post is a collaboration with and cross-posted on <a href="https://duckdb.org/2021/12/03/duck-arrow.html">the DuckDB blog</a>.</p>

<p>Part of <a href="https://arrow.apache.org">Apache Arrow</a> is an in-memory data format optimized for analytical libraries. Like Pandas and R Dataframes, it uses a columnar data model. But the Arrow project contains more than just the format: The Arrow C++ library, which is accessible in Python, R, and Ruby via bindings, has additional features that allow you to compute efficiently on datasets. These additional features are on top of the implementation of the in-memory format described above. The datasets may span multiple files in Parquet, CSV, or other formats, and files may even be on remote or cloud storage like HDFS or Amazon S3. The Arrow C++ query engine supports the streaming of query results, has an efficient implementation of complex data types (e.g., Lists, Structs, Maps), and can perform important scan optimizations like Projection and Filter Pushdown.</p>

<p><a href="https://www.duckdb.org">DuckDB</a> is a new analytical data management system that is designed to run complex SQL queries within other processes. DuckDB has bindings for R and Python, among others. DuckDB can query Arrow datasets directly and stream query results back to Arrow. This integration allows users to query Arrow data using DuckDB’s SQL Interface and API, while taking advantage of DuckDB’s parallel vectorized execution engine, without requiring any extra data copying. Additionally, this integration takes full advantage of Arrow’s predicate and filter pushdown while scanning datasets.</p>

<p>This integration is unique because it uses zero-copy streaming of data between DuckDB and Arrow and vice versa so that you can compose a query using both together. This results in three main benefits:</p>

<ol>
  <li><strong>Larger Than Memory Analysis:</strong> Since both libraries support streaming query results, we are capable of executing on data without fully loading it from disk. Instead, we can execute one batch at a time. This allows us to execute queries on data that is bigger than memory.</li>
  <li><strong>Complex Data Types:</strong> DuckDB can efficiently process complex data types that can be stored in Arrow vectors, including arbitrarily nested structs, lists, and maps.</li>
  <li><strong>Advanced Optimizer:</strong> DuckDB’s state-of-the-art optimizer can push down filters and projections directly into Arrow scans. As a result, only relevant columns and partitions will be read, allowing the system to e.g., take advantage of partition elimination in Parquet files. This significantly accelerates query execution.</li>
</ol>

<p>For those that are just interested in benchmarks, you can jump ahead <a href="#Benchmark Comparison">benchmark section below</a>.</p>

<h2 id="quick-tour">Quick Tour</h2>
<p>Before diving into the details of the integration, in this section we provide a quick motivating example of how powerful and simple to use is the DuckDB-Arrow integration. With a few lines of code, you can already start querying Arrow datasets. Say you want to analyze the infamous <a href="https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page">NYC Taxi Dataset</a> and figure out if groups tip more or less than single riders.</p>

<h3 id="r">R</h3>
<p>Both Arrow and DuckDB support dplyr pipelines for people more comfortable with using dplyr for their data analysis. The Arrow package includes two helper functions that allow us to pass data back and forth between Arrow and DuckDB (<code class="language-plaintext highlighter-rouge">to_duckdb()</code> and <code class="language-plaintext highlighter-rouge">to_arrow()</code>).
This is especially useful in cases where something is supported in one of Arrow or DuckDB but not the other. For example, if you find a complex dplyr pipeline where the SQL translation doesn’t work with DuckDB, use <code class="language-plaintext highlighter-rouge">to_arrow()</code> before the pipeline to use the Arrow engine. Or, if you have a function (e.g., windowed aggregates) that aren’t yet implemented in Arrow, use <code class="language-plaintext highlighter-rouge">to_duckdb()</code> to use the DuckDB engine. All while not paying any cost to (re)serialize the data when you pass it back and forth!</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">duckdb</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">arrow</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span><span class="w">

</span><span class="c1"># Open dataset using year,month folder partition</span><span class="w">
</span><span class="n">ds</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">arrow</span><span class="o">::</span><span class="n">open_dataset</span><span class="p">(</span><span class="s2">"nyc-taxi"</span><span class="p">,</span><span class="w"> </span><span class="n">partitioning</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"year"</span><span class="p">,</span><span class="w"> </span><span class="s2">"month"</span><span class="p">))</span><span class="w">

</span><span class="n">ds</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="c1"># Look only at 2015 on, where the number of passenger is positive, the trip distance is</span><span class="w">
  </span><span class="c1"># greater than a quarter mile, and where the fare amount is positive</span><span class="w">
  </span><span class="n">filter</span><span class="p">(</span><span class="n">year</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">2014</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">passenger_count</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">trip_distance</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">0.25</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">fare_amount</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="c1"># Pass off to DuckDB</span><span class="w">
  </span><span class="n">to_duckdb</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">group_by</span><span class="p">(</span><span class="n">passenger_count</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">mutate</span><span class="p">(</span><span class="n">tip_pct</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tip_amount</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">fare_amount</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">summarise</span><span class="p">(</span><span class="w">
    </span><span class="n">fare_amount</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">fare_amount</span><span class="p">,</span><span class="w"> </span><span class="n">na.rm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">),</span><span class="w">
    </span><span class="n">tip_amount</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">tip_amount</span><span class="p">,</span><span class="w"> </span><span class="n">na.rm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">),</span><span class="w">
    </span><span class="n">tip_pct</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">tip_pct</span><span class="p">,</span><span class="w"> </span><span class="n">na.rm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
  </span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">arrange</span><span class="p">(</span><span class="n">passenger_count</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">collect</span><span class="p">()</span><span class="w">
</span></code></pre></div></div>

<h3 id="python">Python</h3>
<p>The workflow in Python is as simple as it is in R. In this example we use DuckDB’s Relational API.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">duckdb</span>
<span class="kn">import</span> <span class="nn">pyarrow</span> <span class="k">as</span> <span class="n">pa</span>
<span class="kn">import</span> <span class="nn">pyarrow.dataset</span> <span class="k">as</span> <span class="n">ds</span>

<span class="c1"># Open dataset using year,month folder partition
</span><span class="n">nyc</span> <span class="o">=</span> <span class="n">ds</span><span class="p">.</span><span class="n">dataset</span><span class="p">(</span><span class="s">'nyc-taxi/'</span><span class="p">,</span> <span class="n">partitioning</span><span class="o">=</span><span class="p">[</span><span class="s">"year"</span><span class="p">,</span> <span class="s">"month"</span><span class="p">])</span>

<span class="c1"># We transform the nyc dataset into a DuckDB relation
</span><span class="n">nyc</span> <span class="o">=</span> <span class="n">duckdb</span><span class="p">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">nyc</span><span class="p">)</span>

<span class="c1"># Run same query again
</span><span class="n">nyc</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span><span class="s">"year &gt; 2014 &amp; passenger_count &gt; 0 &amp; trip_distance &gt; 0.25 &amp; fare_amount &gt; 0"</span><span class="p">)</span>
    <span class="p">.</span><span class="n">aggregate</span><span class="p">(</span><span class="s">"SELECT AVG(fare_amount), AVG(tip_amount), AVG(tip_amount / fare_amount) as tip_pct"</span><span class="p">,</span><span class="s">"passenger_count"</span><span class="p">).</span><span class="n">arrow</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="duckdb-and-arrow-the-basics">DuckDB and Arrow: The Basics</h2>

<p>In this section, we will look at some basic examples of the code needed to read and output Arrow tables in both Python and R.</p>

<h4 id="setup">Setup</h4>

<p>First we need to install DuckDB and Arrow. The installation process for both libraries in Python and R is shown below.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Python Install</span>
pip <span class="nb">install </span>duckdb
pip <span class="nb">install </span>pyarrow
</code></pre></div></div>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># R Install</span><span class="w">
</span><span class="n">install.packages</span><span class="p">(</span><span class="s2">"duckdb"</span><span class="p">)</span><span class="w">
</span><span class="n">install.packages</span><span class="p">(</span><span class="s2">"arrow"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>To execute the sample-examples in this section, we need to download the following custom parquet files:</p>
<ul>
  <li>https://github.com/duckdb/duckdb-web/blob/master/_posts/data/integers.parquet?raw=true</li>
  <li>https://github.com/cwida/duckdb-data/releases/download/v1.0/lineitemsf1.snappy.parquet</li>
</ul>

<h4 id="python-1">Python</h4>

<p>There are two ways in Python of querying data from Arrow:</p>
<ol>
  <li>
    <p>Through the Relational API</p>

    <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c1"># Reads Parquet File to an Arrow Table
</span> <span class="n">arrow_table</span> <span class="o">=</span> <span class="n">pq</span><span class="p">.</span><span class="n">read_table</span><span class="p">(</span><span class="s">'integers.parquet'</span><span class="p">)</span>

 <span class="c1"># Transforms Arrow Table -&gt; DuckDB Relation
</span> <span class="n">rel_from_arrow</span> <span class="o">=</span> <span class="n">duckdb</span><span class="p">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">arrow_table</span><span class="p">)</span>

 <span class="c1"># we can run a SQL query on this and print the result
</span> <span class="k">print</span><span class="p">(</span><span class="n">rel_from_arrow</span><span class="p">.</span><span class="n">query</span><span class="p">(</span><span class="s">'arrow_table'</span><span class="p">,</span> <span class="s">'SELECT SUM(data) FROM arrow_table WHERE data &gt; 50'</span><span class="p">).</span><span class="n">fetchone</span><span class="p">())</span>

 <span class="c1"># Transforms DuckDB Relation -&gt; Arrow Table
</span> <span class="n">arrow_table_from_duckdb</span> <span class="o">=</span> <span class="n">rel_from_arrow</span><span class="p">.</span><span class="n">arrow</span><span class="p">()</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>By using replacement scans and querying the object directly with SQL:</p>

    <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c1"># Reads Parquet File to an Arrow Table
</span> <span class="n">arrow_table</span> <span class="o">=</span> <span class="n">pq</span><span class="p">.</span><span class="n">read_table</span><span class="p">(</span><span class="s">'integers.parquet'</span><span class="p">)</span>

 <span class="c1"># Gets Database Connection
</span> <span class="n">con</span> <span class="o">=</span> <span class="n">duckdb</span><span class="p">.</span><span class="n">connect</span><span class="p">()</span>

 <span class="c1"># we can run a SQL query on this and print the result
</span> <span class="k">print</span><span class="p">(</span><span class="n">con</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="s">'SELECT SUM(data) FROM arrow_table WHERE data &gt; 50'</span><span class="p">).</span><span class="n">fetchone</span><span class="p">())</span>

 <span class="c1"># Transforms Query Result from DuckDB to Arrow Table
</span> <span class="c1"># We can directly read the arrow object through DuckDB's replacement scans.
</span> <span class="n">con</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="s">"SELECT * FROM arrow_table"</span><span class="p">).</span><span class="n">fetch_arrow_table</span><span class="p">()</span>
</code></pre></div>    </div>
  </li>
</ol>

<p>It is possible to transform both DuckDB Relations and Query Results back to Arrow.</p>

<h4 id="r-1">R</h4>

<p>In R, you can interact with Arrow data in DuckDB by registering the table as a view (an alternative is to use dplyr as shown above).</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">duckdb</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">arrow</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span><span class="w">

</span><span class="c1"># Reads Parquet File to an Arrow Table</span><span class="w">
</span><span class="n">arrow_table</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">arrow</span><span class="o">::</span><span class="n">read_parquet</span><span class="p">(</span><span class="s2">"integers.parquet"</span><span class="p">,</span><span class="w"> </span><span class="n">as_data_frame</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w">

</span><span class="c1"># Gets Database Connection</span><span class="w">
</span><span class="n">con</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dbConnect</span><span class="p">(</span><span class="n">duckdb</span><span class="o">::</span><span class="n">duckdb</span><span class="p">())</span><span class="w">

</span><span class="c1"># Registers arrow table as a DuckDB view</span><span class="w">
</span><span class="n">arrow</span><span class="o">::</span><span class="n">to_duckdb</span><span class="p">(</span><span class="n">arrow_table</span><span class="p">,</span><span class="w"> </span><span class="n">table_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"arrow_table"</span><span class="p">,</span><span class="w"> </span><span class="n">con</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">con</span><span class="p">)</span><span class="w">

</span><span class="c1"># we can run a SQL query on this and print the result</span><span class="w">
</span><span class="n">print</span><span class="p">(</span><span class="n">dbGetQuery</span><span class="p">(</span><span class="n">con</span><span class="p">,</span><span class="w"> </span><span class="s2">"SELECT SUM(data) FROM arrow_table WHERE data &gt; 50"</span><span class="p">))</span><span class="w">

</span><span class="c1"># Transforms Query Result from DuckDB to Arrow Table</span><span class="w">
</span><span class="n">result</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dbSendQuery</span><span class="p">(</span><span class="n">con</span><span class="p">,</span><span class="w"> </span><span class="s2">"SELECT * FROM arrow_table"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<h3 id="streaming-data-fromto-arrow">Streaming Data from/to Arrow</h3>
<p>In the previous section, we depicted how to interact with Arrow tables. However, Arrow also allows users to interact with the data in a streaming fashion. Either consuming it (e.g., from an Arrow Dataset) or producing it (e.g., returning a RecordBatchReader). And of course, DuckDB is able to consume Datasets and produce RecordBatchReaders. This example uses the NYC Taxi Dataset, stored in Parquet files partitioned by year and month, which we can download through the Arrow R package:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">arrow</span><span class="o">::</span><span class="n">copy_files</span><span class="p">(</span><span class="s2">"s3://ursa-labs-taxi-data"</span><span class="p">,</span><span class="w"> </span><span class="s2">"nyc-taxi"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<h4 id="python-2">Python</h4>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Reads dataset partitioning it in year/month folder
</span><span class="n">nyc_dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="p">.</span><span class="n">dataset</span><span class="p">(</span><span class="s">'nyc-taxi/'</span><span class="p">,</span> <span class="n">partitioning</span><span class="o">=</span><span class="p">[</span><span class="s">"year"</span><span class="p">,</span> <span class="s">"month"</span><span class="p">])</span>

<span class="c1"># Gets Database Connection
</span><span class="n">con</span> <span class="o">=</span> <span class="n">duckdb</span><span class="p">.</span><span class="n">connect</span><span class="p">()</span>

<span class="n">query</span> <span class="o">=</span> <span class="n">con</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="s">"SELECT * FROM nyc_dataset"</span><span class="p">)</span>
<span class="c1"># DuckDB's queries can now produce a Record Batch Reader
</span><span class="n">record_batch_reader</span> <span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="n">fetch_record_batch</span><span class="p">()</span>
<span class="c1"># Which means we can stream the whole query per batch.
# This retrieves the first batch
</span><span class="n">chunk</span> <span class="o">=</span> <span class="n">record_batch_reader</span><span class="p">.</span><span class="n">read_next_batch</span><span class="p">()</span>
</code></pre></div></div>
<h4 id="r-2">R</h4>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Reads dataset partitioning it in year/month folder</span><span class="w">
</span><span class="n">nyc_dataset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">open_dataset</span><span class="p">(</span><span class="s2">"nyc-taxi/"</span><span class="p">,</span><span class="w"> </span><span class="n">partitioning</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"year"</span><span class="p">,</span><span class="w"> </span><span class="s2">"month"</span><span class="p">))</span><span class="w">

</span><span class="c1"># Gets Database Connection</span><span class="w">
</span><span class="n">con</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dbConnect</span><span class="p">(</span><span class="n">duckdb</span><span class="o">::</span><span class="n">duckdb</span><span class="p">())</span><span class="w">

</span><span class="c1"># We can use the same function as before to register our arrow dataset</span><span class="w">
</span><span class="n">duckdb</span><span class="o">::</span><span class="n">duckdb_register_arrow</span><span class="p">(</span><span class="n">con</span><span class="p">,</span><span class="w"> </span><span class="s2">"nyc"</span><span class="p">,</span><span class="w"> </span><span class="n">nyc_dataset</span><span class="p">)</span><span class="w">

</span><span class="n">res</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dbSendQuery</span><span class="p">(</span><span class="n">con</span><span class="p">,</span><span class="w"> </span><span class="s2">"SELECT * FROM nyc"</span><span class="p">,</span><span class="w"> </span><span class="n">arrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span><span class="c1"># DuckDB's queries can now produce a Record Batch Reader</span><span class="w">
</span><span class="n">record_batch_reader</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">duckdb</span><span class="o">::</span><span class="n">duckdb_fetch_record_batch</span><span class="p">(</span><span class="n">res</span><span class="p">)</span><span class="w">

</span><span class="c1"># Which means we can stream the whole query per batch.</span><span class="w">
</span><span class="c1"># This retrieves the first batch</span><span class="w">
</span><span class="n">cur_batch</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">record_batch_reader</span><span class="o">$</span><span class="n">read_next_batch</span><span class="p">()</span><span class="w">
</span></code></pre></div></div>

<p>The preceding R code shows in low-level detail how the data is streaming. We provide the helper <code class="language-plaintext highlighter-rouge">to_arrow()</code> in the Arrow package which is a wrapper around this that makes it easy to incorporate this streaming into a dplyr pipeline. <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<h2 id="benchmark-comparison">Benchmark Comparison</h2>

<p>Here we demonstrate in a simple benchmark the performance difference between querying Arrow datasets with DuckDB and querying Arrow datasets with Pandas.
For both the Projection and Filter pushdown comparison, we will use Arrow tables. That is due to Pandas not being capable of consuming Arrow stream objects.</p>

<p>For the NYC Taxi benchmarks, we used the <a href="https://www.monetdb.org/wiki/Scilens-configuration-standard">scilens diamonds configuration</a> and for the TPC-H benchmarks, we used an m1 MacBook Pro. In both cases, parallelism in DuckDB was used (which is now on by default).</p>

<p>For the comparison with Pandas, note that DuckDB runs in parallel, while pandas only support single-threaded execution. Besides that, one should note that we are comparing automatic optimizations. DuckDB’s query optimizer can automatically push down filters and projections. This automatic optimization is not supported in pandas, but it is possible for users to manually perform some of these predicate and filter pushdowns by manually specifying them them in the <code class="language-plaintext highlighter-rouge">read_parquet()</code> call.</p>

<h3 id="projection-pushdown">Projection Pushdown</h3>

<p>In this example we run a simple aggregation on two columns of our lineitem table.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># DuckDB
</span><span class="n">lineitem</span> <span class="o">=</span> <span class="n">pq</span><span class="p">.</span><span class="n">read_table</span><span class="p">(</span><span class="s">'lineitemsf1.snappy.parquet'</span><span class="p">)</span>
<span class="n">con</span> <span class="o">=</span> <span class="n">duckdb</span><span class="p">.</span><span class="n">connect</span><span class="p">()</span>

<span class="c1"># Transforms Query Result from DuckDB to Arrow Table
</span><span class="n">con</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="s">"""SELECT sum(l_extendedprice * l_discount) AS revenue
                FROM
                lineitem;"""</span><span class="p">).</span><span class="n">fetch_arrow_table</span><span class="p">()</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pandas
</span><span class="n">arrow_table</span> <span class="o">=</span> <span class="n">pq</span><span class="p">.</span><span class="n">read_table</span><span class="p">(</span><span class="s">'lineitemsf1.snappy.parquet'</span><span class="p">)</span>

<span class="c1"># Converts an Arrow table to a Dataframe
</span><span class="n">df</span> <span class="o">=</span> <span class="n">arrow_table</span><span class="p">.</span><span class="n">to_pandas</span><span class="p">()</span>

<span class="c1"># Runs aggregation
</span><span class="n">res</span> <span class="o">=</span>  <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'sum'</span><span class="p">:</span> <span class="p">[(</span><span class="n">df</span><span class="p">.</span><span class="n">l_extendedprice</span> <span class="o">*</span> <span class="n">df</span><span class="p">.</span><span class="n">l_discount</span><span class="p">).</span><span class="nb">sum</span><span class="p">()]})</span>

<span class="c1"># Creates an Arrow Table from a Dataframe
</span><span class="n">new_table</span> <span class="o">=</span> <span class="n">pa</span><span class="p">.</span><span class="n">Table</span><span class="p">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>

</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th style="text-align: right">Time (s)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>DuckDB</td>
      <td style="text-align: right">0.19</td>
    </tr>
    <tr>
      <td>Pandas</td>
      <td style="text-align: right">2.13</td>
    </tr>
  </tbody>
</table>

<p>The lineitem table is composed of 16 columns, however, to execute this query only two columns <code class="language-plaintext highlighter-rouge">l_extendedprice</code> and  *  <code class="language-plaintext highlighter-rouge">l_discount</code> are necessary. Since DuckDB can push down the projection of these columns, it is capable of executing this query about one order of magnitude faster than Pandas.</p>

<h3 id="filter-pushdown">Filter Pushdown</h3>

<p>For our filter pushdown we repeat the same aggregation used in the previous section, but add filters on 4 more columns.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># DuckDB
</span><span class="n">lineitem</span> <span class="o">=</span> <span class="n">pq</span><span class="p">.</span><span class="n">read_table</span><span class="p">(</span><span class="s">'lineitemsf1.snappy.parquet'</span><span class="p">)</span>

<span class="c1"># Get database connection
</span><span class="n">con</span> <span class="o">=</span> <span class="n">duckdb</span><span class="p">.</span><span class="n">connect</span><span class="p">()</span>

<span class="c1"># Transforms Query Result from DuckDB to Arrow Table
</span><span class="n">con</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="s">"""SELECT sum(l_extendedprice * l_discount) AS revenue
        FROM
            lineitem
        WHERE
            l_shipdate &gt;= CAST('1994-01-01' AS date)
            AND l_shipdate &lt; CAST('1995-01-01' AS date)
            AND l_discount BETWEEN 0.05
            AND 0.07
            AND l_quantity &lt; 24; """</span><span class="p">).</span><span class="n">fetch_arrow_table</span><span class="p">()</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pandas
</span><span class="n">arrow_table</span> <span class="o">=</span> <span class="n">pq</span><span class="p">.</span><span class="n">read_table</span><span class="p">(</span><span class="s">'lineitemsf1.snappy.parquet'</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">arrow_table</span><span class="p">.</span><span class="n">to_pandas</span><span class="p">()</span>
<span class="n">filtered_df</span> <span class="o">=</span> <span class="n">lineitem</span><span class="p">[</span>
        <span class="p">(</span><span class="n">lineitem</span><span class="p">.</span><span class="n">l_shipdate</span> <span class="o">&gt;=</span> <span class="s">"1994-01-01"</span><span class="p">)</span> <span class="o">&amp;</span>
        <span class="p">(</span><span class="n">lineitem</span><span class="p">.</span><span class="n">l_shipdate</span> <span class="o">&lt;</span> <span class="s">"1995-01-01"</span><span class="p">)</span> <span class="o">&amp;</span>
        <span class="p">(</span><span class="n">lineitem</span><span class="p">.</span><span class="n">l_discount</span> <span class="o">&gt;=</span> <span class="mf">0.05</span><span class="p">)</span> <span class="o">&amp;</span>
        <span class="p">(</span><span class="n">lineitem</span><span class="p">.</span><span class="n">l_discount</span> <span class="o">&lt;=</span> <span class="mf">0.07</span><span class="p">)</span> <span class="o">&amp;</span>
        <span class="p">(</span><span class="n">lineitem</span><span class="p">.</span><span class="n">l_quantity</span> <span class="o">&lt;</span> <span class="mi">24</span><span class="p">)]</span>

<span class="n">res</span> <span class="o">=</span>  <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'sum'</span><span class="p">:</span> <span class="p">[(</span><span class="n">filtered_df</span><span class="p">.</span><span class="n">l_extendedprice</span> <span class="o">*</span> <span class="n">filtered_df</span><span class="p">.</span><span class="n">l_discount</span><span class="p">).</span><span class="nb">sum</span><span class="p">()]})</span>
<span class="n">new_table</span> <span class="o">=</span> <span class="n">pa</span><span class="p">.</span><span class="n">Table</span><span class="p">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Time (s)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>DuckDB</td>
      <td>0.04</td>
    </tr>
    <tr>
      <td>Pandas</td>
      <td>2.29</td>
    </tr>
  </tbody>
</table>

<p>The difference now between DuckDB and Pandas is more drastic, being two orders of magnitude faster than Pandas. Again, since both the filter and projection are pushed down to Arrow, DuckDB reads less data than Pandas, which can’t automatically perform this optimization.</p>

<h3 id="streaming">Streaming</h3>

<p>As demonstrated before, DuckDB is capable of consuming and producing Arrow data in a streaming fashion. In this section we run a simple benchmark, to showcase the benefits in speed and memory usage when comparing it to full materialization and Pandas. This example uses the full NYC taxi dataset which you can download</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># DuckDB
# Open dataset using year,month folder partition
</span><span class="n">nyc</span> <span class="o">=</span> <span class="n">ds</span><span class="p">.</span><span class="n">dataset</span><span class="p">(</span><span class="s">'nyc-taxi/'</span><span class="p">,</span> <span class="n">partitioning</span><span class="o">=</span><span class="p">[</span><span class="s">"year"</span><span class="p">,</span> <span class="s">"month"</span><span class="p">])</span>

<span class="c1"># Get database connection
</span><span class="n">con</span> <span class="o">=</span> <span class="n">duckdb</span><span class="p">.</span><span class="n">connect</span><span class="p">()</span>

<span class="c1"># Run query that selects part of the data
</span><span class="n">query</span> <span class="o">=</span> <span class="n">con</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="s">"SELECT total_amount, passenger_count,year FROM nyc where total_amount &gt; 100 and year &gt; 2014"</span><span class="p">)</span>

<span class="c1"># Create Record Batch Reader from Query Result.
# "fetch_record_batch()" also accepts an extra parameter related to the desired produced chunk size.
</span><span class="n">record_batch_reader</span> <span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="n">fetch_record_batch</span><span class="p">()</span>

<span class="c1"># Retrieve all batch chunks
</span><span class="n">chunk</span> <span class="o">=</span> <span class="n">record_batch_reader</span><span class="p">.</span><span class="n">read_next_batch</span><span class="p">()</span>
<span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">chunk</span> <span class="o">=</span> <span class="n">record_batch_reader</span><span class="p">.</span><span class="n">read_next_batch</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pandas
# We must exclude one of the columns of the NYC dataset due to an unimplemented cast in Arrow.
</span><span class="n">working_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">"vendor_id"</span><span class="p">,</span><span class="s">"pickup_at"</span><span class="p">,</span><span class="s">"dropoff_at"</span><span class="p">,</span><span class="s">"passenger_count"</span><span class="p">,</span><span class="s">"trip_distance"</span><span class="p">,</span><span class="s">"pickup_longitude"</span><span class="p">,</span>
    <span class="s">"pickup_latitude"</span><span class="p">,</span><span class="s">"store_and_fwd_flag"</span><span class="p">,</span><span class="s">"dropoff_longitude"</span><span class="p">,</span><span class="s">"dropoff_latitude"</span><span class="p">,</span><span class="s">"payment_type"</span><span class="p">,</span>
    <span class="s">"fare_amount"</span><span class="p">,</span><span class="s">"extra"</span><span class="p">,</span><span class="s">"mta_tax"</span><span class="p">,</span><span class="s">"tip_amount"</span><span class="p">,</span><span class="s">"tolls_amount"</span><span class="p">,</span><span class="s">"total_amount"</span><span class="p">,</span><span class="s">"year"</span><span class="p">,</span> <span class="s">"month"</span><span class="p">]</span>

<span class="c1"># Open dataset using year,month folder partition
</span><span class="n">nyc_dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="p">.</span><span class="n">dataset</span><span class="p">(</span><span class="nb">dir</span><span class="p">,</span> <span class="n">partitioning</span><span class="o">=</span><span class="p">[</span><span class="s">"year"</span><span class="p">,</span> <span class="s">"month"</span><span class="p">])</span>
<span class="c1"># Generate a scanner to skip problematic column
</span><span class="n">dataset_scanner</span> <span class="o">=</span> <span class="n">nyc_dataset</span><span class="p">.</span><span class="n">scanner</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">working_columns</span><span class="p">)</span>

<span class="c1"># Materialize dataset to an Arrow Table
</span><span class="n">nyc_table</span> <span class="o">=</span> <span class="n">dataset_scanner</span><span class="p">.</span><span class="n">to_table</span><span class="p">()</span>

<span class="c1"># Generate Dataframe from Arow Table
</span><span class="n">nyc_df</span> <span class="o">=</span> <span class="n">nyc_table</span><span class="p">.</span><span class="n">to_pandas</span><span class="p">()</span>

<span class="c1"># Apply Filter
</span><span class="n">filtered_df</span> <span class="o">=</span> <span class="n">nyc_df</span><span class="p">[</span>
    <span class="p">(</span><span class="n">nyc_df</span><span class="p">.</span><span class="n">total_amount</span> <span class="o">&gt;</span> <span class="mi">100</span><span class="p">)</span> <span class="o">&amp;</span>
    <span class="p">(</span><span class="n">nyc_df</span><span class="p">.</span><span class="n">year</span> <span class="o">&gt;</span><span class="mi">2014</span><span class="p">)]</span>

<span class="c1"># Apply Projection
</span><span class="n">res</span> <span class="o">=</span> <span class="n">filtered_df</span><span class="p">[[</span><span class="s">"total_amount"</span><span class="p">,</span> <span class="s">"passenger_count"</span><span class="p">,</span><span class="s">"year"</span><span class="p">]]</span>

<span class="c1"># Transform Result back to an Arrow Table
</span><span class="n">new_table</span> <span class="o">=</span> <span class="n">pa</span><span class="p">.</span><span class="n">Table</span><span class="p">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Time (s)</th>
      <th>Peak Memory Usage (GBs)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>DuckDB</td>
      <td>0.05</td>
      <td>0.3</td>
    </tr>
    <tr>
      <td>Pandas</td>
      <td>146.91</td>
      <td>248</td>
    </tr>
  </tbody>
</table>

<p>The difference in times between DuckDB and Pandas is a combination of all the integration benefits we explored in this article. In DuckDB the filter pushdown is applied to perform partition elimination (i.e., we skip reading the Parquet files where the year is &lt;= 2014). The filter pushdown is also used to eliminate unrelated row_groups (i.e., row groups where the total amount is always &lt;= 100). Due to our projection pushdown, Arrow only has to read the columns of interest from the Parquet files, which allows it to read only 4 out of 20 columns. On the other hand, Pandas is not capable of automatically pushing down any of these optimizations, which means that the full dataset must be read. <strong>This results in the 4 orders of magnitude difference in query execution time.</strong></p>

<p>In the table above, we also depict the comparison of peak memory usage between DuckDB (Streaming) and Pandas (Fully-Materializing).  In DuckDB, we only need to load the row-group of interest into memory. Hence our memory usage is low. We also have constant memory usage since we only have to keep one of these row groups in-memory at a time. Pandas, on the other hand, has to fully materialize all Parquet files when executing the query. Because of this, we see a constant steep increase in its memory consumption. <strong>The total difference in memory consumption of the two solutions is around 3 orders of magnitude.</strong></p>

<h2 id="conclusion-and-feedback">Conclusion and Feedback</h2>
<p>In this blog post, we mainly showcased how to execute queries on Arrow datasets with DuckDB. There are additional libraries that can also consume the Arrow format but they have different purposes and capabilities. As always, we are happy to hear if you want to see benchmarks with different tools for a post in the future! Feel free to drop us an <a href="mailto:pedro@duckdblabs.com;jon@voltrondata.com">email</a> or share your thoughts directly in the Hacker News post.</p>

<p>Last but not least, if you encounter any problems when using our integration, please open an issue in in either <a href="https://github.com/duckdb/duckdb/issues">DuckDB’s - issue tracker</a>  or <a href="https://issues.apache.org/jira/projects/ARROW/">Arrow’s - issue tracker</a>, depending on which library has a problem.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>In Arrow 6.0.0, <code class="language-plaintext highlighter-rouge">to_arrow()</code> currently returns the full table, but will allow full streaming in our upcoming 7.0.0 release. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Pedro Holanda, Jonathan Keane</name></author><category term="application" /><summary type="html"><![CDATA[TLDR: The zero-copy integration between DuckDB and Apache Arrow allows for rapid analysis of larger than memory datasets in Python and R using either SQL or relational APIs. This post is a collaboration with and cross-posted on the DuckDB blog. Part of Apache Arrow is an in-memory data format optimized for analytical libraries. Like Pandas and R Dataframes, it uses a columnar data model. But the Arrow project contains more than just the format: The Arrow C++ library, which is accessible in Python, R, and Ruby via bindings, has additional features that allow you to compute efficiently on datasets. These additional features are on top of the implementation of the in-memory format described above. The datasets may span multiple files in Parquet, CSV, or other formats, and files may even be on remote or cloud storage like HDFS or Amazon S3. The Arrow C++ query engine supports the streaming of query results, has an efficient implementation of complex data types (e.g., Lists, Structs, Maps), and can perform important scan optimizations like Projection and Filter Pushdown. DuckDB is a new analytical data management system that is designed to run complex SQL queries within other processes. DuckDB has bindings for R and Python, among others. DuckDB can query Arrow datasets directly and stream query results back to Arrow. This integration allows users to query Arrow data using DuckDB’s SQL Interface and API, while taking advantage of DuckDB’s parallel vectorized execution engine, without requiring any extra data copying. Additionally, this integration takes full advantage of Arrow’s predicate and filter pushdown while scanning datasets. This integration is unique because it uses zero-copy streaming of data between DuckDB and Arrow and vice versa so that you can compose a query using both together. This results in three main benefits: Larger Than Memory Analysis: Since both libraries support streaming query results, we are capable of executing on data without fully loading it from disk. Instead, we can execute one batch at a time. This allows us to execute queries on data that is bigger than memory. Complex Data Types: DuckDB can efficiently process complex data types that can be stored in Arrow vectors, including arbitrarily nested structs, lists, and maps. Advanced Optimizer: DuckDB’s state-of-the-art optimizer can push down filters and projections directly into Arrow scans. As a result, only relevant columns and partitions will be read, allowing the system to e.g., take advantage of partition elimination in Parquet files. This significantly accelerates query execution. For those that are just interested in benchmarks, you can jump ahead benchmark section below. Quick Tour Before diving into the details of the integration, in this section we provide a quick motivating example of how powerful and simple to use is the DuckDB-Arrow integration. With a few lines of code, you can already start querying Arrow datasets. Say you want to analyze the infamous NYC Taxi Dataset and figure out if groups tip more or less than single riders. R Both Arrow and DuckDB support dplyr pipelines for people more comfortable with using dplyr for their data analysis. The Arrow package includes two helper functions that allow us to pass data back and forth between Arrow and DuckDB (to_duckdb() and to_arrow()). This is especially useful in cases where something is supported in one of Arrow or DuckDB but not the other. For example, if you find a complex dplyr pipeline where the SQL translation doesn’t work with DuckDB, use to_arrow() before the pipeline to use the Arrow engine. Or, if you have a function (e.g., windowed aggregates) that aren’t yet implemented in Arrow, use to_duckdb() to use the DuckDB engine. All while not paying any cost to (re)serialize the data when you pass it back and forth! library(duckdb) library(arrow) library(dplyr) # Open dataset using year,month folder partition ds &lt;- arrow::open_dataset("nyc-taxi", partitioning = c("year", "month")) ds %&gt;% # Look only at 2015 on, where the number of passenger is positive, the trip distance is # greater than a quarter mile, and where the fare amount is positive filter(year &gt; 2014 &amp; passenger_count &gt; 0 &amp; trip_distance &gt; 0.25 &amp; fare_amount &gt; 0) %&gt;% # Pass off to DuckDB to_duckdb() %&gt;% group_by(passenger_count) %&gt;% mutate(tip_pct = tip_amount / fare_amount) %&gt;% summarise( fare_amount = mean(fare_amount, na.rm = TRUE), tip_amount = mean(tip_amount, na.rm = TRUE), tip_pct = mean(tip_pct, na.rm = TRUE) ) %&gt;% arrange(passenger_count) %&gt;% collect() Python The workflow in Python is as simple as it is in R. In this example we use DuckDB’s Relational API. import duckdb import pyarrow as pa import pyarrow.dataset as ds # Open dataset using year,month folder partition nyc = ds.dataset('nyc-taxi/', partitioning=["year", "month"]) # We transform the nyc dataset into a DuckDB relation nyc = duckdb.arrow(nyc) # Run same query again nyc.filter("year &gt; 2014 &amp; passenger_count &gt; 0 &amp; trip_distance &gt; 0.25 &amp; fare_amount &gt; 0") .aggregate("SELECT AVG(fare_amount), AVG(tip_amount), AVG(tip_amount / fare_amount) as tip_pct","passenger_count").arrow() DuckDB and Arrow: The Basics In this section, we will look at some basic examples of the code needed to read and output Arrow tables in both Python and R. Setup First we need to install DuckDB and Arrow. The installation process for both libraries in Python and R is shown below. # Python Install pip install duckdb pip install pyarrow # R Install install.packages("duckdb") install.packages("arrow") To execute the sample-examples in this section, we need to download the following custom parquet files: https://github.com/duckdb/duckdb-web/blob/master/_posts/data/integers.parquet?raw=true https://github.com/cwida/duckdb-data/releases/download/v1.0/lineitemsf1.snappy.parquet Python There are two ways in Python of querying data from Arrow: Through the Relational API # Reads Parquet File to an Arrow Table arrow_table = pq.read_table('integers.parquet') # Transforms Arrow Table -&gt; DuckDB Relation rel_from_arrow = duckdb.arrow(arrow_table) # we can run a SQL query on this and print the result print(rel_from_arrow.query('arrow_table', 'SELECT SUM(data) FROM arrow_table WHERE data &gt; 50').fetchone()) # Transforms DuckDB Relation -&gt; Arrow Table arrow_table_from_duckdb = rel_from_arrow.arrow() By using replacement scans and querying the object directly with SQL: # Reads Parquet File to an Arrow Table arrow_table = pq.read_table('integers.parquet') # Gets Database Connection con = duckdb.connect() # we can run a SQL query on this and print the result print(con.execute('SELECT SUM(data) FROM arrow_table WHERE data &gt; 50').fetchone()) # Transforms Query Result from DuckDB to Arrow Table # We can directly read the arrow object through DuckDB's replacement scans. con.execute("SELECT * FROM arrow_table").fetch_arrow_table() It is possible to transform both DuckDB Relations and Query Results back to Arrow. R In R, you can interact with Arrow data in DuckDB by registering the table as a view (an alternative is to use dplyr as shown above). library(duckdb) library(arrow) library(dplyr) # Reads Parquet File to an Arrow Table arrow_table &lt;- arrow::read_parquet("integers.parquet", as_data_frame = FALSE) # Gets Database Connection con &lt;- dbConnect(duckdb::duckdb()) # Registers arrow table as a DuckDB view arrow::to_duckdb(arrow_table, table_name = "arrow_table", con = con) # we can run a SQL query on this and print the result print(dbGetQuery(con, "SELECT SUM(data) FROM arrow_table WHERE data &gt; 50")) # Transforms Query Result from DuckDB to Arrow Table result &lt;- dbSendQuery(con, "SELECT * FROM arrow_table") Streaming Data from/to Arrow In the previous section, we depicted how to interact with Arrow tables. However, Arrow also allows users to interact with the data in a streaming fashion. Either consuming it (e.g., from an Arrow Dataset) or producing it (e.g., returning a RecordBatchReader). And of course, DuckDB is able to consume Datasets and produce RecordBatchReaders. This example uses the NYC Taxi Dataset, stored in Parquet files partitioned by year and month, which we can download through the Arrow R package: arrow::copy_files("s3://ursa-labs-taxi-data", "nyc-taxi") Python # Reads dataset partitioning it in year/month folder nyc_dataset = ds.dataset('nyc-taxi/', partitioning=["year", "month"]) # Gets Database Connection con = duckdb.connect() query = con.execute("SELECT * FROM nyc_dataset") # DuckDB's queries can now produce a Record Batch Reader record_batch_reader = query.fetch_record_batch() # Which means we can stream the whole query per batch. # This retrieves the first batch chunk = record_batch_reader.read_next_batch() R # Reads dataset partitioning it in year/month folder nyc_dataset = open_dataset("nyc-taxi/", partitioning = c("year", "month")) # Gets Database Connection con &lt;- dbConnect(duckdb::duckdb()) # We can use the same function as before to register our arrow dataset duckdb::duckdb_register_arrow(con, "nyc", nyc_dataset) res &lt;- dbSendQuery(con, "SELECT * FROM nyc", arrow = TRUE) # DuckDB's queries can now produce a Record Batch Reader record_batch_reader &lt;- duckdb::duckdb_fetch_record_batch(res) # Which means we can stream the whole query per batch. # This retrieves the first batch cur_batch &lt;- record_batch_reader$read_next_batch() The preceding R code shows in low-level detail how the data is streaming. We provide the helper to_arrow() in the Arrow package which is a wrapper around this that makes it easy to incorporate this streaming into a dplyr pipeline. 1 Benchmark Comparison Here we demonstrate in a simple benchmark the performance difference between querying Arrow datasets with DuckDB and querying Arrow datasets with Pandas. For both the Projection and Filter pushdown comparison, we will use Arrow tables. That is due to Pandas not being capable of consuming Arrow stream objects. For the NYC Taxi benchmarks, we used the scilens diamonds configuration and for the TPC-H benchmarks, we used an m1 MacBook Pro. In both cases, parallelism in DuckDB was used (which is now on by default). For the comparison with Pandas, note that DuckDB runs in parallel, while pandas only support single-threaded execution. Besides that, one should note that we are comparing automatic optimizations. DuckDB’s query optimizer can automatically push down filters and projections. This automatic optimization is not supported in pandas, but it is possible for users to manually perform some of these predicate and filter pushdowns by manually specifying them them in the read_parquet() call. Projection Pushdown In this example we run a simple aggregation on two columns of our lineitem table. # DuckDB lineitem = pq.read_table('lineitemsf1.snappy.parquet') con = duckdb.connect() # Transforms Query Result from DuckDB to Arrow Table con.execute("""SELECT sum(l_extendedprice * l_discount) AS revenue FROM lineitem;""").fetch_arrow_table() # Pandas arrow_table = pq.read_table('lineitemsf1.snappy.parquet') # Converts an Arrow table to a Dataframe df = arrow_table.to_pandas() # Runs aggregation res = pd.DataFrame({'sum': [(df.l_extendedprice * df.l_discount).sum()]}) # Creates an Arrow Table from a Dataframe new_table = pa.Table.from_pandas(res) Name Time (s) DuckDB 0.19 Pandas 2.13 The lineitem table is composed of 16 columns, however, to execute this query only two columns l_extendedprice and * l_discount are necessary. Since DuckDB can push down the projection of these columns, it is capable of executing this query about one order of magnitude faster than Pandas. Filter Pushdown For our filter pushdown we repeat the same aggregation used in the previous section, but add filters on 4 more columns. # DuckDB lineitem = pq.read_table('lineitemsf1.snappy.parquet') # Get database connection con = duckdb.connect() # Transforms Query Result from DuckDB to Arrow Table con.execute("""SELECT sum(l_extendedprice * l_discount) AS revenue FROM lineitem WHERE l_shipdate &gt;= CAST('1994-01-01' AS date) AND l_shipdate &lt; CAST('1995-01-01' AS date) AND l_discount BETWEEN 0.05 AND 0.07 AND l_quantity &lt; 24; """).fetch_arrow_table() # Pandas arrow_table = pq.read_table('lineitemsf1.snappy.parquet') df = arrow_table.to_pandas() filtered_df = lineitem[ (lineitem.l_shipdate &gt;= "1994-01-01") &amp; (lineitem.l_shipdate &lt; "1995-01-01") &amp; (lineitem.l_discount &gt;= 0.05) &amp; (lineitem.l_discount &lt;= 0.07) &amp; (lineitem.l_quantity &lt; 24)] res = pd.DataFrame({'sum': [(filtered_df.l_extendedprice * filtered_df.l_discount).sum()]}) new_table = pa.Table.from_pandas(res) Name Time (s) DuckDB 0.04 Pandas 2.29 The difference now between DuckDB and Pandas is more drastic, being two orders of magnitude faster than Pandas. Again, since both the filter and projection are pushed down to Arrow, DuckDB reads less data than Pandas, which can’t automatically perform this optimization. Streaming As demonstrated before, DuckDB is capable of consuming and producing Arrow data in a streaming fashion. In this section we run a simple benchmark, to showcase the benefits in speed and memory usage when comparing it to full materialization and Pandas. This example uses the full NYC taxi dataset which you can download # DuckDB # Open dataset using year,month folder partition nyc = ds.dataset('nyc-taxi/', partitioning=["year", "month"]) # Get database connection con = duckdb.connect() # Run query that selects part of the data query = con.execute("SELECT total_amount, passenger_count,year FROM nyc where total_amount &gt; 100 and year &gt; 2014") # Create Record Batch Reader from Query Result. # "fetch_record_batch()" also accepts an extra parameter related to the desired produced chunk size. record_batch_reader = query.fetch_record_batch() # Retrieve all batch chunks chunk = record_batch_reader.read_next_batch() while len(chunk) &gt; 0: chunk = record_batch_reader.read_next_batch() # Pandas # We must exclude one of the columns of the NYC dataset due to an unimplemented cast in Arrow. working_columns = ["vendor_id","pickup_at","dropoff_at","passenger_count","trip_distance","pickup_longitude", "pickup_latitude","store_and_fwd_flag","dropoff_longitude","dropoff_latitude","payment_type", "fare_amount","extra","mta_tax","tip_amount","tolls_amount","total_amount","year", "month"] # Open dataset using year,month folder partition nyc_dataset = ds.dataset(dir, partitioning=["year", "month"]) # Generate a scanner to skip problematic column dataset_scanner = nyc_dataset.scanner(columns=working_columns) # Materialize dataset to an Arrow Table nyc_table = dataset_scanner.to_table() # Generate Dataframe from Arow Table nyc_df = nyc_table.to_pandas() # Apply Filter filtered_df = nyc_df[ (nyc_df.total_amount &gt; 100) &amp; (nyc_df.year &gt;2014)] # Apply Projection res = filtered_df[["total_amount", "passenger_count","year"]] # Transform Result back to an Arrow Table new_table = pa.Table.from_pandas(res) Name Time (s) Peak Memory Usage (GBs) DuckDB 0.05 0.3 Pandas 146.91 248 The difference in times between DuckDB and Pandas is a combination of all the integration benefits we explored in this article. In DuckDB the filter pushdown is applied to perform partition elimination (i.e., we skip reading the Parquet files where the year is &lt;= 2014). The filter pushdown is also used to eliminate unrelated row_groups (i.e., row groups where the total amount is always &lt;= 100). Due to our projection pushdown, Arrow only has to read the columns of interest from the Parquet files, which allows it to read only 4 out of 20 columns. On the other hand, Pandas is not capable of automatically pushing down any of these optimizations, which means that the full dataset must be read. This results in the 4 orders of magnitude difference in query execution time. In the table above, we also depict the comparison of peak memory usage between DuckDB (Streaming) and Pandas (Fully-Materializing). In DuckDB, we only need to load the row-group of interest into memory. Hence our memory usage is low. We also have constant memory usage since we only have to keep one of these row groups in-memory at a time. Pandas, on the other hand, has to fully materialize all Parquet files when executing the query. Because of this, we see a constant steep increase in its memory consumption. The total difference in memory consumption of the two solutions is around 3 orders of magnitude. Conclusion and Feedback In this blog post, we mainly showcased how to execute queries on Arrow datasets with DuckDB. There are additional libraries that can also consume the Arrow format but they have different purposes and capabilities. As always, we are happy to hear if you want to see benchmarks with different tools for a post in the future! Feel free to drop us an email or share your thoughts directly in the Hacker News post. Last but not least, if you encounter any problems when using our integration, please open an issue in in either DuckDB’s - issue tracker or Arrow’s - issue tracker, depending on which library has a problem. In Arrow 6.0.0, to_arrow() currently returns the full table, but will allow full streaming in our upcoming 7.0.0 release. &#8617;]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 6.0.1 Release</title><link href="https://arrow.apache.org/blog/2021/11/22/6.0.1-release/" rel="alternate" type="text/html" title="Apache Arrow 6.0.1 Release" /><published>2021-11-22T01:00:00-05:00</published><updated>2021-11-22T01:00:00-05:00</updated><id>https://arrow.apache.org/blog/2021/11/22/6.0.1-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2021/11/22/6.0.1-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 6.0.1 release.
This is mostly a bugfix release that includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%206.0.1"><strong>30 resolved issues</strong></a>
from <a href="/release/6.0.1.html#contributors"><strong>16 distinct contributors</strong></a>. See the Install Page to learn how to
get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/6.0.1.html">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 6.0.0 release, Joris Van den Bossche has joined the Project Management Committee (PMC). 
Thanks for your contributions and participation in the project!</p>

<h2 id="documentation">Documentation</h2>

<p>A version switcher is now available in the Python and C++ documentation
to access different versions of the documentation.</p>

<h2 id="c-notes">C++ notes</h2>

<ul>
  <li>Trying to join with a list column will now report an unsupported operation
error instead of crashing.</li>
  <li>Dictionaries are now supported as an input in hash joins</li>
  <li>Fixed a potential data loss in S3 multipart upload</li>
</ul>

<h2 id="python-notes">Python notes</h2>

<ul>
  <li>Dataset api now supports <code class="language-plaintext highlighter-rouge">existing_data_behavior</code> option when writing datasets.</li>
  <li>Installing pyarrow from source distribution now works with setuptools 58.5</li>
</ul>

<h2 id="r-notes">R notes</h2>

<ul>
  <li>Fixed a crash when summarizing after filtering to no rows</li>
  <li>Added bindings for <code class="language-plaintext highlighter-rouge">str_count</code></li>
</ul>

<p>For more details, see the <a href="/docs/r/news/">complete R changelog</a>.</p>

<h2 id="go-notes">Go notes</h2>

<ul>
  <li>Arrow and Parquet modules are properly brought together under a top level <code class="language-plaintext highlighter-rouge">github.com/apache/arrow/go</code> module which can be installed as <code class="language-plaintext highlighter-rouge">github.com/apache/arrow/go/v6/arrow@v6.0.1</code>.</li>
</ul>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 6.0.1 release. This is mostly a bugfix release that includes 30 resolved issues from 16 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 6.0.0 release, Joris Van den Bossche has joined the Project Management Committee (PMC). Thanks for your contributions and participation in the project! Documentation A version switcher is now available in the Python and C++ documentation to access different versions of the documentation. C++ notes Trying to join with a list column will now report an unsupported operation error instead of crashing. Dictionaries are now supported as an input in hash joins Fixed a potential data loss in S3 multipart upload Python notes Dataset api now supports existing_data_behavior option when writing datasets. Installing pyarrow from source distribution now works with setuptools 58.5 R notes Fixed a crash when summarizing after filtering to no rows Added bindings for str_count For more details, see the complete R changelog. Go notes Arrow and Parquet modules are properly brought together under a top level github.com/apache/arrow/go module which can be installed as github.com/apache/arrow/go/v6/arrow@v6.0.1.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow DataFusion 6.0.0 Release</title><link href="https://arrow.apache.org/blog/2021/11/19/datafusion-6.0.0/" rel="alternate" type="text/html" title="Apache Arrow DataFusion 6.0.0 Release" /><published>2021-11-19T00:00:00-05:00</published><updated>2021-11-19T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2021/11/19/datafusion-6.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2021/11/19/datafusion-6.0.0/"><![CDATA[<!--

-->

<h1 id="introduction">Introduction</h1>

<p><a href="https://arrow.apache.org/datafusion/">DataFusion</a> is an embedded
query engine which leverages the unique features of
<a href="https://www.rust-lang.org/">Rust</a> and <a href="https://arrow.apache.org/">Apache
Arrow</a> to provide a system that is high
performance, easy to connect, easy to embed, and high quality.</p>

<p>The Apache Arrow team is pleased to announce the DataFusion 6.0.0 release. This covers 4 months of development work
and includes 134 commits from the following 28 distinct contributors.</p>

<!--
git log --pretty=oneline 5.0.0..6.0.0 datafusion datafusion-cli datafusion-examples | wc -l
     134

git shortlog -sn 5.0.0..6.0.0 datafusion datafusion-cli datafusion-examples | wc -l
      29

      Carlos and xudong963 are same individual
-->

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    28  Andrew Lamb
    26  Jiayu Liu
    13  xudong963
     9  rdettai
     9  QP Hou
     6  Matthew Turner
     5  Daniël Heres
     4  Guillaume Balaine
     3  Francis Du
     3  Marco Neumann
     3  Jon Mease
     3  Nga Tran
     2  Yijie Shen
     2  Ruihang Xia
     2  Liang-Chi Hsieh
     2  baishen
     2  Andy Grove
     2  Jason Tianyi Wang
     1  Nan Zhu
     1  Antoine Wendlinger
     1  Krisztián Szűcs
     1  Mike Seddon
     1  Conner Murphy
     1  Patrick More
     1  Taehoon Moon
     1  Tiphaine Ruy
     1  adsharma
     1  lichuan6
</code></pre></div></div>

<p>The release notes below are not exhaustive and only expose selected highlights of the release. Many other bug fixes
and improvements have been made: we refer you to the complete
<a href="https://github.com/apache/arrow-datafusion/blob/6.0.0/datafusion/CHANGELOG.md">changelog</a>.</p>

<h1 id="new-website">New Website</h1>

<p>Befitting a growing project, DataFusion now has its
<a href="https://arrow.apache.org/datafusion/">own website</a> hosted as part of the
main <a href="https://arrow.apache.org">Apache Arrow Website</a></p>

<h1 id="roadmap">Roadmap</h1>

<p>The community worked to gather their thoughts about where we are
taking DataFusion into a public
<a href="https://arrow.apache.org/datafusion/specification/roadmap.html">Roadmap</a>
for the first time</p>

<h1 id="new-features">New Features</h1>

<ul>
  <li>Runtime operator metrics collection framework</li>
  <li>Object store abstraction for unified access to local or remote storage</li>
  <li>Hive style table partitioning support, for Parquet, CSV, Avro and Json files</li>
  <li>DataFrame API support for: <code class="language-plaintext highlighter-rouge">except</code>, <code class="language-plaintext highlighter-rouge">intersect</code>, <code class="language-plaintext highlighter-rouge">show</code>, <code class="language-plaintext highlighter-rouge">limit</code> and window functions</li>
  <li>SQL
    <ul>
      <li><code class="language-plaintext highlighter-rouge">EXPLAIN ANALYZE</code> with runtime metrics</li>
      <li><code class="language-plaintext highlighter-rouge">trim ( [ LEADING | TRAILING | BOTH ] [ FROM ] string text [, characters text ] )</code> syntax</li>
      <li>Postgres style regular expression matching operators <code class="language-plaintext highlighter-rouge">~</code>, <code class="language-plaintext highlighter-rouge">~*</code>, <code class="language-plaintext highlighter-rouge">!~</code>, and <code class="language-plaintext highlighter-rouge">!~*</code></li>
      <li>SQL set operators <code class="language-plaintext highlighter-rouge">UNION</code>, <code class="language-plaintext highlighter-rouge">INTERSECT</code>, and <code class="language-plaintext highlighter-rouge">EXCEPT</code></li>
      <li><code class="language-plaintext highlighter-rouge">cume_dist</code>, <code class="language-plaintext highlighter-rouge">percent_rank</code> window functions</li>
      <li><code class="language-plaintext highlighter-rouge">digest</code>, <code class="language-plaintext highlighter-rouge">blake2s</code>, <code class="language-plaintext highlighter-rouge">blake2b</code>, <code class="language-plaintext highlighter-rouge">blake3</code> crypto functions</li>
      <li>HyperLogLog based <code class="language-plaintext highlighter-rouge">approx_distinct</code></li>
      <li><code class="language-plaintext highlighter-rouge">is distinct from</code> and <code class="language-plaintext highlighter-rouge">is not distinct from</code></li>
      <li><code class="language-plaintext highlighter-rouge">CREATE TABLE AS SELECT</code></li>
      <li>Accessing elements of nested <code class="language-plaintext highlighter-rouge">Struct</code> and <code class="language-plaintext highlighter-rouge">List</code> columns (e.g. <code class="language-plaintext highlighter-rouge">SELECT struct_column['field_name'], array_column[0] FROM ...</code>)</li>
      <li>Boolean expressions in <code class="language-plaintext highlighter-rouge">CASE</code> statement</li>
      <li><code class="language-plaintext highlighter-rouge">DROP TABLE</code></li>
      <li><code class="language-plaintext highlighter-rouge">VALUES</code> List</li>
      <li>Postgres regex match operators</li>
    </ul>
  </li>
  <li>Support for Avro format</li>
  <li>Support for <code class="language-plaintext highlighter-rouge">ScalarValue::Struct</code></li>
  <li>Automatic schema inference for CSV files</li>
  <li>Better interactive editing support in <code class="language-plaintext highlighter-rouge">datafusion-cli</code> as well as <code class="language-plaintext highlighter-rouge">psql</code> style commands such as <code class="language-plaintext highlighter-rouge">\d</code>, <code class="language-plaintext highlighter-rouge">\?</code>, and <code class="language-plaintext highlighter-rouge">\q</code></li>
  <li>Generic constant evaluation and simplification framework</li>
  <li>Added common subexpression eliminate query plan optimization rule</li>
  <li>Python binding 0.4.0 with all Datafusion 6.0.0 features</li>
</ul>

<p>With these new features, we are also now passing TPC-H queries 8, 13 and 21.</p>

<p>For the full list of new features with their relevant PRs, see the
<a href="https://github.com/apache/arrow-datafusion/blob/6.0.0/datafusion/CHANGELOG.md">enhancements section</a>
in the changelog.</p>

<h1 id="async-planning-and-decoupling-file-format-from-table-layout"><code class="language-plaintext highlighter-rouge">async</code> planning and decoupling file format from table layout</h1>

<p>Driven by the need to support Hive style table partitioning, @rdettai
introduced the following design change to the Datafusion core.</p>

<ul>
  <li>The code for reading specific file formats (<code class="language-plaintext highlighter-rouge">Parquet</code>, <code class="language-plaintext highlighter-rouge">Avro</code>, <code class="language-plaintext highlighter-rouge">CSV</code>, and
<code class="language-plaintext highlighter-rouge">JSON</code>) was separated from the logic that handles grouping sets of
files into execution partitions.</li>
  <li>The query planning process was made <code class="language-plaintext highlighter-rouge">async</code>.</li>
</ul>

<p>As a result, we are able to replace the old <code class="language-plaintext highlighter-rouge">Parquet</code>, <code class="language-plaintext highlighter-rouge">CSV</code> and <code class="language-plaintext highlighter-rouge">JSON</code> table
providers with a single <code class="language-plaintext highlighter-rouge">ListingTable</code> table provider.</p>

<p>This also sets up DataFusion and its plug-in ecosystem to
supporting a wide range of catalogs and various object store implementations.
You can read more about this change in the
<a href="https://docs.google.com/document/d/1Bd4-PLLH-pHj0BquMDsJ6cVr_awnxTuvwNJuWsTHxAQ">design document</a>
and on the <a href="https://github.com/apache/arrow-datafusion/pull/1010">arrow-datafusion#1010 PR</a>.</p>

<h1 id="how-to-get-involved">How to Get Involved</h1>

<p>If you are interested in contributing to DataFusion, we would love to have you! You
can help by trying out DataFusion on some of your own data and projects and filing bug reports and helping to
improve the documentation, or contribute to the documentation, tests or code. A list of open issues suitable for
beginners is <a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a>
and the full list is <a href="https://github.com/apache/arrow-datafusion/issues">here</a>.</p>

<p>Check out our new <a href="https://arrow.apache.org/datafusion/community/communication.html">Communication Doc</a> on more
ways to engage with the community.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[Introduction DataFusion is an embedded query engine which leverages the unique features of Rust and Apache Arrow to provide a system that is high performance, easy to connect, easy to embed, and high quality. The Apache Arrow team is pleased to announce the DataFusion 6.0.0 release. This covers 4 months of development work and includes 134 commits from the following 28 distinct contributors. 28 Andrew Lamb 26 Jiayu Liu 13 xudong963 9 rdettai 9 QP Hou 6 Matthew Turner 5 Daniël Heres 4 Guillaume Balaine 3 Francis Du 3 Marco Neumann 3 Jon Mease 3 Nga Tran 2 Yijie Shen 2 Ruihang Xia 2 Liang-Chi Hsieh 2 baishen 2 Andy Grove 2 Jason Tianyi Wang 1 Nan Zhu 1 Antoine Wendlinger 1 Krisztián Szűcs 1 Mike Seddon 1 Conner Murphy 1 Patrick More 1 Taehoon Moon 1 Tiphaine Ruy 1 adsharma 1 lichuan6 The release notes below are not exhaustive and only expose selected highlights of the release. Many other bug fixes and improvements have been made: we refer you to the complete changelog. New Website Befitting a growing project, DataFusion now has its own website hosted as part of the main Apache Arrow Website Roadmap The community worked to gather their thoughts about where we are taking DataFusion into a public Roadmap for the first time New Features Runtime operator metrics collection framework Object store abstraction for unified access to local or remote storage Hive style table partitioning support, for Parquet, CSV, Avro and Json files DataFrame API support for: except, intersect, show, limit and window functions SQL EXPLAIN ANALYZE with runtime metrics trim ( [ LEADING | TRAILING | BOTH ] [ FROM ] string text [, characters text ] ) syntax Postgres style regular expression matching operators ~, ~*, !~, and !~* SQL set operators UNION, INTERSECT, and EXCEPT cume_dist, percent_rank window functions digest, blake2s, blake2b, blake3 crypto functions HyperLogLog based approx_distinct is distinct from and is not distinct from CREATE TABLE AS SELECT Accessing elements of nested Struct and List columns (e.g. SELECT struct_column['field_name'], array_column[0] FROM ...) Boolean expressions in CASE statement DROP TABLE VALUES List Postgres regex match operators Support for Avro format Support for ScalarValue::Struct Automatic schema inference for CSV files Better interactive editing support in datafusion-cli as well as psql style commands such as \d, \?, and \q Generic constant evaluation and simplification framework Added common subexpression eliminate query plan optimization rule Python binding 0.4.0 with all Datafusion 6.0.0 features With these new features, we are also now passing TPC-H queries 8, 13 and 21. For the full list of new features with their relevant PRs, see the enhancements section in the changelog. async planning and decoupling file format from table layout Driven by the need to support Hive style table partitioning, @rdettai introduced the following design change to the Datafusion core. The code for reading specific file formats (Parquet, Avro, CSV, and JSON) was separated from the logic that handles grouping sets of files into execution partitions. The query planning process was made async. As a result, we are able to replace the old Parquet, CSV and JSON table providers with a single ListingTable table provider. This also sets up DataFusion and its plug-in ecosystem to supporting a wide range of catalogs and various object store implementations. You can read more about this change in the design document and on the arrow-datafusion#1010 PR. How to Get Involved If you are interested in contributing to DataFusion, we would love to have you! You can help by trying out DataFusion on some of your own data and projects and filing bug reports and helping to improve the documentation, or contribute to the documentation, tests or code. A list of open issues suitable for beginners is here and the full list is here. Check out our new Communication Doc on more ways to engage with the community.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow Rust 6.0.0 Release</title><link href="https://arrow.apache.org/blog/2021/11/09/6.0.0-rs-release/" rel="alternate" type="text/html" title="Apache Arrow Rust 6.0.0 Release" /><published>2021-11-09T00:00:00-05:00</published><updated>2021-11-09T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2021/11/09/6.0.0-rs-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2021/11/09/6.0.0-rs-release/"><![CDATA[<!--

-->

<p>We recently released the 6.0.0 Rust version of Apache Arrow, which 
coincides with the <a href="/6.0.0.html">Arrow 6.0.0 release</a>. This 
post highlights some of the improvements in the Rust implementation. The full changelog can be 
found <a href="https://github.com/apache/arrow-rs/blob/6.0.0/CHANGELOG.md">here</a>.</p>

<!--
(arrow_dev) bkmgit@Linux:~/arrow-rs$ git log --pretty=oneline 5.0.0..6.0.0 | wc -l
     99
(arrow_dev) bkmgit@Linux:~/arrow-rs$ git shortlog -sn 5.0.0..6.0.0 | wc -l
     35
-->

<p>The Rust Arrow implementation would not be possible without the wonderful work and support of our community, and 
the 6.0.0 release is no exception. It includes 99 commits from 35 individual contributors, many of them with 
their first contribution. Thank you all very much.</p>

<h1 id="arrow">Arrow</h1>

<p>Highlighted features and changes between release 5.0.0 and this release are:</p>
<ol>
  <li>New MapArray support</li>
  <li>Add optimized filter kernel for regular expression matching</li>
  <li>Implement <code class="language-plaintext highlighter-rouge">sort()</code> for <code class="language-plaintext highlighter-rouge">BinaryArray</code></li>
  <li>Replace <code class="language-plaintext highlighter-rouge">ArrayData::new()</code> with <code class="language-plaintext highlighter-rouge">ArrayData::try_new()</code> and <code class="language-plaintext highlighter-rouge">unsafe ArrayData::new_unchecked</code></li>
  <li>Sorting should require less memory and be faster</li>
</ol>

<p>Of course, this release also contains bug fixes, performance improvements, and improved documentation examples. For the full list of changes, 
please consult the <a href="https://github.com/apache/arrow-rs/blob/6.0.0/CHANGELOG.md">changelog</a>.</p>

<h1 id="more-frequent-releases">More Frequent Releases</h1>
<p>Arrow releases major versions every three months. The Rust implementation follows this 
major release cycle, and additionally releases minor version updates approximately every other week
to speed the flow of new features and fixes.</p>

<p>You can always find the latest releases on crates.io: <a href="https://crates.io/crates/arrow"><code class="language-plaintext highlighter-rouge">arrow</code></a>, <a href="https://crates.io/crates/parquet"><code class="language-plaintext highlighter-rouge">parquet</code></a>, 
<a href="https://crates.io/crates/arrow-flight"><code class="language-plaintext highlighter-rouge">arrow-flight</code></a>, and <a href="https://crates.io/crates/parquet-derive"><code class="language-plaintext highlighter-rouge">parquet-derive</code></a>.</p>

<h1 id="datafusion--ballista">DataFusion &amp; Ballista</h1>
<p><a href="https://docs.rs/datafusion/">DataFusion</a> is an in-memory query engine with DataFrame and SQL APIs, built on top of Arrow. 
Ballista is a distributed compute platform. These projects are now in their <a href="https://github.com/apache/arrow-datafusion">own repository</a>, 
and are no longer released in lock-step with Arrow.</p>

<h1 id="highlighted-functionality">Highlighted Functionality</h1>
<p>The memory required to do sorting has been improve by the pull request resolving issue <a href="https://github.com/apache/arrow-rs/issues/553">553</a>. 
A demonstration for how to sort follows.</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">extern</span> <span class="k">crate</span> <span class="n">arrow</span><span class="p">;</span>

<span class="k">use</span> <span class="nn">arrow</span><span class="p">::</span><span class="nn">array</span><span class="p">::{</span>
    <span class="n">Int32Array</span><span class="p">,</span>
    <span class="n">ArrayRef</span><span class="p">,</span>
<span class="p">};</span>
<span class="k">use</span> <span class="nn">std</span><span class="p">::</span><span class="nn">sync</span><span class="p">::</span><span class="nb">Arc</span><span class="p">;</span>
<span class="k">use</span> <span class="nn">arrow</span><span class="p">::</span><span class="nn">compute</span><span class="p">::</span><span class="n">sort</span><span class="p">;</span>

<span class="k">fn</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
    
    <span class="k">let</span> <span class="n">array</span><span class="p">:</span> <span class="n">ArrayRef</span> <span class="o">=</span> <span class="nn">Arc</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nn">Int32Array</span><span class="p">::</span><span class="nf">from</span><span class="p">(</span><span class="nd">vec!</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">]));</span>
    <span class="nd">println!</span><span class="p">(</span><span class="s">"{:?}"</span><span class="p">,</span> <span class="n">array</span><span class="p">);</span>
    <span class="k">let</span> <span class="n">sorted_array</span> <span class="o">=</span> <span class="nf">sort</span><span class="p">(</span><span class="o">&amp;</span><span class="n">array</span><span class="p">,</span> <span class="nb">None</span><span class="p">)</span><span class="nf">.unwrap</span><span class="p">();</span>
    <span class="nd">println!</span><span class="p">(</span><span class="s">"{:?}"</span><span class="p">,</span> <span class="n">sorted_array</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>
<p>For further examples, see the <a href="https://github.com/apache/arrow-rs/blob/master/arrow/src/compute/kernels/sort.rs">source code</a>.</p>

<h1 id="roadmap-for-700-and-beyond">Roadmap for 7.0.0 and Beyond</h1>
<p>Here are some of the initiatives that contributors are currently working on for future releases:</p>

<ul>
  <li>Validate arguments to ArrayData::new and null bit buffer and buffers</li>
  <li>add ilike comparitor</li>
  <li>refactor regexp_is_match_utf8_scalar</li>
  <li>add support for float 16</li>
  <li>Updated <code class="language-plaintext highlighter-rouge">UnionArray</code> support to follow the latest arrow spec</li>
</ul>

<h1 id="contributors-to-600">Contributors to 6.0.0:</h1>
<p>Again, thank you to all the contributors for this release. Here is the raw git listing:</p>

<!--
(arrow_dev) bkmgit@Linux:~/arrow-rs$ git shortlog -sn 5.0.0..6.0.0
.. list below ..
-->

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    23  Andrew Lamb
     8  Ben Chambers
     8  Navin
     7  Jiayu Liu
     5  Wakahisa
     4  Ruihang Xia
     3  Daniël Heres
     3  Matthew Turner
     3  Sumit
     2  Boaz
     2  Chojan Shang
     2  Ilya Biryukov
     2  Krisztián Szűcs
     2  Markus Westerlind
     2  Roee Shlomo
     2  Sergii Mikhtoniuk
     2  Wang Fenjin
     2  baishen
     1  Carol (Nichols || Goulding)
     1  Christian Williams
     1  Felix Yan
     1  Jorge Leitao
     1  Kornelijus Survila
     1  Matthew Zeitlin
     1  Mike Seddon
     1  Mykhailo Osypov
     1  Neal Richardson
     1  Pete Koomen
     1  QP Hou
     1  Richard
     1  Xavier Lange
     1  Yuan Zhou
     1  aiglematth
     1  mathiaspeters-sig
     1  msalib
</code></pre></div></div>

<h1 id="how-to-get-involved">How to Get Involved</h1>
<p>If you are interested in contributing to the Rust implementation of Apache Arrow, we would love to have you! You can help by 
trying out Arrow on some of your own data and projects and filing bug reports and helping to improve the documentation, or 
contribute to the documentation, tests or code. A list of open issues suitable for beginners is 
<a href="https://github.com/apache/arrow-rs/labels/good%20first%20issue">here</a> and the full list is
<a href="https://github.com/apache/arrow-rs/issues">here</a></p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[We recently released the 6.0.0 Rust version of Apache Arrow, which coincides with the Arrow 6.0.0 release. This post highlights some of the improvements in the Rust implementation. The full changelog can be found here. The Rust Arrow implementation would not be possible without the wonderful work and support of our community, and the 6.0.0 release is no exception. It includes 99 commits from 35 individual contributors, many of them with their first contribution. Thank you all very much. Arrow Highlighted features and changes between release 5.0.0 and this release are: New MapArray support Add optimized filter kernel for regular expression matching Implement sort() for BinaryArray Replace ArrayData::new() with ArrayData::try_new() and unsafe ArrayData::new_unchecked Sorting should require less memory and be faster Of course, this release also contains bug fixes, performance improvements, and improved documentation examples. For the full list of changes, please consult the changelog. More Frequent Releases Arrow releases major versions every three months. The Rust implementation follows this major release cycle, and additionally releases minor version updates approximately every other week to speed the flow of new features and fixes. You can always find the latest releases on crates.io: arrow, parquet, arrow-flight, and parquet-derive. DataFusion &amp; Ballista DataFusion is an in-memory query engine with DataFrame and SQL APIs, built on top of Arrow. Ballista is a distributed compute platform. These projects are now in their own repository, and are no longer released in lock-step with Arrow. Highlighted Functionality The memory required to do sorting has been improve by the pull request resolving issue 553. A demonstration for how to sort follows. extern crate arrow; use arrow::array::{ Int32Array, ArrayRef, }; use std::sync::Arc; use arrow::compute::sort; fn main() { let array: ArrayRef = Arc::new(Int32Array::from(vec![5, 4, 23, 1, 20, 2])); println!("{:?}", array); let sorted_array = sort(&amp;array, None).unwrap(); println!("{:?}", sorted_array); } For further examples, see the source code. Roadmap for 7.0.0 and Beyond Here are some of the initiatives that contributors are currently working on for future releases: Validate arguments to ArrayData::new and null bit buffer and buffers add ilike comparitor refactor regexp_is_match_utf8_scalar add support for float 16 Updated UnionArray support to follow the latest arrow spec Contributors to 6.0.0: Again, thank you to all the contributors for this release. Here is the raw git listing: 23 Andrew Lamb 8 Ben Chambers 8 Navin 7 Jiayu Liu 5 Wakahisa 4 Ruihang Xia 3 Daniël Heres 3 Matthew Turner 3 Sumit 2 Boaz 2 Chojan Shang 2 Ilya Biryukov 2 Krisztián Szűcs 2 Markus Westerlind 2 Roee Shlomo 2 Sergii Mikhtoniuk 2 Wang Fenjin 2 baishen 1 Carol (Nichols || Goulding) 1 Christian Williams 1 Felix Yan 1 Jorge Leitao 1 Kornelijus Survila 1 Matthew Zeitlin 1 Mike Seddon 1 Mykhailo Osypov 1 Neal Richardson 1 Pete Koomen 1 QP Hou 1 Richard 1 Xavier Lange 1 Yuan Zhou 1 aiglematth 1 mathiaspeters-sig 1 msalib How to Get Involved If you are interested in contributing to the Rust implementation of Apache Arrow, we would love to have you! You can help by trying out Arrow on some of your own data and projects and filing bug reports and helping to improve the documentation, or contribute to the documentation, tests or code. A list of open issues suitable for beginners is here and the full list is here]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow R 6.0.0 Release</title><link href="https://arrow.apache.org/blog/2021/11/08/r-6.0.0/" rel="alternate" type="text/html" title="Apache Arrow R 6.0.0 Release" /><published>2021-11-08T00:00:00-05:00</published><updated>2021-11-08T00:00:00-05:00</updated><id>https://arrow.apache.org/blog/2021/11/08/r-6.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2021/11/08/r-6.0.0/"><![CDATA[<!--

-->

<p>We are excited to announce the recent release of version 6.0.0 of the Arrow R package on <a href="https://cran.r-project.org/package=arrow">CRAN</a>. While we usually don’t write a dedicated release blog post for the R package, this one is special. There are a number of major new features in this version, some of which we’ve been building up to for several years.</p>

<h1 id="more-dplyr-support">More dplyr support</h1>

<p>In version 0.16.0 (February 2020), we released the first version of the Dataset feature, which allowed you to query multi-file datasets using <code class="language-plaintext highlighter-rouge">dplyr::select()</code> and <code class="language-plaintext highlighter-rouge">filter()</code>. These tools allowed you to find a slice of data in a large dataset that may not fit into memory and pull it into R for further analysis. In version 4.0.0 earlier this year, we added support for <code class="language-plaintext highlighter-rouge">mutate()</code> and a number of other dplyr verbs, and all year we’ve been adding hundreds of functions you can use to transform and filter data in Datasets. However, to aggregate, you’d still need to pull the data into R.</p>

<h2 id="grouped-aggregation">Grouped aggregation</h2>

<p>With <code class="language-plaintext highlighter-rouge">arrow</code> 6.0.0, you can now <code class="language-plaintext highlighter-rouge">summarise()</code> on Arrow data, both with or without <code class="language-plaintext highlighter-rouge">group_by()</code>. These are supported both with in-memory Arrow tables as well as across partitioned datasets. Most common aggregation functions are supported: <code class="language-plaintext highlighter-rouge">n()</code>, <code class="language-plaintext highlighter-rouge">n_distinct()</code>, <code class="language-plaintext highlighter-rouge">min(),</code> <code class="language-plaintext highlighter-rouge">max()</code>, <code class="language-plaintext highlighter-rouge">sum()</code>, <code class="language-plaintext highlighter-rouge">mean()</code>, <code class="language-plaintext highlighter-rouge">var()</code>, <code class="language-plaintext highlighter-rouge">sd()</code>, <code class="language-plaintext highlighter-rouge">any()</code>, and <code class="language-plaintext highlighter-rouge">all()</code>. <code class="language-plaintext highlighter-rouge">median()</code> and <code class="language-plaintext highlighter-rouge">quantile()</code> with one probability are also supported and currently return approximate results using the t-digest algorithm.</p>

<p>As usual, Arrow will read and process data in chunks and in parallel when possible to produce results much faster than one could by loading it all into memory then processing. This allows for operations that wouldn’t fit into memory on a single machine. For example, using the 1.5-billion row NYC Taxi dataset we use for examples in the <a href="https://arrow.apache.org/docs/r/articles/dataset.html">package vignette</a>, we can aggregate over the whole dataset even on a laptop:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ds</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">open_dataset</span><span class="p">(</span><span class="s2">"nyc-taxi"</span><span class="p">,</span><span class="w"> </span><span class="n">partitioning</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"year"</span><span class="p">,</span><span class="w"> </span><span class="s2">"month"</span><span class="p">))</span><span class="w">
</span><span class="n">ds</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">filter</span><span class="p">(</span><span class="w">
    </span><span class="n">passenger_count</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w">
    </span><span class="n">passenger_count</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="m">6</span><span class="p">,</span><span class="w">
    </span><span class="n">grepl</span><span class="p">(</span><span class="s2">"csh"</span><span class="p">,</span><span class="w"> </span><span class="n">payment_type</span><span class="p">,</span><span class="w"> </span><span class="n">ignore.case</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
  </span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">group_by</span><span class="p">(</span><span class="n">passenger_count</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">summarize</span><span class="p">(</span><span class="w">
    </span><span class="n">avg</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">total_amount</span><span class="p">,</span><span class="w"> </span><span class="n">na.rm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">),</span><span class="w">
    </span><span class="n">count</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="p">()</span><span class="w">
  </span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">arrange</span><span class="p">(</span><span class="n">desc</span><span class="p">(</span><span class="n">count</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">collect</span><span class="p">()</span><span class="w">

</span><span class="c1">#&gt; # A tibble: 5 × 3</span><span class="w">
</span><span class="c1">#&gt;   passenger_count   avg     count</span><span class="w">
</span><span class="c1">#&gt;             &lt;int&gt; &lt;dbl&gt;     &lt;int&gt;</span><span class="w">
</span><span class="c1">#&gt; 1               1  11.1 257738064</span><span class="w">
</span><span class="c1">#&gt; 2               2  12.1  58824482</span><span class="w">
</span><span class="c1">#&gt; 3               5  11.4  26056438</span><span class="w">
</span><span class="c1">#&gt; 4               3  12.0  18852606</span><span class="w">
</span><span class="c1">#&gt; 5               4  12.3  10081632</span><span class="w">
</span></code></pre></div></div>

<h2 id="joins">Joins</h2>

<p>In addition to aggregation, Arrow also supports all of dplyr’s mutating joins (inner, left, right, and full) and filtering joins (semi and anti).</p>

<p>Suppose I want to get a table of all the flights from JFK to Las Vegas Airport on
9th October 2013, with the full name of the airline included.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">arrow_table</span><span class="p">(</span><span class="n">nycflights13</span><span class="o">::</span><span class="n">flights</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">filter</span><span class="p">(</span><span class="w">
    </span><span class="n">year</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">2013</span><span class="p">,</span><span class="w">
    </span><span class="n">month</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w">
    </span><span class="n">day</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">9</span><span class="p">,</span><span class="w">
    </span><span class="n">origin</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s2">"JFK"</span><span class="p">,</span><span class="w">
    </span><span class="n">dest</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s2">"LAS"</span><span class="w">
    </span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">select</span><span class="p">(</span><span class="n">dep_time</span><span class="p">,</span><span class="w"> </span><span class="n">arr_time</span><span class="p">,</span><span class="w"> </span><span class="n">carrier</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">left_join</span><span class="p">(</span><span class="w">
    </span><span class="n">arrow_table</span><span class="p">(</span><span class="n">nycflights13</span><span class="o">::</span><span class="n">airlines</span><span class="p">)</span><span class="w">
   </span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">collect</span><span class="p">()</span><span class="w">

</span><span class="c1">#&gt; # A tibble: 12 × 4</span><span class="w">
</span><span class="c1">#&gt;    dep_time arr_time carrier name</span><span class="w">
</span><span class="c1">#&gt;       &lt;int&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;</span><span class="w">
</span><span class="c1">#&gt;  1      637      853 B6      JetBlue Airways</span><span class="w">
</span><span class="c1">#&gt;  2      648      912 AA      American Airlines Inc.</span><span class="w">
</span><span class="c1">#&gt;  3      812     1029 DL      Delta Air Lines Inc.</span><span class="w">
</span><span class="c1">#&gt;  4      945     1206 VX      Virgin America</span><span class="w">
</span><span class="c1">#&gt;  5      955     1219 B6      JetBlue Airways</span><span class="w">
</span><span class="c1">#&gt;  6     1018     1231 DL      Delta Air Lines Inc.</span><span class="w">
</span><span class="c1">#&gt;  7     1120     1338 B6      JetBlue Airways</span><span class="w">
</span><span class="c1">#&gt;  8     1451     1705 DL      Delta Air Lines Inc.</span><span class="w">
</span><span class="c1">#&gt;  9     1656     1915 AA      American Airlines Inc.</span><span class="w">
</span><span class="c1">#&gt; 10     1755     2001 DL      Delta Air Lines Inc.</span><span class="w">
</span><span class="c1">#&gt; 11     1827     2049 B6      JetBlue Airways</span><span class="w">
</span><span class="c1">#&gt; 12     1917     2126 DL      Delta Air Lines Inc.</span><span class="w">
</span></code></pre></div></div>

<p>In this example, we’re working on an in-memory table, so you wouldn’t need <code class="language-plaintext highlighter-rouge">arrow</code> to do this–but the same code would work on a larger-than-memory dataset backed by thousands of Parquet files.</p>

<h2 id="under-the-hood">Under the hood</h2>

<p>To support these features, we’ve made some internal changes to how queries are built up and–importantly–when they are evaluated. As a result, there are some changes in behavior compared to past versions of <code class="language-plaintext highlighter-rouge">arrow</code>.</p>

<p>First, calls to <code class="language-plaintext highlighter-rouge">summarise()</code>, <code class="language-plaintext highlighter-rouge">head()</code>, and <code class="language-plaintext highlighter-rouge">tail()</code> no longer eagerly evaluate: this means you need to call either <code class="language-plaintext highlighter-rouge">compute()</code> (to evaluate it and produce an Arrow Table) or <code class="language-plaintext highlighter-rouge">collect()</code> (to evaluate and pull the Table into an R <code class="language-plaintext highlighter-rouge">data.frame</code>) to see the results.</p>

<p>Second, the order of rows in a dataset query is no longer determinisitic due to the way the parallelization of work happens in the C++ library. This means that you can’t assume that the results of a query will be in the same order as the rows of data in the files on disk. If you do need a stable sort order, call <code class="language-plaintext highlighter-rouge">arrange()</code> to specify ordering.</p>

<p>While these changes are a break from past <code class="language-plaintext highlighter-rouge">arrow</code> behavior, they are consistent with many <code class="language-plaintext highlighter-rouge">dbplyr</code> backends and are needed to allow queries to scale beyond data-frame workflows that can fit into memory.</p>

<h1 id="integration-with-duckdb">Integration with DuckDB</h1>

<p>The Arrow engine is not the only new way to query Arrow Datasets in this release. If you have the <a href="https://cran.r-project.org/package=duckdb">duckdb</a> package installed, you can hand off an Arrow Dataset or query object to <a href="https://duckdb.org/">DuckDB</a> for further querying using the <code class="language-plaintext highlighter-rouge">to_duckdb()</code> function. This allows you to use duckdb’s <code class="language-plaintext highlighter-rouge">dbplyr</code> methods, as well as its SQL interface, to aggregate data. DuckDB supports filter pushdown, so you can take advantage of Arrow Datasets and Arrow-based optimizations even within a DuckDB SQL query with a <code class="language-plaintext highlighter-rouge">where</code> clause. Filtering and column projection specified before the <code class="language-plaintext highlighter-rouge">to_duckdb()</code> call in a pipeline is evaluated in Arrow; this can be helpful in some circumstances like complicated dbplyr pipelines.  You can also hand off DuckDB data (or the result of a query) to arrow with the <code class="language-plaintext highlighter-rouge">to_arrow()</code> call.</p>

<p>In the example below, we are looking at flights between NYC and Chicago, and want to avoid the worst-of-the-worst delays. To do this, we can use <code class="language-plaintext highlighter-rouge">percent_rank()</code>; however that requires a window function which isn’t yet available in Arrow, so let’s try sending the data to DuckDB to do that, then pull it back into Arrow:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">arrow</span><span class="p">,</span><span class="w"> </span><span class="n">warn.conflicts</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">,</span><span class="w"> </span><span class="n">warn.conflicts</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w">

</span><span class="n">flights_filtered</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">arrow_table</span><span class="p">(</span><span class="n">nycflights13</span><span class="o">::</span><span class="n">flights</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">select</span><span class="p">(</span><span class="n">carrier</span><span class="p">,</span><span class="w"> </span><span class="n">origin</span><span class="p">,</span><span class="w"> </span><span class="n">dest</span><span class="p">,</span><span class="w"> </span><span class="n">arr_delay</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="c1"># arriving early doesn't matter, so call negative delays 0</span><span class="w">
  </span><span class="n">mutate</span><span class="p">(</span><span class="n">arr_delay</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pmax</span><span class="p">(</span><span class="n">arr_delay</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">to_duckdb</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="c1"># for each carrier-origin-dest, take the worst 5% of delays</span><span class="w">
  </span><span class="n">group_by</span><span class="p">(</span><span class="n">carrier</span><span class="p">,</span><span class="w"> </span><span class="n">origin</span><span class="p">,</span><span class="w"> </span><span class="n">dest</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">mutate</span><span class="p">(</span><span class="n">arr_delay_rank</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">percent_rank</span><span class="p">(</span><span class="n">arr_delay</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">filter</span><span class="p">(</span><span class="n">arr_delay_rank</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">0.95</span><span class="p">)</span><span class="w">

</span><span class="n">head</span><span class="p">(</span><span class="n">flights_filtered</span><span class="p">)</span><span class="w">
</span><span class="c1">#&gt; # Source:   lazy query [?? x 5]</span><span class="w">
</span><span class="c1">#&gt; # Database: duckdb_connection</span><span class="w">
</span><span class="c1">#&gt; # Groups:   carrier, origin, dest</span><span class="w">
</span><span class="c1">#&gt;   carrier origin dest  arr_delay arr_delay_rank</span><span class="w">
</span><span class="c1">#&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;          &lt;dbl&gt;</span><span class="w">
</span><span class="c1">#&gt; 1 9E      JFK    RIC         119          0.952</span><span class="w">
</span><span class="c1">#&gt; 2 9E      JFK    RIC         125          0.956</span><span class="w">
</span><span class="c1">#&gt; 3 9E      JFK    RIC         137          0.960</span><span class="w">
</span><span class="c1">#&gt; 4 9E      JFK    RIC         137          0.960</span><span class="w">
</span><span class="c1">#&gt; 5 9E      JFK    RIC         158          0.968</span><span class="w">
</span><span class="c1">#&gt; 6 9E      JFK    RIC         163          0.972</span><span class="w">
</span></code></pre></div></div>

<p>Now we have all of the flights filtered to those that are the worst-of-the-worst, and stored as a dbplyr lazy <code class="language-plaintext highlighter-rouge">tbl</code> with our DuckDB connection. This is an example of using Arrow -&gt; DuckDB.</p>

<p>But we can do more: we can then bring that data back into Arrow just as easily. For the rest of our analysis, we pick up where we left off with the <code class="language-plaintext highlighter-rouge">tbl</code> referring to the DuckDB query:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># pull data back into arrow to complete analysis</span><span class="w">
</span><span class="n">flights_filtered</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">to_arrow</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="c1"># now summarise to get mean/min</span><span class="w">
  </span><span class="n">group_by</span><span class="p">(</span><span class="n">carrier</span><span class="p">,</span><span class="w"> </span><span class="n">origin</span><span class="p">,</span><span class="w"> </span><span class="n">dest</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">summarise</span><span class="p">(</span><span class="w">
    </span><span class="n">arr_delay_mean</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">arr_delay</span><span class="p">),</span><span class="w">
    </span><span class="n">arr_delay_min</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">arr_delay</span><span class="p">),</span><span class="w">
    </span><span class="n">num_flights</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="p">()</span><span class="w">
  </span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">filter</span><span class="p">(</span><span class="n">dest</span><span class="w"> </span><span class="o">%in%</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"ORD"</span><span class="p">,</span><span class="w"> </span><span class="s2">"MDW"</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">arrange</span><span class="p">(</span><span class="n">desc</span><span class="p">(</span><span class="n">arr_delay_mean</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">collect</span><span class="p">()</span><span class="w">
</span><span class="c1">#&gt; # A tibble: 10 × 6</span><span class="w">
</span><span class="c1">#&gt; # Groups:   carrier, origin [10]</span><span class="w">
</span><span class="c1">#&gt;    carrier origin dest  arr_delay_mean arr_delay_min num_flights</span><span class="w">
</span><span class="c1">#&gt;    &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;</span><span class="w">
</span><span class="c1">#&gt;  1 MQ      EWR    ORD             190.           103         113</span><span class="w">
</span><span class="c1">#&gt;  2 9E      JFK    ORD             185.           134          52</span><span class="w">
</span><span class="c1">#&gt;  3 UA      LGA    ORD             179.           101         157</span><span class="w">
</span><span class="c1">#&gt;  4 WN      LGA    MDW             178.           107         103</span><span class="w">
</span><span class="c1">#&gt;  5 AA      JFK    ORD             178.           133          19</span><span class="w">
</span><span class="c1">#&gt;  6 B6      JFK    ORD             174.           129          46</span><span class="w">
</span><span class="c1">#&gt;  7 WN      EWR    MDW             167.           107         103</span><span class="w">
</span><span class="c1">#&gt;  8 UA      EWR    ORD             149.            87         189</span><span class="w">
</span><span class="c1">#&gt;  9 AA      LGA    ORD             135.            78         280</span><span class="w">
</span><span class="c1">#&gt; 10 EV      EWR    ORD              35             35           1</span><span class="w">
</span></code></pre></div></div>

<p>And just like that, we’ve passed data back and forth between Arrow and DuckDB without having to write a single file to disk!</p>

<h1 id="expanded-use-of-altrep">Expanded use of ALTREP</h1>

<p>We are continuing our use of R’s <a href="https://svn.r-project.org/R/branches/ALTREP/ALTREP.html">ALTREP</a> where possible. In 5.0.0 there were a limited set of circumstances that took advantage of ALTREP, but in 6.0.0 we have expanded types to include strings, as well as vectors with <code class="language-plaintext highlighter-rouge">NA</code>s.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">microbenchmark</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">arrow</span><span class="p">)</span><span class="w">

</span><span class="n">tbl</span><span class="w"> </span><span class="o">&lt;-</span><span class="w">
  </span><span class="n">arrow_table</span><span class="p">(</span><span class="n">data.frame</span><span class="p">(</span><span class="w">
    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="m">10000000</span><span class="p">),</span><span class="w">
    </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sample</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="nb">letters</span><span class="p">,</span><span class="w"> </span><span class="kc">NA</span><span class="p">),</span><span class="w"> </span><span class="m">10000000</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
  </span><span class="p">))</span><span class="w">

</span><span class="n">with_altrep</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">data</span><span class="p">){</span><span class="w">
  </span><span class="n">options</span><span class="p">(</span><span class="n">arrow.use_altrep</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
  </span><span class="n">as.data.frame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">without_altrep</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">data</span><span class="p">){</span><span class="w">
  </span><span class="n">options</span><span class="p">(</span><span class="n">arrow.use_altrep</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
  </span><span class="n">as.data.frame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">microbenchmark</span><span class="p">(</span><span class="w">
  </span><span class="n">without_altrep</span><span class="p">(</span><span class="n">tbl</span><span class="p">),</span><span class="w">
  </span><span class="n">with_altrep</span><span class="p">(</span><span class="n">tbl</span><span class="p">)</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="c1">#&gt; Unit: milliseconds</span><span class="w">
</span><span class="c1">#&gt;                 expr      min        lq      mean    median        uq      max neval</span><span class="w">
</span><span class="c1">#&gt;  without_altrep(tbl) 191.0788 213.82235 249.65076 225.52120 244.26977 512.1652   100</span><span class="w">
</span><span class="c1">#&gt;     with_altrep(tbl)  48.7152  50.97269  65.56832  52.93795  55.24505 338.4602   100</span><span class="w">
</span></code></pre></div></div>

<h1 id="airgapped-installation-on-linux">Airgapped installation on Linux</h1>

<p>With every release, we continue to improve the installation experience on Linux. Unlike macOS and Windows, CRAN does not host binary packages for Linux, and unless you’re using a service like RStudio Package Manger that hosts binaries, you have to build <code class="language-plaintext highlighter-rouge">arrow</code> from source. Because Arrow involves a large C++ project, this can be slow and sensitive to differences in build environments. To ensure a reliable installation experience, we work hard to test on a wide range of platforms and configurations and eagerly seek to simplify the process so that <code class="language-plaintext highlighter-rouge">install.packages("arrow")</code> just works and you don’t have to think about it.</p>

<p>A big improvement in 6.0.0 is that <code class="language-plaintext highlighter-rouge">arrow</code> can now install in a fully offline mode. The R package now includes the C++ source, so it does not need to be downloaded at build time. This does not include optional dependencies like compression libraries, the AWS SDK for accessing data in S3, and more. For folks who need to install Arrow on an airgapped server with all of those features, we have included a helper function to download and assemble a “fat” pacakge that contains everything that would be downloaded lazily at build time.
The function <code class="language-plaintext highlighter-rouge">create_package_with_all_dependencies()</code> can be run from a computer that does have access to the internet, and creates a fat-source package which can then be transferred and installed on a server without connectivity. This helper is also available on GitHub without installing the arrow package. For more installation <a href="https://arrow.apache.org/docs/r/articles/install.html#offline-installation">see the docs</a>.</p>

<p>Another installation change is that we’ve changed the source build to fail cleanly if the C++ library is not found or cannot be built. Previously, if the C++ library failed to build, you would get a successful R package installation, but the package wouldn’t do anything useful, it would just tell you to reinstall. This was helpful back in the early days of the package when we weren’t confident it would build everywhere that CRAN checked, but we now have much more experience (and extensive testing). In recent months this failure mode caused more confusion than it was worth, and it led many people to think that after you install arrow, you always have to <code class="language-plaintext highlighter-rouge">install_arrow()</code> again.</p>

<h1 id="thanks">Thanks</h1>

<p>This is a significant milestone for Arrow, and the R package specifically, and there is much gratitude to go around. In the 6.0.0 release, there were 77 individuals who contributed to Arrow, many of whom did the heavy lifting in the C++ library to make the new dataset query features a reality. Specifically in the R package, we wanted to acknowledge Phillip Cloud, Dewey Dunnington, Dragoș Moldovan-Grünfeld, Matt Peterson, and Percy Camilo Triveño Aucahuasi for their
their first contributions to the R package. And a special thanks goes to Karl Dunkle Werner for the hard work on the offline package build!</p>

<p>We also want to thank you in advance for your help. For this release of the Arrow query engine, we’ve focused our effort on getting the core functionality implemented. (In fact, this first release is something of an R-exclusive: bindings for these features haven’t yet been added to pyarrow, the Python Arrow library!) By focusing on the essentials, it means that there are a number of performance optimizations we plan to do but didn’t have time for in this release–and there are surely more issues to improve that we don’t yet know. We are eager for your feedback: please <a href="https://issues.apache.org/jira/browse/ARROW">let us know</a> of any issues you encounter so that we can improve these for our next release.</p>]]></content><author><name>Nic Crane, Jonathan Keane, Neal Richardson</name></author><category term="release" /><summary type="html"><![CDATA[We are excited to announce the recent release of version 6.0.0 of the Arrow R package on CRAN. While we usually don’t write a dedicated release blog post for the R package, this one is special. There are a number of major new features in this version, some of which we’ve been building up to for several years. More dplyr support In version 0.16.0 (February 2020), we released the first version of the Dataset feature, which allowed you to query multi-file datasets using dplyr::select() and filter(). These tools allowed you to find a slice of data in a large dataset that may not fit into memory and pull it into R for further analysis. In version 4.0.0 earlier this year, we added support for mutate() and a number of other dplyr verbs, and all year we’ve been adding hundreds of functions you can use to transform and filter data in Datasets. However, to aggregate, you’d still need to pull the data into R. Grouped aggregation With arrow 6.0.0, you can now summarise() on Arrow data, both with or without group_by(). These are supported both with in-memory Arrow tables as well as across partitioned datasets. Most common aggregation functions are supported: n(), n_distinct(), min(), max(), sum(), mean(), var(), sd(), any(), and all(). median() and quantile() with one probability are also supported and currently return approximate results using the t-digest algorithm. As usual, Arrow will read and process data in chunks and in parallel when possible to produce results much faster than one could by loading it all into memory then processing. This allows for operations that wouldn’t fit into memory on a single machine. For example, using the 1.5-billion row NYC Taxi dataset we use for examples in the package vignette, we can aggregate over the whole dataset even on a laptop: ds &lt;- open_dataset("nyc-taxi", partitioning = c("year", "month")) ds %&gt;% filter( passenger_count &gt; 0, passenger_count &lt; 6, grepl("csh", payment_type, ignore.case = TRUE) ) %&gt;% group_by(passenger_count) %&gt;% summarize( avg = mean(total_amount, na.rm = TRUE), count = n() ) %&gt;% arrange(desc(count)) %&gt;% collect() #&gt; # A tibble: 5 × 3 #&gt; passenger_count avg count #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1 11.1 257738064 #&gt; 2 2 12.1 58824482 #&gt; 3 5 11.4 26056438 #&gt; 4 3 12.0 18852606 #&gt; 5 4 12.3 10081632 Joins In addition to aggregation, Arrow also supports all of dplyr’s mutating joins (inner, left, right, and full) and filtering joins (semi and anti). Suppose I want to get a table of all the flights from JFK to Las Vegas Airport on 9th October 2013, with the full name of the airline included. arrow_table(nycflights13::flights) %&gt;% filter( year == 2013, month == 10, day == 9, origin == "JFK", dest == "LAS" ) %&gt;% select(dep_time, arr_time, carrier) %&gt;% left_join( arrow_table(nycflights13::airlines) ) %&gt;% collect() #&gt; # A tibble: 12 × 4 #&gt; dep_time arr_time carrier name #&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 637 853 B6 JetBlue Airways #&gt; 2 648 912 AA American Airlines Inc. #&gt; 3 812 1029 DL Delta Air Lines Inc. #&gt; 4 945 1206 VX Virgin America #&gt; 5 955 1219 B6 JetBlue Airways #&gt; 6 1018 1231 DL Delta Air Lines Inc. #&gt; 7 1120 1338 B6 JetBlue Airways #&gt; 8 1451 1705 DL Delta Air Lines Inc. #&gt; 9 1656 1915 AA American Airlines Inc. #&gt; 10 1755 2001 DL Delta Air Lines Inc. #&gt; 11 1827 2049 B6 JetBlue Airways #&gt; 12 1917 2126 DL Delta Air Lines Inc. In this example, we’re working on an in-memory table, so you wouldn’t need arrow to do this–but the same code would work on a larger-than-memory dataset backed by thousands of Parquet files. Under the hood To support these features, we’ve made some internal changes to how queries are built up and–importantly–when they are evaluated. As a result, there are some changes in behavior compared to past versions of arrow. First, calls to summarise(), head(), and tail() no longer eagerly evaluate: this means you need to call either compute() (to evaluate it and produce an Arrow Table) or collect() (to evaluate and pull the Table into an R data.frame) to see the results. Second, the order of rows in a dataset query is no longer determinisitic due to the way the parallelization of work happens in the C++ library. This means that you can’t assume that the results of a query will be in the same order as the rows of data in the files on disk. If you do need a stable sort order, call arrange() to specify ordering. While these changes are a break from past arrow behavior, they are consistent with many dbplyr backends and are needed to allow queries to scale beyond data-frame workflows that can fit into memory. Integration with DuckDB The Arrow engine is not the only new way to query Arrow Datasets in this release. If you have the duckdb package installed, you can hand off an Arrow Dataset or query object to DuckDB for further querying using the to_duckdb() function. This allows you to use duckdb’s dbplyr methods, as well as its SQL interface, to aggregate data. DuckDB supports filter pushdown, so you can take advantage of Arrow Datasets and Arrow-based optimizations even within a DuckDB SQL query with a where clause. Filtering and column projection specified before the to_duckdb() call in a pipeline is evaluated in Arrow; this can be helpful in some circumstances like complicated dbplyr pipelines. You can also hand off DuckDB data (or the result of a query) to arrow with the to_arrow() call. In the example below, we are looking at flights between NYC and Chicago, and want to avoid the worst-of-the-worst delays. To do this, we can use percent_rank(); however that requires a window function which isn’t yet available in Arrow, so let’s try sending the data to DuckDB to do that, then pull it back into Arrow: library(arrow, warn.conflicts = FALSE) library(dplyr, warn.conflicts = FALSE) flights_filtered &lt;- arrow_table(nycflights13::flights) %&gt;% select(carrier, origin, dest, arr_delay) %&gt;% # arriving early doesn't matter, so call negative delays 0 mutate(arr_delay = pmax(arr_delay, 0)) %&gt;% to_duckdb() %&gt;% # for each carrier-origin-dest, take the worst 5% of delays group_by(carrier, origin, dest) %&gt;% mutate(arr_delay_rank = percent_rank(arr_delay)) %&gt;% filter(arr_delay_rank &gt; 0.95) head(flights_filtered) #&gt; # Source: lazy query [?? x 5] #&gt; # Database: duckdb_connection #&gt; # Groups: carrier, origin, dest #&gt; carrier origin dest arr_delay arr_delay_rank #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 9E JFK RIC 119 0.952 #&gt; 2 9E JFK RIC 125 0.956 #&gt; 3 9E JFK RIC 137 0.960 #&gt; 4 9E JFK RIC 137 0.960 #&gt; 5 9E JFK RIC 158 0.968 #&gt; 6 9E JFK RIC 163 0.972 Now we have all of the flights filtered to those that are the worst-of-the-worst, and stored as a dbplyr lazy tbl with our DuckDB connection. This is an example of using Arrow -&gt; DuckDB. But we can do more: we can then bring that data back into Arrow just as easily. For the rest of our analysis, we pick up where we left off with the tbl referring to the DuckDB query: # pull data back into arrow to complete analysis flights_filtered %&gt;% to_arrow() %&gt;% # now summarise to get mean/min group_by(carrier, origin, dest) %&gt;% summarise( arr_delay_mean = mean(arr_delay), arr_delay_min = min(arr_delay), num_flights = n() ) %&gt;% filter(dest %in% c("ORD", "MDW")) %&gt;% arrange(desc(arr_delay_mean)) %&gt;% collect() #&gt; # A tibble: 10 × 6 #&gt; # Groups: carrier, origin [10] #&gt; carrier origin dest arr_delay_mean arr_delay_min num_flights #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 MQ EWR ORD 190. 103 113 #&gt; 2 9E JFK ORD 185. 134 52 #&gt; 3 UA LGA ORD 179. 101 157 #&gt; 4 WN LGA MDW 178. 107 103 #&gt; 5 AA JFK ORD 178. 133 19 #&gt; 6 B6 JFK ORD 174. 129 46 #&gt; 7 WN EWR MDW 167. 107 103 #&gt; 8 UA EWR ORD 149. 87 189 #&gt; 9 AA LGA ORD 135. 78 280 #&gt; 10 EV EWR ORD 35 35 1 And just like that, we’ve passed data back and forth between Arrow and DuckDB without having to write a single file to disk! Expanded use of ALTREP We are continuing our use of R’s ALTREP where possible. In 5.0.0 there were a limited set of circumstances that took advantage of ALTREP, but in 6.0.0 we have expanded types to include strings, as well as vectors with NAs. library(microbenchmark) library(arrow) tbl &lt;- arrow_table(data.frame( x = rnorm(10000000), y = sample(c(letters, NA), 10000000, replace = TRUE) )) with_altrep &lt;- function(data){ options(arrow.use_altrep = TRUE) as.data.frame(data) } without_altrep &lt;- function(data){ options(arrow.use_altrep = FALSE) as.data.frame(data) } microbenchmark( without_altrep(tbl), with_altrep(tbl) ) #&gt; Unit: milliseconds #&gt; expr min lq mean median uq max neval #&gt; without_altrep(tbl) 191.0788 213.82235 249.65076 225.52120 244.26977 512.1652 100 #&gt; with_altrep(tbl) 48.7152 50.97269 65.56832 52.93795 55.24505 338.4602 100 Airgapped installation on Linux With every release, we continue to improve the installation experience on Linux. Unlike macOS and Windows, CRAN does not host binary packages for Linux, and unless you’re using a service like RStudio Package Manger that hosts binaries, you have to build arrow from source. Because Arrow involves a large C++ project, this can be slow and sensitive to differences in build environments. To ensure a reliable installation experience, we work hard to test on a wide range of platforms and configurations and eagerly seek to simplify the process so that install.packages("arrow") just works and you don’t have to think about it. A big improvement in 6.0.0 is that arrow can now install in a fully offline mode. The R package now includes the C++ source, so it does not need to be downloaded at build time. This does not include optional dependencies like compression libraries, the AWS SDK for accessing data in S3, and more. For folks who need to install Arrow on an airgapped server with all of those features, we have included a helper function to download and assemble a “fat” pacakge that contains everything that would be downloaded lazily at build time. The function create_package_with_all_dependencies() can be run from a computer that does have access to the internet, and creates a fat-source package which can then be transferred and installed on a server without connectivity. This helper is also available on GitHub without installing the arrow package. For more installation see the docs. Another installation change is that we’ve changed the source build to fail cleanly if the C++ library is not found or cannot be built. Previously, if the C++ library failed to build, you would get a successful R package installation, but the package wouldn’t do anything useful, it would just tell you to reinstall. This was helpful back in the early days of the package when we weren’t confident it would build everywhere that CRAN checked, but we now have much more experience (and extensive testing). In recent months this failure mode caused more confusion than it was worth, and it led many people to think that after you install arrow, you always have to install_arrow() again. Thanks This is a significant milestone for Arrow, and the R package specifically, and there is much gratitude to go around. In the 6.0.0 release, there were 77 individuals who contributed to Arrow, many of whom did the heavy lifting in the C++ library to make the new dataset query features a reality. Specifically in the R package, we wanted to acknowledge Phillip Cloud, Dewey Dunnington, Dragoș Moldovan-Grünfeld, Matt Peterson, and Percy Camilo Triveño Aucahuasi for their their first contributions to the R package. And a special thanks goes to Karl Dunkle Werner for the hard work on the offline package build! We also want to thank you in advance for your help. For this release of the Arrow query engine, we’ve focused our effort on getting the core functionality implemented. (In fact, this first release is something of an R-exclusive: bindings for these features haven’t yet been added to pyarrow, the Python Arrow library!) By focusing on the essentials, it means that there are a number of performance optimizations we plan to do but didn’t have time for in this release–and there are surely more issues to improve that we don’t yet know. We are eager for your feedback: please let us know of any issues you encounter so that we can improve these for our next release.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>