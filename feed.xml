<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://arrow.apache.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arrow.apache.org/" rel="alternate" type="text/html" /><updated>2023-06-28T17:26:02-04:00</updated><id>https://arrow.apache.org/feed.xml</id><title type="html">Apache Arrow</title><subtitle>Apache Arrow is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, C++, C#, Go, Java, JavaScript, MATLAB, Python, R, Ruby, and Rust.</subtitle><entry><title type="html">Apache Arrow ADBC 0.5.1 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/06/27/adbc-0.5.1-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.5.1 (Libraries) Release" /><published>2023-06-27T00:00:00-04:00</published><updated>2023-06-27T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/06/27/adbc-0.5.1-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/06/27/adbc-0.5.1-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.5.1 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/7"><strong>8
resolved issues</strong></a> from <a href="#contributors"><strong>8 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version
0.5.1.  The <strong>API specification</strong> is versioned separately and is
at version 1.0.0.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.5.1/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>This is a patch release primarily aimed at fixing a <a href="https://github.com/apache/arrow-adbc/commit/f35485a5f3c9597668c0b4a8936621c97c4adc15">deadlock in the Snowflake driver</a> that was discovered post-release.</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.5.0..apache-arrow-adbc-0.5.1
     9	David Li
     5	Dewey Dunnington
     4	Matt Topol
     3	William Ayd
     2	davidhcoe
     1	Curt Hagenlocher
     1	Diego Fernández Giraldo
     1	Sutou Kouhei
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>Work for the proposed <a href="https://github.com/apache/arrow-adbc/milestone/3">1.1.0 API revision</a> is progressing on a <a href="https://github.com/apache/arrow-adbc/tree/spec-1.1.0">branch</a>, with several API proposals merged in and others awaiting review.  They will then receive prototype implementations before being submitted for review/voting.  This is not currently targeting any release of the ADBC libraries, although we hope to wrap up this effort in the next few months.</p>

<p>A milestone has been created to explore <a href="https://github.com/apache/arrow-adbc/milestone/9">asynchronous APIs</a>, which have been requested for a while now.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.5.1 release of the Apache Arrow ADBC libraries. This covers includes 8 resolved issues from 8 distinct contributors. This is a release of the libraries, which are at version 0.5.1. The API specification is versioned separately and is at version 1.0.0. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights This is a patch release primarily aimed at fixing a deadlock in the Snowflake driver that was discovered post-release. Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.5.0..apache-arrow-adbc-0.5.1 9 David Li 5 Dewey Dunnington 4 Matt Topol 3 William Ayd 2 davidhcoe 1 Curt Hagenlocher 1 Diego Fernández Giraldo 1 Sutou Kouhei Roadmap Work for the proposed 1.1.0 API revision is progressing on a branch, with several API proposals merged in and others awaiting review. They will then receive prototype implementations before being submitted for review/voting. This is not currently targeting any release of the ADBC libraries, although we hope to wrap up this effort in the next few months. A milestone has been created to explore asynchronous APIs, which have been requested for a while now. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Our journey at F5 with Apache Arrow (part 2): Adaptive Schemas and Sorting to Optimize Arrow Usage</title><link href="https://arrow.apache.org/blog/2023/06/26/our-journey-at-f5-with-apache-arrow-part-2/" rel="alternate" type="text/html" title="Our journey at F5 with Apache Arrow (part 2): Adaptive Schemas and Sorting to Optimize Arrow Usage" /><published>2023-06-26T00:00:00-04:00</published><updated>2023-06-26T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/06/26/our-journey-at-f5-with-apache-arrow-part-2</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/06/26/our-journey-at-f5-with-apache-arrow-part-2/"><![CDATA[<!--

-->

<p>In the previous <a href="https://arrow.apache.org/blog/2023/04/11/our-journey-at-f5-with-apache-arrow-part-1/">article</a>, we discussed our use of Apache Arrow within the context of the OpenTelemetry project. We investigated various techniques to maximize the efficiency of Apache Arrow, aiming to find the optimal balance between data compression ratio and queryability. The compression results speak for themselves, boasting improvements ranging from 1.5x to 5x better than the original OTLP protocol. In this article, we will delve into three techniques that have enabled us to enhance both the compression ratio and memory usage of Apache Arrow buffers within the current version of the <a href="https://github.com/f5/otel-arrow-adapter">OTel Arrow protocol</a>.</p>

<p>The first technique we’ll discuss aims to optimize schemas in terms of memory usage. As you’ll see, the gains can be substantial, potentially halving memory usage in certain cases. The second section will delve more deeply into the various approaches that can be used to handle recursive schema definitions. Lastly, we’ll emphasize that the design of your schema(s), coupled with the sorts you can apply at the record level, play a pivotal role in maximizing the benefits of Apache Arrow and its columnar representation.</p>

<h2 id="handling-dynamic-and-unknown-data-distributions">Handling dynamic and unknown data distributions</h2>

<p>In certain contexts, the comprehensive definition of an Arrow schema can end up being overly broad and complex in order to cover all possible cases that you intend to represent in columnar form. However, as is often the case with complex schemas, only a subset of this schema will actually be utilized for a specific deployment. Similarly, it’s not always possible to determine the optimal dictionary encoding for one or more fields in advance. Employing a broad and very general schema that covers all cases is usually more memory-intensive. This is because, for most implementations, a column without value still continues to consume memory space. Likewise, a column with dictionary encoding that indexes a uint64 will occupy four times more memory than the same column with a dictionary encoding based on a uint8.</p>

<p>To illustrate this more concretely, let’s consider an OTel collector positioned at the output of a production environment, receiving a telemetry data stream produced by a large and dynamic set of servers. Invariably, the content of this telemetry stream will change in volume and nature over time. It’s challenging to predict the optimal schema in such a scenario, and it’s equally difficult to know in advance the distribution of a particular attribute of the telemetry data passing through this point.</p>

<p>To optimize such scenarios, we have adopted an intermediary approach that we have named <strong>dynamic Arrow schema</strong>, aiming to gradually adapt the schema based on the observed data. The general principle is relatively simple. We start with a general schema defining the maximum envelope of what should be represented. Some fields of this schema will be declared optional, while other fields will be encoded with multiple possible options depending on the observed distribution. In theory, this principle can be applied to other types of transformations (e.g., recursive column creation) but we will let your imagination explore these other options. So if you encounter data streams where certain fields are not utilized, some union variants remain unused, and/or the value distribution of a field cannot be determined a priori, it may be worthwhile to invest time in implementing this model. This can lead to improved efficiency in terms of compression ratio, memory usage, and processing speed.</p>

<p>The following Go Arrow schema definition provides an example of such a schema, instrumented with a collection of annotations. These annotations will be processed by an enhanced Record Builder, equipped with the ability to dynamically adapt the schema. The structure of this system is illustrated in Figure 1.</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">var</span> <span class="p">(</span>
  <span class="c">// Arrow schema for the OTLP Arrow Traces record (without attributes, links, and events).</span>
  <span class="n">TracesSchema</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">.</span><span class="n">NewSchema</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
      <span class="c">// Nullabe:true means the field is optional, in this case of 16 bit unsigned integers</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">ID</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint16</span><span class="p">,</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">Resource</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">ID</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint16</span><span class="p">,</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
        <span class="c">// --- Use dictionary with 8 bit integers initially ----</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">SchemaUrl</span><span class="p">,</span><span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span><span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">DroppedAttributesCount</span><span class="p">,</span><span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint32</span><span class="p">,</span><span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">}</span><span class="o">...</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">Scope</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
          <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">ID</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint16</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">acommon</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">acommon</span><span class="o">.</span><span class="n">DeltaEncoding</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
          <span class="c">// --- Use dictionary with 8 bit integers initially ----</span>
          <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">Name</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">acommon</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">acommon</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
          <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">Version</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">acommon</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">acommon</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
          <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">DroppedAttributesCount</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint32</span><span class="p">,</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">}</span><span class="o">...</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">SchemaUrl</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">StartTimeUnixNano</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FixedWidthTypes</span><span class="o">.</span><span class="n">Timestamp_ns</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">DurationTimeUnixNano</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FixedWidthTypes</span><span class="o">.</span><span class="n">Duration_ms</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">)},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">TraceId</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">FixedSizeBinaryType</span><span class="p">{</span><span class="n">ByteWidth</span><span class="o">:</span> <span class="m">16</span><span class="p">}},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">SpanId</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">FixedSizeBinaryType</span><span class="p">{</span><span class="n">ByteWidth</span><span class="o">:</span> <span class="m">8</span><span class="p">}},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">TraceState</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">ParentSpanId</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">FixedSizeBinaryType</span><span class="p">{</span><span class="n">ByteWidth</span><span class="o">:</span> <span class="m">8</span><span class="p">},</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">Name</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">)},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">KIND</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Int32</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">DroppedAttributesCount</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint32</span><span class="p">,</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">DroppedEventsCount</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint32</span><span class="p">,</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">DroppedLinksCount</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint32</span><span class="p">,</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">Status</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">StatusCode</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Int32</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">StatusMessage</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span> <span class="n">Metadata</span><span class="o">:</span> <span class="n">schema</span><span class="o">.</span><span class="n">Metadata</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">Dictionary8</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
      <span class="p">}</span><span class="o">...</span><span class="p">),</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
    <span class="p">},</span> <span class="no">nil</span><span class="p">)</span>
  <span class="p">)</span>
</code></pre></div></div>

<p>In this example, Arrow field-level metadata are employed to designate when a field is optional (Nullable: true) or to specify the minimal dictionary encoding applicable to a particular field (Metadata Dictionary8/16/…). Now let’s imagine a scenario utilizing this schema in a straightforward scenario, wherein only a handful of fields are actually in use, and the cardinality of most dictionary-encoded fields is low (i.e., below 2^8). Ideally, we’d want a system capable of dynamically constructing the following simplified schema, which, in essence, is a strict subset of the original schema.</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">var</span> <span class="p">(</span>
  <span class="c">// Simplified schema definition generated by the Arrow Record encoder based on</span>
  <span class="c">// the data observed.</span>
  <span class="n">TracesSchema</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">.</span><span class="n">NewSchema</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">ID</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint16</span><span class="p">,</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">StartTimeUnixNano</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FixedWidthTypes</span><span class="o">.</span><span class="n">Timestamp_ns</span><span class="p">},</span>
    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">TraceId</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">FixedSizeBinaryType</span><span class="p">{</span><span class="n">ByteWidth</span><span class="o">:</span> <span class="m">16</span><span class="p">}},</span>
    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">SpanId</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">FixedSizeBinaryType</span><span class="p">{</span><span class="n">ByteWidth</span><span class="o">:</span> <span class="m">8</span><span class="p">}},</span>
    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">Name</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">DictionaryType</span> <span class="p">{</span>
      <span class="n">IndexType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint8</span><span class="p">,</span>
      <span class="n">ValueType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">}},</span>
    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="n">constants</span><span class="o">.</span><span class="n">KIND</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">DictionaryType</span> <span class="p">{</span>
      <span class="n">IndexType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint8</span><span class="p">,</span>
      <span class="n">ValueType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Int32</span><span class="p">,</span>
    <span class="p">},</span> <span class="n">Nullable</span><span class="o">:</span> <span class="no">true</span><span class="p">},</span>
  <span class="p">},</span> <span class="no">nil</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Additionally, we desire a system capable of automatically adapting the aforementioned schema if it encounters new fields or existing fields with a cardinality exceeding the size of the current dictionary definition in future batches. In extreme scenarios, if the cardinality of a specific field surpasses a certain threshold, we would prefer the system to automatically revert to the non-dictionary representation (mechanism of dictionary overflow). That is precisely what we will elaborate on in the remainder of this section.</p>

<p>An overview of the different components and events used to implement this approach is depicted in figure 1.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/adaptive-schema-architecture.svg" width="100%" class="img-responsive" alt="Fig 1: Adaptive Arrow schema architecture overview." />
  <figcaption>Fig 1: Adaptive Arrow schema architecture overview.</figcaption>
</figure>

<p>The overall Adaptive Arrow schema component takes a data stream segmented into batches and produces one or multiple streams of Arrow Records (one schema per stream). Each of these records is defined with an Arrow schema, which is based both on the annotated Arrow schema and the shape of fields observed in the incoming data.</p>

<p>More specifically, the process of the Adaptive Arrow schema component consists of four main phases</p>

<p><strong>Initialization phase</strong></p>

<p>During the initialization phase, the Arrow Record Encoder reads the annotated Arrow schema (i.e. the reference schema) and generates a collection of transformations. When these transformations are applied to the reference schema, they yield the first minimal Arrow schema that adheres to the constraints depicted by these annotations. In this initial iteration, all optional fields are eliminated, and all dictionary-encoded fields are configured to utilize the smallest encoding as defined by the annotation (only <code class="language-plaintext highlighter-rouge">Dictionary8</code> in the previous example). These transformations form a tree, reflecting the structure of the reference schema.</p>

<p><strong>Feeding phase</strong></p>

<p>Following the initialization is the feeding phase. Here, the Arrow Record Encoder scans the batch and attempts to store all the fields in an Arrow Record Builder, which is defined by the schema created in the prior step. If a field exists in the data but is not included in the schema, the encoder will trigger a <code class="language-plaintext highlighter-rouge">missing field</code> event. This process continues until the current batch is completely processed. An additional internal check is conducted on all dictionary-encoded fields in the Arrow Record builder to ensure there’s no dictionary overflow (i.e. more unique entries than the cardinality of the index permits). <code class="language-plaintext highlighter-rouge">Dictionary overflow</code> events are generated if such a situation is detected. Consequently, by the end, all unknown fields and dictionary overflow would have been detected, or alternatively, no discrepancies would have surfaced if the data aligns perfectly with the schema.</p>

<p><strong>Corrective phase</strong></p>

<p>If at least one event has been generated, a corrective phase will be initiated to fix the schema. This optional stage considers all the events generated in the previous stage and adjusts the transformation tree accordingly to align with the observed data. A <code class="language-plaintext highlighter-rouge">missing field</code> event will remove a NoField transformation for the corresponding field. A <code class="language-plaintext highlighter-rouge">dictionary overflow</code> event will modify the dictionary transformation to mirror the event (e.g. changing the index type from uint8 to uint16, or if the maximum index size has been reached, the transformation will remove the dictionary-encoding and revert to the original non-dictionary-encoded type). The updated transformation tree is subsequently used to create a new schema and a fresh Arrow Record Builder. This Record Builder is then utilized to replay the preceding feeding phase with the batch that wasn’t processed correctly.</p>

<p><strong>Routing phase</strong></p>

<p>Once a Record Builder has been properly fed, an Arrow Record is created, and the system transitions into the routing phase. The router component calculates a schema signature of the record and utilizes this signature to route the record to an existing Arrow stream compatible with the signature, or it initiates a new stream if there is no match.</p>

<p>This four-phase process should gradually adapt and stabilize the schema to a structure and definition that is optimized for a specific data stream. Unused fields will never unnecessarily consume memory. Dictionary-encoded fields will be defined with the most optimal index size based on the observed data cardinality, and fields with a cardinality exceeding a certain threshold (defined by configuration) will automatically revert to their non-dictionary-encoded versions.</p>

<p>To effectively execute this approach, you must ensure that there is a sufficient level of flexibility on the receiver side. It’s crucial that your downstream pipeline remains functional even when some fields are missing in the schema or when various dictionary index configurations are employed. While this may not always be feasible without implementing additional transformations upon reception, it proves worthwhile in certain scenarios.</p>

<p>The following results highlight the significant memory usage reduction achieved through the application of various optimization techniques. These results were gathered using a schema akin to the one previously presented. The considerable memory efficiency underscores the effectiveness of this approach.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/memory-usage-25k-traces.png" width="100%" class="img-responsive" alt="Fig 2: Comparative analysis of memory usage for different schema optimizations." />
  <figcaption>Fig 2: Comparative analysis of memory usage for different schema optimizations.</figcaption>
</figure>

<p>The concept of a transformation tree enables a generalized approach to perform various types of schema optimizations based on the knowledge acquired from the data. This architecture is highly flexible; the current implementation allows for the removal of unused fields, the application of the most specific dictionary encoding, and the optimization of union type variants. In the future, there is potential for introducing additional optimizations that can be expressed as transformations on the initial schema. An implementation of this approach is available <a href="https://github.com/f5/otel-arrow-adapter/tree/main/pkg/otel/common/schema">here</a>.</p>

<h2 id="handling-recursive-schema-definition">Handling recursive schema definition</h2>

<p>Apache Arrow does not support recursive schema definitions, implying that data structures with variable depth cannot be directly represented. Figure 3 exemplifies such a recursive definition where the value of an attribute can either be a simple data type, a list of values, or a map of values. The depth of this definition cannot be predetermined.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/recursive-def-otel-attributes.svg" width="100%" class="img-responsive" alt="Fig 3: Recursive definition of OTel attributes." />
  <figcaption>Fig 3: Recursive definition of OTel attributes.</figcaption>
</figure>

<p>Several strategies can be employed to circumvent this limitation. Technically, the dynamic schema concept we’ve presented could be expanded to dynamically update the schema to include any missing level of recursion. However, for this use case, this method is complex and has the notable downside of not offering any assurance on the maximum size of the schema. This lack of constraint can pose security issues; hence, this approach isn’t elaborated upon.</p>

<p>The second approach consists of breaking the recursion by employing a serialization format that supports the definition of a recursive schema. The result of this serialization can then be integrated into the Arrow record as a binary type column, effectively halting the recursion at a specific level. To fully leverage the advantages of columnar representation, it is crucial to apply this ad-hoc serialization as deeply within the data structure as feasible. In the context of OpenTelemetry, this is performed at the attribute level – more specifically, at the second level of attributes.</p>

<p>A variety of serialization formats, such as protobuf or CBOR, can be employed to encode recursive data. Without particular treatment, these binary columns may not be easily queryable by the existing Arrow query engines. Therefore, it’s crucial to thoughtfully ascertain when and where to apply such a technique. While I’m not aware of any attempts to address this limitation within the Arrow system, it doesn’t seem insurmountable and would constitute a valuable extension. This would help reduce the complexity of integrating Arrow with other systems that rely on such recursive definitions.</p>

<h2 id="importance-of-sorting">Importance of sorting</h2>

<p>In our preceding <a href="https://arrow.apache.org/blog/2023/04/11/our-journey-at-f5-with-apache-arrow-part-1/">article</a>, we explored a variety of strategies to represent hierarchical data models, including nested structures based on struct/list/map/union, denormalization and flattening representations, as well as a multi-record approach. Each method presents its unique advantages and disadvantages. However, in this last section, we’ll delve deeper into the multi-record approach, focusing specifically on its ability to offer versatile sorting options and how these options contribute to an enhanced compression ratio.</p>

<p>In the OTel Arrow protocol, we leverage the multi-record approach to represent metrics, logs, and traces. The following entity-relationship diagram offers a simplified version of various record schemas and illustrates their relationships, specifically those used to represent gauges and sums. A comprehensive description of the Arrow data model employed in OpenTelemetry can be accessed <a href="https://github.com/f5/otel-arrow-adapter/blob/main/docs/data_model.md">here</a>.</p>

<p>These Arrow records, also referred to as tables, form a hierarchy with <code class="language-plaintext highlighter-rouge">METRICS</code> acting as the primary entry point. Each table can be independently sorted according to one or more columns. This sorting strategy facilitates the grouping of duplicated data, thereby improving the compression ratio.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/metric-dp-data-model.png" width="100%" class="img-responsive" alt="Fig 4: A simplified entity-relationship diagram representing OTel sum &amp; gauge metrics." />
  <figcaption>Fig 4: A simplified entity-relationship diagram representing OTel sum &amp; gauge metrics.</figcaption>
</figure>

<p>The relationship between the primary <code class="language-plaintext highlighter-rouge">METRICS</code> table and the secondary <code class="language-plaintext highlighter-rouge">RESOURCE_ATTRS</code>, <code class="language-plaintext highlighter-rouge">SCOPE_ATTRS</code>, and <code class="language-plaintext highlighter-rouge">NUMBER_DATA_POINTS</code> tables is established through a unique <code class="language-plaintext highlighter-rouge">id</code> in the main table and a <code class="language-plaintext highlighter-rouge">parent_id</code> column in each of the secondary tables. This {id,parent_id} pair represents an overhead that should be minimized to the greatest extent possible post-compression.</p>

<p>To achieve this, the ordering process for the different tables adheres to the hierarchy, starting from the main table down to the leaf. The main table is sorted (by one or multiple columns), and then an incremental id is assigned to each row. This numerical id is stored using delta-encoding, which is implemented on top of Arrow.</p>

<p>The secondary tables directly connected to the main table are sorted using the same principle, but the <code class="language-plaintext highlighter-rouge">parent_id</code> column is consistently utilized as the last column in the sort statement. Including the <code class="language-plaintext highlighter-rouge">parent_id</code> column in the sort statement enables the use of a variation of delta encoding. The efficiency of this approach is summarized in the chart below.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/compressed-message-size.png" width="100%" class="img-responsive" alt="Fig 5: Comparative analysis of compression ratios - OTLP Protocol vs. Two variations of the OTel Arrow Protocol with multivariate metrics stream. (lower percentage is better)" />
  <figcaption>Fig 5: Comparative analysis of compression ratios - OTLP Protocol vs. Two variations of the OTel Arrow Protocol with multivariate metrics stream. (lower percentage is better)</figcaption>
</figure>

<p>The second column presents the average size of the OTLP batch both pre- and post-ZSTD compression for batches of varying sizes. This column serves as a reference point for the ensuing two columns. The third column displays results for the OTel Arrow protocol without any sorting applied, while the final column showcases results for the OTel Arrow protocol with sorting enabled.</p>

<p>Before compression, the average batch sizes for the two OTel Arrow configurations are predictably similar. However, post-compression, the benefits of sorting each individual table on the compression ratio become immediately apparent. Without sorting, the OTel Arrow protocol exhibits a compression ratio that’s 1.40 to 1.67 times better than the reference. When sorting is enabled, the OTel Arrow protocol outperforms the reference by a factor ranging from 4.94 to 7.21 times!</p>

<p>The gains in terms of compression obviously depend on your data and the redundancy of information present in your data batches. According to our observations, the choice of a good sort generally improves the compression ratio by a factor of 1.5 to 8.</p>

<p>Decomposing a complex schema into multiple simpler schemas to enhance sorting capabilities, coupled with a targeted approach to efficiently encode the identifiers representing the relationships, emerges as an effective strategy for enhancing overall data compression. This method also eliminates complex Arrow data types, such as lists, maps, and unions. Consequently, it not only improves but also simplifies data query-ability. This simplification proves beneficial for existing query engines, which may struggle to operate on intricate schemas.</p>

<h2 id="conclusion-and-next-steps">Conclusion and next steps</h2>

<p>This article concludes our two-part series on Apache Arrow, wherein we have explored various strategies to maximize the utility of Apache Arrow within specific contexts. The adaptive schema architecture presented in the second part of this series paves the way for future optimization possibilities. We look forward to seeing what the community can add based on this contribution.</p>

<p>Apache Arrow is an exceptional project, continually enhanced by a thriving ecosystem. However, throughout our exploration, we have noticed certain gaps or points of friction that, if addressed, could significantly enrich the overall experience.</p>
<ul>
  <li>Designing an efficient Arrow schema can, in some cases, prove to be a challenging task. Having the <strong>ability to collect statistics</strong> at the record level could facilitate this design phase (data distribution per field, dictionary stats, Arrow array sizes before/after compression, and so on). These statistics would also assist in identifying the most effective columns on which to base the record sorting.</li>
  <li><strong>Native support for recursive schemas</strong> would also increase adoption by simplifying the use of Arrow in complex scenarios. While I’m not aware of any attempts to address this limitation within the Arrow system, it doesn’t seem insurmountable and would constitute a valuable extension. This would help reduce the complexity of integrating Arrow with other systems that rely on such recursive definitions.</li>
  <li><strong>Harmonizing the support for data types as well as IPC stream capabilities</strong> would also be a major benefit. Predominant client libraries support nested and hierarchical schemas, but their use is limited due to a lack of full support across the rest of the ecosystem. For example, list and/or union types are not well supported by query engines or Parquet bridges. Also, the advanced dictionary support within IPC streams is not consistent across different implementations (i.e. delta dictionaries and replacement dictionaries are not supported by all implementations).</li>
  <li><strong>Optimizing the support of complex schemas</strong> in terms of memory consumption and compression rate could be improved by natively integrating the concept of the dynamic schema presented in this article.</li>
  <li><strong>Detecting dictionary overflows</strong> (index level) is not something that is easy to test on the fly. The API could be improved to indicate this overflow as soon as an insertion occurs.</li>
</ul>

<p>Our effort to utilize Apache Arrow in conjunction with OpenTelemetry has produced encouraging results. While this has necessitated considerable investment in terms of development, exploration, and benchmarking, we hope that these articles will aid in accelerating your journey with Apache Arrow. Looking ahead, we envision an end-to-end integration with Apache Arrow and plan to significantly extend our use of the Arrow ecosystem. This extension involves providing a bridge with Parquet and integrating with a query engine such as DataFusion, with the goal of processing telemetry streams within the collector.</p>]]></content><author><name>Laurent Quérel</name></author><category term="application" /><summary type="html"><![CDATA[In the previous article, we discussed our use of Apache Arrow within the context of the OpenTelemetry project. We investigated various techniques to maximize the efficiency of Apache Arrow, aiming to find the optimal balance between data compression ratio and queryability. The compression results speak for themselves, boasting improvements ranging from 1.5x to 5x better than the original OTLP protocol. In this article, we will delve into three techniques that have enabled us to enhance both the compression ratio and memory usage of Apache Arrow buffers within the current version of the OTel Arrow protocol. The first technique we’ll discuss aims to optimize schemas in terms of memory usage. As you’ll see, the gains can be substantial, potentially halving memory usage in certain cases. The second section will delve more deeply into the various approaches that can be used to handle recursive schema definitions. Lastly, we’ll emphasize that the design of your schema(s), coupled with the sorts you can apply at the record level, play a pivotal role in maximizing the benefits of Apache Arrow and its columnar representation. Handling dynamic and unknown data distributions In certain contexts, the comprehensive definition of an Arrow schema can end up being overly broad and complex in order to cover all possible cases that you intend to represent in columnar form. However, as is often the case with complex schemas, only a subset of this schema will actually be utilized for a specific deployment. Similarly, it’s not always possible to determine the optimal dictionary encoding for one or more fields in advance. Employing a broad and very general schema that covers all cases is usually more memory-intensive. This is because, for most implementations, a column without value still continues to consume memory space. Likewise, a column with dictionary encoding that indexes a uint64 will occupy four times more memory than the same column with a dictionary encoding based on a uint8. To illustrate this more concretely, let’s consider an OTel collector positioned at the output of a production environment, receiving a telemetry data stream produced by a large and dynamic set of servers. Invariably, the content of this telemetry stream will change in volume and nature over time. It’s challenging to predict the optimal schema in such a scenario, and it’s equally difficult to know in advance the distribution of a particular attribute of the telemetry data passing through this point. To optimize such scenarios, we have adopted an intermediary approach that we have named dynamic Arrow schema, aiming to gradually adapt the schema based on the observed data. The general principle is relatively simple. We start with a general schema defining the maximum envelope of what should be represented. Some fields of this schema will be declared optional, while other fields will be encoded with multiple possible options depending on the observed distribution. In theory, this principle can be applied to other types of transformations (e.g., recursive column creation) but we will let your imagination explore these other options. So if you encounter data streams where certain fields are not utilized, some union variants remain unused, and/or the value distribution of a field cannot be determined a priori, it may be worthwhile to invest time in implementing this model. This can lead to improved efficiency in terms of compression ratio, memory usage, and processing speed. The following Go Arrow schema definition provides an example of such a schema, instrumented with a collection of annotations. These annotations will be processed by an enhanced Record Builder, equipped with the ability to dynamically adapt the schema. The structure of this system is illustrated in Figure 1. var ( // Arrow schema for the OTLP Arrow Traces record (without attributes, links, and events). TracesSchema = arrow.NewSchema([]arrow.Field{ // Nullabe:true means the field is optional, in this case of 16 bit unsigned integers {Name: constants.ID, Type: arrow.PrimitiveTypes.Uint16, Nullable: true}, {Name: constants.Resource, Type: arrow.StructOf([]arrow.Field{ {Name: constants.ID, Type: arrow.PrimitiveTypes.Uint16, Nullable: true}, // --- Use dictionary with 8 bit integers initially ---- {Name: constants.SchemaUrl,Type: arrow.BinaryTypes.String,Metadata: schema.Metadata(schema.Dictionary8), Nullable: true}, {Name: constants.DroppedAttributesCount,Type: arrow.PrimitiveTypes.Uint32,Nullable: true}, }...), Nullable: true}, {Name: constants.Scope, Type: arrow.StructOf([]arrow.Field{ {Name: constants.ID, Type: arrow.PrimitiveTypes.Uint16, Metadata: acommon.Metadata(acommon.DeltaEncoding), Nullable: true}, // --- Use dictionary with 8 bit integers initially ---- {Name: constants.Name, Type: arrow.BinaryTypes.String, Metadata: acommon.Metadata(acommon.Dictionary8), Nullable: true}, {Name: constants.Version, Type: arrow.BinaryTypes.String, Metadata: acommon.Metadata(acommon.Dictionary8), Nullable: true}, {Name: constants.DroppedAttributesCount, Type: arrow.PrimitiveTypes.Uint32, Nullable: true}, }...), Nullable: true}, {Name: constants.SchemaUrl, Type: arrow.BinaryTypes.String, Metadata: schema.Metadata(schema.Dictionary8), Nullable: true}, {Name: constants.StartTimeUnixNano, Type: arrow.FixedWidthTypes.Timestamp_ns}, {Name: constants.DurationTimeUnixNano, Type: arrow.FixedWidthTypes.Duration_ms, Metadata: schema.Metadata(schema.Dictionary8)}, {Name: constants.TraceId, Type: &amp;arrow.FixedSizeBinaryType{ByteWidth: 16}}, {Name: constants.SpanId, Type: &amp;arrow.FixedSizeBinaryType{ByteWidth: 8}}, {Name: constants.TraceState, Type: arrow.BinaryTypes.String, Metadata: schema.Metadata(schema.Dictionary8), Nullable: true}, {Name: constants.ParentSpanId, Type: &amp;arrow.FixedSizeBinaryType{ByteWidth: 8}, Nullable: true}, {Name: constants.Name, Type: arrow.BinaryTypes.String, Metadata: schema.Metadata(schema.Dictionary8)}, {Name: constants.KIND, Type: arrow.PrimitiveTypes.Int32, Metadata: schema.Metadata(schema.Dictionary8), Nullable: true}, {Name: constants.DroppedAttributesCount, Type: arrow.PrimitiveTypes.Uint32, Nullable: true}, {Name: constants.DroppedEventsCount, Type: arrow.PrimitiveTypes.Uint32, Nullable: true}, {Name: constants.DroppedLinksCount, Type: arrow.PrimitiveTypes.Uint32, Nullable: true}, {Name: constants.Status, Type: arrow.StructOf([]arrow.Field{ {Name: constants.StatusCode, Type: arrow.PrimitiveTypes.Int32, Metadata: schema.Metadata(schema.Dictionary8), Nullable: true}, {Name: constants.StatusMessage, Type: arrow.BinaryTypes.String, Metadata: schema.Metadata(schema.Dictionary8), Nullable: true}, }...), Nullable: true}, }, nil) ) In this example, Arrow field-level metadata are employed to designate when a field is optional (Nullable: true) or to specify the minimal dictionary encoding applicable to a particular field (Metadata Dictionary8/16/…). Now let’s imagine a scenario utilizing this schema in a straightforward scenario, wherein only a handful of fields are actually in use, and the cardinality of most dictionary-encoded fields is low (i.e., below 2^8). Ideally, we’d want a system capable of dynamically constructing the following simplified schema, which, in essence, is a strict subset of the original schema. var ( // Simplified schema definition generated by the Arrow Record encoder based on // the data observed. TracesSchema = arrow.NewSchema([]arrow.Field{ {Name: constants.ID, Type: arrow.PrimitiveTypes.Uint16, Nullable: true}, {Name: constants.StartTimeUnixNano, Type: arrow.FixedWidthTypes.Timestamp_ns}, {Name: constants.TraceId, Type: &amp;arrow.FixedSizeBinaryType{ByteWidth: 16}}, {Name: constants.SpanId, Type: &amp;arrow.FixedSizeBinaryType{ByteWidth: 8}}, {Name: constants.Name, Type: &amp;arrow.DictionaryType { IndexType: arrow.PrimitiveTypes.Uint8, ValueType: arrow.BinaryTypes.String}}, {Name: constants.KIND, Type: &amp;arrow.DictionaryType { IndexType: arrow.PrimitiveTypes.Uint8, ValueType: arrow.PrimitiveTypes.Int32, }, Nullable: true}, }, nil) ) Additionally, we desire a system capable of automatically adapting the aforementioned schema if it encounters new fields or existing fields with a cardinality exceeding the size of the current dictionary definition in future batches. In extreme scenarios, if the cardinality of a specific field surpasses a certain threshold, we would prefer the system to automatically revert to the non-dictionary representation (mechanism of dictionary overflow). That is precisely what we will elaborate on in the remainder of this section. An overview of the different components and events used to implement this approach is depicted in figure 1. Fig 1: Adaptive Arrow schema architecture overview. The overall Adaptive Arrow schema component takes a data stream segmented into batches and produces one or multiple streams of Arrow Records (one schema per stream). Each of these records is defined with an Arrow schema, which is based both on the annotated Arrow schema and the shape of fields observed in the incoming data. More specifically, the process of the Adaptive Arrow schema component consists of four main phases Initialization phase During the initialization phase, the Arrow Record Encoder reads the annotated Arrow schema (i.e. the reference schema) and generates a collection of transformations. When these transformations are applied to the reference schema, they yield the first minimal Arrow schema that adheres to the constraints depicted by these annotations. In this initial iteration, all optional fields are eliminated, and all dictionary-encoded fields are configured to utilize the smallest encoding as defined by the annotation (only Dictionary8 in the previous example). These transformations form a tree, reflecting the structure of the reference schema. Feeding phase Following the initialization is the feeding phase. Here, the Arrow Record Encoder scans the batch and attempts to store all the fields in an Arrow Record Builder, which is defined by the schema created in the prior step. If a field exists in the data but is not included in the schema, the encoder will trigger a missing field event. This process continues until the current batch is completely processed. An additional internal check is conducted on all dictionary-encoded fields in the Arrow Record builder to ensure there’s no dictionary overflow (i.e. more unique entries than the cardinality of the index permits). Dictionary overflow events are generated if such a situation is detected. Consequently, by the end, all unknown fields and dictionary overflow would have been detected, or alternatively, no discrepancies would have surfaced if the data aligns perfectly with the schema. Corrective phase If at least one event has been generated, a corrective phase will be initiated to fix the schema. This optional stage considers all the events generated in the previous stage and adjusts the transformation tree accordingly to align with the observed data. A missing field event will remove a NoField transformation for the corresponding field. A dictionary overflow event will modify the dictionary transformation to mirror the event (e.g. changing the index type from uint8 to uint16, or if the maximum index size has been reached, the transformation will remove the dictionary-encoding and revert to the original non-dictionary-encoded type). The updated transformation tree is subsequently used to create a new schema and a fresh Arrow Record Builder. This Record Builder is then utilized to replay the preceding feeding phase with the batch that wasn’t processed correctly. Routing phase Once a Record Builder has been properly fed, an Arrow Record is created, and the system transitions into the routing phase. The router component calculates a schema signature of the record and utilizes this signature to route the record to an existing Arrow stream compatible with the signature, or it initiates a new stream if there is no match. This four-phase process should gradually adapt and stabilize the schema to a structure and definition that is optimized for a specific data stream. Unused fields will never unnecessarily consume memory. Dictionary-encoded fields will be defined with the most optimal index size based on the observed data cardinality, and fields with a cardinality exceeding a certain threshold (defined by configuration) will automatically revert to their non-dictionary-encoded versions. To effectively execute this approach, you must ensure that there is a sufficient level of flexibility on the receiver side. It’s crucial that your downstream pipeline remains functional even when some fields are missing in the schema or when various dictionary index configurations are employed. While this may not always be feasible without implementing additional transformations upon reception, it proves worthwhile in certain scenarios. The following results highlight the significant memory usage reduction achieved through the application of various optimization techniques. These results were gathered using a schema akin to the one previously presented. The considerable memory efficiency underscores the effectiveness of this approach. Fig 2: Comparative analysis of memory usage for different schema optimizations. The concept of a transformation tree enables a generalized approach to perform various types of schema optimizations based on the knowledge acquired from the data. This architecture is highly flexible; the current implementation allows for the removal of unused fields, the application of the most specific dictionary encoding, and the optimization of union type variants. In the future, there is potential for introducing additional optimizations that can be expressed as transformations on the initial schema. An implementation of this approach is available here. Handling recursive schema definition Apache Arrow does not support recursive schema definitions, implying that data structures with variable depth cannot be directly represented. Figure 3 exemplifies such a recursive definition where the value of an attribute can either be a simple data type, a list of values, or a map of values. The depth of this definition cannot be predetermined. Fig 3: Recursive definition of OTel attributes. Several strategies can be employed to circumvent this limitation. Technically, the dynamic schema concept we’ve presented could be expanded to dynamically update the schema to include any missing level of recursion. However, for this use case, this method is complex and has the notable downside of not offering any assurance on the maximum size of the schema. This lack of constraint can pose security issues; hence, this approach isn’t elaborated upon. The second approach consists of breaking the recursion by employing a serialization format that supports the definition of a recursive schema. The result of this serialization can then be integrated into the Arrow record as a binary type column, effectively halting the recursion at a specific level. To fully leverage the advantages of columnar representation, it is crucial to apply this ad-hoc serialization as deeply within the data structure as feasible. In the context of OpenTelemetry, this is performed at the attribute level – more specifically, at the second level of attributes. A variety of serialization formats, such as protobuf or CBOR, can be employed to encode recursive data. Without particular treatment, these binary columns may not be easily queryable by the existing Arrow query engines. Therefore, it’s crucial to thoughtfully ascertain when and where to apply such a technique. While I’m not aware of any attempts to address this limitation within the Arrow system, it doesn’t seem insurmountable and would constitute a valuable extension. This would help reduce the complexity of integrating Arrow with other systems that rely on such recursive definitions. Importance of sorting In our preceding article, we explored a variety of strategies to represent hierarchical data models, including nested structures based on struct/list/map/union, denormalization and flattening representations, as well as a multi-record approach. Each method presents its unique advantages and disadvantages. However, in this last section, we’ll delve deeper into the multi-record approach, focusing specifically on its ability to offer versatile sorting options and how these options contribute to an enhanced compression ratio. In the OTel Arrow protocol, we leverage the multi-record approach to represent metrics, logs, and traces. The following entity-relationship diagram offers a simplified version of various record schemas and illustrates their relationships, specifically those used to represent gauges and sums. A comprehensive description of the Arrow data model employed in OpenTelemetry can be accessed here. These Arrow records, also referred to as tables, form a hierarchy with METRICS acting as the primary entry point. Each table can be independently sorted according to one or more columns. This sorting strategy facilitates the grouping of duplicated data, thereby improving the compression ratio. Fig 4: A simplified entity-relationship diagram representing OTel sum &amp; gauge metrics. The relationship between the primary METRICS table and the secondary RESOURCE_ATTRS, SCOPE_ATTRS, and NUMBER_DATA_POINTS tables is established through a unique id in the main table and a parent_id column in each of the secondary tables. This {id,parent_id} pair represents an overhead that should be minimized to the greatest extent possible post-compression. To achieve this, the ordering process for the different tables adheres to the hierarchy, starting from the main table down to the leaf. The main table is sorted (by one or multiple columns), and then an incremental id is assigned to each row. This numerical id is stored using delta-encoding, which is implemented on top of Arrow. The secondary tables directly connected to the main table are sorted using the same principle, but the parent_id column is consistently utilized as the last column in the sort statement. Including the parent_id column in the sort statement enables the use of a variation of delta encoding. The efficiency of this approach is summarized in the chart below. Fig 5: Comparative analysis of compression ratios - OTLP Protocol vs. Two variations of the OTel Arrow Protocol with multivariate metrics stream. (lower percentage is better) The second column presents the average size of the OTLP batch both pre- and post-ZSTD compression for batches of varying sizes. This column serves as a reference point for the ensuing two columns. The third column displays results for the OTel Arrow protocol without any sorting applied, while the final column showcases results for the OTel Arrow protocol with sorting enabled. Before compression, the average batch sizes for the two OTel Arrow configurations are predictably similar. However, post-compression, the benefits of sorting each individual table on the compression ratio become immediately apparent. Without sorting, the OTel Arrow protocol exhibits a compression ratio that’s 1.40 to 1.67 times better than the reference. When sorting is enabled, the OTel Arrow protocol outperforms the reference by a factor ranging from 4.94 to 7.21 times! The gains in terms of compression obviously depend on your data and the redundancy of information present in your data batches. According to our observations, the choice of a good sort generally improves the compression ratio by a factor of 1.5 to 8. Decomposing a complex schema into multiple simpler schemas to enhance sorting capabilities, coupled with a targeted approach to efficiently encode the identifiers representing the relationships, emerges as an effective strategy for enhancing overall data compression. This method also eliminates complex Arrow data types, such as lists, maps, and unions. Consequently, it not only improves but also simplifies data query-ability. This simplification proves beneficial for existing query engines, which may struggle to operate on intricate schemas. Conclusion and next steps This article concludes our two-part series on Apache Arrow, wherein we have explored various strategies to maximize the utility of Apache Arrow within specific contexts. The adaptive schema architecture presented in the second part of this series paves the way for future optimization possibilities. We look forward to seeing what the community can add based on this contribution. Apache Arrow is an exceptional project, continually enhanced by a thriving ecosystem. However, throughout our exploration, we have noticed certain gaps or points of friction that, if addressed, could significantly enrich the overall experience. Designing an efficient Arrow schema can, in some cases, prove to be a challenging task. Having the ability to collect statistics at the record level could facilitate this design phase (data distribution per field, dictionary stats, Arrow array sizes before/after compression, and so on). These statistics would also assist in identifying the most effective columns on which to base the record sorting. Native support for recursive schemas would also increase adoption by simplifying the use of Arrow in complex scenarios. While I’m not aware of any attempts to address this limitation within the Arrow system, it doesn’t seem insurmountable and would constitute a valuable extension. This would help reduce the complexity of integrating Arrow with other systems that rely on such recursive definitions. Harmonizing the support for data types as well as IPC stream capabilities would also be a major benefit. Predominant client libraries support nested and hierarchical schemas, but their use is limited due to a lack of full support across the rest of the ecosystem. For example, list and/or union types are not well supported by query engines or Parquet bridges. Also, the advanced dictionary support within IPC streams is not consistent across different implementations (i.e. delta dictionaries and replacement dictionaries are not supported by all implementations). Optimizing the support of complex schemas in terms of memory consumption and compression rate could be improved by natively integrating the concept of the dynamic schema presented in this article. Detecting dictionary overflows (index level) is not something that is easy to test on the fly. The API could be improved to indicate this overflow as soon as an insertion occurs. Our effort to utilize Apache Arrow in conjunction with OpenTelemetry has produced encouraging results. While this has necessitated considerable investment in terms of development, exploration, and benchmarking, we hope that these articles will aid in accelerating your journey with Apache Arrow. Looking ahead, we envision an end-to-end integration with Apache Arrow and plan to significantly extend our use of the Arrow ecosystem. This extension involves providing a bridge with Parquet and integrating with a query engine such as DataFusion, with the goal of processing telemetry streams within the collector.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow DataFusion 26.0.0</title><link href="https://arrow.apache.org/blog/2023/06/24/datafusion-25.0.0/" rel="alternate" type="text/html" title="Apache Arrow DataFusion 26.0.0" /><published>2023-06-24T00:00:00-04:00</published><updated>2023-06-24T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/06/24/datafusion-25.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/06/24/datafusion-25.0.0/"><![CDATA[<!--

-->

<p>It has been a whirlwind 6 months of DataFusion development since <a href="https://arrow.apache.org/blog/2023/01/19/datafusion-16.0.0">our
last update</a>: the community has grown, many features have been added,
performance improved and we are <a href="https://github.com/apache/arrow-datafusion/discussions/6475">discussing</a> branching out to our own
top level Apache Project.</p>

<h2 id="background">Background</h2>

<p><a href="https://arrow.apache.org/datafusion/">Apache Arrow DataFusion</a> is an extensible query engine and database
toolkit, written in <a href="https://www.rust-lang.org/">Rust</a>, that uses <a href="https://arrow.apache.org">Apache Arrow</a> as its in-memory
format.</p>

<p>DataFusion, along with <a href="https://calcite.apache.org">Apache Calcite</a>, Facebook’s <a href="https://github.com/facebookincubator/velox">Velox</a> and
similar technology are part of the next generation “<a href="https://www.usenix.org/publications/login/winter2018/khurana">Deconstructed
Database</a>” architectures, where new systems are built on a foundation
of fast, modular components, rather as a single tightly integrated
system.</p>

<p>While single tightly integrated systems such as <a href="https://spark.apache.org/">Spark</a>, <a href="https://duckdb.org">DuckDB</a> and
<a href="https://www.pola.rs/">Pola.rs</a> are great pieces of technology, our community believes that
anyone developing new data heavy application, such as those common in
machine learning in the next 5 years, will <strong>require</strong> a high
performance, vectorized, query engine to remain relevant. The only
practical way to gain access to such technology without investing many
millions of dollars to build a new tightly integrated engine, is
though open source projects like DataFusion and similar enabling
technologies such as <a href="https://arrow.apache.org">Apache Arrow</a> and <a href="https://www.rust-lang.org/">Rust</a>.</p>

<p>DataFusion is targeted primarily at developers creating other data
intensive analytics, and offers:</p>

<ul>
  <li>High performance, native, parallel streaming execution engine</li>
  <li>Mature <a href="https://arrow.apache.org/datafusion/user-guide/sql/index.html">SQL support</a>, featuring  subqueries, window functions, grouping sets, and more</li>
  <li>Built in support for Parquet, Avro, CSV, JSON and Arrow formats and easy extension for others</li>
  <li>Native DataFrame API and <a href="https://arrow.apache.org/datafusion-python/">python bindings</a></li>
  <li><a href="https://docs.rs/datafusion/latest/datafusion/index.html">Well documented</a> source code and architecture, designed to be customized to suit downstream project needs</li>
  <li>High quality, easy to use code <a href="https://crates.io/crates/datafusion/versions">released every 2 weeks to crates.io</a></li>
  <li>Welcoming, open community, governed by the highly regarded and well understood <a href="https://www.apache.org/">Apache Software Foundation</a></li>
</ul>

<p>The rest of this post highlights some of the improvements we have made
to DataFusion over the last 6 months and a preview of where we are
heading. You can see a list of all changes in the detailed
<a href="https://github.com/apache/arrow-datafusion/blob/main/datafusion/CHANGELOG.md">CHANGELOG</a>.</p>

<h2 id="even-better-performance">(Even) Better Performance</h2>

<p><a href="https://voltrondata.com/resources/speeds-and-feeds-hardware-and-software-matter">Various</a> benchmarks show DataFusion to be quite close or <a href="https://github.com/tustvold/access-log-bench">even
faster</a> to the state of the art in analytic performance (at the moment
this seems to be DuckDB). We continually work on improving performance
(see <a href="https://github.com/apache/arrow-datafusion/issues/5546">#5546</a> for a list) and would love additional help in this area.</p>

<p>DataFusion now reads single large Parquet files significantly faster by
<a href="https://github.com/apache/arrow-datafusion/pull/5057">parallelizing across multiple cores</a>. Native speeds for reading JSON
and CSV files are also up to 2.5x faster thanks to improvements
upstream in arrow-rs <a href="https://github.com/apache/arrow-rs/pull/3479#issuecomment-1384353159">JSON reader</a> and <a href="https://github.com/apache/arrow-rs/pull/3365">CSV reader</a>.</p>

<p>Also, we have integrated the <a href="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1/">arrow-rs Row Format</a> into DataFusion resulting in up to <a href="https://github.com/apache/arrow-datafusion/pull/6163">2-3x faster sorting and merging</a>.</p>

<h2 id="improved-documentation-and-website">Improved Documentation and Website</h2>

<p>Part of growing the DataFusion community is ensuring that DataFusion’s
features are understood and that it is easy to contribute and
participate. To that end the <a href="https://arrow.apache.org/datafusion/">website</a> has been cleaned up, <a href="https://docs.rs/datafusion/latest/datafusion/index.html#architecture">the
architecture guide</a> expanded, the <a href="https://arrow.apache.org/datafusion/contributor-guide/roadmap.html">roadmap</a> updated, and several
overview talks created:</p>

<ul>
  <li>Apr 2023 <em>Query Engine</em>: <a href="https://youtu.be/NVKujPxwSBA">recording</a> and <a href="https://docs.google.com/presentation/d/1D3GDVas-8y0sA4c8EOgdCvEjVND4s2E7I6zfs67Y4j8/edit#slide=id.p">slides</a></li>
  <li>April 2023 <em>Logical Plan and Expressions</em>: <a href="https://youtu.be/EzZTLiSJnhY">recording</a> and <a href="https://docs.google.com/presentation/d/1ypylM3-w60kVDW7Q6S99AHzvlBgciTdjsAfqNP85K30">slides</a></li>
  <li>April 2023 <em>Physical Plan and Execution</em>: <a href="https://youtu.be/2jkWU3_w6z0">recording</a> and <a href="https://docs.google.com/presentation/d/1cA2WQJ2qg6tx6y4Wf8FH2WVSm9JQ5UgmBWATHdik0hg">slides</a></li>
</ul>

<h2 id="new-features">New Features</h2>

<h3 id="more-streaming-less-memory">More Streaming, Less Memory</h3>

<p>We have made significant progress on the <a href="https://github.com/apache/arrow-datafusion/issues/4285">streaming execution roadmap</a>
such as <a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/trait.ExecutionPlan.html#method.unbounded_output">unbounded datasources</a>, <a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/aggregates/enum.GroupByOrderMode.html">streaming group by</a>, sophisticated
<a href="https://docs.rs/datafusion/latest/datafusion/physical_optimizer/global_sort_selection/index.html">sort</a> and <a href="https://docs.rs/datafusion/latest/datafusion/physical_optimizer/repartition/index.html">repartitioning</a> improvements in the optimizer, and support
for <a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/joins/struct.SymmetricHashJoinExec.html">symmetric hash join</a> (read more about that in the great <a href="https://www.synnada.ai/blog/general-purpose-stream-joins-via-pruning-symmetric-hash-joins">Synnada
Blog Post</a> on the topic). Together, these features both 1) make it
easier to build streaming systems using DataFusion that can
incrementally generate output before (or ever) seeing the end of the
input and 2) allow general queries to use less memory and generate their
results faster.</p>

<p>We have also improved the runtime <a href="https://docs.rs/datafusion/latest/datafusion/execution/memory_pool/index.html">memory management</a> system so that
DataFusion now stays within its declared memory budget <a href="https://github.com/apache/arrow-datafusion/issues/3941">generate
runtime errors</a>.</p>

<h3 id="dml-support-insert-delete-update-etc">DML Support (<code class="language-plaintext highlighter-rouge">INSERT</code>, <code class="language-plaintext highlighter-rouge">DELETE</code>, <code class="language-plaintext highlighter-rouge">UPDATE</code>, etc)</h3>

<p>Part of building high performance data systems includes writing data,
and DataFusion supports several features for creating new files:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">INSERT INTO</code> and <code class="language-plaintext highlighter-rouge">SELECT ... INTO </code> support for memory backed and CSV tables</li>
  <li>New <a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/insert/trait.DataSink.html">API for writing data into TableProviders</a></li>
</ul>

<p>We are working on easier to use <a href="https://github.com/apache/arrow-datafusion/issues/5654">COPY INTO</a> syntax, better support
for writing parquet, JSON, and AVRO, and more – see our <a href="https://github.com/apache/arrow-datafusion/issues/6569">tracking epic</a>
for more details.</p>

<h3 id="timestamp-and-intervals">Timestamp and Intervals</h3>

<p>One mark of the maturity of a SQL engine is how it handles the tricky
world of timestamp, date, times and interval arithmetic. DataFusion is
feature complete in this area and behaves as you would expect,
supporting queries such as</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="n">now</span><span class="p">()</span> <span class="o">+</span> <span class="s1">'1 month'</span> <span class="k">FROM</span> <span class="n">my_table</span><span class="p">;</span>
</code></pre></div></div>

<p>We still have a long tail of <a href="https://github.com/apache/arrow-datafusion/issues/3148">date and time improvements</a>, which we are working on as well.</p>

<h3 id="querying-structured-types-list-and-structs">Querying Structured Types (<code class="language-plaintext highlighter-rouge">List</code> and <code class="language-plaintext highlighter-rouge">Struct</code>s)</h3>

<p>Arrow and Parquet <a href="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/">support nested data</a> well and DataFusion lets you
easily query such <code class="language-plaintext highlighter-rouge">Struct</code> and <code class="language-plaintext highlighter-rouge">List</code>. For example, you can use
DataFusion to read and query the <a href="https://data.mendeley.com/datasets/ct8f9skv97">JSON Datasets for Exploratory OLAP -
Mendeley Data</a> like this:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">----------</span>
<span class="c1">-- Explore structured data using SQL</span>
<span class="c1">----------</span>
<span class="k">SELECT</span> <span class="k">delete</span> <span class="k">FROM</span> <span class="s1">'twitter-sample-head-100000.parquet'</span> <span class="k">WHERE</span> <span class="k">delete</span> <span class="k">IS</span> <span class="k">NOT</span> <span class="k">NULL</span> <span class="k">limit</span> <span class="mi">10</span><span class="p">;</span>
<span class="o">+</span><span class="c1">---------------------------------------------------------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="k">delete</span>                                                                                                                    <span class="o">|</span>
<span class="o">+</span><span class="c1">---------------------------------------------------------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">135037425050320896</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">135037425050320896</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">334902461</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">334902461</span><span class="p">}}</span> <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">134703982051463168</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">134703982051463168</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">405383453</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">405383453</span><span class="p">}}</span> <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">134773741740765184</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">134773741740765184</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">64823441</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">64823441</span><span class="p">}}</span>   <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">132543659655704576</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">132543659655704576</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">45917834</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">45917834</span><span class="p">}}</span>   <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">133786431926697984</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">133786431926697984</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">67229952</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">67229952</span><span class="p">}}</span>   <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">134619093570560002</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">134619093570560002</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">182430773</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">182430773</span><span class="p">}}</span> <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">134019857527214080</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">134019857527214080</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">257396311</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">257396311</span><span class="p">}}</span> <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">133931546469076993</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">133931546469076993</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">124539548</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">124539548</span><span class="p">}}</span> <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">134397743350296576</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">134397743350296576</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">139836391</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">139836391</span><span class="p">}}</span> <span class="o">|</span>
<span class="o">|</span> <span class="p">{</span><span class="n">status</span><span class="p">:</span> <span class="p">{</span><span class="n">id</span><span class="p">:</span> <span class="p">{</span><span class="err">$</span><span class="n">numberLong</span><span class="p">:</span> <span class="mi">127833661767823360</span><span class="p">},</span> <span class="n">id_str</span><span class="p">:</span> <span class="mi">127833661767823360</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="mi">244442687</span><span class="p">,</span> <span class="n">user_id_str</span><span class="p">:</span> <span class="mi">244442687</span><span class="p">}}</span> <span class="o">|</span>
<span class="o">+</span><span class="c1">---------------------------------------------------------------------------------------------------------------------------+</span>

<span class="c1">----------</span>
<span class="c1">-- Select some deeply nested fields</span>
<span class="c1">----------</span>
<span class="k">SELECT</span>
  <span class="k">delete</span><span class="p">[</span><span class="s1">'status'</span><span class="p">][</span><span class="s1">'id'</span><span class="p">][</span><span class="s1">'$numberLong'</span><span class="p">]</span> <span class="k">as</span> <span class="n">delete_id</span><span class="p">,</span>
  <span class="k">delete</span><span class="p">[</span><span class="s1">'status'</span><span class="p">][</span><span class="s1">'user_id'</span><span class="p">]</span> <span class="k">as</span> <span class="n">delete_user_id</span>
<span class="k">FROM</span> <span class="s1">'twitter-sample-head-100000.parquet'</span> <span class="k">WHERE</span> <span class="k">delete</span> <span class="k">IS</span> <span class="k">NOT</span> <span class="k">NULL</span> <span class="k">LIMIT</span> <span class="mi">10</span><span class="p">;</span>

<span class="o">+</span><span class="c1">--------------------+----------------+</span>
<span class="o">|</span> <span class="n">delete_id</span>          <span class="o">|</span> <span class="n">delete_user_id</span> <span class="o">|</span>
<span class="o">+</span><span class="c1">--------------------+----------------+</span>
<span class="o">|</span> <span class="mi">135037425050320896</span> <span class="o">|</span> <span class="mi">334902461</span>      <span class="o">|</span>
<span class="o">|</span> <span class="mi">134703982051463168</span> <span class="o">|</span> <span class="mi">405383453</span>      <span class="o">|</span>
<span class="o">|</span> <span class="mi">134773741740765184</span> <span class="o">|</span> <span class="mi">64823441</span>       <span class="o">|</span>
<span class="o">|</span> <span class="mi">132543659655704576</span> <span class="o">|</span> <span class="mi">45917834</span>       <span class="o">|</span>
<span class="o">|</span> <span class="mi">133786431926697984</span> <span class="o">|</span> <span class="mi">67229952</span>       <span class="o">|</span>
<span class="o">|</span> <span class="mi">134619093570560002</span> <span class="o">|</span> <span class="mi">182430773</span>      <span class="o">|</span>
<span class="o">|</span> <span class="mi">134019857527214080</span> <span class="o">|</span> <span class="mi">257396311</span>      <span class="o">|</span>
<span class="o">|</span> <span class="mi">133931546469076993</span> <span class="o">|</span> <span class="mi">124539548</span>      <span class="o">|</span>
<span class="o">|</span> <span class="mi">134397743350296576</span> <span class="o">|</span> <span class="mi">139836391</span>      <span class="o">|</span>
<span class="o">|</span> <span class="mi">127833661767823360</span> <span class="o">|</span> <span class="mi">244442687</span>      <span class="o">|</span>
<span class="o">+</span><span class="c1">--------------------+----------------+</span>
</code></pre></div></div>

<h3 id="subqueries-all-the-way-down">Subqueries All the Way Down</h3>

<p>DataFusion can run many different subqueries by rewriting them to
joins. It has been able to run the full suite of TPC-H queries for at
least the last year, but recently we have implemented significant
improvements to this logic, sufficient to run almost all queries in
the TPC-DS benchmark as well.</p>

<h2 id="community-and-project-growth">Community and Project Growth</h2>

<p>The six months since <a href="https://arrow.apache.org/blog/2023/01/19/datafusion-16.0.0">our last update</a> saw significant growth in
the DataFusion community. Between versions <code class="language-plaintext highlighter-rouge">17.0.0</code> and <code class="language-plaintext highlighter-rouge">26.0.0</code>,
DataFusion merged 711 PRs from 107 distinct contributors, not
including all the work that goes into our core dependencies such as
<a href="https://crates.io/crates/arrow">arrow</a>,
<a href="https://crates.io/crates/parquet">parquet</a>, and
<a href="https://crates.io/crates/object_store">object_store</a>, that much of
the same community helps support.</p>

<p>In addition, we have added 7 new committers and 1 new PMC member to
the Apache Arrow project, largely focused on DataFusion, and we
learned about some of the cool <a href="https://arrow.apache.org/datafusion/user-guide/introduction.html#known-users">new systems</a> which are using
DataFusion. Given the growth of the community and interest in the
project, we also clarified the <a href="https://github.com/apache/arrow-datafusion/discussions/6441">mission statement</a> and are
<a href="https://github.com/apache/arrow-datafusion/discussions/6475">discussing</a> “graduate”ing DataFusion to a new top level
Apache Software Foundation project.</p>

<!--
$ git log --pretty=oneline 17.0.0..26.0.0 . | wc -l
     711

$ git shortlog -sn 17.0.0..26.0.0 . | wc -l
      107
-->

<h1 id="how-to-get-involved">How to Get Involved</h1>

<p>Kudos to everyone in the community who has contributed ideas,
discussions, bug reports, documentation and code. It is exciting to be
innovating on the next generation of database architectures together!</p>

<p>If you are interested in contributing to DataFusion, we would love to
have you join us. You can try out DataFusion on some of your own
data and projects and let us know how it goes or contribute a PR with
documentation, tests or code. A list of open issues suitable for
beginners is <a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a>.</p>

<p>Check out our <a href="https://arrow.apache.org/datafusion/contributor-guide/communication.html">Communication Doc</a> for more ways to engage with the
community.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[It has been a whirlwind 6 months of DataFusion development since our last update: the community has grown, many features have been added, performance improved and we are discussing branching out to our own top level Apache Project. Background Apache Arrow DataFusion is an extensible query engine and database toolkit, written in Rust, that uses Apache Arrow as its in-memory format. DataFusion, along with Apache Calcite, Facebook’s Velox and similar technology are part of the next generation “Deconstructed Database” architectures, where new systems are built on a foundation of fast, modular components, rather as a single tightly integrated system. While single tightly integrated systems such as Spark, DuckDB and Pola.rs are great pieces of technology, our community believes that anyone developing new data heavy application, such as those common in machine learning in the next 5 years, will require a high performance, vectorized, query engine to remain relevant. The only practical way to gain access to such technology without investing many millions of dollars to build a new tightly integrated engine, is though open source projects like DataFusion and similar enabling technologies such as Apache Arrow and Rust. DataFusion is targeted primarily at developers creating other data intensive analytics, and offers: High performance, native, parallel streaming execution engine Mature SQL support, featuring subqueries, window functions, grouping sets, and more Built in support for Parquet, Avro, CSV, JSON and Arrow formats and easy extension for others Native DataFrame API and python bindings Well documented source code and architecture, designed to be customized to suit downstream project needs High quality, easy to use code released every 2 weeks to crates.io Welcoming, open community, governed by the highly regarded and well understood Apache Software Foundation The rest of this post highlights some of the improvements we have made to DataFusion over the last 6 months and a preview of where we are heading. You can see a list of all changes in the detailed CHANGELOG. (Even) Better Performance Various benchmarks show DataFusion to be quite close or even faster to the state of the art in analytic performance (at the moment this seems to be DuckDB). We continually work on improving performance (see #5546 for a list) and would love additional help in this area. DataFusion now reads single large Parquet files significantly faster by parallelizing across multiple cores. Native speeds for reading JSON and CSV files are also up to 2.5x faster thanks to improvements upstream in arrow-rs JSON reader and CSV reader. Also, we have integrated the arrow-rs Row Format into DataFusion resulting in up to 2-3x faster sorting and merging. Improved Documentation and Website Part of growing the DataFusion community is ensuring that DataFusion’s features are understood and that it is easy to contribute and participate. To that end the website has been cleaned up, the architecture guide expanded, the roadmap updated, and several overview talks created: Apr 2023 Query Engine: recording and slides April 2023 Logical Plan and Expressions: recording and slides April 2023 Physical Plan and Execution: recording and slides New Features More Streaming, Less Memory We have made significant progress on the streaming execution roadmap such as unbounded datasources, streaming group by, sophisticated sort and repartitioning improvements in the optimizer, and support for symmetric hash join (read more about that in the great Synnada Blog Post on the topic). Together, these features both 1) make it easier to build streaming systems using DataFusion that can incrementally generate output before (or ever) seeing the end of the input and 2) allow general queries to use less memory and generate their results faster. We have also improved the runtime memory management system so that DataFusion now stays within its declared memory budget generate runtime errors. DML Support (INSERT, DELETE, UPDATE, etc) Part of building high performance data systems includes writing data, and DataFusion supports several features for creating new files: INSERT INTO and SELECT ... INTO support for memory backed and CSV tables New API for writing data into TableProviders We are working on easier to use COPY INTO syntax, better support for writing parquet, JSON, and AVRO, and more – see our tracking epic for more details. Timestamp and Intervals One mark of the maturity of a SQL engine is how it handles the tricky world of timestamp, date, times and interval arithmetic. DataFusion is feature complete in this area and behaves as you would expect, supporting queries such as SELECT now() + '1 month' FROM my_table; We still have a long tail of date and time improvements, which we are working on as well. Querying Structured Types (List and Structs) Arrow and Parquet support nested data well and DataFusion lets you easily query such Struct and List. For example, you can use DataFusion to read and query the JSON Datasets for Exploratory OLAP - Mendeley Data like this: ---------- -- Explore structured data using SQL ---------- SELECT delete FROM 'twitter-sample-head-100000.parquet' WHERE delete IS NOT NULL limit 10; +---------------------------------------------------------------------------------------------------------------------------+ | delete | +---------------------------------------------------------------------------------------------------------------------------+ | {status: {id: {$numberLong: 135037425050320896}, id_str: 135037425050320896, user_id: 334902461, user_id_str: 334902461}} | | {status: {id: {$numberLong: 134703982051463168}, id_str: 134703982051463168, user_id: 405383453, user_id_str: 405383453}} | | {status: {id: {$numberLong: 134773741740765184}, id_str: 134773741740765184, user_id: 64823441, user_id_str: 64823441}} | | {status: {id: {$numberLong: 132543659655704576}, id_str: 132543659655704576, user_id: 45917834, user_id_str: 45917834}} | | {status: {id: {$numberLong: 133786431926697984}, id_str: 133786431926697984, user_id: 67229952, user_id_str: 67229952}} | | {status: {id: {$numberLong: 134619093570560002}, id_str: 134619093570560002, user_id: 182430773, user_id_str: 182430773}} | | {status: {id: {$numberLong: 134019857527214080}, id_str: 134019857527214080, user_id: 257396311, user_id_str: 257396311}} | | {status: {id: {$numberLong: 133931546469076993}, id_str: 133931546469076993, user_id: 124539548, user_id_str: 124539548}} | | {status: {id: {$numberLong: 134397743350296576}, id_str: 134397743350296576, user_id: 139836391, user_id_str: 139836391}} | | {status: {id: {$numberLong: 127833661767823360}, id_str: 127833661767823360, user_id: 244442687, user_id_str: 244442687}} | +---------------------------------------------------------------------------------------------------------------------------+ ---------- -- Select some deeply nested fields ---------- SELECT delete['status']['id']['$numberLong'] as delete_id, delete['status']['user_id'] as delete_user_id FROM 'twitter-sample-head-100000.parquet' WHERE delete IS NOT NULL LIMIT 10; +--------------------+----------------+ | delete_id | delete_user_id | +--------------------+----------------+ | 135037425050320896 | 334902461 | | 134703982051463168 | 405383453 | | 134773741740765184 | 64823441 | | 132543659655704576 | 45917834 | | 133786431926697984 | 67229952 | | 134619093570560002 | 182430773 | | 134019857527214080 | 257396311 | | 133931546469076993 | 124539548 | | 134397743350296576 | 139836391 | | 127833661767823360 | 244442687 | +--------------------+----------------+ Subqueries All the Way Down DataFusion can run many different subqueries by rewriting them to joins. It has been able to run the full suite of TPC-H queries for at least the last year, but recently we have implemented significant improvements to this logic, sufficient to run almost all queries in the TPC-DS benchmark as well. Community and Project Growth The six months since our last update saw significant growth in the DataFusion community. Between versions 17.0.0 and 26.0.0, DataFusion merged 711 PRs from 107 distinct contributors, not including all the work that goes into our core dependencies such as arrow, parquet, and object_store, that much of the same community helps support. In addition, we have added 7 new committers and 1 new PMC member to the Apache Arrow project, largely focused on DataFusion, and we learned about some of the cool new systems which are using DataFusion. Given the growth of the community and interest in the project, we also clarified the mission statement and are discussing “graduate”ing DataFusion to a new top level Apache Software Foundation project. How to Get Involved Kudos to everyone in the community who has contributed ideas, discussions, bug reports, documentation and code. It is exciting to be innovating on the next generation of database architectures together! If you are interested in contributing to DataFusion, we would love to have you join us. You can try out DataFusion on some of your own data and projects and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable for beginners is here. Check out our Communication Doc for more ways to engage with the community.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow nanoarrow 0.2 Release</title><link href="https://arrow.apache.org/blog/2023/06/22/nanoarrow-0.120-release/" rel="alternate" type="text/html" title="Apache Arrow nanoarrow 0.2 Release" /><published>2023-06-22T00:00:00-04:00</published><updated>2023-06-22T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/06/22/nanoarrow-0.120-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/06/22/nanoarrow-0.120-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.2.0 release of
Apache Arrow nanoarrow. This initial release covers 19 resolved issues from
6 contributors.</p>

<h2 id="release-highlights">Release Highlights</h2>

<ul>
  <li>Addition of the Arrow <a href="#ipc-stream-support">IPC stream reader extension</a></li>
  <li>Addition of the <a href="#getting-started-with-nanoarrow">Getting Started with nanoarrow</a>
tutorial</li>
  <li>Improvements in reliability and platform test coverage of the <a href="#c-library">C library</a></li>
  <li>Improvements in reliability and type support in the <a href="#r-bindings">R bindings</a></li>
</ul>

<p>See the
<a href="https://github.com/apache/arrow-nanoarrow/blob/apache-arrow-nanoarrow-0.2.0/CHANGELOG.md">Changelog</a>
for a detailed list of contributions to this release.</p>

<h2 id="ipc-stream-support">IPC stream support</h2>

<p>This release includes support for reading schemas and record batches serialized
using the
<a href="https://arrow.apache.org/docs/format/Columnar.html#serialization-and-interprocess-communication-ipc">Arrow IPC format</a>. Based on the
<a href="https://github.com/dvidelabs/flatcc">flatcc</a>
flatbuffers implementation, the nanoarrow IPC read support is implemented as
an optional extension to the core nanoarrow library. The easiest way to get
started is with the <code class="language-plaintext highlighter-rouge">ArrowArrayStream</code> provider using one of the built-in
<code class="language-plaintext highlighter-rouge">ArrowIpcInputStream</code> implementations:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;stdbool.h&gt;</span><span class="cp">
</span>
<span class="cp">#include</span> <span class="cpf">"nanoarrow_ipc.h"</span><span class="cp">
</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">*</span> <span class="n">argv</span><span class="p">[])</span> <span class="p">{</span>
  <span class="kt">FILE</span><span class="o">*</span> <span class="n">file_ptr</span> <span class="o">=</span> <span class="n">freopen</span><span class="p">(</span><span class="nb">NULL</span><span class="p">,</span> <span class="s">"rb"</span><span class="p">,</span> <span class="n">stdin</span><span class="p">);</span>

  <span class="k">struct</span> <span class="n">ArrowIpcInputStream</span> <span class="n">input</span><span class="p">;</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowIpcInputStreamInitFile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">input</span><span class="p">,</span> <span class="n">file_ptr</span><span class="p">,</span> <span class="nb">false</span><span class="p">));</span>

  <span class="k">struct</span> <span class="n">ArrowArrayStream</span> <span class="n">stream</span><span class="p">;</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">ArrowIpcArrayStreamReaderInit</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">input</span><span class="p">,</span> <span class="nb">NULL</span><span class="p">));</span>

  <span class="k">struct</span> <span class="n">ArrowSchema</span> <span class="n">schema</span><span class="p">;</span>
  <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">stream</span><span class="p">.</span><span class="n">get_schema</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">schema</span><span class="p">));</span>

  <span class="k">struct</span> <span class="n">ArrowArray</span> <span class="n">array</span><span class="p">;</span>
  <span class="k">while</span> <span class="p">(</span><span class="nb">true</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">NANOARROW_RETURN_NOT_OK</span><span class="p">(</span><span class="n">stream</span><span class="p">.</span><span class="n">get_next</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">array</span><span class="p">));</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">array</span><span class="p">.</span><span class="n">release</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">break</span><span class="p">;</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Facilities for advanced usage are also provided via the low-level <code class="language-plaintext highlighter-rouge">ArrowIpcDecoder</code>,
which takes care of the details of deserializing the flatbuffer headers and
assembling buffers into an <code class="language-plaintext highlighter-rouge">ArrowArray</code>. The current implementation can read
schema and record batch messages that contain any Arrow type that supported by
the C data interface. The initial version can read both big and little endian
streams originating from platforms of either endian. Dictionary encoding and
compression are not currently supported.</p>

<h2 id="getting-started-with-nanoarrow">Getting started with nanoarrow</h2>

<p>Early users of the nanoarrow C library respectfully noted that it was difficult
to know where to begin. This release includes improvements in reference documentation
but also includes a long-form tutorial for those just getting started with or
considering adopting the library. You can find the tutorial at the
<a href="https://apache.github.io/arrow-nanoarrow/dev/getting-started.html">nanoarrow documentation site</a>.</p>

<h2 id="c-library">C library</h2>

<p>The nanoarrow 0.2.0 release also includes a number of bugfixes and improvements
to the core C library, many of which were identified as a result of usage
connected with development of the IPC extension.</p>

<ul>
  <li>Helpers for extracting/appending <code class="language-plaintext highlighter-rouge">Decimal128</code> and <code class="language-plaintext highlighter-rouge">Decimal256</code> elements
from/to arrays were added.</li>
  <li>The C library can now perform “full” validation to validate untrusted input
(e.g., serialized IPC from the wire).</li>
  <li>The C library can now perform “minimal” validation that performs all checks
that do not access any buffer data. This feature was added to facilitate
future support for the <code class="language-plaintext highlighter-rouge">ArrowDeviceArray</code> that was recently added as the
Arrow C device interface.</li>
  <li>Release verification on Ubuntu (x86_64, arm64), Fedora (x86_64),
Archlinux (x86_64), Centos 7 (x86_64, arm64), Alpine (x86_64, arm64, s390x),
Windows (x86_64), and MacOS (x86_64) were added to the continuous
integration system. Linux verification is implemented using <code class="language-plaintext highlighter-rouge">docker compose</code>
to facilitate local checks when developing features that may affect
a specific platform.</li>
</ul>

<h2 id="r-bindings">R bindings</h2>

<p>The nanoarrow R bindings are distributed as the <code class="language-plaintext highlighter-rouge">nanoarrow</code> package on
<a href="https://cran.r-project.org/">CRAN</a>. The 0.2.0 release of the R bindings includes
improvements in type support, improvements in stability, and features required
for the forthcoming release of Arrow Database Connectivity (ADBC) R bindings.
Notably:</p>

<ul>
  <li>Support for conversion of union arrays to R objects was added to facilitate
support for an ADBC function that returns such an array.</li>
  <li>Support for adding an R-level finalizer to an <code class="language-plaintext highlighter-rouge">ArrowArrayStream</code> was added
to facilitate safely wrapping a stream resulting from an ADBC call at the
R level.</li>
</ul>

<h2 id="python-bindings">Python bindings?</h2>

<p>The nanoarrow 0.2.0 release does not include Python bindings, but improvements to the
<a href="https://github.com/apache/arrow-nanoarrow/tree/main/python">unreleased draft bindings</a>
were added to facilitate discussion among Python developers regarding the useful
scope of a potential future nanoarrow Python package. If one of those developers is
you, feel free to
<a href="https://github.com/apache/arrow-nanoarrow/issues/new/choose">open an issue</a>
or send a post to the
<a href="https://lists.apache.org/list.html?dev@arrow.apache.org">developer mailing list</a>
to engage in the discussion.</p>

<h2 id="contributors">Contributors</h2>

<p>This initial release consists of contributions from 6 contributors in addition
to the invaluable advice and support of the Apache Arrow developer mailing list.
Special thanks to David Li for reviewing nearly every PR in this release!</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog -sn d91c35b33c7b6ff94f5f929384879352e241ed71..apache-arrow-nanoarrow-0.2.0 | grep -v "GitHub Actions"
    61  Dewey Dunnington
     2  Dirk Eddelbuettel
     2  Joris Van den Bossche
     2  Kirill Müller
     2  William Ayd
     1  David Li
</code></pre></div></div>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.2.0 release of Apache Arrow nanoarrow. This initial release covers 19 resolved issues from 6 contributors. Release Highlights Addition of the Arrow IPC stream reader extension Addition of the Getting Started with nanoarrow tutorial Improvements in reliability and platform test coverage of the C library Improvements in reliability and type support in the R bindings See the Changelog for a detailed list of contributions to this release. IPC stream support This release includes support for reading schemas and record batches serialized using the Arrow IPC format. Based on the flatcc flatbuffers implementation, the nanoarrow IPC read support is implemented as an optional extension to the core nanoarrow library. The easiest way to get started is with the ArrowArrayStream provider using one of the built-in ArrowIpcInputStream implementations: #include &lt;stdio.h&gt; #include &lt;stdbool.h&gt; #include "nanoarrow_ipc.h" int main(int argc, char* argv[]) { FILE* file_ptr = freopen(NULL, "rb", stdin); struct ArrowIpcInputStream input; NANOARROW_RETURN_NOT_OK(ArrowIpcInputStreamInitFile(&amp;input, file_ptr, false)); struct ArrowArrayStream stream; NANOARROW_RETURN_NOT_OK(ArrowIpcArrayStreamReaderInit(&amp;stream, &amp;input, NULL)); struct ArrowSchema schema; NANOARROW_RETURN_NOT_OK(stream.get_schema(&amp;stream, &amp;schema)); struct ArrowArray array; while (true) { NANOARROW_RETURN_NOT_OK(stream.get_next(&amp;stream, &amp;array)); if (array.release == NULL) { break; } } return 0; } Facilities for advanced usage are also provided via the low-level ArrowIpcDecoder, which takes care of the details of deserializing the flatbuffer headers and assembling buffers into an ArrowArray. The current implementation can read schema and record batch messages that contain any Arrow type that supported by the C data interface. The initial version can read both big and little endian streams originating from platforms of either endian. Dictionary encoding and compression are not currently supported. Getting started with nanoarrow Early users of the nanoarrow C library respectfully noted that it was difficult to know where to begin. This release includes improvements in reference documentation but also includes a long-form tutorial for those just getting started with or considering adopting the library. You can find the tutorial at the nanoarrow documentation site. C library The nanoarrow 0.2.0 release also includes a number of bugfixes and improvements to the core C library, many of which were identified as a result of usage connected with development of the IPC extension. Helpers for extracting/appending Decimal128 and Decimal256 elements from/to arrays were added. The C library can now perform “full” validation to validate untrusted input (e.g., serialized IPC from the wire). The C library can now perform “minimal” validation that performs all checks that do not access any buffer data. This feature was added to facilitate future support for the ArrowDeviceArray that was recently added as the Arrow C device interface. Release verification on Ubuntu (x86_64, arm64), Fedora (x86_64), Archlinux (x86_64), Centos 7 (x86_64, arm64), Alpine (x86_64, arm64, s390x), Windows (x86_64), and MacOS (x86_64) were added to the continuous integration system. Linux verification is implemented using docker compose to facilitate local checks when developing features that may affect a specific platform. R bindings The nanoarrow R bindings are distributed as the nanoarrow package on CRAN. The 0.2.0 release of the R bindings includes improvements in type support, improvements in stability, and features required for the forthcoming release of Arrow Database Connectivity (ADBC) R bindings. Notably: Support for conversion of union arrays to R objects was added to facilitate support for an ADBC function that returns such an array. Support for adding an R-level finalizer to an ArrowArrayStream was added to facilitate safely wrapping a stream resulting from an ADBC call at the R level. Python bindings? The nanoarrow 0.2.0 release does not include Python bindings, but improvements to the unreleased draft bindings were added to facilitate discussion among Python developers regarding the useful scope of a potential future nanoarrow Python package. If one of those developers is you, feel free to open an issue or send a post to the developer mailing list to engage in the discussion. Contributors This initial release consists of contributions from 6 contributors in addition to the invaluable advice and support of the Apache Arrow developer mailing list. Special thanks to David Li for reviewing nearly every PR in this release! $ git shortlog -sn d91c35b33c7b6ff94f5f929384879352e241ed71..apache-arrow-nanoarrow-0.2.0 | grep -v "GitHub Actions" 61 Dewey Dunnington 2 Dirk Eddelbuettel 2 Joris Van den Bossche 2 Kirill Müller 2 William Ayd 1 David Li]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 0.5.0 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/06/21/adbc-0.5.0-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.5.0 (Libraries) Release" /><published>2023-06-21T00:00:00-04:00</published><updated>2023-06-21T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/06/21/adbc-0.5.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/06/21/adbc-0.5.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.5.0 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/6?closed=1"><strong>37
resolved issues</strong></a> from <a href="#contributors"><strong>12 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version
0.5.0.  The <strong>API specification</strong> is versioned separately and is
at version 1.0.0.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.5.0/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>Experimental C# and Rust codebases were added to the source tree.  No packages are released for them yet.</p>

<p>Go-based drivers, when using FFI, will now catch panics at the interface boundary and return them as errors.  Afterwards, all calls to the driver will immediately fail.  This lets applications handle errors and gracefully terminate instead of immediately aborting.</p>

<p>The JDBC adapter allows customizing the type mapping between JDBC types and Arrow types, using all the information provided by JDBC.  This allows proper mappings for certain databases (e.g. the PostgreSQL JDBC driver returns both tz-aware and tz-naive timestamps under the same type code, but differentiates them via the type name).</p>

<p>The PostgreSQL driver has better support for reading and writing different types, including datetime types in particular.  More metadata is now returned in GetObjects, including tables, columns, and constraints (primary/foreign keys).  The driver also now handles non-SELECT queries in <code class="language-plaintext highlighter-rouge">ExecuteQuery</code>.</p>

<p>The Python driver manager lets you choose whether to disable or enable autocommit.</p>

<p>The R driver manager now exposes easy convenience functions for reading/writing data.</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.4.0..apache-arrow-adbc-0.5.0
    36	David Li
    21	William Ayd
     6	Dewey Dunnington
     4	Matt Topol
     3	Kirill Müller
     2	Sutou Kouhei
     2	vipere
     1	Curt Hagenlocher
     1	Diego Fernández Giraldo
     1	Matthijs Brobbel
     1	Will Jones
     1	davidhcoe
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>Work for the proposed <a href="https://github.com/apache/arrow-adbc/milestone/3">1.1.0 API revision</a> is progressing on a <a href="https://github.com/apache/arrow-adbc/tree/spec-1.1.0">branch</a>, with several API proposals merged in and others awaiting review.  They will then receive prototype implementations before being submitted for review/voting.  This is not currently targeting any release of the ADBC libraries, although we hope to wrap up this effort in the next few months.</p>

<p>A milestone has been created to explore <a href="https://github.com/apache/arrow-adbc/milestone/9">asynchronous APIs</a>, which have been requested for a while now.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.5.0 release of the Apache Arrow ADBC libraries. This covers includes 37 resolved issues from 12 distinct contributors. This is a release of the libraries, which are at version 0.5.0. The API specification is versioned separately and is at version 1.0.0. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights Experimental C# and Rust codebases were added to the source tree. No packages are released for them yet. Go-based drivers, when using FFI, will now catch panics at the interface boundary and return them as errors. Afterwards, all calls to the driver will immediately fail. This lets applications handle errors and gracefully terminate instead of immediately aborting. The JDBC adapter allows customizing the type mapping between JDBC types and Arrow types, using all the information provided by JDBC. This allows proper mappings for certain databases (e.g. the PostgreSQL JDBC driver returns both tz-aware and tz-naive timestamps under the same type code, but differentiates them via the type name). The PostgreSQL driver has better support for reading and writing different types, including datetime types in particular. More metadata is now returned in GetObjects, including tables, columns, and constraints (primary/foreign keys). The driver also now handles non-SELECT queries in ExecuteQuery. The Python driver manager lets you choose whether to disable or enable autocommit. The R driver manager now exposes easy convenience functions for reading/writing data. Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.4.0..apache-arrow-adbc-0.5.0 36 David Li 21 William Ayd 6 Dewey Dunnington 4 Matt Topol 3 Kirill Müller 2 Sutou Kouhei 2 vipere 1 Curt Hagenlocher 1 Diego Fernández Giraldo 1 Matthijs Brobbel 1 Will Jones 1 davidhcoe Roadmap Work for the proposed 1.1.0 API revision is progressing on a branch, with several API proposals merged in and others awaiting review. They will then receive prototype implementations before being submitted for review/voting. This is not currently targeting any release of the ADBC libraries, although we hope to wrap up this effort in the next few months. A milestone has been created to explore asynchronous APIs, which have been requested for a while now. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 12.0.1 Release</title><link href="https://arrow.apache.org/blog/2023/06/13/12.0.1-release/" rel="alternate" type="text/html" title="Apache Arrow 12.0.1 Release" /><published>2023-06-13T00:00:00-04:00</published><updated>2023-06-13T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/06/13/12.0.1-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/06/13/12.0.1-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 12.0.1 release.
This is mostly a bugfix release that includes <a href="https://github.com/apache/arrow/milestone/54?closed=1"><strong>38 resolved issues</strong></a>
from <a href="/release/12.0.1.html#contributors"><strong>12 distinct contributors</strong></a>. See the Install Page to learn how to
get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/12.0.1.html#changelog">complete changelog</a>.</p>

<h2 id="c-notes">C++ notes</h2>

<ul>
  <li>Fixed a performance regression when writing data from non-arrow sources (e.g. pandas) (<a href="https://github.com/apache/arrow/pull/35565">GH-35498</a>)</li>
  <li>Fixed a “Data size too large” error that could occur when reading valid parquet files (<a href="https://github.com/apache/arrow/pull/35428">GH-35423</a>)</li>
  <li>It is now possible to specify field-level metadata in dataset writes (<a href="https://github.com/apache/arrow/pull/35860">GH-35730</a>)</li>
</ul>

<h2 id="go-notes">Go notes</h2>

<ul>
  <li>Fixed builds of the Go Arrow package on 32-bit systems (<a href="https://github.com/apache/arrow/pull/35767">GH-34784</a>)</li>
  <li>Added <code class="language-plaintext highlighter-rouge">ValueString(int) string</code> method to <code class="language-plaintext highlighter-rouge">arrow.Array</code> (<a href="https://github.com/apache/arrow/pull/34986">GH-34657</a>)</li>
  <li>Fixed ASAN failure when using go1.20+ by using <code class="language-plaintext highlighter-rouge">unsafe.StringData</code> (<a href="https://github.com/apache/arrow/pull/35338">GH-35337</a>)</li>
</ul>

<h2 id="java-notes">Java notes</h2>

<ul>
  <li>Bumped jackson-databind dependency version to avoid CVE-2022-42003. (<a href="https://github.com/apache/arrow/pull/35771">GH-35771</a>)</li>
</ul>

<h2 id="python-notes">Python notes</h2>

<ul>
  <li>Fix <code class="language-plaintext highlighter-rouge">Table.join</code> respecting the <code class="language-plaintext highlighter-rouge">coalesce_keys=False</code> option again (<a href="https://github.com/apache/arrow/issues/35389">GH-35389</a>)</li>
</ul>

<h2 id="r-notes">R notes</h2>

<ul>
  <li>Update the version of the date library vendored with Arrow C++ library for compatibility with tzdb 0.4.0 (<a href="https://github.com/apache/arrow/issues/35594">GH-35594</a>, <a href="https://github.com/apache/arrow/issues/35612">GH-35612</a>)</li>
</ul>

<h2 id="other-modules-and-languages">Other modules and languages</h2>

<p>No general changes were made to the other libraries or languages.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 12.0.1 release. This is mostly a bugfix release that includes 38 resolved issues from 12 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. C++ notes Fixed a performance regression when writing data from non-arrow sources (e.g. pandas) (GH-35498) Fixed a “Data size too large” error that could occur when reading valid parquet files (GH-35423) It is now possible to specify field-level metadata in dataset writes (GH-35730) Go notes Fixed builds of the Go Arrow package on 32-bit systems (GH-34784) Added ValueString(int) string method to arrow.Array (GH-34657) Fixed ASAN failure when using go1.20+ by using unsafe.StringData (GH-35337) Java notes Bumped jackson-databind dependency version to avoid CVE-2022-42003. (GH-35771) Python notes Fix Table.join respecting the coalesce_keys=False option again (GH-35389) R notes Update the version of the date library vendored with Arrow C++ library for compatibility with tzdb 0.4.0 (GH-35594, GH-35612) Other modules and languages No general changes were made to the other libraries or languages.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow ADBC 0.4.0 (Libraries) Release</title><link href="https://arrow.apache.org/blog/2023/05/15/adbc-0.4.0-release/" rel="alternate" type="text/html" title="Apache Arrow ADBC 0.4.0 (Libraries) Release" /><published>2023-05-15T00:00:00-04:00</published><updated>2023-05-15T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/05/15/adbc-0.4.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/05/15/adbc-0.4.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 0.4.0 release of
the Apache Arrow ADBC libraries. This covers includes <a href="https://github.com/apache/arrow-adbc/milestone/5"><strong>47
resolved issues</strong></a> from <a href="#contributors"><strong>8 distinct contributors</strong></a>.</p>

<p>This is a release of the <strong>libraries</strong>, which are at version
0.4.0.  The <strong>API specification</strong> is versioned separately and is
at version 1.0.0.</p>

<p>The release notes below are not exhaustive and only expose selected
highlights of the release. Many other bugfixes and improvements have
been made: we refer you to the <a href="https://github.com/apache/arrow-adbc/blob/apache-arrow-adbc-0.4.0/CHANGELOG.md">complete changelog</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>

<p>A Go-based driver for <a href="https://www.snowflake.com/en/">Snowflake</a> was added, along with bindings for Python and R.</p>

<p>The PostgreSQL driver now has much better support for different types, and properly handles NULL values.
It now also implements <code class="language-plaintext highlighter-rouge">AdbcConnectionGetTableSchema</code>.
All C/C++ components have been consolidated into a single build project, simplifying integration with IDEs.</p>

<p>Several <code class="language-plaintext highlighter-rouge">ArrowBuf</code> leaks in the Java libraries have been fixed, along with miscellaneous bugs in the JDBC adapter.
There was breaking change in the Java libraries: static driver instances were removed, as this was prone to the aformentioned leaks.
Instead, <code class="language-plaintext highlighter-rouge">AdbcDriver</code> instances can now be created with a <code class="language-plaintext highlighter-rouge">BufferAllocator</code>, giving the application control over allocations.</p>

<p>The Python, GLib, and Ruby bindings expose more of the API functions.
The Python bindings are now tested against the <a href="https://www.pola.rs/">polars</a> dataframe project, which has experimental integration with ADBC.
The release process was fixed to properly upload the Windows Python wheels.
The R bindings now include packages for the PostgreSQL driver.</p>

<p>There is a known issue with the Snowflake driver: it will not build on non-x86_64 (AMD64) platforms, including Linux/macOS arm64/aarch64.
This has already been fixed for the next release.</p>

<h2 id="contributors">Contributors</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.3.0..apache-arrow-adbc-0.4.0
    31	David Li
    15	Sutou Kouhei
     9	Dewey Dunnington
     7	William Ayd
     5	Matt Topol
     1	Jacob Marble
     1	Tornike Gurgenidze
     1	eitsupi
</code></pre></div></div>

<h2 id="roadmap">Roadmap</h2>

<p>Work for the proposed <a href="https://github.com/apache/arrow-adbc/milestone/3">1.1.0 API revision</a> has begun on a new <a href="https://github.com/apache/arrow-adbc/tree/spec-1.1.0">branch</a>.
This is not currently targeting any release of the ADBC libraries.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>We welcome questions and contributions from all interested.  Issues
can be filed on <a href="https://github.com/apache/arrow-adbc/issues">GitHub</a>, and questions can be directed to GitHub
or the <a href="/community/">Arrow mailing lists</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 0.4.0 release of the Apache Arrow ADBC libraries. This covers includes 47 resolved issues from 8 distinct contributors. This is a release of the libraries, which are at version 0.4.0. The API specification is versioned separately and is at version 1.0.0. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Release Highlights A Go-based driver for Snowflake was added, along with bindings for Python and R. The PostgreSQL driver now has much better support for different types, and properly handles NULL values. It now also implements AdbcConnectionGetTableSchema. All C/C++ components have been consolidated into a single build project, simplifying integration with IDEs. Several ArrowBuf leaks in the Java libraries have been fixed, along with miscellaneous bugs in the JDBC adapter. There was breaking change in the Java libraries: static driver instances were removed, as this was prone to the aformentioned leaks. Instead, AdbcDriver instances can now be created with a BufferAllocator, giving the application control over allocations. The Python, GLib, and Ruby bindings expose more of the API functions. The Python bindings are now tested against the polars dataframe project, which has experimental integration with ADBC. The release process was fixed to properly upload the Windows Python wheels. The R bindings now include packages for the PostgreSQL driver. There is a known issue with the Snowflake driver: it will not build on non-x86_64 (AMD64) platforms, including Linux/macOS arm64/aarch64. This has already been fixed for the next release. Contributors $ git shortlog --perl-regexp --author='^((?!dependabot\[bot\]).*)$' -sn apache-arrow-adbc-0.3.0..apache-arrow-adbc-0.4.0 31 David Li 15 Sutou Kouhei 9 Dewey Dunnington 7 William Ayd 5 Matt Topol 1 Jacob Marble 1 Tornike Gurgenidze 1 eitsupi Roadmap Work for the proposed 1.1.0 API revision has begun on a new branch. This is not currently targeting any release of the ADBC libraries. Getting Involved We welcome questions and contributions from all interested. Issues can be filed on GitHub, and questions can be directed to GitHub or the Arrow mailing lists.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Adopting Apache Arrow at CloudQuery</title><link href="https://arrow.apache.org/blog/2023/05/04/adopting-apache-arrow-at-cloudquery/" rel="alternate" type="text/html" title="Adopting Apache Arrow at CloudQuery" /><published>2023-05-04T00:00:00-04:00</published><updated>2023-05-04T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/05/04/adopting-apache-arrow-at-cloudquery</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/05/04/adopting-apache-arrow-at-cloudquery/"><![CDATA[<!--

-->

<p>This post is a collaboration with CloudQuery and cross-posted on the CloudQuery <a href="https://cloudquery.io/blog/adopting-apache-arrow-at-cloudquery">blog</a>.</p>

<p><a href="https://github.com/cloudquery/cloudquery">CloudQuery</a> is an open source high performance ELT framework written in Go. We <a href="https://www.cloudquery.io/blog/building-cloudquery">previously</a> discussed some of the <a href="https://www.cloudquery.io/docs/developers/architecture">architecture</a> and design decisions that we took to build a performant ELT framework. A type system is a key component for creating a performant and scalable ELT framework where sources and destinations are decoupled. In this blog post we will go through why we decided to adopt Apache Arrow as our type system and replace our in-house implementation.</p>

<h1 id="what-is-a-type-system">What is a Type System?</h1>

<p>Let’s quickly <a href="https://www.cloudquery.io/blog/building-cloudquery#type-system">recap</a> what a type system is and why an ELT framework needs one. At a very high level, an ELT framework extracts data from some source and moves it to some destination with a specific schema.</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>API ---&gt; [Source Plugin]  -----&gt;    [Destination Plugin]
                          -----&gt;    [Destination Plugin]
                           gRPC
</code></pre></div></div>

<p>Sources and destinations are decoupled and communicate via gRPC. This is crucial to allowing the addition of new destinations and updating old destinations without requiring updates to source plugin code (which otherwise would introduce an unmaintainable architecture).</p>

<p>This is where a type system comes in. Source plugins extract information from APIs in the most performant way possible, defining a schema and then transforming the result from the API (JSON or any other format) to a well-defined type system. The destination plugin can then easily create the schema for its database and transform the incoming data to the destination types. So to recap, the source plugin sends mainly two things to a destination: 1) the schema 2) the records that fit the defined schema. In Arrow terminology, these are a schema and a record batch.</p>

<h1 id="why-arrow">Why Arrow?</h1>

<p>Before Arrow, we used our own type system that supported more than 14 types. This served us well, but we started to hit limitations in various use-cases. For example, in database to database replication, we needed to support many more types, including nested types. Also, performance-wise, lots of the time spent in an ELT process is around converting data from one format to another, so we wanted to take a step back and see if we can avoid this <a href="https://xkcd.com/927/">famous XKCD</a> (by building yet another format):</p>

<figure style="text-align: center;">
  <img src="https://imgs.xkcd.com/comics/standards.png" width="100%" class="img-responsive" alt="Yet another standard XKCD" />
</figure>

<p>This is where Arrow comes in. Apache Arrow defines a language-independent columnar format for flat and hierarchical data, and brings the following advantages:</p>

<ol>
  <li>Cross-language with extensive libraries for different languages - The <a href="https://arrow.apache.org/docs/format/Columnar.html">format</a> is defined via flatbuffers in such way that you can parse it in any language and already has extensive support in C/C++, C#, Go, Java, JavaScript, Julia, Matlab, Python, R, Ruby and Rust (at the time of writing). For CloudQuery this is important as it makes it much easier to develop source or destination plugins in different languages.</li>
  <li>Performance: Arrow adoption is rising especially in columnar based databases (<a href="https://duckdb.org/2021/12/03/duck-arrow.html">DuckDB</a>, <a href="https://clickhouse.com/docs/en/integrations/data-formats/arrow-avro-orc">ClickHouse</a>, <a href="https://cloud.google.com/bigquery/docs/samples/bigquerystorage-arrow-quickstart">BigQuery</a>) and file formats (<a href="https://arrow.apache.org/docs/python/parquet.html">Parquet</a>) which makes it easier to write CloudQuery destination or source plugins for databases that already support Arrow as well as much more efficient as we remove the need for additional serialization and transformation step. Moreover, just the performance of sending Arrow format from source plugin to destination is already more performant and memory efficient, given its “zero-copy” nature and not needing serialization/deserialization.</li>
  <li>Rich Data Types: Arrow supports more than <a href="https://arrow.apache.org/docs/python/api/datatypes.html">35 types</a> including composite types (i.e. lists, structs and maps of all the available types) and ability to extend the type system with custom types. Also, there is already built-in mapping from/to the arrow type system and the parquet type system (including nested types) which already supported in many of the arrow libraries as explained <a href="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/">here</a>.</li>
</ol>

<h1 id="summary">Summary</h1>

<p>Adopting Apache Arrow as the CloudQuery in-memory type system enables us to gain better performance, data interoperability and developer experience. Some plugins that are going to gain an immediate boost of rich type systems are our database-to-database replication plugins such as <a href="https://www.cloudquery.io/docs/plugins/sources/postgresql/overview">PostgreSQL CDC</a> source plugin (and all <a href="https://www.cloudquery.io/docs/plugins/destinations/overview">database destinations</a>) that are going to get support for all available types including nested ones.</p>

<p>We are excited about this step and joining the growing Arrow community. We already contributed more than <a href="https://github.com/search?q=is%3Apr+author%3Ayevgenypats+author%3Ahermanschaaf+author%3Acandiduslynx+author%3Adisq+label%3A%22Component%3A+Go%22++is%3Amerged+&amp;ref=simplesearch">30</a> upstream pull requests that were quickly reviewed by the Arrow maintainers, thank you!</p>]]></content><author><name>Yevgeny Pats</name></author><category term="application" /><summary type="html"><![CDATA[This post is a collaboration with CloudQuery and cross-posted on the CloudQuery blog. CloudQuery is an open source high performance ELT framework written in Go. We previously discussed some of the architecture and design decisions that we took to build a performant ELT framework. A type system is a key component for creating a performant and scalable ELT framework where sources and destinations are decoupled. In this blog post we will go through why we decided to adopt Apache Arrow as our type system and replace our in-house implementation. What is a Type System? Let’s quickly recap what a type system is and why an ELT framework needs one. At a very high level, an ELT framework extracts data from some source and moves it to some destination with a specific schema. API ---&gt; [Source Plugin]  -----&gt;    [Destination Plugin] -----&gt;    [Destination Plugin] gRPC Sources and destinations are decoupled and communicate via gRPC. This is crucial to allowing the addition of new destinations and updating old destinations without requiring updates to source plugin code (which otherwise would introduce an unmaintainable architecture). This is where a type system comes in. Source plugins extract information from APIs in the most performant way possible, defining a schema and then transforming the result from the API (JSON or any other format) to a well-defined type system. The destination plugin can then easily create the schema for its database and transform the incoming data to the destination types. So to recap, the source plugin sends mainly two things to a destination: 1) the schema 2) the records that fit the defined schema. In Arrow terminology, these are a schema and a record batch. Why Arrow? Before Arrow, we used our own type system that supported more than 14 types. This served us well, but we started to hit limitations in various use-cases. For example, in database to database replication, we needed to support many more types, including nested types. Also, performance-wise, lots of the time spent in an ELT process is around converting data from one format to another, so we wanted to take a step back and see if we can avoid this famous XKCD (by building yet another format): This is where Arrow comes in. Apache Arrow defines a language-independent columnar format for flat and hierarchical data, and brings the following advantages: Cross-language with extensive libraries for different languages - The format is defined via flatbuffers in such way that you can parse it in any language and already has extensive support in C/C++, C#, Go, Java, JavaScript, Julia, Matlab, Python, R, Ruby and Rust (at the time of writing). For CloudQuery this is important as it makes it much easier to develop source or destination plugins in different languages. Performance: Arrow adoption is rising especially in columnar based databases (DuckDB, ClickHouse, BigQuery) and file formats (Parquet) which makes it easier to write CloudQuery destination or source plugins for databases that already support Arrow as well as much more efficient as we remove the need for additional serialization and transformation step. Moreover, just the performance of sending Arrow format from source plugin to destination is already more performant and memory efficient, given its “zero-copy” nature and not needing serialization/deserialization. Rich Data Types: Arrow supports more than 35 types including composite types (i.e. lists, structs and maps of all the available types) and ability to extend the type system with custom types. Also, there is already built-in mapping from/to the arrow type system and the parquet type system (including nested types) which already supported in many of the arrow libraries as explained here. Summary Adopting Apache Arrow as the CloudQuery in-memory type system enables us to gain better performance, data interoperability and developer experience. Some plugins that are going to gain an immediate boost of rich type systems are our database-to-database replication plugins such as PostgreSQL CDC source plugin (and all database destinations) that are going to get support for all available types including nested ones. We are excited about this step and joining the growing Arrow community. We already contributed more than 30 upstream pull requests that were quickly reviewed by the Arrow maintainers, thank you!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 12.0.0 Release</title><link href="https://arrow.apache.org/blog/2023/05/02/12.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 12.0.0 Release" /><published>2023-05-02T00:00:00-04:00</published><updated>2023-05-02T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/05/02/12.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/05/02/12.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 12.0.0 release. This covers
over 3 months of development work and includes <a href="https://github.com/apache/arrow/milestone/51?closed=1"><strong>476 resolved issues</strong></a>
with <a href="/release/12.0.0.html#contributors"><strong>531 commits from 97 distinct contributors</strong></a>.
See the <a href="https://arrow.apache.org/install/">Install Page</a>
to learn how to get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/12.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 11.0.0 release, Wang Mingming, Mustafa Akur and Ruihang Xia
have been invited to be committers.
Will Jones have joined the Project Management Committee (PMC).</p>

<p>Thanks for your contributions and participation in the project!</p>

<h2 id="columnar-format-notes">Columnar Format Notes</h2>

<p>A first “canonical” extension type has been formalized: <code class="language-plaintext highlighter-rouge">arrow.fixed_shape_tensor</code> to
represent an Array where each slot contains a tensor, with all tensors having the same
dimension and shape, <a href="https://github.com/apache/arrow/issues/33923">GH-33923</a>.
This is based on a Fixed-Size List layout as storage array
(<a href="https://arrow.apache.org/docs/dev/format/CanonicalExtensions.html#fixed-shape-tensor-extension">https://arrow.apache.org/docs/dev/format/CanonicalExtensions.html#fixed-shape-tensor-extension</a>).</p>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>

<p>The JDBC driver for Arrow Flight SQL has had some bugfixes, and has been refactored into a core library (which is not distributed as an uberjar with shaded names) and a driver (which is distributed as an uberjar).</p>

<p>The Java server builder API now offers easier access to the underlying gRPC builder.</p>

<p>Go now implements the Flight SQL extensions for Substrait and transaction support.</p>

<h2 id="plasma-notes">Plasma notes</h2>

<p>Plasma was deprecated since 10.0.0. Plasma is removed in this
release. <a href="https://github.com/apache/arrow/issues/33243">GH-33243</a></p>

<h2 id="linux-packages-notes">Linux packages notes</h2>

<p>We dropped support for Ubuntu 18.04 because Ubuntu 18.04 reached EOL.</p>

<h2 id="c-notes">C++ notes</h2>

<ul>
  <li>Run-End Encoded Arrays have been implemented and are accessible (<a href="https://github.com/apache/arrow/issues/32104">GH-32104</a>)</li>
  <li>The FixedShapeTensor Logical value type has been implemented using ExtensionType (<a href="https://github.com/apache/arrow/issues/15483">GH-15483</a>, <a href="https://github.com/apache/arrow/issues/34796">GH-34796</a>)</li>
</ul>

<h3 id="compute">Compute</h3>

<ul>
  <li>New kernel to convert timestamp with timezone to wall time (<a href="https://github.com/apache/arrow/issues/33143">GH-33143</a>)</li>
  <li>Cast kernels are now built into libarrow by default (<a href="https://github.com/apache/arrow/issues/34388">GH-34388</a>)</li>
</ul>

<h3 id="acero">Acero</h3>

<ul>
  <li>Acero has been moved out of libarrow into it’s own shared library, allowing for smaller builds of the core libarrow (<a href="https://github.com/apache/arrow/issues/15280">GH-15280</a>)</li>
  <li>Exec nodes now can have a concept of “ordering” and will reject non-sensible plans (<a href="https://github.com/apache/arrow/issues/34136">GH-34136</a>)</li>
  <li>New exec nodes: “pivot_longer” (<a href="https://github.com/apache/arrow/issues/34266">GH-34266</a>), “order_by” (<a href="https://github.com/apache/arrow/issues/34248">GH-34248</a>) and “fetch” (<a href="https://github.com/apache/arrow/issues/34059">GH-34059</a>)</li>
  <li><em>Breaking Change:</em> Reorder output fields of “group_by” node so that keys/segment keys come before aggregates (<a href="https://github.com/apache/arrow/issues/33616">GH-33616</a>)</li>
</ul>

<h3 id="substrait">Substrait</h3>

<ul>
  <li>Add support for the <code class="language-plaintext highlighter-rouge">round</code> function <a href="https://github.com/apache/arrow/issues/33588">GH-33588</a></li>
  <li>Add support for the <code class="language-plaintext highlighter-rouge">cast</code> expression element <a href="https://github.com/apache/arrow/issues/31910">GH-31910</a></li>
  <li>Added API reference documentation <a href="https://github.com/apache/arrow/issues/34011">GH-34011</a></li>
  <li>Added an extension relation to support segmented aggregation <a href="https://github.com/apache/arrow/issues/34626">GH-34626</a></li>
  <li>The output of the aggregate relation now conforms to the spec <a href="https://github.com/apache/arrow/issues/34786">GH-34786</a></li>
</ul>

<h3 id="parquet">Parquet</h3>

<ul>
  <li>Added support for DeltaLengthByteArray encoding to the Parquet writer (<a href="https://github.com/apache/arrow/issues/33024">GH-33024</a>)</li>
  <li>NaNs are correctly handled now for Parquet predicate push-downs (<a href="https://github.com/apache/arrow/issues/18481">GH-18481</a>)</li>
  <li>Added support for reading Parquet page indexes (<a href="https://github.com/apache/arrow/issues/33596">GH-33596</a>) and writing page indexes (<a href="https://github.com/apache/arrow/issues/34053">GH-34053</a>)</li>
  <li>Parquet writer can write columns in parallel now (<a href="https://github.com/apache/arrow/issues/33655">GH-33655</a>)</li>
  <li>Fixed incorrect number of rows in Parquet V2 page headers (<a href="https://github.com/apache/arrow/issues/34086">GH-34086</a>)</li>
  <li>Fixed incorrect Parquet page null_count when stats are disabled (<a href="https://github.com/apache/arrow/issues/34326">GH-34326</a>)</li>
  <li>Added support for reading BloomFilters to the Parquet Reader (<a href="https://github.com/apache/arrow/issues/34665">GH-34665</a>)</li>
  <li>Parquet File-writer can now add additional key-value metadata after it has been opened (<a href="https://github.com/apache/arrow/issues/34888">GH-34888</a>)</li>
  <li><em>Breaking Change:</em> The default row group size for the Arrow writer changed from 64Mi rows to 1Mi rows. <a href="https://github.com/apache/arrow/issues/34280">GH-34280</a></li>
</ul>

<h3 id="orc">ORC</h3>

<ul>
  <li>Added support for the union type in ORC writer (<a href="https://github.com/apache/arrow/issues/34262">GH-34262</a>)</li>
  <li>Fixed ORC CHAR type mapping with Arrow (<a href="https://github.com/apache/arrow/issues/34823">GH-34823</a>)</li>
  <li>Fixed timestamp type mapping between ORC and arrow (<a href="https://github.com/apache/arrow/issues/34590">GH-34590</a>)</li>
</ul>

<h3 id="datasets">Datasets</h3>

<ul>
  <li>Added support for reading JSON datasets (<a href="https://github.com/apache/arrow/issues/33209">GH-33209</a>)</li>
  <li>Dataset writer now supports specifying a function callback to construct the file name in addition to the existing file name template (<a href="https://github.com/apache/arrow/issues/34565">GH-34565</a>)</li>
</ul>

<h3 id="filesystems">Filesystems</h3>

<ul>
  <li>GcsFileSystem::OpenInputFile avoids unnecessary downloads (<a href="https://github.com/apache/arrow/issues/34051">GH-34051</a>)</li>
</ul>

<h3 id="other-changes">Other changes</h3>

<ul>
  <li>Convenience Append(std::optional<T>...) methods have been added to array builders ([GH-14863](https://github.com/apache/arrow/issues/14863))</T></li>
  <li>A deprecated OpenTelemetry header was removed from the Flight library (<a href="https://github.com/apache/arrow/issues/34417">GH-34417</a>)</li>
  <li>Fixed crash in “take” kernels on ExtensionArrays with an underlying dictionary type (<a href="https://github.com/apache/arrow/issues/34619">GH-34619</a>)</li>
  <li>Fixed bug where the C-Data bridge did not preserve nullability of map values on import (<a href="https://github.com/apache/arrow/issues/34983">GH-34983</a>)</li>
  <li>Added support for EqualOptions to RecordBatch::Equals (<a href="https://github.com/apache/arrow/issues/34968">GH-34968</a>)</li>
  <li>zstd dependency upgraded to v1.5.5 (<a href="https://github.com/apache/arrow/issues/34899">GH-34899</a>)</li>
  <li>Improved handling of “logical” nulls such as with union and RunEndEncoded arrays (<a href="https://github.com/apache/arrow/issues/34361">GH-34361</a>)</li>
  <li>Fixed incorrect handling of uncompressed body buffers in IPC reader, added IpcWriteOptions::min_space_savings for optional compression optimizations (<a href="https://github.com/apache/arrow/issues/15102">GH-15102</a>)</li>
</ul>

<h2 id="c-notes-1">C# notes</h2>

<ul>
  <li>Support added for importing / exporting schemas and types via the
C data interface <a href="https://github.com/apache/arrow/issues/34737">GH-34737</a></li>
  <li>Support added for the half-float data type <a href="https://github.com/apache/arrow/issues/25163">GH-25163</a></li>
  <li>Schemas are now allowed to have multiple fields with the same name
<a href="https://github.com/apache/arrow/issues/34076">GH-34076</a></li>
  <li>Added support for reading compressed IPC files <a href="https://github.com/apache/arrow/issues/32240">GH-32240</a></li>
  <li>Add [] operator to Schema <a href="https://github.com/apache/arrow/issues/32240">GH-34119</a></li>
</ul>

<h2 id="go-notes">Go notes</h2>

<ul>
  <li>Run-End Encoded Arrays have been added to the Golang implementation (<a href="https://github.com/apache/arrow/issues/32104">GH-32104</a>, <a href="https://github.com/apache/arrow/issues/32946">GH-32946</a>, <a href="https://github.com/apache/arrow/issues/20407">GH-20407</a>, <a href="https://github.com/apache/arrow/issues/32949">GH-32949</a>)</li>
  <li>The SQLite Flight SQL Example has been improved and you can now <code class="language-plaintext highlighter-rouge">go get</code> a simple SQLite Flight SQL Server mainprog using <code class="language-plaintext highlighter-rouge">go get github.com/apache/arrow/go/v12/arrow/flight/flightsql/example/cmd/sqlite_flightsql_server</code> (<a href="https://github.com/apache/arrow/issues/33840">GH-33840</a>)</li>
  <li>Fixed bug causing builds to fail with the <code class="language-plaintext highlighter-rouge">noasm</code> build tag (<a href="https://github.com/apache/arrow/issues/34044">GH-34044</a>) and added a CI test run that uses the <code class="language-plaintext highlighter-rouge">noasm</code> tag (<a href="https://github.com/apache/arrow/issues/34055">GH-34055</a>)</li>
  <li>Fixed issue allowing building on riscv64-freebsd (<a href="https://github.com/apache/arrow/issues/34629">GH-34629</a>)</li>
  <li>Fixed issue preventing building on 32-bit platforms (<a href="https://github.com/apache/arrow/issues/35133">GH-35133</a>)</li>
</ul>

<h3 id="arrow">Arrow</h3>

<ul>
  <li>Fixed bug in C-Data handling of <code class="language-plaintext highlighter-rouge">ArrowArrayStream.get_next</code> when handling uninitialized <code class="language-plaintext highlighter-rouge">ArrowArrays</code> (<a href="https://github.com/apache/arrow/issues/33767">GH-33767</a>)</li>
  <li><em>Breaking Change:</em> Added <code class="language-plaintext highlighter-rouge">Err()</code> method to <code class="language-plaintext highlighter-rouge">RecordReader</code> interface so that it can propagate errors (<a href="https://github.com/apache/arrow/issues/33789">GH-33789</a>)</li>
  <li>Fixed potential panic in C-Data API for misusing an invalid handle (<a href="https://github.com/apache/arrow/issues/33864">GH-33864</a>)</li>
  <li>A new cgo-based Allocator that does not depend on libarrow has been added to the memory package (<a href="https://github.com/apache/arrow/issues/33901">GH-33901</a>)</li>
  <li>CSV Reader and Writer now support Extension type arrays (<a href="https://github.com/apache/arrow/issues/34334">GH-34334</a>)</li>
  <li>Fixed bug preventing the reading of IPC streams/files with compression enabled but uncompressed buffers (<a href="https://github.com/apache/arrow/issues/34385">GH-34385</a>)</li>
  <li>Added interface which can be added to an <code class="language-plaintext highlighter-rouge">ExtensionType</code> to allow Builders to be created via <code class="language-plaintext highlighter-rouge">NewBuilder</code>, enabling easy building of nested fields containing extension types (<a href="https://github.com/apache/arrow/issues/34453">GH-34453</a>)</li>
  <li>Added utilities to perform Array diffing (<a href="https://github.com/apache/arrow/issues/34790">GH-34790</a>)</li>
  <li>Added <code class="language-plaintext highlighter-rouge">SetColumn</code> method to <code class="language-plaintext highlighter-rouge">arrow.Record</code> (<a href="https://github.com/apache/arrow/issues/34832">GH-34832</a>)</li>
  <li>Added <code class="language-plaintext highlighter-rouge">GetValue</code> method to <code class="language-plaintext highlighter-rouge">arrow.Metadata</code> (<a href="https://github.com/apache/arrow/issues/34855">GH-34855</a>)</li>
  <li>Added <code class="language-plaintext highlighter-rouge">Pow</code> method to <code class="language-plaintext highlighter-rouge">decimal128.Num</code> and <code class="language-plaintext highlighter-rouge">decimal256.Num</code> (<a href="https://github.com/apache/arrow/issues/34863">GH-34863</a>)</li>
</ul>

<h4 id="flight">Flight</h4>

<ul>
  <li>Fixed bug in <code class="language-plaintext highlighter-rouge">StreamChunks</code> for Flight SQL to correctly propagate to the gRPC client (<a href="https://github.com/apache/arrow/issues/33717">GH-33717</a>)</li>
  <li>Fixed issue that prevented compatibility with gRPC &lt; v1.45 (<a href="https://github.com/apache/arrow/issues/33734">GH-33734</a>)</li>
  <li>Added support to bind a <code class="language-plaintext highlighter-rouge">RecordReader</code> for supplying parameters to a Flight SQL Prepared statement (<a href="https://github.com/apache/arrow/issues/33794">GH-33794</a>)</li>
  <li>Prepared Statement methods for FlightSQL client now allows gRPC Call Options (<a href="https://github.com/apache/arrow/issues/33867">GH-33867</a>)</li>
  <li>FlightSQL Extensions have been implemented (for transactions and Substrait support) (<a href="https://github.com/apache/arrow/issues/33935">GH-33935</a>)</li>
  <li>A driver compatible with <code class="language-plaintext highlighter-rouge">database/sql</code> for FlightSQL has been added (<a href="https://github.com/apache/arrow/issues/34332">GH-34332</a>)</li>
</ul>

<h4 id="compute-1">Compute</h4>

<ul>
  <li>“run_end_encode” and “run_end_decode” functions added to compute functions (<a href="https://github.com/apache/arrow/issues/20408">GH-20408</a>)</li>
  <li>“unique” function added (<a href="https://github.com/apache/arrow/issues/34171">GH-34171</a>)</li>
</ul>

<h3 id="parquet-1">Parquet</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">pqarrow</code> pkg now handles DICTIONARY fields natively (<a href="https://github.com/apache/arrow/issues/33466">GH-33466</a>)</li>
  <li>Fixed rare panic when writing list of 8 structs (<a href="https://github.com/apache/arrow/issues/33600">GH-33600</a>)</li>
  <li>Added support for <code class="language-plaintext highlighter-rouge">pqarrow</code> pkg to write LargeString and LargeBinary types (<a href="https://github.com/apache/arrow/issues/33875">GH-33875</a>)</li>
  <li>Fixed bug where <code class="language-plaintext highlighter-rouge">pqarrow.NewSchemaManifest</code> created the wrong field type for Array Object fields (<a href="https://github.com/apache/arrow/issues/34101">GH-34101</a>)</li>
  <li>Added support to Parquet Writer for Extension type Arrays (<a href="https://github.com/apache/arrow/issues/34330">GH-34330</a>)</li>
</ul>

<h2 id="java-notes">Java notes</h2>

<ul>
  <li>Update Java JNI modules to consider Arrow ACERO <a href="https://github.com/apache/arrow/issues/34862">GH-34862</a></li>
  <li>Ability to register additional GRPC services with FlightServer <a href="https://github.com/apache/arrow/issues/34778">GH-34778</a></li>
  <li>Allow sending custom headers/properties through Arrow Flight SQL JDBC <a href="https://github.com/apache/arrow/issues/33874">GH-33874</a></li>
</ul>

<h2 id="javascript-notes">JavaScript notes</h2>

<p>No changes.</p>

<h2 id="python-notes">Python notes</h2>

<p>Compatibility notes:</p>

<ul>
  <li>Plasma has been removed in this release (<a href="https://github.com/apache/arrow/issues/33243">GH-33243</a>). In addition, the deprecated serialization module in PyArrow was also removed (<a href="https://github.com/apache/arrow/issues/29705">GH-29705</a>). IPC (Inter-Process Communication) functionality of pyarrow or the standard library pickle should be used instead.</li>
  <li>The deprecated <code class="language-plaintext highlighter-rouge">use_async</code> keyword has been removed from the dataset module (<a href="https://github.com/apache/arrow/issues/30774">GH-30774</a>)</li>
  <li>Minimum Cython version to build PyArrow from source has been raised to 0.29.31 (<a href="https://github.com/apache/arrow/issues/34933">GH-34933</a>). In addition, PyArrow can now be compiled using Cython 3 (<a href="https://github.com/apache/arrow/issues/34564">GH-34564</a>).</li>
</ul>

<p>New features:</p>

<ul>
  <li>A new <code class="language-plaintext highlighter-rouge">pyarrow.acero</code> module with initial bindings for the Acero execution engine has been added (<a href="https://github.com/apache/arrow/issues/33976">GH-33976</a>)</li>
  <li>A new canonical extension type for fixed shaped tensor data has been defined. This is exposed in PyArrow as the FixedShapeTensorType (<a href="https://github.com/apache/arrow/issues/34882">GH-34882</a>, <a href="https://github.com/apache/arrow/issues/34956">GH-34956</a>)</li>
  <li>Run-End Encoded arrays binding has been implemented (<a href="https://github.com/apache/arrow/issues/34686">GH-34686</a>, <a href="https://github.com/apache/arrow/issues/34568">GH-34568</a>)</li>
  <li>Method <code class="language-plaintext highlighter-rouge">is_nan</code> has been added to Array, ChunkedArray and Expression (<a href="https://github.com/apache/arrow/issues/34154">GH-34154</a>)</li>
  <li>Dataframe interchange protocol has been implemented for RecordBatch (<a href="https://github.com/apache/arrow/issues/33926">GH-33926</a>)</li>
</ul>

<p>Other improvements:</p>

<ul>
  <li>Extension arrays can now be concatenated (<a href="https://github.com/apache/arrow/issues/31868">GH-31868</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">get_partition_keys</code> helper function is implemented in the <code class="language-plaintext highlighter-rouge">dataset</code> module to access the partitioning field’s key/value from the partition expression of a certain dataset fragment (<a href="https://github.com/apache/arrow/issues/33825">GH-33825</a>)</li>
  <li>PyArrow Array objects can now be accepted by the <code class="language-plaintext highlighter-rouge">pa.array()</code> constructor (<a href="https://github.com/apache/arrow/issues/34411">GH-34411</a>)</li>
  <li>The default row group size when writing parquet files has been changed (<a href="https://github.com/apache/arrow/issues/34280">GH-34280</a>)</li>
  <li>RecordBatch has the <code class="language-plaintext highlighter-rouge">select()</code> method implemented (<a href="https://github.com/apache/arrow/issues/34359">GH-34359</a>)</li>
  <li>New method <code class="language-plaintext highlighter-rouge">drop_column</code> on the <code class="language-plaintext highlighter-rouge">pyarrow.Table</code> supports passing a single column as a string (<a href="https://github.com/apache/arrow/issues/33377">GH-33377</a>)</li>
  <li>User-defined tabular functions, which are a user-functions implemented in Python that return a stateful stream of tabular data, are now also supported (<a href="https://github.com/apache/arrow/issues/32916">GH-32916</a>)</li>
  <li><a href="https://arrow.apache.org/docs/dev/developers/continuous_integration/archery.html">Arrow Archery tool</a> now includes linting of the Cython files (<a href="https://github.com/apache/arrow/issues/31905">GH-31905</a>)</li>
  <li><em>Breaking Change:</em> Reorder output fields of “group_by” node so that keys/segment keys come before aggregates (<a href="https://github.com/apache/arrow/issues/33616">GH-33616</a>)</li>
</ul>

<p>Relevant bug fixes:</p>

<ul>
  <li>Acero can now detect and raise an error in case a join operation needs too much bytes of key data (<a href="https://github.com/apache/arrow/issues/34474">GH-34474</a>)</li>
  <li>Fix for converting non-sequence object in <code class="language-plaintext highlighter-rouge">pa.array()</code> (<a href="https://github.com/apache/arrow/issues/34944">GH-34944</a>)</li>
  <li>Fix erroneous table conversion to pandas if table includes an extension array that does not implement <code class="language-plaintext highlighter-rouge">to_pandas_dtype</code> (<a href="https://github.com/apache/arrow/issues/34906">GH-34906</a>)</li>
  <li>Reading from a closed <code class="language-plaintext highlighter-rouge">ArrayStreamBatchReader</code> now returns invalid status instead of segfaulting (<a href="https://github.com/apache/arrow/issues/34165">GH-34165</a>)</li>
  <li><code class="language-plaintext highlighter-rouge">array()</code> now returns <code class="language-plaintext highlighter-rouge">pyarrow.Array</code> and not <code class="language-plaintext highlighter-rouge">pyarrow.ChunkedArray</code> for columns with <code class="language-plaintext highlighter-rouge">__arrow_array__</code> method and only one chunk so that the conversion of pandas dataframe with categorical column of dtype <code class="language-plaintext highlighter-rouge">string[pyarrow]</code> does not fail (<a href="https://github.com/apache/arrow/issues/33727">GH-33727</a>)</li>
  <li>Custom type mapper in <code class="language-plaintext highlighter-rouge">to_pandas</code> now converts index dtypes together with column dtypes (<a href="https://github.com/apache/arrow/issues/34283">GH-34283</a>)</li>
</ul>

<h2 id="r-notes">R notes</h2>

<ul>
  <li>The <code class="language-plaintext highlighter-rouge">read_parquet()</code> and <code class="language-plaintext highlighter-rouge">read_feather()</code> functions can now accept URL
arguments.</li>
  <li>The <code class="language-plaintext highlighter-rouge">json_credentials</code> argument in <code class="language-plaintext highlighter-rouge">GcsFileSystem$create()</code> now accepts
a file path containing the appropriate authentication token.</li>
  <li>The <code class="language-plaintext highlighter-rouge">$options</code> member of <code class="language-plaintext highlighter-rouge">GcsFileSystem</code> objects can now be inspected.</li>
  <li>The <code class="language-plaintext highlighter-rouge">read_csv_arrow()</code> and <code class="language-plaintext highlighter-rouge">read_json_arrow()</code> functions now accept literal text input wrapped in
<code class="language-plaintext highlighter-rouge">I()</code> to improve compatability with <code class="language-plaintext highlighter-rouge">readr::read_csv()</code>.</li>
  <li>Nested fields can now be accessed using <code class="language-plaintext highlighter-rouge">$</code> and <code class="language-plaintext highlighter-rouge">[[</code> in dplyr expressions.</li>
</ul>

<p>For more on what’s in the 12.0.0 R package, see the <a href="/docs/r/news/">R changelog</a>.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<ul>
  <li>Flight SQL: Added support for authentication. <a href="https://github.com/apache/arrow/issues/34074">GH-34074</a></li>
  <li>Compute: Added <code class="language-plaintext highlighter-rouge">GArrowRankOptions</code>. <a href="https://github.com/apache/arrow/issues/34425">GH-34425</a></li>
  <li>Compute: Added <code class="language-plaintext highlighter-rouge">GArrowFilterOptions</code>. <a href="https://github.com/apache/arrow/issues/34650">GH-34650</a></li>
  <li>Compute: Added <code class="language-plaintext highlighter-rouge">GArrowIndexOptions</code>. <a href="https://github.com/apache/arrow/issues/15286">GH-15286</a></li>
  <li>Compute: Added <code class="language-plaintext highlighter-rouge">GArrowMatchSubstringOptions</code>. <a href="https://github.com/apache/arrow/issues/15285">GH-15285</a></li>
  <li>Added <code class="language-plaintext highlighter-rouge">GArrowDenseUnionArrayBuilder</code>. <a href="https://github.com/apache/arrow/issues/21429">GH-21429</a></li>
  <li>Added <code class="language-plaintext highlighter-rouge">GArrowSparseUnionArrayBuilder</code>. <a href="https://github.com/apache/arrow/issues/21430">GH-21430</a></li>
</ul>

<h3 id="ruby-notes">Ruby notes</h3>

<ul>
  <li>Improved <code class="language-plaintext highlighter-rouge">Arrow::Table#join</code> API. <a href="https://github.com/apache/arrow/issues/15287">GH-15287</a></li>
  <li>Flight SQL: Added <code class="language-plaintext highlighter-rouge">ArrowFlight::RecordBatchReader#each</code>. <a href="https://github.com/apache/arrow/issues/15287">GH-15287</a></li>
  <li>Added <code class="language-plaintext highlighter-rouge">Arrow::DenseUnionArray#get_value</code>. <a href="https://github.com/apache/arrow/issues/34837">GH-34837</a></li>
  <li>Added <code class="language-plaintext highlighter-rouge">Arrow::SparseUnionArray#get_value</code>. <a href="https://github.com/apache/arrow/issues/34837">GH-34837</a></li>
  <li>Expression: Added support for
<code class="language-plaintext highlighter-rouge">table.slice {|slicer| slicer.column.match_substring(string)</code> and
related shortcuts. <a href="https://github.com/apache/arrow/issues/34819">GH-34819</a> <a href="https://github.com/apache/arrow/issues/34951">GH-34951</a></li>
  <li><em>Breaking change:</em> <code class="language-plaintext highlighter-rouge">Arrow::Table#slice</code> with a filter removes null
records. <a href="https://github.com/apache/arrow/issues/34953">GH-34953</a></li>
</ul>

<h2 id="rust-notes">Rust notes</h2>

<p>The Rust projects have moved to separate repositories outside the
main Arrow monorepo. For notes on the latest release of the Rust
implementation, see the latest <a href="https://github.com/apache/arrow-rs/tags">Arrow Rust changelog</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 12.0.0 release. This covers over 3 months of development work and includes 476 resolved issues with 531 commits from 97 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 11.0.0 release, Wang Mingming, Mustafa Akur and Ruihang Xia have been invited to be committers. Will Jones have joined the Project Management Committee (PMC). Thanks for your contributions and participation in the project! Columnar Format Notes A first “canonical” extension type has been formalized: arrow.fixed_shape_tensor to represent an Array where each slot contains a tensor, with all tensors having the same dimension and shape, GH-33923. This is based on a Fixed-Size List layout as storage array (https://arrow.apache.org/docs/dev/format/CanonicalExtensions.html#fixed-shape-tensor-extension). Arrow Flight RPC notes The JDBC driver for Arrow Flight SQL has had some bugfixes, and has been refactored into a core library (which is not distributed as an uberjar with shaded names) and a driver (which is distributed as an uberjar). The Java server builder API now offers easier access to the underlying gRPC builder. Go now implements the Flight SQL extensions for Substrait and transaction support. Plasma notes Plasma was deprecated since 10.0.0. Plasma is removed in this release. GH-33243 Linux packages notes We dropped support for Ubuntu 18.04 because Ubuntu 18.04 reached EOL. C++ notes Run-End Encoded Arrays have been implemented and are accessible (GH-32104) The FixedShapeTensor Logical value type has been implemented using ExtensionType (GH-15483, GH-34796) Compute New kernel to convert timestamp with timezone to wall time (GH-33143) Cast kernels are now built into libarrow by default (GH-34388) Acero Acero has been moved out of libarrow into it’s own shared library, allowing for smaller builds of the core libarrow (GH-15280) Exec nodes now can have a concept of “ordering” and will reject non-sensible plans (GH-34136) New exec nodes: “pivot_longer” (GH-34266), “order_by” (GH-34248) and “fetch” (GH-34059) Breaking Change: Reorder output fields of “group_by” node so that keys/segment keys come before aggregates (GH-33616) Substrait Add support for the round function GH-33588 Add support for the cast expression element GH-31910 Added API reference documentation GH-34011 Added an extension relation to support segmented aggregation GH-34626 The output of the aggregate relation now conforms to the spec GH-34786 Parquet Added support for DeltaLengthByteArray encoding to the Parquet writer (GH-33024) NaNs are correctly handled now for Parquet predicate push-downs (GH-18481) Added support for reading Parquet page indexes (GH-33596) and writing page indexes (GH-34053) Parquet writer can write columns in parallel now (GH-33655) Fixed incorrect number of rows in Parquet V2 page headers (GH-34086) Fixed incorrect Parquet page null_count when stats are disabled (GH-34326) Added support for reading BloomFilters to the Parquet Reader (GH-34665) Parquet File-writer can now add additional key-value metadata after it has been opened (GH-34888) Breaking Change: The default row group size for the Arrow writer changed from 64Mi rows to 1Mi rows. GH-34280 ORC Added support for the union type in ORC writer (GH-34262) Fixed ORC CHAR type mapping with Arrow (GH-34823) Fixed timestamp type mapping between ORC and arrow (GH-34590) Datasets Added support for reading JSON datasets (GH-33209) Dataset writer now supports specifying a function callback to construct the file name in addition to the existing file name template (GH-34565) Filesystems GcsFileSystem::OpenInputFile avoids unnecessary downloads (GH-34051) Other changes Convenience Append(std::optional...) methods have been added to array builders ([GH-14863](https://github.com/apache/arrow/issues/14863)) A deprecated OpenTelemetry header was removed from the Flight library (GH-34417) Fixed crash in “take” kernels on ExtensionArrays with an underlying dictionary type (GH-34619) Fixed bug where the C-Data bridge did not preserve nullability of map values on import (GH-34983) Added support for EqualOptions to RecordBatch::Equals (GH-34968) zstd dependency upgraded to v1.5.5 (GH-34899) Improved handling of “logical” nulls such as with union and RunEndEncoded arrays (GH-34361) Fixed incorrect handling of uncompressed body buffers in IPC reader, added IpcWriteOptions::min_space_savings for optional compression optimizations (GH-15102) C# notes Support added for importing / exporting schemas and types via the C data interface GH-34737 Support added for the half-float data type GH-25163 Schemas are now allowed to have multiple fields with the same name GH-34076 Added support for reading compressed IPC files GH-32240 Add [] operator to Schema GH-34119 Go notes Run-End Encoded Arrays have been added to the Golang implementation (GH-32104, GH-32946, GH-20407, GH-32949) The SQLite Flight SQL Example has been improved and you can now go get a simple SQLite Flight SQL Server mainprog using go get github.com/apache/arrow/go/v12/arrow/flight/flightsql/example/cmd/sqlite_flightsql_server (GH-33840) Fixed bug causing builds to fail with the noasm build tag (GH-34044) and added a CI test run that uses the noasm tag (GH-34055) Fixed issue allowing building on riscv64-freebsd (GH-34629) Fixed issue preventing building on 32-bit platforms (GH-35133) Arrow Fixed bug in C-Data handling of ArrowArrayStream.get_next when handling uninitialized ArrowArrays (GH-33767) Breaking Change: Added Err() method to RecordReader interface so that it can propagate errors (GH-33789) Fixed potential panic in C-Data API for misusing an invalid handle (GH-33864) A new cgo-based Allocator that does not depend on libarrow has been added to the memory package (GH-33901) CSV Reader and Writer now support Extension type arrays (GH-34334) Fixed bug preventing the reading of IPC streams/files with compression enabled but uncompressed buffers (GH-34385) Added interface which can be added to an ExtensionType to allow Builders to be created via NewBuilder, enabling easy building of nested fields containing extension types (GH-34453) Added utilities to perform Array diffing (GH-34790) Added SetColumn method to arrow.Record (GH-34832) Added GetValue method to arrow.Metadata (GH-34855) Added Pow method to decimal128.Num and decimal256.Num (GH-34863) Flight Fixed bug in StreamChunks for Flight SQL to correctly propagate to the gRPC client (GH-33717) Fixed issue that prevented compatibility with gRPC &lt; v1.45 (GH-33734) Added support to bind a RecordReader for supplying parameters to a Flight SQL Prepared statement (GH-33794) Prepared Statement methods for FlightSQL client now allows gRPC Call Options (GH-33867) FlightSQL Extensions have been implemented (for transactions and Substrait support) (GH-33935) A driver compatible with database/sql for FlightSQL has been added (GH-34332) Compute “run_end_encode” and “run_end_decode” functions added to compute functions (GH-20408) “unique” function added (GH-34171) Parquet pqarrow pkg now handles DICTIONARY fields natively (GH-33466) Fixed rare panic when writing list of 8 structs (GH-33600) Added support for pqarrow pkg to write LargeString and LargeBinary types (GH-33875) Fixed bug where pqarrow.NewSchemaManifest created the wrong field type for Array Object fields (GH-34101) Added support to Parquet Writer for Extension type Arrays (GH-34330) Java notes Update Java JNI modules to consider Arrow ACERO GH-34862 Ability to register additional GRPC services with FlightServer GH-34778 Allow sending custom headers/properties through Arrow Flight SQL JDBC GH-33874 JavaScript notes No changes. Python notes Compatibility notes: Plasma has been removed in this release (GH-33243). In addition, the deprecated serialization module in PyArrow was also removed (GH-29705). IPC (Inter-Process Communication) functionality of pyarrow or the standard library pickle should be used instead. The deprecated use_async keyword has been removed from the dataset module (GH-30774) Minimum Cython version to build PyArrow from source has been raised to 0.29.31 (GH-34933). In addition, PyArrow can now be compiled using Cython 3 (GH-34564). New features: A new pyarrow.acero module with initial bindings for the Acero execution engine has been added (GH-33976) A new canonical extension type for fixed shaped tensor data has been defined. This is exposed in PyArrow as the FixedShapeTensorType (GH-34882, GH-34956) Run-End Encoded arrays binding has been implemented (GH-34686, GH-34568) Method is_nan has been added to Array, ChunkedArray and Expression (GH-34154) Dataframe interchange protocol has been implemented for RecordBatch (GH-33926) Other improvements: Extension arrays can now be concatenated (GH-31868) get_partition_keys helper function is implemented in the dataset module to access the partitioning field’s key/value from the partition expression of a certain dataset fragment (GH-33825) PyArrow Array objects can now be accepted by the pa.array() constructor (GH-34411) The default row group size when writing parquet files has been changed (GH-34280) RecordBatch has the select() method implemented (GH-34359) New method drop_column on the pyarrow.Table supports passing a single column as a string (GH-33377) User-defined tabular functions, which are a user-functions implemented in Python that return a stateful stream of tabular data, are now also supported (GH-32916) Arrow Archery tool now includes linting of the Cython files (GH-31905) Breaking Change: Reorder output fields of “group_by” node so that keys/segment keys come before aggregates (GH-33616) Relevant bug fixes: Acero can now detect and raise an error in case a join operation needs too much bytes of key data (GH-34474) Fix for converting non-sequence object in pa.array() (GH-34944) Fix erroneous table conversion to pandas if table includes an extension array that does not implement to_pandas_dtype (GH-34906) Reading from a closed ArrayStreamBatchReader now returns invalid status instead of segfaulting (GH-34165) array() now returns pyarrow.Array and not pyarrow.ChunkedArray for columns with __arrow_array__ method and only one chunk so that the conversion of pandas dataframe with categorical column of dtype string[pyarrow] does not fail (GH-33727) Custom type mapper in to_pandas now converts index dtypes together with column dtypes (GH-34283) R notes The read_parquet() and read_feather() functions can now accept URL arguments. The json_credentials argument in GcsFileSystem$create() now accepts a file path containing the appropriate authentication token. The $options member of GcsFileSystem objects can now be inspected. The read_csv_arrow() and read_json_arrow() functions now accept literal text input wrapped in I() to improve compatability with readr::read_csv(). Nested fields can now be accessed using $ and [[ in dplyr expressions. For more on what’s in the 12.0.0 R package, see the R changelog. Ruby and C GLib notes Flight SQL: Added support for authentication. GH-34074 Compute: Added GArrowRankOptions. GH-34425 Compute: Added GArrowFilterOptions. GH-34650 Compute: Added GArrowIndexOptions. GH-15286 Compute: Added GArrowMatchSubstringOptions. GH-15285 Added GArrowDenseUnionArrayBuilder. GH-21429 Added GArrowSparseUnionArrayBuilder. GH-21430 Ruby notes Improved Arrow::Table#join API. GH-15287 Flight SQL: Added ArrowFlight::RecordBatchReader#each. GH-15287 Added Arrow::DenseUnionArray#get_value. GH-34837 Added Arrow::SparseUnionArray#get_value. GH-34837 Expression: Added support for table.slice {|slicer| slicer.column.match_substring(string) and related shortcuts. GH-34819 GH-34951 Breaking change: Arrow::Table#slice with a filter removes null records. GH-34953 Rust notes The Rust projects have moved to separate repositories outside the main Arrow monorepo. For notes on the latest release of the Rust implementation, see the latest Arrow Rust changelog.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Our journey at F5 with Apache Arrow (part 1)</title><link href="https://arrow.apache.org/blog/2023/04/11/our-journey-at-f5-with-apache-arrow-part-1/" rel="alternate" type="text/html" title="Our journey at F5 with Apache Arrow (part 1)" /><published>2023-04-11T00:00:00-04:00</published><updated>2023-04-11T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2023/04/11/our-journey-at-f5-with-apache-arrow-part-1</id><content type="html" xml:base="https://arrow.apache.org/blog/2023/04/11/our-journey-at-f5-with-apache-arrow-part-1/"><![CDATA[<!--

-->

<p>Apache Arrow is a technology widely adopted in big data, analytics, and machine learning applications. In this article, we share <a href="https://www.f5.com/">F5</a>’s experience with Arrow, specifically its application to telemetry, and the challenges we encountered while optimizing the OpenTelemetry protocol to significantly reduce bandwidth costs. The promising results we achieved inspired us to share our insights. This article specifically focuses on transforming relatively complex data structure from various formats into an efficient Arrow representation that optimizes both compression ratio, transport, and data processing. We also explore the trade-offs between different mapping and normalization strategies, as well as the nuances of streaming and batch communication using Arrow and Arrow Flight. Our benchmarks thus far have shown promising results, with compression ratio improvements ranging from 1.5x to 5x, depending on the data type (metrics, logs, traces), distribution, and compression algorithm. The approaches presented for addressing these challenges may be applicable to other Arrow domains as well. This article serves as the first installment in a two-part series.</p>

<h2 id="what-is-apache-arrow">What is Apache Arrow</h2>

<p><a href="https://arrow.apache.org/docs/index.html">Apache Arrow</a> is an open-source project offering a standardized, language-agnostic in-memory format for representing structured and semi-structured data. This enables data sharing and zero-copy data access between systems, eliminating the need for serialization and deserialization when exchanging datasets between varying CPU architectures and programming languages. Furthermore, Arrow libraries feature an extensive set of high-performance, parallel, and vectorized kernel functions designed for efficiently processing massive amounts of columnar data. These features make Arrow an appealing technology for big data processing, data transport, analytics, and machine learning applications. The growing number of <a href="https://arrow.apache.org/powered_by/">products and open-source projects</a> that have adopted Apache Arrow at their core or offer Arrow support reflects the widespread recognition and appreciation of its benefits (refer to this <a href="https://www.dremio.com/blog/apache-arrows-rapid-growth-over-the-years/">article</a> for an in-depth overview of the Arrow ecosystem and adoption). Over 11,000 GitHub users support this project, and 840+ are contributors who make this project an undeniable success.</p>

<p>Very often people ask about the differences between Arrow and <a href="https://parquet.apache.org/">Apache Parquet</a> or other columnar file formats. Arrow is designed and optimized for in-memory processing, while Parquet is tailored for disk-based storage. In reality, these technologies are complementary, with bridges existing between them to simplify interoperability. In both cases, data is represented in columns to optimize access, data locality and compressibility. However, the tradeoffs differ slightly. Arrow prioritizes data processing speed over the optimal data encoding. Complex encodings that don’t benefit from SIMD instruction sets are generally not natively supported by Arrow, unlike formats such as Parquet. Storing data in Parquet format and processing and transporting it in Arrow format has become a prevalent model within the big data community.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/row-vs-columnar.svg" width="100%" class="img-responsive" alt="Memory representations: row vs columnar data." />
  <figcaption>Fig 1: Memory representations: row vs columnar data.</figcaption>
</figure>

<p>Figure 1 illustrates the differences in memory representation between row-oriented and column-oriented approaches. The column-oriented approach groups data from the same column in a continuous memory area, which facilitates parallel processing (SIMD) and enhances compression performance.</p>

<h2 id="why-are-we-interested-in-apache-arrow">Why are we interested in Apache Arrow</h2>

<p>At <a href="https://www.f5.com/">F5</a>, we’ve adopted <a href="https://opentelemetry.io/">OpenTelemetry</a> (OTel) as the standard for all telemetry across our products, such as BIGIP and NGINX. These products may generate large volumes of metrics and logs for various reasons, from performance evaluation to forensic purposes. The data produced by these systems is typically centralized and processed in dedicated systems. Transporting and processing this data accounts for a significant portion of the cost associated with telemetry pipelines. In this context, we became interested in Apache Arrow. Instead of reinventing yet another telemetry solution, we decided to invest in the OpenTelemetry project, working on improvements to the protocol to significantly increase its efficiency with high telemetry data volumes. We collaborated with <a href="https://github.com/jmacd">Joshua MacDonald</a> from <a href="https://lightstep.com/">Lightstep</a> to integrate these optimizations into an <a href="https://github.com/open-telemetry/experimental-arrow-collector">experimental OTel collector</a> and are currently in discussions with the OTel technical committee to finalize a code <a href="https://github.com/open-telemetry/community/issues/1332">donation</a>.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/performance.svg" width="100%" class="img-responsive" alt="Performance improvement in the OpenTelemetry Arrow experimental project." />
  <figcaption>Fig 2: Performance improvement in the OpenTelemetry Arrow experimental project.</figcaption>
</figure>

<p>This project has been divided into two phases. The first phase, which is nearing completion, aims to enhance the protocol’s compression ratio. The second phase, planned for the future, focuses on improving end-to-end performance by incorporating Apache Arrow throughout all levels, eliminating the need for conversion between old and new protocols. The results so far are promising, with our benchmarks showing compression ratio improvements ranging from x1.5 to x5, depending on the data type (metrics, logs, traces), distribution, and compression algorithm. For the second phase, our estimates suggest that data processing acceleration could range from x2 to x12, again depending on the data’s nature and distribution. For more information, we encourage you to review the <a href="https://github.com/lquerel/oteps/blob/main/text/0156-columnar-encoding.md">specifications</a> and the <a href="https://github.com/f5/otel-arrow-adapter">reference implementation</a>.</p>

<p>Arrow relies on a schema to define the structure of data batches that it processes and transports. The subsequent sections will discuss various techniques that can be employed to optimize the creation of these schemas.</p>

<h2 id="how-to-leverage-arrow-to-optimize-network-transport-cost">How to leverage Arrow to optimize network transport cost</h2>

<p>Apache Arrow is a complex project with a rapidly evolving ecosystem, which can sometimes be overwhelming for newcomers. Fortunately the Arrow community has published three introductory articles <a href="https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/">1</a>, <a href="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/">2</a>, and <a href="https://arrow.apache.org/blog/2022/10/17/arrow-parquet-encoding-part-3/">3</a> that we recommend for those interested in exploring this technology.</p>

<p>This article primarily focuses on transforming data from an XYZ format into an efficient Arrow representation that optimizes both compression ratio and data processing. There are numerous approaches to this transformation, and we will examine how these methods can impact compression ratio, CPU usage, and memory consumption during the conversion process, among other factors.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/schema-optim-process.svg" width="100%" class="img-responsive" alt="Fig 3: Optimization process for the definition of an Arrow schema." />
  <figcaption>Fig 3: Optimization process for the definition of an Arrow schema.</figcaption>
</figure>

<p>The complexity of your initial model significantly impacts the Arrow mapping choices you need to make. To begin, it’s essential to identify the properties you want to optimize for your specific context. Compression rate, conversion speed, memory consumption, speed and ease of use of the final model, compatibility, and extensibility are all factors that can influence your final mapping decisions. From there, you must explore multiple alternative schemas.</p>

<p>The choice of the Arrow type and data encoding for each individual field will affect the performance of your schema. There are various ways to represent hierarchical data or highly dynamic data models, and multiple options need to be evaluated in coordination with the configuration of the transport layer. This transport layer should also be carefully considered. Arrow supports compression mechanisms and dictionary deltas that may not be active by default.</p>

<p>After several iterations of this process, you should arrive at an optimized schema that meets the goals you initially set. It’s crucial to compare the performance of your different approaches using real data, as the distribution of data in each individual field may influence whether you use dictionary encoding or not. We will now examine these choices in greater detail throughout the remainder of this article.</p>

<h2 id="arrow-data-type-selection">Arrow data type selection</h2>

<p>The principles of selecting an Arrow data type are quite similar to those used when defining a data model for databases. Arrow supports a wide range of data types. Some of these types are supported by all implementations, while others are only available for languages with the strongest Arrow community support (see this <a href="https://arrow.apache.org/docs/status.html">page</a> for a comparison matrix of the different implementations). For primitive types, it is generally preferable to choose the type that offers the most concise representation and is closest to the semantics of your initial field. For example, while it’s possible to represent a timestamp with an int64, it’s more advantageous to use the native Arrow Timestamp type. This choice isn’t due to a more efficient binary representation, but rather because it will be easier to process and manipulate in your pipeline. Query engines such as <a href="https://github.com/apache/arrow-datafusion">DataFusion</a> offer dedicated timestamp handling functions for columns of this type. The same choices can be made for primitive types such as date, time, duration, and interval. However, if your project requires maximum compatibility, it may be crucial in some cases to favor types with universal support instead of the most optimal type in terms of memory occupation.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/data-types.svg" width="100%" class="img-responsive" alt="Fig 4: Data types supported by Apache Arrow." />
  <figcaption>Fig 4: Data types supported by Apache Arrow.</figcaption>
</figure>

<p>When selecting the Arrow data type, it’s important to consider the size of the data before and after compression. It’s quite possible that the size after compression is the same for two different types, but the actual size in memory may be two, four, or even eight times larger (e.g., uint8 vs. uint64). This difference will impact your ability to process large batches of data and will also significantly influence the speed of processing these data in memory (e.g., cache optimization, SIMD instruction efficiency).</p>

<p>It’s also possible to extend these types using an <a href="https://arrow.apache.org/docs/format/Columnar.html#extension-types">extension type</a> mechanism that builds upon one of the currently supported primitive types while adding specific semantics. This extension mechanism can simplify the use of this data in your own project, while remaining transparent to intermediate systems that will interpret this data as a basic primitive type.</p>

<p>There are some variations in the encoding of primitive types, which we will explore next.</p>

<h2 id="data-encoding">Data encoding</h2>

<p>Another crucial aspect of optimizing your Arrow schema is analyzing the cardinality of your data. Fields that can have only a limited number of values will typically be more efficiently represented with a dictionary encoding.</p>

<p>The maximum cardinality of a field determines the data type characteristics of your dictionary. For instance, for a field representing the status code of an HTTP transaction, it’s preferable to use a dictionary with an index of type ‘uint8’ and a value of type ‘uint16’ (notation: ‘Dictionary&lt;uint8, uint16&gt;’). This consumes less memory because the main array will be of type ‘[]uint8’. Even if the range of possible values is greater than 255, as long as the number of distinct values does not exceed 255, the representation remains efficient. Similarly, the representation of a ‘user-agent’ will be more efficient with a dictionary of type ‘Dictionary&lt;uint16, string&gt;’ (see figure 5). In this case, the main array will be of type ‘uint16’, allowing a compact representation in memory and during transfers at the cost of an indirection during reverse conversion.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/dictionary-encoding.svg" width="90%" class="img-responsive" alt="Fig 5: Dictionary encoding." />
  <figcaption>Fig 5: Dictionary encoding.</figcaption>
</figure>

<p>Dictionary encoding is highly flexible in Apache Arrow, allowing the creation of encodings for any Arrow primitive type. The size of the indices can also be configured based on the context.</p>

<p>In general, it is advisable to use dictionaries in the following cases:</p>
<ul>
  <li>Representation of enumerations</li>
  <li>Representation of textual or binary fields with a high probability of having redundant values.</li>
  <li>Representation of fields with cardinalities known to be below 2^16 or 2^32.</li>
</ul>

<p>Sometimes, the cardinality of a field is not known a priori. For example, a proxy that transforms a data stream from a row-oriented format into a series of columnar-encoded batches (e.g., OpenTelemetry collector) may not be able to predict in advance whether a field will have a fixed number of distinct values. Two approaches are possible: 
1) a conservative approach using the largest data type (e.g., ‘int64’, ‘string’, etc., instead of dictionary), 
2) an adaptive approach that modifies the schema on the fly based on the observed cardinality of the field(s). In this second approach, without cardinality information, you can optimistically start by using a ‘Dictionary&lt;uint8, original-field-type&gt;’ dictionary, then detect a potential dictionary overflow during conversion, and change the schema to a ‘Dictionary&lt;uint16, original-field-type&gt;’ in case of an overflow. This technique of automatic management of dictionary overflows will be presented in greater detail in a future article.</p>

<p>Recent advancements in Apache Arrow include the implementation of <a href="https://arrow.apache.org/docs/format/Columnar.html#run-end-encoded-layout">run-end encoding</a>, a technique that efficiently represents data with sequences of repeated values. This encoding method is particularly beneficial for handling data sets containing long stretches of identical values, as it offers a more compact and optimized representation.</p>

<p>In conclusion, dictionary encoding not only occupies less space in memory and during transfers but also significantly improves the compression ratio and data processing speed. However, this type of representation requires indirection when extracting the initial values (although this isn’t always necessary, even during some data processing operations). Additionally, it is important to manage dictionary index overflow, especially when the encoded field doesn’t have a well-defined cardinality.</p>

<h2 id="hierarchical-data">Hierarchical data</h2>

<p>Basic hierarchical data structures translate relatively well into Arrow. However, as we will see, there are some complications to handle in more  general cases (see figure 6). While Arrow schemas do support nested structures, maps, and unions, some components of the Arrow ecosystem do not fully support them, which can make these Arrow data types unsuitable for certain scenarios. Additionally, unlike most languages and formats, such as Protobuf, Arrow doesn’t support the concept of a recursively defined schema. An Arrow schema is static in its definition, and the depth of its nested elements must be known in advance. There are multiple strategies to work around this limitation and we’ll explore these in the following sections.</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/simple-vs-complex-data-model.svg" width="100%" class="img-responsive" alt="Fig 6: simple vs complex data model." />
  <figcaption>Fig 6: simple vs complex data model.</figcaption>
</figure>

<h3 id="natural-representation">Natural representation</h3>

<p>The most straightforward and intuitive approach to representing a simple hierarchical data model is to use Arrow’s list, map, and union data types. However, it’s important to note that some of these data types are not fully supported throughout the entire Arrow ecosystem. For example, the conversion of unions to Parquet is <a href="https://issues.apache.org/jira/browse/PARQUET-756">not directly supported</a> and requires a transformation step (see <a href="https://docs.google.com/document/d/11lG7Go2IgKOyW-RReBRW6r7HIdV1X7lu5WrDGlW5LbQ/edit#heading=h.nlplaj34c4ke">denormalization &amp; flattening representation</a> to decompose a sparse union into a nullable struct and type ids column). Similarly, lists and maps are <a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html">not yet supported</a> in DataFusion version 20 (nested structures are partially supported).</p>

<figure style="text-align: center;">
  <img src="/img/journey-apache-arrow/hierarchical-data-model.svg" width="80%" class="img-responsive" alt="Fig 7: initial data model." />
  <figcaption>Fig 7: initial data model.</figcaption>
</figure>

<p>The following example is a Go program snippet of an Arrow schema using these different data types to represent the model above.</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="s">"github.com/apache/arrow/go/v11/arrow"</span>


<span class="k">const</span> <span class="p">(</span>
  <span class="n">GaugeMetricCode</span> <span class="n">arrow</span><span class="o">.</span><span class="n">UnionTypeCode</span> <span class="o">=</span> <span class="m">0</span>
  <span class="n">SumMetricCode</span>   <span class="n">arrow</span><span class="o">.</span><span class="n">UnionTypeCode</span> <span class="o">=</span> <span class="m">1</span>
<span class="p">)</span>


<span class="k">var</span> <span class="p">(</span>
  <span class="c">// uint8Dictionary represent a Dictionary&lt;Uint8, String&gt;</span>
  <span class="n">uint8Dictionary</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">DictionaryType</span><span class="p">{</span>
     <span class="n">IndexType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint8</span><span class="p">,</span>
     <span class="n">ValueType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span>
  <span class="p">}</span>
  <span class="c">// uint16Dictionary represent a Dictionary&lt;Uint16, String&gt;</span>
  <span class="n">uint16Dictionary</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">arrow</span><span class="o">.</span><span class="n">DictionaryType</span><span class="p">{</span>
     <span class="n">IndexType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">PrimitiveTypes</span><span class="o">.</span><span class="n">Uint16</span><span class="p">,</span>
     <span class="n">ValueType</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BinaryTypes</span><span class="o">.</span><span class="n">String</span><span class="p">,</span>
  <span class="p">}</span>


  <span class="n">Schema</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">.</span><span class="n">NewSchema</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
     <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"resource_metrics"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">ListOf</span><span class="p">(</span><span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"scope"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
           <span class="c">// Name and Version are declared as dictionaries (Dictionary&lt;Uint16, String&gt;)).</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"version"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
        <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">ListOf</span><span class="p">(</span><span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"unit"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint8Dictionary</span><span class="p">},</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"timestamp"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">TIMESTAMP</span><span class="p">},</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metric_type"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">UINT8</span><span class="p">},</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"data_point"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">ListOf</span><span class="p">(</span><span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
              <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metric"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">DenseUnionOf</span><span class="p">(</span>
                 <span class="p">[]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
                    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"gauge"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
                       <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"data_point"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FLOAT64</span><span class="p">},</span>
                    <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
                    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"sum"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
                       <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"data_point"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FLOAT64</span><span class="p">},</span>
                       <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"is_monotonic"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BOOL</span><span class="p">},</span>
                    <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
                 <span class="p">},</span>
                 <span class="p">[]</span><span class="n">arrow</span><span class="o">.</span><span class="n">UnionTypeCode</span><span class="p">{</span><span class="n">GaugeMetricCode</span><span class="p">,</span> <span class="n">SumMetricCode</span><span class="p">},</span>
              <span class="p">)},</span>
           <span class="p">}</span><span class="o">...</span><span class="p">))},</span>
        <span class="p">}</span><span class="o">...</span><span class="p">))},</span>
     <span class="p">}</span><span class="o">...</span><span class="p">))},</span>
  <span class="p">},</span> <span class="no">nil</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div>

<p>In this pattern, we use a union type to represent an inheritance relationship. There are two types of Arrow union that are optimized for different cases. The dense union type has a relatively succinct memory representation but doesn’t support vectorizable operations, making it less efficient during the processing phase. Conversely, a sparse union supports vectorization operations, but comes with a memory overhead directly proportional to the number of variants in the union. Dense and sparse unions have quite similar compression rates, with sometimes a slight advantage for sparse unions. In addition, sparse unions with a large number of variants should generally be avoided, as they can lead to excessive memory consumption. For more details on the memory representation of unions, you can consult this <a href="https://wesm.github.io/arrow-site-test/format/Layout.html#dense-union-type">page</a>.</p>

<p>In certain scenarios, it may be more idiomatic to represent the inheritance relationship using multiple schemas (i.e., one schema per subtype), thereby avoiding the use of the union type. However, applying this approach to the aforementioned model may not be optimal, as the data preceding the inheritance relationship (i.e., <code class="language-plaintext highlighter-rouge">ResourceMetrics</code>, <code class="language-plaintext highlighter-rouge">Scope</code>, and <code class="language-plaintext highlighter-rouge">Metrics</code>) could potentially be duplicated numerous times. If the relationships between <code class="language-plaintext highlighter-rouge">ResourceMetrics</code>, <code class="language-plaintext highlighter-rouge">Metrics</code>, and <code class="language-plaintext highlighter-rouge">DataPoint</code> were 0..1 (zero-to-one) relationships, then the multi-schema approach would likely be the simplest and most idiomatic solution.</p>

<h3 id="denormalization--flattening-representations">Denormalization &amp; Flattening representations</h3>

<p>If the <code class="language-plaintext highlighter-rouge">List</code> type is not supported in your telemetry pipeline, you can denormalize your data model. This process is often used in the database world to remove a join between two tables for optimization purposes. In the Arrow world, denormalization is employed to eliminate the <code class="language-plaintext highlighter-rouge">List</code> type by duplicating some data. Once transformed, the previous Arrow schema becomes.</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Schema</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">.</span><span class="n">NewSchema</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"resource_metrics"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
     <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"scope"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
        <span class="c">// Name and Version are declared as dictionaries (Dictionary&lt;Uint16, String&gt;)).</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"version"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
     <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
     <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"unit"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint8Dictionary</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"timestamp"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">TIMESTAMP</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metric_type"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">UINT8</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"data_point"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metric"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">DenseUnionOf</span><span class="p">(</span>
              <span class="p">[]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
                 <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"gauge"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
                    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"value"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FLOAT64</span><span class="p">},</span>
                 <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
                 <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"sum"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
                    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"value"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FLOAT64</span><span class="p">},</span>
                    <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"is_monotonic"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BOOL</span><span class="p">},</span>
                 <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
              <span class="p">},</span>
              <span class="p">[]</span><span class="n">arrow</span><span class="o">.</span><span class="n">UnionTypeCode</span><span class="p">{</span><span class="n">GaugeMetricCode</span><span class="p">,</span> <span class="n">SumMetricCode</span><span class="p">},</span>
           <span class="p">)},</span>
        <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
     <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
  <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
<span class="p">},</span> <span class="no">nil</span><span class="p">)</span>
</code></pre></div></div>

<p>List types are eliminated at all levels. The initial semantics of the model are preserved by duplicating the data of the levels below each data point value. The memory representation will generally be much larger than the previous one, but a query engine that does not support the <code class="language-plaintext highlighter-rouge">List</code> type will still be able to process this data. Interestingly, once compressed, this way of representing data may not necessarily be larger than the previous approach. This is because the columnar representation compresses very well when there is redundancy in the data.</p>

<p>If the union type is not supported by some components of your pipeline, it is also possible to eliminate them by merging the union variants (the nested structure ‘metric’ is removed, see below).</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Schema</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">.</span><span class="n">NewSchema</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"resource_metrics"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
     <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"scope"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
        <span class="c">// Name and Version are declared as dictionaries (Dictionary&lt;Uint16, String&gt;)).</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"version"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
     <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
     <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"unit"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint8Dictionary</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"timestamp"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">TIMESTAMP</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metric_type"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">UINT8</span><span class="p">},</span>
        <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"data_point"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">StructOf</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"value"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FLOAT64</span><span class="p">},</span>
           <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"is_monotonic"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BOOL</span><span class="p">},</span>
        <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
     <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
  <span class="p">}</span><span class="o">...</span><span class="p">)},</span>
<span class="p">},</span> <span class="no">nil</span><span class="p">)</span>
</code></pre></div></div>

<p>The final schema has evolved into a series of nested structures, where the fields of the union variants are merged into one structure. The trade-off of this approach is similar to that of sparse union - the more variants, the higher the memory occupation. Arrow supports the concept of bitmap validity to identify null values (1 bit per entry) for various data types, including those that do not have a unique null representation (e.g., primitive types). The use of bitmap validity makes the query part easier, and query engines such as DataFusion know how to use if efficiently, Columns with numerous nulls typically compress quite efficiently since the underlying arrays are generally initialized with 0’s. Upon compression, these extensive sequences of 0’s result in high compression efficiency, despite the memory overhead before compression in the case of sparse unions. Consequently, it is essential to select the appropriate trade-off based on your specific context.</p>

<p>In some extreme situations where nested structures are not supported, a flattening approach can be used to address this problem.</p>

<div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Schema</span> <span class="o">=</span> <span class="n">arrow</span><span class="o">.</span><span class="n">NewSchema</span><span class="p">([]</span><span class="n">arrow</span><span class="o">.</span><span class="n">Field</span><span class="p">{</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"scope_name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"scope_version"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics_name"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint16Dictionary</span><span class="p">},</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics_unit"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">uint8Dictionary</span><span class="p">},</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics_timestamp"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">TIMESTAMP</span><span class="p">},</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics_metric_type"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">UINT8</span><span class="p">},</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics_data_point_value"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">FLOAT64</span><span class="p">},</span>
  <span class="p">{</span><span class="n">Name</span><span class="o">:</span> <span class="s">"metrics_data_point_is_monotonic"</span><span class="p">,</span> <span class="n">Type</span><span class="o">:</span> <span class="n">arrow</span><span class="o">.</span><span class="n">BOOL</span><span class="p">},</span>
<span class="p">},</span> <span class="no">nil</span><span class="p">)</span>
</code></pre></div></div>

<p>The terminal fields (leaves) are renamed by concatenating the names of the parent structures to provide proper scoping. This type of structure is supported by all components of the Arrow ecosystem. This approach can be useful if compatibility is a crucial criterion for your system. However, it shares the same drawbacks as other alternative denormalization models.</p>

<p>The Arrow ecosystem is evolving rapidly, so it is likely that support for List, Map, and Union data types in query engines will improve quickly. If kernel functions are sufficient or preferable for your application, it is usually possible to utilize these nested types.</p>

<h3 id="adaptivedynamic-representation">Adaptive/Dynamic representation</h3>

<p>Some data models can be more challenging to translate into an Arrow schema, such as the following Protobuf example. In this example, a collection of attributes is added to each data point. These attributes are defined using a recursive definition that most languages and formats, like Protobuf, support (see the ‘AnyValue’ definition below). Unfortunately, Arrow (like most classical database schemas) does not support such recursive definition within schemas.</p>

<div class="language-protobuf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">syntax</span> <span class="o">=</span> <span class="s">"proto3"</span><span class="p">;</span>


<span class="kd">message</span> <span class="nc">Metric</span> <span class="p">{</span>
 <span class="kd">message</span> <span class="nc">DataPoint</span> <span class="p">{</span>
   <span class="k">repeated</span> <span class="n">Attribute</span> <span class="na">attributes</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
   <span class="k">oneof</span> <span class="n">value</span> <span class="p">{</span>
     <span class="kt">int64</span> <span class="na">int_value</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
     <span class="kt">double</span> <span class="na">double_value</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
   <span class="p">}</span>
 <span class="p">}</span>


 <span class="kd">enum</span> <span class="n">MetricType</span> <span class="p">{</span>
   <span class="na">UNSPECIFIED</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
   <span class="na">GAUGE</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
   <span class="na">SUM</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
 <span class="p">}</span>


 <span class="kd">message</span> <span class="nc">Gauge</span> <span class="p">{</span>
   <span class="n">DataPoint</span> <span class="na">data_point</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
 <span class="p">}</span>


 <span class="kd">message</span> <span class="nc">Sum</span> <span class="p">{</span>
   <span class="n">DataPoint</span> <span class="na">data_point</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
   <span class="kt">bool</span> <span class="na">is_monotonic</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
 <span class="p">}</span>


 <span class="kt">string</span> <span class="na">name</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
 <span class="kt">int64</span> <span class="na">timestamp</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
 <span class="kt">string</span> <span class="na">unit</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
 <span class="n">MetricType</span> <span class="na">type</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span>
 <span class="k">oneof</span> <span class="n">metric</span> <span class="p">{</span>
   <span class="n">Gauge</span> <span class="na">gauge</span> <span class="o">=</span> <span class="mi">5</span><span class="p">;</span>
   <span class="n">Sum</span> <span class="na">sum</span> <span class="o">=</span> <span class="mi">6</span><span class="p">;</span>
 <span class="p">}</span>
<span class="p">}</span>


<span class="kd">message</span> <span class="nc">Attribute</span> <span class="p">{</span>
 <span class="kt">string</span> <span class="na">name</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
 <span class="n">AnyValue</span> <span class="na">value</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
<span class="p">}</span>


<span class="c1">// Recursive definition of AnyValue. AnyValue can be a primitive value, a list</span>
<span class="c1">// of AnyValues, or a list of key-value pairs where the key is a string and</span>
<span class="c1">// the value is an AnyValue.</span>
<span class="kd">message</span> <span class="nc">AnyValue</span> <span class="p">{</span>
 <span class="kd">message</span> <span class="nc">ArrayValue</span> <span class="p">{</span>
   <span class="k">repeated</span> <span class="n">AnyValue</span> <span class="na">values</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
 <span class="p">}</span>
 <span class="kd">message</span> <span class="nc">KeyValueList</span> <span class="p">{</span>
   <span class="kd">message</span> <span class="nc">KeyValue</span> <span class="p">{</span>
     <span class="kt">string</span> <span class="na">key</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
     <span class="n">AnyValue</span> <span class="na">value</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
   <span class="p">}</span>
   <span class="k">repeated</span> <span class="n">KeyValue</span> <span class="na">values</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
 <span class="p">}</span>


 <span class="k">oneof</span> <span class="n">value</span> <span class="p">{</span>
   <span class="kt">int64</span> <span class="na">int_value</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
   <span class="kt">double</span> <span class="na">double_value</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
   <span class="kt">string</span> <span class="na">string_value</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
   <span class="n">ArrayValue</span> <span class="na">list_value</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span>
   <span class="n">KeyValueList</span> <span class="na">kvlist_value</span> <span class="o">=</span> <span class="mi">5</span><span class="p">;</span>
 <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>If the definition of the attributes were non-recursive, it would have been possible to directly translate them into an Arrow Map type.</p>

<p>To address this kind of issue and further optimize Arrow schema definitions, you can employ an adaptive and iterative method that automatically constructs the Arrow schema based on the data being translated. With this approach, fields are automatically dictionary-encoded according to their cardinalities, unused fields are eliminated, and recursive structures are represented in a specific manner. Another solution involves using a multi-schema approach, in which attributes are depicted in a separate Arrow Record, and the inheritance relation is represented by a self-referential relationship. These strategies will be covered in more depth in a future article. For those eager to learn more, the first method is utilized in the reference implementation of the <a href="https://github.com/f5/otel-arrow-adapter">OTel Arrow Adapter</a>.</p>

<h2 id="data-transport">Data transport</h2>

<p>Unlike to Protobuf, an Arrow schema is generally not known a priori by the two parties participating in an exchange. Before being able to exchange data in Arrow format, the sender must first communicate the schema to the receiver, as well as the contents of the dictionaries used in the data. Only after this initialization phase has been completed can the sender transmit batches of data in Arrow format. This process, known as <a href="https://wesmckinney.com/blog/arrow-streaming-columnar/">Arrow IPC Stream</a>, plays an essential role transporting Arrow data between systems. Several approaches can be employed to communicate these Arrow IPC Streams. The simplest method is to use <a href="https://arrow.apache.org/docs/format/Flight.html">Arrow Flight</a>, which encapsulates Arrow IPC streams in a gRPC-based protocol. However, it is also possible to use your own implementation for specific contexts. Regardless of the solution you choose, it is crucial to understand that the underlying protocol must be stateful to take full advantage of the Arrow IPC stream approach. To achieve the best compression rates, it is vital to send schemas and dictionaries only once in order to amortize the cost and minimize data redundancy between batches. This necessitates a transport that supports stream-oriented communications, such as gRPC.</p>

<p>Using a stateless protocol is possible for large batches because the overhead of the schema will be negligible compared to the compression gains achieved using dictionary encoding and columnar representation. However, dictionaries will have to be communicated for each batch, making this approach generally less efficient than a stream-oriented approach.</p>

<p>Arrow IPC Stream also supports the concept of “delta dictionaries,” which allows for further optimization of batch transport. When a batch adds data to an existing dictionary (at the sender’s end), Arrow IPC enables sending the delta dictionary followed by the batch that references it. On the receiver side, this delta is used to update the existing dictionary, eliminating the need to retransmit the entire dictionary when changes occur. This optimization is only possible with a stateful protocol.</p>

<p>To fully leverage the column-oriented format of Apache Arrow, it is essential to consider sorting and compression. If your data model is simple (i.e., flat) and has one or more columns representing a natural order for your data (e.g., timestamp), it might be beneficial to sort your data to optimize the final compression ratio. Before implementing this optimization, it is recommended to perform tests on real data since the benefits may vary. In any case, using a compression algorithm when sending your batches is advantageous. Arrow IPC generally supports the ZSTD compression algorithm, which strikes an excellent balance between speed and compression efficiency, especially for column-oriented data.</p>

<p>Lastly, some implementations (e.g., Arrow Go) are not configured by default to support delta dictionaries and compression algorithms. Therefore, it is crucial to ensure that your code employs these options to maximize data transport efficiency.</p>

<h2 id="experiments">Experiments</h2>

<p>If your initial data is complex, it is advisable to conduct your own experiments to optimize the Arrow representation according to your data and goals (e.g., optimizing the compression ratio or enhancing the query-ability of your data in Arrow format). In our case, we developed an overlay for Apache Arrow that enables us to carry out these experiments with ease, without having to deal with the intrinsic complexity of Arrow APIs. However, this comes at the expense of a slower conversion phase compared to using Arrow APIs directly. While this library is not currently public, it may become available if there is sufficient interest.</p>

<p>We also employed a “black box optimization” approach, which automatically finds the best combination to meet the objectives we aimed to optimize (refer to “<a href="https://cloud.google.com/blog/products/ai-machine-learning/optimize-your-applications-using-google-vertex-ai-vizier">Optimize your applications using Google Vertex AI Vizier</a>” for a description of this approach).</p>

<h2 id="conclusion-and-next-steps">Conclusion and next steps</h2>

<p>Essentially, the key concept behind Apache Arrow is that it eliminates the need for serialization and deserialization, enabling zero-copy data sharing. Arrow achieves this by defining a language-agnostic, in-memory format that remains consistent across various implementations. Consequently, raw memory bytes can be transmitted directly over a network without requiring any serialization or deserialization, significantly enhancing data processing efficiency.</p>

<p>Converting a data model to Apache Arrow necessitates adaptation and optimization work, as we have begun to describe in this article. Many parameters must be considered, and it is recommended to perform a series of experiments to validate the various choices made during this process.</p>

<p>Handling highly dynamic data with Arrow can be challenging. Arrow requires the definition of a static schema, which can sometimes make representing this type of data complex or suboptimal, especially when the initial schema contains recursive definitions. This article has discussed several approaches to address this issue. The next article will be dedicated to a hybrid strategy that involves adapting the Arrow schema on-the-fly to optimize memory usage, compression ratio, and processing speed based on the data being represented. This approach is quite unique and deserves a separate article.</p>]]></content><author><name>Laurent Quérel</name></author><category term="application" /><summary type="html"><![CDATA[Apache Arrow is a technology widely adopted in big data, analytics, and machine learning applications. In this article, we share F5’s experience with Arrow, specifically its application to telemetry, and the challenges we encountered while optimizing the OpenTelemetry protocol to significantly reduce bandwidth costs. The promising results we achieved inspired us to share our insights. This article specifically focuses on transforming relatively complex data structure from various formats into an efficient Arrow representation that optimizes both compression ratio, transport, and data processing. We also explore the trade-offs between different mapping and normalization strategies, as well as the nuances of streaming and batch communication using Arrow and Arrow Flight. Our benchmarks thus far have shown promising results, with compression ratio improvements ranging from 1.5x to 5x, depending on the data type (metrics, logs, traces), distribution, and compression algorithm. The approaches presented for addressing these challenges may be applicable to other Arrow domains as well. This article serves as the first installment in a two-part series. What is Apache Arrow Apache Arrow is an open-source project offering a standardized, language-agnostic in-memory format for representing structured and semi-structured data. This enables data sharing and zero-copy data access between systems, eliminating the need for serialization and deserialization when exchanging datasets between varying CPU architectures and programming languages. Furthermore, Arrow libraries feature an extensive set of high-performance, parallel, and vectorized kernel functions designed for efficiently processing massive amounts of columnar data. These features make Arrow an appealing technology for big data processing, data transport, analytics, and machine learning applications. The growing number of products and open-source projects that have adopted Apache Arrow at their core or offer Arrow support reflects the widespread recognition and appreciation of its benefits (refer to this article for an in-depth overview of the Arrow ecosystem and adoption). Over 11,000 GitHub users support this project, and 840+ are contributors who make this project an undeniable success. Very often people ask about the differences between Arrow and Apache Parquet or other columnar file formats. Arrow is designed and optimized for in-memory processing, while Parquet is tailored for disk-based storage. In reality, these technologies are complementary, with bridges existing between them to simplify interoperability. In both cases, data is represented in columns to optimize access, data locality and compressibility. However, the tradeoffs differ slightly. Arrow prioritizes data processing speed over the optimal data encoding. Complex encodings that don’t benefit from SIMD instruction sets are generally not natively supported by Arrow, unlike formats such as Parquet. Storing data in Parquet format and processing and transporting it in Arrow format has become a prevalent model within the big data community. Fig 1: Memory representations: row vs columnar data. Figure 1 illustrates the differences in memory representation between row-oriented and column-oriented approaches. The column-oriented approach groups data from the same column in a continuous memory area, which facilitates parallel processing (SIMD) and enhances compression performance. Why are we interested in Apache Arrow At F5, we’ve adopted OpenTelemetry (OTel) as the standard for all telemetry across our products, such as BIGIP and NGINX. These products may generate large volumes of metrics and logs for various reasons, from performance evaluation to forensic purposes. The data produced by these systems is typically centralized and processed in dedicated systems. Transporting and processing this data accounts for a significant portion of the cost associated with telemetry pipelines. In this context, we became interested in Apache Arrow. Instead of reinventing yet another telemetry solution, we decided to invest in the OpenTelemetry project, working on improvements to the protocol to significantly increase its efficiency with high telemetry data volumes. We collaborated with Joshua MacDonald from Lightstep to integrate these optimizations into an experimental OTel collector and are currently in discussions with the OTel technical committee to finalize a code donation. Fig 2: Performance improvement in the OpenTelemetry Arrow experimental project. This project has been divided into two phases. The first phase, which is nearing completion, aims to enhance the protocol’s compression ratio. The second phase, planned for the future, focuses on improving end-to-end performance by incorporating Apache Arrow throughout all levels, eliminating the need for conversion between old and new protocols. The results so far are promising, with our benchmarks showing compression ratio improvements ranging from x1.5 to x5, depending on the data type (metrics, logs, traces), distribution, and compression algorithm. For the second phase, our estimates suggest that data processing acceleration could range from x2 to x12, again depending on the data’s nature and distribution. For more information, we encourage you to review the specifications and the reference implementation. Arrow relies on a schema to define the structure of data batches that it processes and transports. The subsequent sections will discuss various techniques that can be employed to optimize the creation of these schemas. How to leverage Arrow to optimize network transport cost Apache Arrow is a complex project with a rapidly evolving ecosystem, which can sometimes be overwhelming for newcomers. Fortunately the Arrow community has published three introductory articles 1, 2, and 3 that we recommend for those interested in exploring this technology. This article primarily focuses on transforming data from an XYZ format into an efficient Arrow representation that optimizes both compression ratio and data processing. There are numerous approaches to this transformation, and we will examine how these methods can impact compression ratio, CPU usage, and memory consumption during the conversion process, among other factors. Fig 3: Optimization process for the definition of an Arrow schema. The complexity of your initial model significantly impacts the Arrow mapping choices you need to make. To begin, it’s essential to identify the properties you want to optimize for your specific context. Compression rate, conversion speed, memory consumption, speed and ease of use of the final model, compatibility, and extensibility are all factors that can influence your final mapping decisions. From there, you must explore multiple alternative schemas. The choice of the Arrow type and data encoding for each individual field will affect the performance of your schema. There are various ways to represent hierarchical data or highly dynamic data models, and multiple options need to be evaluated in coordination with the configuration of the transport layer. This transport layer should also be carefully considered. Arrow supports compression mechanisms and dictionary deltas that may not be active by default. After several iterations of this process, you should arrive at an optimized schema that meets the goals you initially set. It’s crucial to compare the performance of your different approaches using real data, as the distribution of data in each individual field may influence whether you use dictionary encoding or not. We will now examine these choices in greater detail throughout the remainder of this article. Arrow data type selection The principles of selecting an Arrow data type are quite similar to those used when defining a data model for databases. Arrow supports a wide range of data types. Some of these types are supported by all implementations, while others are only available for languages with the strongest Arrow community support (see this page for a comparison matrix of the different implementations). For primitive types, it is generally preferable to choose the type that offers the most concise representation and is closest to the semantics of your initial field. For example, while it’s possible to represent a timestamp with an int64, it’s more advantageous to use the native Arrow Timestamp type. This choice isn’t due to a more efficient binary representation, but rather because it will be easier to process and manipulate in your pipeline. Query engines such as DataFusion offer dedicated timestamp handling functions for columns of this type. The same choices can be made for primitive types such as date, time, duration, and interval. However, if your project requires maximum compatibility, it may be crucial in some cases to favor types with universal support instead of the most optimal type in terms of memory occupation. Fig 4: Data types supported by Apache Arrow. When selecting the Arrow data type, it’s important to consider the size of the data before and after compression. It’s quite possible that the size after compression is the same for two different types, but the actual size in memory may be two, four, or even eight times larger (e.g., uint8 vs. uint64). This difference will impact your ability to process large batches of data and will also significantly influence the speed of processing these data in memory (e.g., cache optimization, SIMD instruction efficiency). It’s also possible to extend these types using an extension type mechanism that builds upon one of the currently supported primitive types while adding specific semantics. This extension mechanism can simplify the use of this data in your own project, while remaining transparent to intermediate systems that will interpret this data as a basic primitive type. There are some variations in the encoding of primitive types, which we will explore next. Data encoding Another crucial aspect of optimizing your Arrow schema is analyzing the cardinality of your data. Fields that can have only a limited number of values will typically be more efficiently represented with a dictionary encoding. The maximum cardinality of a field determines the data type characteristics of your dictionary. For instance, for a field representing the status code of an HTTP transaction, it’s preferable to use a dictionary with an index of type ‘uint8’ and a value of type ‘uint16’ (notation: ‘Dictionary&lt;uint8, uint16&gt;’). This consumes less memory because the main array will be of type ‘[]uint8’. Even if the range of possible values is greater than 255, as long as the number of distinct values does not exceed 255, the representation remains efficient. Similarly, the representation of a ‘user-agent’ will be more efficient with a dictionary of type ‘Dictionary&lt;uint16, string&gt;’ (see figure 5). In this case, the main array will be of type ‘uint16’, allowing a compact representation in memory and during transfers at the cost of an indirection during reverse conversion. Fig 5: Dictionary encoding. Dictionary encoding is highly flexible in Apache Arrow, allowing the creation of encodings for any Arrow primitive type. The size of the indices can also be configured based on the context. In general, it is advisable to use dictionaries in the following cases: Representation of enumerations Representation of textual or binary fields with a high probability of having redundant values. Representation of fields with cardinalities known to be below 2^16 or 2^32. Sometimes, the cardinality of a field is not known a priori. For example, a proxy that transforms a data stream from a row-oriented format into a series of columnar-encoded batches (e.g., OpenTelemetry collector) may not be able to predict in advance whether a field will have a fixed number of distinct values. Two approaches are possible: 1) a conservative approach using the largest data type (e.g., ‘int64’, ‘string’, etc., instead of dictionary), 2) an adaptive approach that modifies the schema on the fly based on the observed cardinality of the field(s). In this second approach, without cardinality information, you can optimistically start by using a ‘Dictionary&lt;uint8, original-field-type&gt;’ dictionary, then detect a potential dictionary overflow during conversion, and change the schema to a ‘Dictionary&lt;uint16, original-field-type&gt;’ in case of an overflow. This technique of automatic management of dictionary overflows will be presented in greater detail in a future article. Recent advancements in Apache Arrow include the implementation of run-end encoding, a technique that efficiently represents data with sequences of repeated values. This encoding method is particularly beneficial for handling data sets containing long stretches of identical values, as it offers a more compact and optimized representation. In conclusion, dictionary encoding not only occupies less space in memory and during transfers but also significantly improves the compression ratio and data processing speed. However, this type of representation requires indirection when extracting the initial values (although this isn’t always necessary, even during some data processing operations). Additionally, it is important to manage dictionary index overflow, especially when the encoded field doesn’t have a well-defined cardinality. Hierarchical data Basic hierarchical data structures translate relatively well into Arrow. However, as we will see, there are some complications to handle in more general cases (see figure 6). While Arrow schemas do support nested structures, maps, and unions, some components of the Arrow ecosystem do not fully support them, which can make these Arrow data types unsuitable for certain scenarios. Additionally, unlike most languages and formats, such as Protobuf, Arrow doesn’t support the concept of a recursively defined schema. An Arrow schema is static in its definition, and the depth of its nested elements must be known in advance. There are multiple strategies to work around this limitation and we’ll explore these in the following sections. Fig 6: simple vs complex data model. Natural representation The most straightforward and intuitive approach to representing a simple hierarchical data model is to use Arrow’s list, map, and union data types. However, it’s important to note that some of these data types are not fully supported throughout the entire Arrow ecosystem. For example, the conversion of unions to Parquet is not directly supported and requires a transformation step (see denormalization &amp; flattening representation to decompose a sparse union into a nullable struct and type ids column). Similarly, lists and maps are not yet supported in DataFusion version 20 (nested structures are partially supported). Fig 7: initial data model. The following example is a Go program snippet of an Arrow schema using these different data types to represent the model above. import "github.com/apache/arrow/go/v11/arrow" const ( GaugeMetricCode arrow.UnionTypeCode = 0 SumMetricCode arrow.UnionTypeCode = 1 ) var ( // uint8Dictionary represent a Dictionary&lt;Uint8, String&gt; uint8Dictionary = &amp;arrow.DictionaryType{ IndexType: arrow.PrimitiveTypes.Uint8, ValueType: arrow.BinaryTypes.String, } // uint16Dictionary represent a Dictionary&lt;Uint16, String&gt; uint16Dictionary = &amp;arrow.DictionaryType{ IndexType: arrow.PrimitiveTypes.Uint16, ValueType: arrow.BinaryTypes.String, } Schema = arrow.NewSchema([]arrow.Field{ {Name: "resource_metrics", Type: arrow.ListOf(arrow.StructOf([]arrow.Field{ {Name: "scope", Type: arrow.StructOf([]arrow.Field{ // Name and Version are declared as dictionaries (Dictionary&lt;Uint16, String&gt;)). {Name: "name", Type: uint16Dictionary}, {Name: "version", Type: uint16Dictionary}, }...)}, {Name: "metrics", Type: arrow.ListOf(arrow.StructOf([]arrow.Field{ {Name: "name", Type: uint16Dictionary}, {Name: "unit", Type: uint8Dictionary}, {Name: "timestamp", Type: arrow.TIMESTAMP}, {Name: "metric_type", Type: arrow.UINT8}, {Name: "data_point", Type: arrow.ListOf(arrow.StructOf([]arrow.Field{ {Name: "metric", Type: arrow.DenseUnionOf( []arrow.Field{ {Name: "gauge", Type: arrow.StructOf([]arrow.Field{ {Name: "data_point", Type: arrow.FLOAT64}, }...)}, {Name: "sum", Type: arrow.StructOf([]arrow.Field{ {Name: "data_point", Type: arrow.FLOAT64}, {Name: "is_monotonic", Type: arrow.BOOL}, }...)}, }, []arrow.UnionTypeCode{GaugeMetricCode, SumMetricCode}, )}, }...))}, }...))}, }...))}, }, nil) ) In this pattern, we use a union type to represent an inheritance relationship. There are two types of Arrow union that are optimized for different cases. The dense union type has a relatively succinct memory representation but doesn’t support vectorizable operations, making it less efficient during the processing phase. Conversely, a sparse union supports vectorization operations, but comes with a memory overhead directly proportional to the number of variants in the union. Dense and sparse unions have quite similar compression rates, with sometimes a slight advantage for sparse unions. In addition, sparse unions with a large number of variants should generally be avoided, as they can lead to excessive memory consumption. For more details on the memory representation of unions, you can consult this page. In certain scenarios, it may be more idiomatic to represent the inheritance relationship using multiple schemas (i.e., one schema per subtype), thereby avoiding the use of the union type. However, applying this approach to the aforementioned model may not be optimal, as the data preceding the inheritance relationship (i.e., ResourceMetrics, Scope, and Metrics) could potentially be duplicated numerous times. If the relationships between ResourceMetrics, Metrics, and DataPoint were 0..1 (zero-to-one) relationships, then the multi-schema approach would likely be the simplest and most idiomatic solution. Denormalization &amp; Flattening representations If the List type is not supported in your telemetry pipeline, you can denormalize your data model. This process is often used in the database world to remove a join between two tables for optimization purposes. In the Arrow world, denormalization is employed to eliminate the List type by duplicating some data. Once transformed, the previous Arrow schema becomes. Schema = arrow.NewSchema([]arrow.Field{ {Name: "resource_metrics", Type: arrow.StructOf([]arrow.Field{ {Name: "scope", Type: arrow.StructOf([]arrow.Field{ // Name and Version are declared as dictionaries (Dictionary&lt;Uint16, String&gt;)). {Name: "name", Type: uint16Dictionary}, {Name: "version", Type: uint16Dictionary}, }...)}, {Name: "metrics", Type: arrow.StructOf([]arrow.Field{ {Name: "name", Type: uint16Dictionary}, {Name: "unit", Type: uint8Dictionary}, {Name: "timestamp", Type: arrow.TIMESTAMP}, {Name: "metric_type", Type: arrow.UINT8}, {Name: "data_point", Type: arrow.StructOf([]arrow.Field{ {Name: "metric", Type: arrow.DenseUnionOf( []arrow.Field{ {Name: "gauge", Type: arrow.StructOf([]arrow.Field{ {Name: "value", Type: arrow.FLOAT64}, }...)}, {Name: "sum", Type: arrow.StructOf([]arrow.Field{ {Name: "value", Type: arrow.FLOAT64}, {Name: "is_monotonic", Type: arrow.BOOL}, }...)}, }, []arrow.UnionTypeCode{GaugeMetricCode, SumMetricCode}, )}, }...)}, }...)}, }...)}, }, nil) List types are eliminated at all levels. The initial semantics of the model are preserved by duplicating the data of the levels below each data point value. The memory representation will generally be much larger than the previous one, but a query engine that does not support the List type will still be able to process this data. Interestingly, once compressed, this way of representing data may not necessarily be larger than the previous approach. This is because the columnar representation compresses very well when there is redundancy in the data. If the union type is not supported by some components of your pipeline, it is also possible to eliminate them by merging the union variants (the nested structure ‘metric’ is removed, see below). Schema = arrow.NewSchema([]arrow.Field{ {Name: "resource_metrics", Type: arrow.StructOf([]arrow.Field{ {Name: "scope", Type: arrow.StructOf([]arrow.Field{ // Name and Version are declared as dictionaries (Dictionary&lt;Uint16, String&gt;)). {Name: "name", Type: uint16Dictionary}, {Name: "version", Type: uint16Dictionary}, }...)}, {Name: "metrics", Type: arrow.StructOf([]arrow.Field{ {Name: "name", Type: uint16Dictionary}, {Name: "unit", Type: uint8Dictionary}, {Name: "timestamp", Type: arrow.TIMESTAMP}, {Name: "metric_type", Type: arrow.UINT8}, {Name: "data_point", Type: arrow.StructOf([]arrow.Field{ {Name: "value", Type: arrow.FLOAT64}, {Name: "is_monotonic", Type: arrow.BOOL}, }...)}, }...)}, }...)}, }, nil) The final schema has evolved into a series of nested structures, where the fields of the union variants are merged into one structure. The trade-off of this approach is similar to that of sparse union - the more variants, the higher the memory occupation. Arrow supports the concept of bitmap validity to identify null values (1 bit per entry) for various data types, including those that do not have a unique null representation (e.g., primitive types). The use of bitmap validity makes the query part easier, and query engines such as DataFusion know how to use if efficiently, Columns with numerous nulls typically compress quite efficiently since the underlying arrays are generally initialized with 0’s. Upon compression, these extensive sequences of 0’s result in high compression efficiency, despite the memory overhead before compression in the case of sparse unions. Consequently, it is essential to select the appropriate trade-off based on your specific context. In some extreme situations where nested structures are not supported, a flattening approach can be used to address this problem. Schema = arrow.NewSchema([]arrow.Field{ {Name: "scope_name", Type: uint16Dictionary}, {Name: "scope_version", Type: uint16Dictionary}, {Name: "metrics_name", Type: uint16Dictionary}, {Name: "metrics_unit", Type: uint8Dictionary}, {Name: "metrics_timestamp", Type: arrow.TIMESTAMP}, {Name: "metrics_metric_type", Type: arrow.UINT8}, {Name: "metrics_data_point_value", Type: arrow.FLOAT64}, {Name: "metrics_data_point_is_monotonic", Type: arrow.BOOL}, }, nil) The terminal fields (leaves) are renamed by concatenating the names of the parent structures to provide proper scoping. This type of structure is supported by all components of the Arrow ecosystem. This approach can be useful if compatibility is a crucial criterion for your system. However, it shares the same drawbacks as other alternative denormalization models. The Arrow ecosystem is evolving rapidly, so it is likely that support for List, Map, and Union data types in query engines will improve quickly. If kernel functions are sufficient or preferable for your application, it is usually possible to utilize these nested types. Adaptive/Dynamic representation Some data models can be more challenging to translate into an Arrow schema, such as the following Protobuf example. In this example, a collection of attributes is added to each data point. These attributes are defined using a recursive definition that most languages and formats, like Protobuf, support (see the ‘AnyValue’ definition below). Unfortunately, Arrow (like most classical database schemas) does not support such recursive definition within schemas. syntax = "proto3"; message Metric { message DataPoint { repeated Attribute attributes = 1; oneof value { int64 int_value = 2; double double_value = 3; } } enum MetricType { UNSPECIFIED = 0; GAUGE = 1; SUM = 2; } message Gauge { DataPoint data_point = 1; } message Sum { DataPoint data_point = 1; bool is_monotonic = 2; } string name = 1; int64 timestamp = 2; string unit = 3; MetricType type = 4; oneof metric { Gauge gauge = 5; Sum sum = 6; } } message Attribute { string name = 1; AnyValue value = 2; } // Recursive definition of AnyValue. AnyValue can be a primitive value, a list // of AnyValues, or a list of key-value pairs where the key is a string and // the value is an AnyValue. message AnyValue { message ArrayValue { repeated AnyValue values = 1; } message KeyValueList { message KeyValue { string key = 1; AnyValue value = 2; } repeated KeyValue values = 1; } oneof value { int64 int_value = 1; double double_value = 2; string string_value = 3; ArrayValue list_value = 4; KeyValueList kvlist_value = 5; } } If the definition of the attributes were non-recursive, it would have been possible to directly translate them into an Arrow Map type. To address this kind of issue and further optimize Arrow schema definitions, you can employ an adaptive and iterative method that automatically constructs the Arrow schema based on the data being translated. With this approach, fields are automatically dictionary-encoded according to their cardinalities, unused fields are eliminated, and recursive structures are represented in a specific manner. Another solution involves using a multi-schema approach, in which attributes are depicted in a separate Arrow Record, and the inheritance relation is represented by a self-referential relationship. These strategies will be covered in more depth in a future article. For those eager to learn more, the first method is utilized in the reference implementation of the OTel Arrow Adapter. Data transport Unlike to Protobuf, an Arrow schema is generally not known a priori by the two parties participating in an exchange. Before being able to exchange data in Arrow format, the sender must first communicate the schema to the receiver, as well as the contents of the dictionaries used in the data. Only after this initialization phase has been completed can the sender transmit batches of data in Arrow format. This process, known as Arrow IPC Stream, plays an essential role transporting Arrow data between systems. Several approaches can be employed to communicate these Arrow IPC Streams. The simplest method is to use Arrow Flight, which encapsulates Arrow IPC streams in a gRPC-based protocol. However, it is also possible to use your own implementation for specific contexts. Regardless of the solution you choose, it is crucial to understand that the underlying protocol must be stateful to take full advantage of the Arrow IPC stream approach. To achieve the best compression rates, it is vital to send schemas and dictionaries only once in order to amortize the cost and minimize data redundancy between batches. This necessitates a transport that supports stream-oriented communications, such as gRPC. Using a stateless protocol is possible for large batches because the overhead of the schema will be negligible compared to the compression gains achieved using dictionary encoding and columnar representation. However, dictionaries will have to be communicated for each batch, making this approach generally less efficient than a stream-oriented approach. Arrow IPC Stream also supports the concept of “delta dictionaries,” which allows for further optimization of batch transport. When a batch adds data to an existing dictionary (at the sender’s end), Arrow IPC enables sending the delta dictionary followed by the batch that references it. On the receiver side, this delta is used to update the existing dictionary, eliminating the need to retransmit the entire dictionary when changes occur. This optimization is only possible with a stateful protocol. To fully leverage the column-oriented format of Apache Arrow, it is essential to consider sorting and compression. If your data model is simple (i.e., flat) and has one or more columns representing a natural order for your data (e.g., timestamp), it might be beneficial to sort your data to optimize the final compression ratio. Before implementing this optimization, it is recommended to perform tests on real data since the benefits may vary. In any case, using a compression algorithm when sending your batches is advantageous. Arrow IPC generally supports the ZSTD compression algorithm, which strikes an excellent balance between speed and compression efficiency, especially for column-oriented data. Lastly, some implementations (e.g., Arrow Go) are not configured by default to support delta dictionaries and compression algorithms. Therefore, it is crucial to ensure that your code employs these options to maximize data transport efficiency. Experiments If your initial data is complex, it is advisable to conduct your own experiments to optimize the Arrow representation according to your data and goals (e.g., optimizing the compression ratio or enhancing the query-ability of your data in Arrow format). In our case, we developed an overlay for Apache Arrow that enables us to carry out these experiments with ease, without having to deal with the intrinsic complexity of Arrow APIs. However, this comes at the expense of a slower conversion phase compared to using Arrow APIs directly. While this library is not currently public, it may become available if there is sufficient interest. We also employed a “black box optimization” approach, which automatically finds the best combination to meet the objectives we aimed to optimize (refer to “Optimize your applications using Google Vertex AI Vizier” for a description of this approach). Conclusion and next steps Essentially, the key concept behind Apache Arrow is that it eliminates the need for serialization and deserialization, enabling zero-copy data sharing. Arrow achieves this by defining a language-agnostic, in-memory format that remains consistent across various implementations. Consequently, raw memory bytes can be transmitted directly over a network without requiring any serialization or deserialization, significantly enhancing data processing efficiency. Converting a data model to Apache Arrow necessitates adaptation and optimization work, as we have begun to describe in this article. Many parameters must be considered, and it is recommended to perform a series of experiments to validate the various choices made during this process. Handling highly dynamic data with Arrow can be challenging. Arrow requires the definition of a static schema, which can sometimes make representing this type of data complex or suboptimal, especially when the initial schema contains recursive definitions. This article has discussed several approaches to address this issue. The next article will be dedicated to a hybrid strategy that involves adapting the Arrow schema on-the-fly to optimize memory usage, compression ratio, and processing speed based on the data being represented. This approach is quite unique and deserves a separate article.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>