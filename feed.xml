<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://arrow.apache.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arrow.apache.org/" rel="alternate" type="text/html" /><updated>2022-10-30T21:38:11-04:00</updated><id>https://arrow.apache.org/feed.xml</id><title type="html">Apache Arrow</title><subtitle>Apache Arrow is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, C++, C#, Go, Java, JavaScript, MATLAB, Python, R, Ruby, and Rust.</subtitle><entry><title type="html">Apache Arrow DataFusion 13.0.0 Project Update</title><link href="https://arrow.apache.org/blog/2022/10/25/datafusion-13.0.0/" rel="alternate" type="text/html" title="Apache Arrow DataFusion 13.0.0 Project Update" /><published>2022-10-25T00:00:00-04:00</published><updated>2022-10-25T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/10/25/datafusion-13.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/10/25/datafusion-13.0.0/"><![CDATA[<!--

-->

<h1 id="introduction">Introduction</h1>

<p><a href="https://arrow.apache.org/datafusion/">Apache Arrow DataFusion</a> <a href="https://crates.io/crates/datafusion"><code class="language-plaintext highlighter-rouge">13.0.0</code></a> is released, and this blog contains an update on the project for the 5 months since our <a href="https://arrow.apache.org/blog/2022/05/16/datafusion-8.0.0/">last update in May 2022</a>.</p>

<p>DataFusion is an extensible and embeddable query engine, written in Rust used to create modern, fast and efficient data pipelines, ETL processes, and database systems. You may want to check out DataFusion to extend your Rust project to:</p>

<ul>
  <li>Support <a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html">SQL support</a></li>
  <li>Support <a href="https://docs.rs/datafusion/13.0.0/datafusion/dataframe/struct.DataFrame.html">DataFrame API</a></li>
  <li>Support a Domain Specific Query Language</li>
  <li>Easily and quickly read and process Parquet, JSON, Avro or CSV data.</li>
  <li>Read from remote object stores such as AWS S3, Azure Blob Storage, GCP.</li>
</ul>

<p>Even though DataFusion is 4 years “young,” it has seen significant community growth in the last few months and the momentum continues to accelerate.</p>

<h1 id="background">Background</h1>

<p>DataFusion is used as the engine in <a href="https://github.com/apache/arrow-datafusion#known-uses">many open source and commercial projects</a> and was one of the early open source projects to provide this capability. 2022 has validated our belief in the need for such a <a href="https://docs.google.com/presentation/d/1iNX_35sWUakee2q3zMFPyHE4IV2nC3lkCK_H6Y2qK84/edit#slide=id.p">“LLVM for database and AI systems”</a><a href="https://www.slideshare.net/AndrewLamb32/20220623-apache-arrow-and-datafusion-changing-the-game-for-implementing-database-systemspdf">(alternate link)</a> with announcements such as the <a href="https://engineering.fb.com/2022/08/31/open-source/velox/">release of FaceBook’s Velox</a> engine, the major investments in <a href="https://arrow.apache.org/docs/cpp/streaming_execution.html">Acero</a> as well as the continued popularity of <a href="https://calcite.apache.org/">Apache Calcite</a> and other similar technologies.</p>

<p>While Velox and Acero focus on execution engines, DataFusion provides the entire suite of components needed to build most analytic systems, including a SQL frontend, a dataframe API, and  extension points for just about everything. Some <a href="https://github.com/apache/arrow-datafusion#known-uses">DataFusion users</a> use a subset of the features such as the frontend (e.g. <a href="https://dask-sql.readthedocs.io/en/latest/">dask-sql</a>) or the execution engine, (e.g.  <a href="https://github.com/blaze-init/blaze">Blaze</a>), and some use many different components to build both SQL based and customized DSL based systems such as <a href="https://github.com/influxdata/influxdb_iox/pulls">InfluxDB IOx</a> and <a href="https://github.com/vegafusion/vegafusion">VegaFusion</a>.</p>

<p>One of DataFusion’s advantages is its implementation in <a href="https://www.rust-lang.org/">Rust</a> and thus its easy integration with the broader Rust ecosystem. Rust continues to be a major source of benefit, from the <a href="https://www.influxdata.com/blog/using-rustlangs-async-tokio-runtime-for-cpu-bound-tasks/">ease of parallelization with the high quality and standardized <code class="language-plaintext highlighter-rouge">async</code> ecosystem</a> , as well as its modern dependency management system and wonderful performance. <!-- I wonder if we should link to clickbench?? -->
<!--While we haven’t invested in the benchmarking ratings game datafusion continues to be quite speedy (todo quantity this, with some evidence) – maybe clickbench?--></p>

<!--
Maybe we can do this un a future post
# DataFusion in Action

While DataFusion really shines as an embeddable query engine, if you want to try it out and get a feel for its power, you can use the basic[`datafusion-cli`](https://docs.rs/datafusion-cli/13.0.0/datafusion_cli/) tool to get a sense for what is possible to add in your application

(TODO example here of using datafusion-cli to query from local parquet files on disk)

TODO: also mention you can use the same thing to query data from S3
-->

<h1 id="summary">Summary</h1>

<p>We have increased the frequency of DataFusion releases to monthly instead of quarterly. This
makes it easier for the increasing number of projects that now depend on DataFusion.</p>

<p>We have also completed the “graduation” of <a href="https://github.com/apache/arrow-ballista">Ballista to its own top-level arrow-ballista repository</a>
which decouples the two projects and allows each project to move even faster.</p>

<p>Along with numerous other bug fixes and smaller improvements, here are some of the major advances:</p>

<h1 id="improved-support-for-cloud-object-stores">Improved Support for Cloud Object Stores</h1>

<p>DataFusion now supports many major cloud object stores (Amazon S3, Azure Blob Storage, and Google Cloud Storage) “out of the box” via the <a href="https://crates.io/crates/object_store">object_store</a> crate. Using this integration, DataFusion optimizes reading parquet files by reading only the parts of the files that are needed.</p>

<h2 id="advanced-sql">Advanced SQL</h2>

<p>DataFusion now supports correlated subqueries, by rewriting them as joins. See the <a href="https://arrow.apache.org/datafusion/user-guide/sql/subqueries.html">Subquery</a> page in the User Guide for more information.</p>

<p>In addition to numerous other small improvements, the following SQL features are now supported:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ROWS</code>, <code class="language-plaintext highlighter-rouge">RANGE</code>, <code class="language-plaintext highlighter-rouge">PRECEDING</code> and <code class="language-plaintext highlighter-rouge">FOLLOWING</code> in <code class="language-plaintext highlighter-rouge">OVER</code> clauses <a href="https://github.com/apache/arrow-datafusion/issues/3570">#3570</a></li>
  <li><code class="language-plaintext highlighter-rouge">ROLLUP</code> and <code class="language-plaintext highlighter-rouge">CUBE</code> grouping set expressions  <a href="https://github.com/apache/arrow-datafusion/issues/2446">#2446</a></li>
  <li><code class="language-plaintext highlighter-rouge">SUM DISTINCT</code> aggregate support  <a href="https://github.com/apache/arrow-datafusion/issues/2405">#2405</a></li>
  <li><code class="language-plaintext highlighter-rouge">IN</code> and <code class="language-plaintext highlighter-rouge">NOT IN</code> Subqueries by rewriting them to <code class="language-plaintext highlighter-rouge">SEMI</code> / <code class="language-plaintext highlighter-rouge">ANTI</code> <a href="https://github.com/apache/arrow-datafusion/issues/2885">#2421</a></li>
  <li>Non equality predicates in  <code class="language-plaintext highlighter-rouge">ON</code> clause of  <code class="language-plaintext highlighter-rouge">LEFT</code>, <code class="language-plaintext highlighter-rouge">RIGHT, </code>and <code class="language-plaintext highlighter-rouge">FULL</code> joins <a href="https://github.com/apache/arrow-datafusion/issues/2591">#2591</a></li>
  <li>Exact <code class="language-plaintext highlighter-rouge">MEDIAN</code> <a href="https://github.com/apache/arrow-datafusion/issues/3009">#3009</a></li>
  <li><code class="language-plaintext highlighter-rouge">GROUPING SETS</code>/<code class="language-plaintext highlighter-rouge">CUBE</code>/<code class="language-plaintext highlighter-rouge">ROLLUP</code> <a href="https://github.com/apache/arrow-datafusion/issues/2716">#2716</a></li>
</ul>

<h1 id="more-ddl-support">More DDL Support</h1>

<p>Just as it is important to query, it is also important to give users the ability to define their data sources. We have added:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">CREATE VIEW</code> <a href="https://github.com/apache/arrow-datafusion/issues/2279">#2279</a></li>
  <li><code class="language-plaintext highlighter-rouge">DESCRIBE &lt;table&gt;</code> <a href="https://github.com/apache/arrow-datafusion/issues/2642">#2642</a></li>
  <li>Custom / Dynamic table provider factories <a href="https://github.com/apache/arrow-datafusion/issues/3311">#3311</a></li>
  <li><code class="language-plaintext highlighter-rouge">SHOW CREATE TABLE</code> for support for views <a href="https://github.com/apache/arrow-datafusion/issues/2830">#2830</a></li>
</ul>

<h1 id="faster-execution">Faster Execution</h1>
<p>Performance is always an important goal for DataFusion, and there are a number of significant new optimizations such as</p>

<ul>
  <li>Optimizations of TopK (queries with a <code class="language-plaintext highlighter-rouge">LIMIT</code> or <code class="language-plaintext highlighter-rouge">OFFSET</code> clause):  <a href="https://github.com/apache/arrow-datafusion/issues/3527">#3527</a>, <a href="https://github.com/apache/arrow-datafusion/issues/2521">#2521</a></li>
  <li>Reduce <code class="language-plaintext highlighter-rouge">left</code>/<code class="language-plaintext highlighter-rouge">right</code>/<code class="language-plaintext highlighter-rouge">full</code> joins to <code class="language-plaintext highlighter-rouge">inner</code> join <a href="https://github.com/apache/arrow-datafusion/issues/2750">#2750</a></li>
  <li>Convert  cross joins to inner joins when possible <a href="https://github.com/apache/arrow-datafusion/issues/3482">#3482</a></li>
  <li>Sort preserving <code class="language-plaintext highlighter-rouge">SortMergeJoin</code> <a href="https://github.com/apache/arrow-datafusion/issues/2699">#2699</a></li>
  <li>Improvements in group by and sort performance <a href="https://github.com/apache/arrow-datafusion/issues/2375">#2375</a></li>
  <li>Adaptive <code class="language-plaintext highlighter-rouge">regex_replace</code> implementation <a href="https://github.com/apache/arrow-datafusion/issues/3518">#3518</a></li>
</ul>

<h1 id="optimizer-enhancements">Optimizer Enhancements</h1>
<p>Internally the optimizer has been significantly enhanced as well.</p>

<ul>
  <li>Casting / coercion now happens during logical planning <a href="https://github.com/apache/arrow-datafusion/issues/3396">#3185</a> <a href="https://github.com/apache/arrow-datafusion/issues/3636">#3636</a></li>
  <li>More sophisticated expression analysis and simplification is available</li>
</ul>

<h1 id="parquet">Parquet</h1>
<ul>
  <li>The parquet reader can now read directly from parquet files on remote object storage <a href="https://github.com/apache/arrow-datafusion/issues/2677">#2489</a> <a href="https://github.com/apache/arrow-datafusion/issues/3051">#3051</a></li>
  <li>Experimental support for “predicate pushdown” with late materialization after filtering during the scan (another blog post on this topic is coming soon).</li>
  <li>Support reading directly from AWS S3 and other object stores via <code class="language-plaintext highlighter-rouge">datafusion-cli </code> <a href="https://github.com/apache/arrow-datafusion/issues/3631">#3631</a></li>
</ul>

<h1 id="datatype-support">DataType Support</h1>
<ul>
  <li>Support for <code class="language-plaintext highlighter-rouge">TimestampTz</code> <a href="https://github.com/apache/arrow-datafusion/issues/3660">#3660</a></li>
  <li>Expanded support for the <code class="language-plaintext highlighter-rouge">Decimal</code> type, including  <code class="language-plaintext highlighter-rouge">IN</code> list and better built in coercion.</li>
  <li>Expanded support for date/time manipulation such as  <code class="language-plaintext highlighter-rouge">date_bin</code> built-in function , timestamp <code class="language-plaintext highlighter-rouge">+/-</code> interval, <code class="language-plaintext highlighter-rouge">TIME</code> literal values <a href="https://github.com/apache/arrow-datafusion/issues/3010">#3010</a>, <a href="https://github.com/apache/arrow-datafusion/issues/3110">#3110</a>, <a href="https://github.com/apache/arrow-datafusion/issues/3034">#3034</a></li>
  <li>Binary operations (<code class="language-plaintext highlighter-rouge">AND</code>, <code class="language-plaintext highlighter-rouge">XOR</code>, etc):  <a href="https://github.com/apache/arrow-datafusion/issues/1619">#3037</a> <a href="https://github.com/apache/arrow-datafusion/issues/3430">#3420</a></li>
  <li><code class="language-plaintext highlighter-rouge">IS TRUE/FALSE</code> and <code class="language-plaintext highlighter-rouge">IS [NOT] UNKNOWN</code> <a href="https://github.com/apache/arrow-datafusion/issues/3235">#3235</a>, <a href="https://github.com/apache/arrow-datafusion/issues/3246">#3246</a></li>
</ul>

<h2 id="upcoming-work">Upcoming Work</h2>
<p>With the community growing and code accelerating, there is so much great stuff on the horizon. Some features we expect to land in the next few months:</p>

<ul>
  <li><a href="https://github.com/apache/arrow-datafusion/issues/3462">Complete Parquet Pushdown</a></li>
  <li><a href="https://github.com/apache/arrow-datafusion/issues/3148">Additional date/time support</a></li>
  <li>Cost models, Nested Join Optimizations, analysis framework <a href="https://github.com/apache/arrow-datafusion/issues/128">#128</a>, <a href="https://github.com/apache/arrow-datafusion/issues/3843">#3843</a>, <a href="https://github.com/apache/arrow-datafusion/issues/3845">#3845</a></li>
</ul>

<h1 id="community-growth">Community Growth</h1>

<p>The DataFusion 9.0.0 and 13.0.0 releases consists of 433 PRs from 64 distinct contributors. This does not count all the work that goes into our dependencies such as <a href="https://crates.io/crates/arrow">arrow</a>,  <a href="https://crates.io/crates/parquet">parquet</a>, and <a href="https://crates.io/crates/object_store">object_store</a>, that much of the same community helps nurture.</p>

<!--
$ git log --pretty=oneline 9.0.0..13.0.0 . | wc -l
433

$ git shortlog -sn 9.0.0..13.0.0 . | wc -l
65
-->

<h1 id="how-to-get-involved">How to Get Involved</h1>

<p>Kudos to everyone in the community who contributed ideas, discussions, bug reports, documentation and code. It is exciting to be building something so cool together!</p>

<p>If you are interested in contributing to DataFusion, we would love to
have you join us on our journey to create the most advanced open
source query engine. You can try out DataFusion on some of your own
data and projects and let us know how it goes or contribute a PR with
documentation, tests or code. A list of open issues suitable for
beginners is
<a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a>.</p>

<p>Check out our <a href="https://arrow.apache.org/datafusion/community/communication.html">Communication Doc</a> on more
ways to engage with the community.</p>

<h2 id="appendix-contributor-shoutout">Appendix: Contributor Shoutout</h2>

<p>To give a sense of the number of people who contribute to this project regularly, we present for your consideration the following list derived from <code class="language-plaintext highlighter-rouge">git shortlog -sn 9.0.0..13.0.0 .</code> Thank you all again!</p>

<!-- Note: combined kmitchener and Kirk Mitchener -->

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    87	Andy Grove
    71	Andrew Lamb
    29	Kun Liu
    29	Kirk Mitchener
    17	Wei-Ting Kuo
    14	Yang Jiang
    12	Raphael Taylor-Davies
    11	Batuhan Taskaya
    10	Brent Gardner
    10	Remzi Yang
    10	comphead
    10	xudong.w
     8	AssHero
     7	Ruihang Xia
     6	Dan Harris
     6	Daniël Heres
     6	Ian Alexander Joiner
     6	Mike Roberts
     6	askoa
     4	BaymaxHWY
     4	gorkem
     4	jakevin
     3	George Andronchik
     3	Sarah Yurick
     3	Stuart Carnie
     2	Dalton Modlin
     2	Dmitry Patsura
     2	JasonLi
     2	Jon Mease
     2	Marco Neumann
     2	yahoNanJing
     1	Adilet Sarsembayev
     1	Ayush Dattagupta
     1	Dezhi Wu
     1	Dhamotharan Sritharan
     1	Eduard Karacharov
     1	Francis Du
     1	Harbour Zheng
     1	Ismaël Mejía
     1	Jack Klamer
     1	Jeremy Dyer
     1	Jiayu Liu
     1	Kamil Konior
     1	Liang-Chi Hsieh
     1	Martin Grigorov
     1	Matthijs Brobbel
     1	Mehmet Ozan Kabak
     1	Metehan Yıldırım
     1	Morgan Cassels
     1	Nitish Tiwari
     1	Renjie Liu
     1	Rito Takeuchi
     1	Robert Pack
     1	Thomas Cameron
     1	Vrishabh
     1	Xin Hao
     1	Yijie Shen
     1	byteink
     1	kamille
     1	mateuszkj
     1	nvartolomei
     1	yourenawo
     1	Özgür Akkurt
</code></pre></div></div>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[Introduction Apache Arrow DataFusion 13.0.0 is released, and this blog contains an update on the project for the 5 months since our last update in May 2022. DataFusion is an extensible and embeddable query engine, written in Rust used to create modern, fast and efficient data pipelines, ETL processes, and database systems. You may want to check out DataFusion to extend your Rust project to: Support SQL support Support DataFrame API Support a Domain Specific Query Language Easily and quickly read and process Parquet, JSON, Avro or CSV data. Read from remote object stores such as AWS S3, Azure Blob Storage, GCP. Even though DataFusion is 4 years “young,” it has seen significant community growth in the last few months and the momentum continues to accelerate. Background DataFusion is used as the engine in many open source and commercial projects and was one of the early open source projects to provide this capability. 2022 has validated our belief in the need for such a “LLVM for database and AI systems”(alternate link) with announcements such as the release of FaceBook’s Velox engine, the major investments in Acero as well as the continued popularity of Apache Calcite and other similar technologies. While Velox and Acero focus on execution engines, DataFusion provides the entire suite of components needed to build most analytic systems, including a SQL frontend, a dataframe API, and extension points for just about everything. Some DataFusion users use a subset of the features such as the frontend (e.g. dask-sql) or the execution engine, (e.g. Blaze), and some use many different components to build both SQL based and customized DSL based systems such as InfluxDB IOx and VegaFusion. One of DataFusion’s advantages is its implementation in Rust and thus its easy integration with the broader Rust ecosystem. Rust continues to be a major source of benefit, from the ease of parallelization with the high quality and standardized async ecosystem , as well as its modern dependency management system and wonderful performance. Summary We have increased the frequency of DataFusion releases to monthly instead of quarterly. This makes it easier for the increasing number of projects that now depend on DataFusion. We have also completed the “graduation” of Ballista to its own top-level arrow-ballista repository which decouples the two projects and allows each project to move even faster. Along with numerous other bug fixes and smaller improvements, here are some of the major advances: Improved Support for Cloud Object Stores DataFusion now supports many major cloud object stores (Amazon S3, Azure Blob Storage, and Google Cloud Storage) “out of the box” via the object_store crate. Using this integration, DataFusion optimizes reading parquet files by reading only the parts of the files that are needed. Advanced SQL DataFusion now supports correlated subqueries, by rewriting them as joins. See the Subquery page in the User Guide for more information. In addition to numerous other small improvements, the following SQL features are now supported: ROWS, RANGE, PRECEDING and FOLLOWING in OVER clauses #3570 ROLLUP and CUBE grouping set expressions #2446 SUM DISTINCT aggregate support #2405 IN and NOT IN Subqueries by rewriting them to SEMI / ANTI #2421 Non equality predicates in ON clause of LEFT, RIGHT, and FULL joins #2591 Exact MEDIAN #3009 GROUPING SETS/CUBE/ROLLUP #2716 More DDL Support Just as it is important to query, it is also important to give users the ability to define their data sources. We have added: CREATE VIEW #2279 DESCRIBE &lt;table&gt; #2642 Custom / Dynamic table provider factories #3311 SHOW CREATE TABLE for support for views #2830 Faster Execution Performance is always an important goal for DataFusion, and there are a number of significant new optimizations such as Optimizations of TopK (queries with a LIMIT or OFFSET clause): #3527, #2521 Reduce left/right/full joins to inner join #2750 Convert cross joins to inner joins when possible #3482 Sort preserving SortMergeJoin #2699 Improvements in group by and sort performance #2375 Adaptive regex_replace implementation #3518 Optimizer Enhancements Internally the optimizer has been significantly enhanced as well. Casting / coercion now happens during logical planning #3185 #3636 More sophisticated expression analysis and simplification is available Parquet The parquet reader can now read directly from parquet files on remote object storage #2489 #3051 Experimental support for “predicate pushdown” with late materialization after filtering during the scan (another blog post on this topic is coming soon). Support reading directly from AWS S3 and other object stores via datafusion-cli #3631 DataType Support Support for TimestampTz #3660 Expanded support for the Decimal type, including IN list and better built in coercion. Expanded support for date/time manipulation such as date_bin built-in function , timestamp +/- interval, TIME literal values #3010, #3110, #3034 Binary operations (AND, XOR, etc): #3037 #3420 IS TRUE/FALSE and IS [NOT] UNKNOWN #3235, #3246 Upcoming Work With the community growing and code accelerating, there is so much great stuff on the horizon. Some features we expect to land in the next few months: Complete Parquet Pushdown Additional date/time support Cost models, Nested Join Optimizations, analysis framework #128, #3843, #3845 Community Growth The DataFusion 9.0.0 and 13.0.0 releases consists of 433 PRs from 64 distinct contributors. This does not count all the work that goes into our dependencies such as arrow, parquet, and object_store, that much of the same community helps nurture. How to Get Involved Kudos to everyone in the community who contributed ideas, discussions, bug reports, documentation and code. It is exciting to be building something so cool together! If you are interested in contributing to DataFusion, we would love to have you join us on our journey to create the most advanced open source query engine. You can try out DataFusion on some of your own data and projects and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable for beginners is here. Check out our Communication Doc on more ways to engage with the community. Appendix: Contributor Shoutout To give a sense of the number of people who contribute to this project regularly, we present for your consideration the following list derived from git shortlog -sn 9.0.0..13.0.0 . Thank you all again! 87 Andy Grove 71 Andrew Lamb 29 Kun Liu 29 Kirk Mitchener 17 Wei-Ting Kuo 14 Yang Jiang 12 Raphael Taylor-Davies 11 Batuhan Taskaya 10 Brent Gardner 10 Remzi Yang 10 comphead 10 xudong.w 8 AssHero 7 Ruihang Xia 6 Dan Harris 6 Daniël Heres 6 Ian Alexander Joiner 6 Mike Roberts 6 askoa 4 BaymaxHWY 4 gorkem 4 jakevin 3 George Andronchik 3 Sarah Yurick 3 Stuart Carnie 2 Dalton Modlin 2 Dmitry Patsura 2 JasonLi 2 Jon Mease 2 Marco Neumann 2 yahoNanJing 1 Adilet Sarsembayev 1 Ayush Dattagupta 1 Dezhi Wu 1 Dhamotharan Sritharan 1 Eduard Karacharov 1 Francis Du 1 Harbour Zheng 1 Ismaël Mejía 1 Jack Klamer 1 Jeremy Dyer 1 Jiayu Liu 1 Kamil Konior 1 Liang-Chi Hsieh 1 Martin Grigorov 1 Matthijs Brobbel 1 Mehmet Ozan Kabak 1 Metehan Yıldırım 1 Morgan Cassels 1 Nitish Tiwari 1 Renjie Liu 1 Rito Takeuchi 1 Robert Pack 1 Thomas Cameron 1 Vrishabh 1 Xin Hao 1 Yijie Shen 1 byteink 1 kamille 1 mateuszkj 1 nvartolomei 1 yourenawo 1 Özgür Akkurt]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Arrow and Parquet Part 3: Arbitrary Nesting with Lists of Structs and Structs of Lists</title><link href="https://arrow.apache.org/blog/2022/10/17/arrow-parquet-encoding-part-3/" rel="alternate" type="text/html" title="Arrow and Parquet Part 3: Arbitrary Nesting with Lists of Structs and Structs of Lists" /><published>2022-10-17T00:00:00-04:00</published><updated>2022-10-17T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/10/17/arrow-parquet-encoding-part-3</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/10/17/arrow-parquet-encoding-part-3/"><![CDATA[<!--

-->

<h2 id="introduction">Introduction</h2>

<p>This is the third of a three part series exploring how projects such as <a href="https://github.com/apache/arrow-rs">Rust Apache Arrow</a> support conversion between <a href="https://arrow.apache.org/">Apache Arrow</a> for in memory processing and <a href="https://parquet.apache.org/">Apache Parquet</a> for efficient storage. <a href="https://arrow.apache.org/">Apache Arrow</a> is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. <a href="https://parquet.apache.org/">Apache Parquet</a> is an open, column-oriented data file format designed for very efficient data encoding and retrieval.</p>

<p><a href="https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/">Arrow and Parquet Part 1: Primitive Types and Nullability</a> covered the basics of primitive types.  <a href="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/">Arrow and Parquet Part 2: Nested and Hierarchical Data using Structs and Lists</a> covered the <code class="language-plaintext highlighter-rouge">Struct</code> and <code class="language-plaintext highlighter-rouge">List</code> types. This post builds on this foundation to show how both formats combine these to support arbitrary nesting.</p>

<p>Some libraries, such as Rust <a href="https://crates.io/crates/parquet">parquet</a> implementation, offer complete support for such combinations, and users of those libraries do not need to worry about these details except to satisfy their own curiosity. Other libraries may not handle some corner cases and this post gives some flavor of why it is so complicated to do so.</p>

<h1 id="structs-with-lists">Structs with Lists</h1>
<p>Consider the following three json documents</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>                     <span class="c1"># &lt;-- First record
</span>  <span class="s">"a"</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span>           <span class="c1"># &lt;-- top-level field a containing list of integers
</span>  <span class="s">"b"</span><span class="p">:</span> <span class="p">[</span>              <span class="c1"># &lt;-- top-level field b containing list of structures
</span>    <span class="p">{</span>                 <span class="c1"># &lt;-- list element of b containing two field b1 and b2
</span>      <span class="s">"b1"</span><span class="p">:</span> <span class="mi">1</span>         <span class="c1"># &lt;-- b1 is always provided (non nullable)
</span>    <span class="p">},</span>
    <span class="p">{</span>
      <span class="s">"b1"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
      <span class="s">"b2"</span><span class="p">:</span> <span class="p">[</span>         <span class="c1"># &lt;-- b2 contains list of integers
</span>        <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span>          <span class="c1"># &lt;-- list elements of b.b2 always provided (non nullable)
</span>      <span class="p">]</span>
    <span class="p">}</span>
  <span class="p">]</span>
<span class="p">}</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
  <span class="s">"b"</span><span class="p">:</span> <span class="p">[</span>              <span class="c1"># &lt;-- b is always provided (non nullable)
</span>    <span class="p">{</span>
      <span class="s">"b1"</span><span class="p">:</span> <span class="mi">2</span>
    <span class="p">},</span>
  <span class="p">]</span>
<span class="p">}</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
  <span class="s">"a"</span><span class="p">:</span> <span class="p">[</span><span class="n">null</span><span class="p">,</span> <span class="n">null</span><span class="p">],</span>  <span class="c1"># &lt;-- list elements of a are nullable
</span>  <span class="s">"b"</span><span class="p">:</span> <span class="p">[</span><span class="n">null</span><span class="p">]</span>         <span class="c1"># &lt;-- list elements of b are nullable
</span><span class="p">}</span>
</code></pre></div></div>

<p>Documents of this format could be stored in this Arrow schema</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"a"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">List</span><span class="p">(</span>
  <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"element"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Int32</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"b"</span><span class="p">),</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">false</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">List</span><span class="p">(</span>
  <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"element"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Struct</span><span class="p">[</span>
    <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"b1"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">false</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Int32</span><span class="p">),</span>
    <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"b2"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">List</span><span class="p">(</span>
      <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"element"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">false</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Int32</span><span class="p">)</span>
    <span class="p">))</span>
  <span class="p">])</span>
<span class="p">))</span>
</code></pre></div></div>

<p>As explained previously, Arrow chooses to represent this in a hierarchical fashion. <code class="language-plaintext highlighter-rouge">StructArray</code>s are stored as child arrays that contain each field of the struct.  <code class="language-plaintext highlighter-rouge">ListArray</code>s are stored as lists of monotonically increasing integers called offsets, with values stored in a single child array. Each consecutive pair of elements in the offset array identifies a slice of the child array for that array index.</p>

<p>The Arrow encoding of the example would be:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐
                     ┌──────────────────┐
│ ┌─────┐   ┌─────┐  │ ┌─────┐   ┌─────┐│ │
  │  1  │   │  0  │  │ │  1  │   │  1  ││
│ ├─────┤   ├─────┤  │ ├─────┤   ├─────┤│ │
  │  0  │   │  1  │  │ │  0  │   │ ??  ││
│ ├─────┤   ├─────┤  │ ├─────┤   ├─────┤│ │
  │  1  │   │  1  │  │ │  0  │   │ ??  ││
│ └─────┘   ├─────┤  │ └─────┘   └─────┘│ │
            │  3  │  │ Validity   Values│
│ Validity  └─────┘  │                  │ │
                     │ child[0]         │
│ "a"       Offsets  │ PrimitiveArray   │ │
  ListArray          └──────────────────┘
└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘

┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
           ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ │
│                    ┌──────────┐ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
  ┌─────┐  │ ┌─────┐ │ ┌─────┐  │   ┌─────┐ ┌─────┐ ┌──────────┐ │ │ │
│ │  0  │    │  1  │ │ │  1  │  │ │ │  0  │ │  0  │ │ ┌─────┐  │
  ├─────┤  │ ├─────┤ │ ├─────┤  │   ├─────┤ ├─────┤ │ │  3  │  │ │ │ │
│ │  2  │    │  1  │ │ │  1  │  │ │ │  1  │ │  0  │ │ ├─────┤  │
  ├─────┤  │ ├─────┤ │ ├─────┤  │   ├─────┤ ├─────┤ │ │  4  │  │ │ │ │
│ │  3  │    │  1  │ │ │  2  │  │ │ │  0  │ │  2  │ │ └─────┘  │
  ├─────┤  │ ├─────┤ │ ├─────┤  │   ├─────┤ ├─────┤ │          │ │ │ │
│ │  4  │    │  0  │ │ │ ??  │  │ │ │ ??  │ │  2  │ │  Values  │
  └─────┘  │ └─────┘ │ └─────┘  │   └─────┘ ├─────┤ │          │ │ │ │
│                    │          │ │         │  2  │ │          │
  Offsets  │ Validity│ Values   │           └─────┘ │          │ │ │ │
│                    │          │ │Validity         │ child[0] │
           │         │ "b1"     │           Offsets │ Primitive│ │ │ │
│                    │ Primitive│ │ "b2"            │ Array    │
           │         │ Array    │   ListArray       └──────────┘ │ │ │
│                    └──────────┘ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
           │ "element"                                             │ │
│            StructArray
  "b"      └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ │
│ ListArray
 ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘
</code></pre></div></div>

<p>Documents of this format could be stored in this Parquet schema</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">message</span> <span class="n">schema</span> <span class="p">{</span>
  <span class="n">optional</span> <span class="n">group</span> <span class="n">a</span> <span class="p">(</span><span class="n">LIST</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">repeated</span> <span class="n">group</span> <span class="nb">list</span> <span class="p">{</span>
      <span class="n">optional</span> <span class="n">int32</span> <span class="n">element</span><span class="p">;</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="n">required</span> <span class="n">group</span> <span class="n">b</span> <span class="p">(</span><span class="n">LIST</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">repeated</span> <span class="n">group</span> <span class="nb">list</span> <span class="p">{</span>
      <span class="n">optional</span> <span class="n">group</span> <span class="n">element</span> <span class="p">{</span>
        <span class="n">required</span> <span class="n">int32</span> <span class="n">b1</span><span class="p">;</span>
        <span class="n">optional</span> <span class="n">group</span> <span class="n">b2</span> <span class="p">(</span><span class="n">LIST</span><span class="p">)</span> <span class="p">{</span>
          <span class="n">repeated</span> <span class="n">group</span> <span class="nb">list</span> <span class="p">{</span>
            <span class="n">required</span> <span class="n">int32</span> <span class="n">element</span><span class="p">;</span>
          <span class="p">}</span>
        <span class="p">}</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>As explained in our previous posts, Parquet uses repetition levels and definition levels to encode nested structures and nullability.</p>

<p>Definition and repetition levels is a non trivial topic. For more detail, you can read the <a href="https://research.google/pubs/pub36632/">Google Dremel Paper</a> which offers an academic description of the algorithm. You can also explore this <a href="https://gist.github.com/alamb/acd653c49e318ff70672b61325ba3443">gist</a> to see Rust <a href="https://crates.io/crates/parquet">parquet</a> code which generates the example below.</p>

<p>The Parquet encoding of the example would be:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌───────────────────────────────┐ ┌────────────────────────────────┐
│ ┌─────┐    ┌─────┐    ┌─────┐ │ │  ┌─────┐    ┌─────┐    ┌─────┐ │
│ │  3  │    │  0  │    │  1  │ │ │  │  2  │    │  0  │    │  1  │ │
│ ├─────┤    ├─────┤    └─────┘ │ │  ├─────┤    ├─────┤    ├─────┤ │
│ │  0  │    │  0  │            │ │  │  2  │    │  1  │    │  1  │ │
│ ├─────┤    ├─────┤      Data  │ │  ├─────┤    ├─────┤    ├─────┤ │
│ │  2  │    │  0  │            │ │  │  2  │    │  0  │    │  2  │ │
│ ├─────┤    ├─────┤            │ │  ├─────┤    ├─────┤    └─────┘ │
│ │  2  │    │  1  │            │ │  │  1  │    │  0  │            │
│ └─────┘    └─────┘            │ │  └─────┘    └─────┘     Data   │
│                               │ │                                │
│Definition Repetition          │ │ Definition Repetition          │
│  Levels     Levels            │ │   Levels     Levels            │
│                               │ │                                │
│ "a"                           │ │  "b.b1"                        │
└───────────────────────────────┘ └────────────────────────────────┘

┌───────────────────────────────┐
│  ┌─────┐    ┌─────┐    ┌─────┐│
│  │  2  │    │  0  │    │  3  ││
│  ├─────┤    ├─────┤    ├─────┤│
│  │  4  │    │  1  │    │  4  ││
│  ├─────┤    ├─────┤    └─────┘│
│  │  4  │    │  2  │           │
│  ├─────┤    ├─────┤           │
│  │  2  │    │  0  │           │
│  ├─────┤    ├─────┤     Data  │
│  │  1  │    │  0  │           │
│  └─────┘    └─────┘           │
│Definition  Repetition         │
│  Levels      Levels           │
│                               │
│  "b.b2"                       │
└───────────────────────────────┘
</code></pre></div></div>

<h2 id="additional-complications">Additional Complications</h2>

<p>This series of posts has necessarily glossed over a number of details that further complicate actual implementations:</p>

<ul>
  <li>A <code class="language-plaintext highlighter-rouge">ListArray</code> may contain a non-empty offset range that is masked by a validity mask</li>
  <li>Reading a given number of rows from a nullable field requires reading the definition levels and determining the number of values to read based on the number of nulls present</li>
  <li>Reading a given number of rows from a repeated field requires reading the repetition levels and detecting a new row based on a repetition level of 0</li>
  <li>A Parquet file may contain multiple row groups, each containing multiple column chunks</li>
  <li>A column chunk may contain multiple pages, and there is no relationship between pages across columns</li>
  <li>Parquet has alternative schema for representing lists with varying degrees of nullability</li>
  <li>And more…</li>
</ul>

<h2 id="summary">Summary</h2>
<p>Both Parquet and Arrow are columnar formats and support nested structs and lists, however, the way they represent such nesting differs significantly and conversion between the two formats is complex.</p>

<p>Fortunately, with the Rust <a href="https://crates.io/crates/parquet">parquet</a> implementation, reading and writing nested data in Arrow, in Parquet or converting between the two is as simple as reading unnested data. The library handles all the complex record shredding and reconstruction automatically. With this and other exciting features, such as support for <a href="https://docs.rs/parquet/22.0.0/parquet/arrow/async_reader/index.html">reading asynchronously</a> from <a href="https://docs.rs/object_store">object storage</a>, it is the fastest and most feature complete Rust parquet implementation available. We look forward to seeing what you build with it!</p>]]></content><author><name>tustvold and alamb</name></author><category term="parquet" /><category term="arrow" /><summary type="html"><![CDATA[Introduction This is the third of a three part series exploring how projects such as Rust Apache Arrow support conversion between Apache Arrow for in memory processing and Apache Parquet for efficient storage. Apache Arrow is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. Apache Parquet is an open, column-oriented data file format designed for very efficient data encoding and retrieval. Arrow and Parquet Part 1: Primitive Types and Nullability covered the basics of primitive types. Arrow and Parquet Part 2: Nested and Hierarchical Data using Structs and Lists covered the Struct and List types. This post builds on this foundation to show how both formats combine these to support arbitrary nesting. Some libraries, such as Rust parquet implementation, offer complete support for such combinations, and users of those libraries do not need to worry about these details except to satisfy their own curiosity. Other libraries may not handle some corner cases and this post gives some flavor of why it is so complicated to do so. Structs with Lists Consider the following three json documents { # &lt;-- First record "a": [1], # &lt;-- top-level field a containing list of integers "b": [ # &lt;-- top-level field b containing list of structures { # &lt;-- list element of b containing two field b1 and b2 "b1": 1 # &lt;-- b1 is always provided (non nullable) }, { "b1": 1, "b2": [ # &lt;-- b2 contains list of integers 3, 4 # &lt;-- list elements of b.b2 always provided (non nullable) ] } ] } { "b": [ # &lt;-- b is always provided (non nullable) { "b1": 2 }, ] } { "a": [null, null], # &lt;-- list elements of a are nullable "b": [null] # &lt;-- list elements of b are nullable } Documents of this format could be stored in this Arrow schema Field(name: "a", nullable: true, datatype: List( Field(name: "element", nullable: true, datatype: Int32), ) Field(name: "b"), nullable: false, datatype: List( Field(name: "element", nullable: true, datatype: Struct[ Field(name: "b1", nullable: false, datatype: Int32), Field(name: "b2", nullable: true, datatype: List( Field(name: "element", nullable: false, datatype: Int32) )) ]) )) As explained previously, Arrow chooses to represent this in a hierarchical fashion. StructArrays are stored as child arrays that contain each field of the struct. ListArrays are stored as lists of monotonically increasing integers called offsets, with values stored in a single child array. Each consecutive pair of elements in the offset array identifies a slice of the child array for that array index. The Arrow encoding of the example would be: ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┌──────────────────┐ │ ┌─────┐ ┌─────┐ │ ┌─────┐ ┌─────┐│ │ │ 1 │ │ 0 │ │ │ 1 │ │ 1 ││ │ ├─────┤ ├─────┤ │ ├─────┤ ├─────┤│ │ │ 0 │ │ 1 │ │ │ 0 │ │ ?? ││ │ ├─────┤ ├─────┤ │ ├─────┤ ├─────┤│ │ │ 1 │ │ 1 │ │ │ 0 │ │ ?? ││ │ └─────┘ ├─────┤ │ └─────┘ └─────┘│ │ │ 3 │ │ Validity Values│ │ Validity └─────┘ │ │ │ │ child[0] │ │ "a" Offsets │ PrimitiveArray │ │ ListArray └──────────────────┘ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ │ │ ┌──────────┐ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌─────┐ │ ┌─────┐ │ ┌─────┐ │ ┌─────┐ ┌─────┐ ┌──────────┐ │ │ │ │ │ 0 │ │ 1 │ │ │ 1 │ │ │ │ 0 │ │ 0 │ │ ┌─────┐ │ ├─────┤ │ ├─────┤ │ ├─────┤ │ ├─────┤ ├─────┤ │ │ 3 │ │ │ │ │ │ │ 2 │ │ 1 │ │ │ 1 │ │ │ │ 1 │ │ 0 │ │ ├─────┤ │ ├─────┤ │ ├─────┤ │ ├─────┤ │ ├─────┤ ├─────┤ │ │ 4 │ │ │ │ │ │ │ 3 │ │ 1 │ │ │ 2 │ │ │ │ 0 │ │ 2 │ │ └─────┘ │ ├─────┤ │ ├─────┤ │ ├─────┤ │ ├─────┤ ├─────┤ │ │ │ │ │ │ │ 4 │ │ 0 │ │ │ ?? │ │ │ │ ?? │ │ 2 │ │ Values │ └─────┘ │ └─────┘ │ └─────┘ │ └─────┘ ├─────┤ │ │ │ │ │ │ │ │ │ │ 2 │ │ │ Offsets │ Validity│ Values │ └─────┘ │ │ │ │ │ │ │ │ │Validity │ child[0] │ │ │ "b1" │ Offsets │ Primitive│ │ │ │ │ │ Primitive│ │ "b2" │ Array │ │ │ Array │ ListArray └──────────┘ │ │ │ │ └──────────┘ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ "element" │ │ │ StructArray "b" └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ │ │ ListArray ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ Documents of this format could be stored in this Parquet schema message schema { optional group a (LIST) { repeated group list { optional int32 element; } } required group b (LIST) { repeated group list { optional group element { required int32 b1; optional group b2 (LIST) { repeated group list { required int32 element; } } } } } } As explained in our previous posts, Parquet uses repetition levels and definition levels to encode nested structures and nullability. Definition and repetition levels is a non trivial topic. For more detail, you can read the Google Dremel Paper which offers an academic description of the algorithm. You can also explore this gist to see Rust parquet code which generates the example below. The Parquet encoding of the example would be: ┌───────────────────────────────┐ ┌────────────────────────────────┐ │ ┌─────┐ ┌─────┐ ┌─────┐ │ │ ┌─────┐ ┌─────┐ ┌─────┐ │ │ │ 3 │ │ 0 │ │ 1 │ │ │ │ 2 │ │ 0 │ │ 1 │ │ │ ├─────┤ ├─────┤ └─────┘ │ │ ├─────┤ ├─────┤ ├─────┤ │ │ │ 0 │ │ 0 │ │ │ │ 2 │ │ 1 │ │ 1 │ │ │ ├─────┤ ├─────┤ Data │ │ ├─────┤ ├─────┤ ├─────┤ │ │ │ 2 │ │ 0 │ │ │ │ 2 │ │ 0 │ │ 2 │ │ │ ├─────┤ ├─────┤ │ │ ├─────┤ ├─────┤ └─────┘ │ │ │ 2 │ │ 1 │ │ │ │ 1 │ │ 0 │ │ │ └─────┘ └─────┘ │ │ └─────┘ └─────┘ Data │ │ │ │ │ │Definition Repetition │ │ Definition Repetition │ │ Levels Levels │ │ Levels Levels │ │ │ │ │ │ "a" │ │ "b.b1" │ └───────────────────────────────┘ └────────────────────────────────┘ ┌───────────────────────────────┐ │ ┌─────┐ ┌─────┐ ┌─────┐│ │ │ 2 │ │ 0 │ │ 3 ││ │ ├─────┤ ├─────┤ ├─────┤│ │ │ 4 │ │ 1 │ │ 4 ││ │ ├─────┤ ├─────┤ └─────┘│ │ │ 4 │ │ 2 │ │ │ ├─────┤ ├─────┤ │ │ │ 2 │ │ 0 │ │ │ ├─────┤ ├─────┤ Data │ │ │ 1 │ │ 0 │ │ │ └─────┘ └─────┘ │ │Definition Repetition │ │ Levels Levels │ │ │ │ "b.b2" │ └───────────────────────────────┘ Additional Complications This series of posts has necessarily glossed over a number of details that further complicate actual implementations: A ListArray may contain a non-empty offset range that is masked by a validity mask Reading a given number of rows from a nullable field requires reading the definition levels and determining the number of values to read based on the number of nulls present Reading a given number of rows from a repeated field requires reading the repetition levels and detecting a new row based on a repetition level of 0 A Parquet file may contain multiple row groups, each containing multiple column chunks A column chunk may contain multiple pages, and there is no relationship between pages across columns Parquet has alternative schema for representing lists with varying degrees of nullability And more… Summary Both Parquet and Arrow are columnar formats and support nested structs and lists, however, the way they represent such nesting differs significantly and conversion between the two formats is complex. Fortunately, with the Rust parquet implementation, reading and writing nested data in Arrow, in Parquet or converting between the two is as simple as reading unnested data. The library handles all the complex record shredding and reconstruction automatically. With this and other exciting features, such as support for reading asynchronously from object storage, it is the fastest and most feature complete Rust parquet implementation available. We look forward to seeing what you build with it!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Arrow and Parquet Part 2: Nested and Hierarchical Data using Structs and Lists</title><link href="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/" rel="alternate" type="text/html" title="Arrow and Parquet Part 2: Nested and Hierarchical Data using Structs and Lists" /><published>2022-10-08T00:00:00-04:00</published><updated>2022-10-08T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/"><![CDATA[<!--

-->

<h2 id="introduction">Introduction</h2>

<p>This is the second, in a three part series exploring how projects such as <a href="https://github.com/apache/arrow-rs">Rust Apache Arrow</a> support conversion between <a href="https://arrow.apache.org/">Apache Arrow</a> and <a href="https://parquet.apache.org/">Apache Parquet</a>. The <a href="https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/">first post</a> covered the basics of data storage and validity encoding, and this post will cover the more complex <code class="language-plaintext highlighter-rouge">Struct</code> and <code class="language-plaintext highlighter-rouge">List</code> types.</p>

<p><a href="https://arrow.apache.org/">Apache Arrow</a> is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. <a href="https://parquet.apache.org/">Apache Parquet</a> is an open, column-oriented data file format designed for very efficient data encoding and retrieval.</p>

<h2 id="struct--group-columns">Struct / Group Columns</h2>

<p>Both Parquet and Arrow have the concept of a <em>struct</em> column, which is a column containing one or more other columns in named fields and is analogous to a JSON object.</p>

<p>For example, consider the following three JSON documents</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>              <span class="c1"># &lt;-- First record
</span>  <span class="s">"a"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>      <span class="c1"># &lt;-- the top level fields are a, b, c, and d
</span>  <span class="s">"b"</span><span class="p">:</span> <span class="p">{</span>       <span class="c1"># &lt;-- b is always provided (not nullable)
</span>    <span class="s">"b1"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>   <span class="c1"># &lt;-- b1 and b2 are "nested" fields of "b"
</span>    <span class="s">"b2"</span><span class="p">:</span> <span class="mi">3</span>    <span class="c1"># &lt;-- b2 is always provided (not nullable)
</span>   <span class="p">},</span>
 <span class="s">"d"</span><span class="p">:</span> <span class="p">{</span>
   <span class="s">"d1"</span><span class="p">:</span>  <span class="mi">1</span>    <span class="c1"># &lt;-- d1 is a "nested" field of "d"
</span>  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>              <span class="c1"># &lt;-- Second record
</span>  <span class="s">"a"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
  <span class="s">"b"</span><span class="p">:</span> <span class="p">{</span>
    <span class="s">"b2"</span><span class="p">:</span> <span class="mi">4</span>    <span class="c1"># &lt;-- note "b1" is NULL in this record
</span>  <span class="p">},</span>
  <span class="s">"c"</span><span class="p">:</span> <span class="p">{</span>       <span class="c1"># &lt;-- note "c" was NULL in the first record
</span>    <span class="s">"c1"</span><span class="p">:</span> <span class="mi">6</span>        <span class="n">but</span> <span class="n">when</span> <span class="s">"c"</span> <span class="ow">is</span> <span class="n">provided</span><span class="p">,</span> <span class="n">c1</span> <span class="ow">is</span> <span class="n">also</span>
  <span class="p">},</span>               <span class="n">always</span> <span class="n">provided</span> <span class="p">(</span><span class="ow">not</span> <span class="n">nullable</span><span class="p">)</span>
  <span class="s">"d"</span><span class="p">:</span> <span class="p">{</span>
    <span class="s">"d1"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s">"d2"</span><span class="p">:</span> <span class="mi">1</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>              <span class="c1"># &lt;-- Third record
</span>  <span class="s">"b"</span><span class="p">:</span> <span class="p">{</span>
    <span class="s">"b1"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="s">"b2"</span><span class="p">:</span> <span class="mi">6</span>
  <span class="p">},</span>
  <span class="s">"c"</span><span class="p">:</span> <span class="p">{</span>
    <span class="s">"c1"</span><span class="p">:</span> <span class="mi">7</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>
<p>Documents of this format could be stored in an Arrow <code class="language-plaintext highlighter-rouge">StructArray</code> with this schema</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"a"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Int32</span><span class="p">)</span>
<span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"b"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">false</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Struct</span><span class="p">[</span>
  <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"b1"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Int32</span><span class="p">),</span>
  <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"b2"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">false</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Int32</span><span class="p">)</span>
<span class="p">])</span>
<span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"c"</span><span class="p">),</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Struct</span><span class="p">[</span>
  <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"c1"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">false</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Int32</span><span class="p">)</span>
<span class="p">])</span>
<span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"d"</span><span class="p">),</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Struct</span><span class="p">[</span>
  <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"d1"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">false</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Int32</span><span class="p">)</span>
  <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"d2"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Int32</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<p>Arrow represents each <code class="language-plaintext highlighter-rouge">StructArray</code> hierarchically using a parent child relationship, with separate validity masks on each of the individual nullable arrays</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  ┌───────────────────┐        ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐
  │                   │           ┌─────────────────┐ ┌────────────┐
  │ ┌─────┐   ┌─────┐ │        │  │┌─────┐   ┌─────┐│ │  ┌─────┐   │ │
  │ │  1  │   │  1  │ │           ││  1  │   │  1  ││ │  │  3  │   │
  │ ├─────┤   ├─────┤ │        │  │├─────┤   ├─────┤│ │  ├─────┤   │ │
  │ │  1  │   │  2  │ │           ││  0  │   │ ??  ││ │  │  4  │   │
  │ ├─────┤   ├─────┤ │        │  │├─────┤   ├─────┤│ │  ├─────┤   │ │
  │ │  0  │   │ ??  │ │           ││  1  │   │  5  ││ │  │  6  │   │
  │ └─────┘   └─────┘ │        │  │└─────┘   └─────┘│ │  └─────┘   │ │
  │ Validity   Values │           │Validity   Values│ │   Values   │
  │                   │        │  │                 │ │            │ │
  │ "a"               │           │"b.b1"           │ │  "b.b2"    │
  │ PrimitiveArray    │        │  │PrimitiveArray   │ │  Primitive │ │
  └───────────────────┘           │                 │ │  Array     │
                               │  └─────────────────┘ └────────────┘ │
                                    "b"
                               │    StructArray                      │
                                ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─

┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┌─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
            ┌───────────┐                ┌──────────┐┌─────────────────┐ │
│  ┌─────┐  │ ┌─────┐   │ │ │  ┌─────┐   │┌─────┐   ││ ┌─────┐  ┌─────┐│
   │  0  │  │ │ ??  │   │      │  1  │   ││  1  │   ││ │  0  │  │ ??  ││ │
│  ├─────┤  │ ├─────┤   │ │ │  ├─────┤   │├─────┤   ││ ├─────┤  ├─────┤│
   │  1  │  │ │  6  │   │      │  1  │   ││  2  │   ││ │  1  │  │  1  ││ │
│  ├─────┤  │ ├─────┤   │ │ │  ├─────┤   │├─────┤   ││ ├─────┤  ├─────┤│
   │  1  │  │ │  7  │   │      │  0  │   ││ ??  │   ││ │ ??  │  │ ??  ││ │
│  └─────┘  │ └─────┘   │ │ │  └─────┘   │└─────┘   ││ └─────┘  └─────┘│
   Validity │  Values   │      Validity  │ Values   ││ Validity  Values│ │
│           │           │ │ │            │          ││                 │
            │ "c.c1"    │                │"d.d1"    ││ "d.d2"          │ │
│           │ Primitive │ │ │            │Primitive ││ PrimitiveArray  │
            │ Array     │                │Array     ││                 │ │
│           └───────────┘ │ │            └──────────┘└─────────────────┘
    "c"                         "d"                                      │
│   StructArray           │ │   StructArray
  ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘
</code></pre></div></div>

<p>More technical detail is available in the <a href="https://arrow.apache.org/docs/format/Columnar.html#struct-layout">StructArray format specification</a>.</p>

<h3 id="definition-levels">Definition Levels</h3>
<p>Unlike Arrow, Parquet does not encode validity in a structured fashion, instead only storing definition levels for each of the primitive columns, i.e. those that don’t contain other columns. The definition level of a given element is the depth in the schema at which it is fully defined.</p>

<p>For example consider the case of <code class="language-plaintext highlighter-rouge">d.d2</code>, which contains two nullable levels <code class="language-plaintext highlighter-rouge">d</code> and <code class="language-plaintext highlighter-rouge">d2</code>.</p>

<p>A definition level of <code class="language-plaintext highlighter-rouge">0</code> would imply a null at the level of <code class="language-plaintext highlighter-rouge">d</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
<span class="p">}</span>
</code></pre></div></div>

<p>A definition level of <code class="language-plaintext highlighter-rouge">1</code> would imply a null at the level of <code class="language-plaintext highlighter-rouge">d</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
  <span class="s">"d"</span><span class="p">:</span> <span class="p">{</span> <span class="n">null</span> <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>A definition level of <code class="language-plaintext highlighter-rouge">2</code> would imply a defined value for <code class="language-plaintext highlighter-rouge">d.d2</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
  <span class="s">"d"</span><span class="p">:</span> <span class="p">{</span> <span class="s">"d2"</span><span class="p">:</span> <span class="p">..</span> <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Going back to the three JSON documents above, they could be stored in Parquet with this schema</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">message</span> <span class="n">schema</span> <span class="p">{</span>
  <span class="n">optional</span> <span class="n">int32</span> <span class="n">a</span><span class="p">;</span>
  <span class="n">required</span> <span class="n">group</span> <span class="n">b</span> <span class="p">{</span>
    <span class="n">optional</span> <span class="n">int32</span> <span class="n">b1</span><span class="p">;</span>
    <span class="n">required</span> <span class="n">int32</span> <span class="n">b2</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="n">optional</span> <span class="n">group</span> <span class="n">c</span> <span class="p">{</span>
    <span class="n">required</span> <span class="n">int32</span> <span class="n">c1</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="n">optional</span> <span class="n">group</span> <span class="n">d</span> <span class="p">{</span>
    <span class="n">required</span> <span class="n">int32</span> <span class="n">d1</span><span class="p">;</span>
    <span class="n">optional</span> <span class="n">int32</span> <span class="n">d2</span><span class="p">;</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The Parquet encoding of the example would be:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code> ┌────────────────────────┐  ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
 │  ┌─────┐     ┌─────┐   │    ┌──────────────────────┐ ┌───────────┐ │
 │  │  1  │     │  1  │   │  │ │  ┌─────┐    ┌─────┐  │ │  ┌─────┐  │
 │  ├─────┤     ├─────┤   │    │  │  1  │    │  1  │  │ │  │  3  │  │ │
 │  │  1  │     │  2  │   │  │ │  ├─────┤    ├─────┤  │ │  ├─────┤  │
 │  ├─────┤     └─────┘   │    │  │  0  │    │  5  │  │ │  │  4  │  │ │
 │  │  0  │               │  │ │  ├─────┤    └─────┘  │ │  ├─────┤  │
 │  └─────┘               │    │  │  1  │             │ │  │  6  │  │ │
 │                        │  │ │  └─────┘             │ │  └─────┘  │
 │  Definition    Data    │    │                      │ │           │ │
 │    Levels              │  │ │  Definition   Data   │ │   Data    │
 │                        │    │    Levels            │ │           │ │
 │  "a"                   │  │ │                      │ │           │
 └────────────────────────┘    │  "b.b1"              │ │  "b.b2"   │ │
                             │ └──────────────────────┘ └───────────┘
                                  "b"                                 │
                             └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─


┌ ─ ─ ─ ─ ─ ── ─ ─ ─ ─ ─   ┌ ─ ─ ─ ─ ── ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
  ┌────────────────────┐ │   ┌────────────────────┐ ┌──────────────────┐ │
│ │  ┌─────┐   ┌─────┐ │   │ │  ┌─────┐   ┌─────┐ │ │ ┌─────┐  ┌─────┐ │
  │  │  0  │   │  6  │ │ │   │  │  1  │   │  1  │ │ │ │  1  │  │  1  │ │ │
│ │  ├─────┤   ├─────┤ │   │ │  ├─────┤   ├─────┤ │ │ ├─────┤  └─────┘ │
  │  │  1  │   │  7  │ │ │   │  │  1  │   │  2  │ │ │ │  2  │          │ │
│ │  ├─────┤   └─────┘ │   │ │  ├─────┤   └─────┘ │ │ ├─────┤          │
  │  │  1  │           │ │   │  │  0  │           │ │ │  0  │          │ │
│ │  └─────┘           │   │ │  └─────┘           │ │ └─────┘          │
  │                    │ │   │                    │ │                  │ │
│ │  Definition  Data  │   │ │  Definition  Data  │ │ Definition Data  │
  │    Levels          │ │   │    Levels          │ │   Levels         │ │
│ │                    │   │ │                    │ │                  │
  │  "c.1"             │ │   │  "d.1"             │ │  "d.d2"          │ │
│ └────────────────────┘   │ └────────────────────┘ └──────────────────┘
     "c"                 │      "d"                                      │
└ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─  └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
</code></pre></div></div>

<h2 id="list--repeated-columns">List / Repeated Columns</h2>

<p>Closing out support for nested types are <em>lists</em>, which contain a variable number of other values. For example, the following four documents each have a (nullable) field <code class="language-plaintext highlighter-rouge">a</code> containing a list of integers</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>                     <span class="c1"># &lt;-- First record
</span>  <span class="s">"a"</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span>           <span class="c1"># &lt;-- top-level field a containing list of integers
</span><span class="p">}</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>                     <span class="c1"># &lt;-- "a" is not provided (is null)
</span><span class="p">}</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>                     <span class="c1"># &lt;-- "a" is non-null but empty
</span>  <span class="s">"a"</span><span class="p">:</span> <span class="p">[]</span>
<span class="p">}</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
  <span class="s">"a"</span><span class="p">:</span> <span class="p">[</span><span class="n">null</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>     <span class="c1"># &lt;-- "a" has a null and non-null elements
</span><span class="p">}</span>
</code></pre></div></div>

<p>Documents of this format could be stored in this Arrow schema</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"a"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">List</span><span class="p">(</span>
  <span class="n">Field</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="s">"element"</span><span class="p">,</span> <span class="n">nullable</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span> <span class="n">datatype</span><span class="p">:</span> <span class="n">Int32</span><span class="p">),</span>
<span class="p">)</span>
</code></pre></div></div>

<p>As before, Arrow chooses to represent this in a hierarchical fashion as a <code class="language-plaintext highlighter-rouge">ListArray</code>. A <code class="language-plaintext highlighter-rouge">ListArray</code> contains a list of monotonically increasing integers called <em>offsets</em>, a validity mask if the list is nullable, and a child array containing the list elements. Each consecutive pair of elements in the offset array identifies a slice of the child array for that index in the ListArray</p>

<p>For example, a list with offsets <code class="language-plaintext highlighter-rouge">[0, 2, 3, 3]</code> contains 3 pairs of offsets, <code class="language-plaintext highlighter-rouge">(0,2)</code>, <code class="language-plaintext highlighter-rouge">(2,3)</code>, and <code class="language-plaintext highlighter-rouge">(3,3)</code>, and therefore represents a <code class="language-plaintext highlighter-rouge">ListArray</code> of length 3 with the following values:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">0</span><span class="p">:</span> <span class="p">[</span><span class="n">child</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">child</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
<span class="mi">1</span><span class="p">:</span> <span class="p">[]</span>
<span class="mi">2</span><span class="p">:</span> <span class="p">[</span><span class="n">child</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span>
</code></pre></div></div>

<p>For the example above with 4 JSON documents, this would be encoded in Arrow as</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─
                          ┌──────────────────┐ │
│    ┌─────┐   ┌─────┐    │ ┌─────┐   ┌─────┐│
     │  1  │   │  0  │    │ │  1  │   │  1  ││ │
│    ├─────┤   ├─────┤    │ ├─────┤   ├─────┤│
     │  0  │   │  1  │    │ │  0  │   │ ??  ││ │
│    ├─────┤   ├─────┤    │ ├─────┤   ├─────┤│
     │  1  │   │  1  │    │ │  1  │   │  2  ││ │
│    ├─────┤   ├─────┤    │ └─────┘   └─────┘│
     │  1  │   │  1  │    │ Validity   Values│ │
│    └─────┘   ├─────┤    │                  │
               │  3  │    │ child[0]         │ │
│    Validity  └─────┘    │ PrimitiveArray   │
                          │                  │ │
│              Offsets    └──────────────────┘
     "a"                                       │
│    ListArray
 ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘
</code></pre></div></div>

<p>More technical detail is available in the <a href="https://arrow.apache.org/docs/format/Columnar.html#variable-size-list-layout">ListArray format specification</a>.</p>

<h3 id="parquet-repetition-levels">Parquet Repetition Levels</h3>

<p>The example above with 4 JSON documents can be stored in this Parquet schema</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">message</span> <span class="n">schema</span> <span class="p">{</span>
  <span class="n">optional</span> <span class="n">group</span> <span class="n">a</span> <span class="p">(</span><span class="n">LIST</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">repeated</span> <span class="n">group</span> <span class="nb">list</span> <span class="p">{</span>
      <span class="n">optional</span> <span class="n">int32</span> <span class="n">element</span><span class="p">;</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>In order to encode lists, Parquet stores an integer <em>repetition level</em> in addition to a definition level. A repetition level identifies where in the hierarchy of repeated fields the current value is to be inserted. A value of <code class="language-plaintext highlighter-rouge">0</code> means a new list in the top-most repeated list, a value of <code class="language-plaintext highlighter-rouge">1</code> means a new element within the top-most repeated list, a value of <code class="language-plaintext highlighter-rouge">2</code> means a new element within the second top-most repeated list, and so on.</p>

<p>A consequence of this encoding is that the number of zeros in the <code class="language-plaintext highlighter-rouge">repetition</code> levels is the total number of rows in the column, and the first level in a column must be 0.</p>

<p>Each repeated field also has a corresponding definition level, however, in this case rather than indicating a null value, they indicate an empty array.</p>

<p>The example above would therefore be encoded as</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────────────────────────────────────┐
│  ┌─────┐      ┌─────┐               │
│  │  3  │      │  0  │               │
│  ├─────┤      ├─────┤               │
│  │  0  │      │  0  │               │
│  ├─────┤      ├─────┤      ┌─────┐  │
│  │  1  │      │  0  │      │  1  │  │
│  ├─────┤      ├─────┤      ├─────┤  │
│  │  2  │      │  0  │      │  2  │  │
│  ├─────┤      ├─────┤      └─────┘  │
│  │  3  │      │  1  │               │
│  └─────┘      └─────┘               │
│                                     │
│ Definition  Repetition      Values  │
│   Levels      Levels                │
│  "a"                                │
│                                     │
└─────────────────────────────────────┘
</code></pre></div></div>

<h2 id="next-up-arbitrary-nesting-lists-of-structs-and-structs-of-lists">Next up: Arbitrary Nesting: Lists of Structs and Structs of Lists</h2>

<p>In our <a href="https://arrow.apache.org/blog/2022/10/17/arrow-parquet-encoding-part-3/">final blog post</a>, we explain how Parquet and Arrow combine these concepts to support arbitrary nesting of potentially nullable data structures.</p>

<p>If you want to store and process structured types, you will be pleased to hear that the Rust <a href="https://crates.io/crates/parquet">parquet</a> implementation fully supports reading and writing directly into Arrow, as simply as any other type. All the complex record shredding and reconstruction is handled automatically. With this and other exciting features such as  <a href="https://docs.rs/parquet/22.0.0/parquet/arrow/async_reader/index.html">reading asynchronously</a> from <a href="https://docs.rs/object_store/0.5.0/object_store/">object storage</a>, and advanced row filter pushdown, it is the fastest and most feature complete Rust parquet implementation. We look forward to seeing what you build with it!</p>]]></content><author><name>tustvold and alamb</name></author><category term="parquet" /><category term="arrow" /><summary type="html"><![CDATA[Introduction This is the second, in a three part series exploring how projects such as Rust Apache Arrow support conversion between Apache Arrow and Apache Parquet. The first post covered the basics of data storage and validity encoding, and this post will cover the more complex Struct and List types. Apache Arrow is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. Apache Parquet is an open, column-oriented data file format designed for very efficient data encoding and retrieval. Struct / Group Columns Both Parquet and Arrow have the concept of a struct column, which is a column containing one or more other columns in named fields and is analogous to a JSON object. For example, consider the following three JSON documents { # &lt;-- First record "a": 1, # &lt;-- the top level fields are a, b, c, and d "b": { # &lt;-- b is always provided (not nullable) "b1": 1, # &lt;-- b1 and b2 are "nested" fields of "b" "b2": 3 # &lt;-- b2 is always provided (not nullable) }, "d": { "d1": 1 # &lt;-- d1 is a "nested" field of "d" } } { # &lt;-- Second record "a": 2, "b": { "b2": 4 # &lt;-- note "b1" is NULL in this record }, "c": { # &lt;-- note "c" was NULL in the first record "c1": 6 but when "c" is provided, c1 is also }, always provided (not nullable) "d": { "d1": 2, "d2": 1 } } { # &lt;-- Third record "b": { "b1": 5, "b2": 6 }, "c": { "c1": 7 } } Documents of this format could be stored in an Arrow StructArray with this schema Field(name: "a", nullable: true, datatype: Int32) Field(name: "b", nullable: false, datatype: Struct[ Field(name: "b1", nullable: true, datatype: Int32), Field(name: "b2", nullable: false, datatype: Int32) ]) Field(name: "c"), nullable: true, datatype: Struct[ Field(name: "c1", nullable: false, datatype: Int32) ]) Field(name: "d"), nullable: true, datatype: Struct[ Field(name: "d1", nullable: false, datatype: Int32) Field(name: "d2", nullable: true, datatype: Int32) ]) Arrow represents each StructArray hierarchically using a parent child relationship, with separate validity masks on each of the individual nullable arrays ┌───────────────────┐ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ │ │ ┌─────────────────┐ ┌────────────┐ │ ┌─────┐ ┌─────┐ │ │ │┌─────┐ ┌─────┐│ │ ┌─────┐ │ │ │ │ 1 │ │ 1 │ │ ││ 1 │ │ 1 ││ │ │ 3 │ │ │ ├─────┤ ├─────┤ │ │ │├─────┤ ├─────┤│ │ ├─────┤ │ │ │ │ 1 │ │ 2 │ │ ││ 0 │ │ ?? ││ │ │ 4 │ │ │ ├─────┤ ├─────┤ │ │ │├─────┤ ├─────┤│ │ ├─────┤ │ │ │ │ 0 │ │ ?? │ │ ││ 1 │ │ 5 ││ │ │ 6 │ │ │ └─────┘ └─────┘ │ │ │└─────┘ └─────┘│ │ └─────┘ │ │ │ Validity Values │ │Validity Values│ │ Values │ │ │ │ │ │ │ │ │ │ "a" │ │"b.b1" │ │ "b.b2" │ │ PrimitiveArray │ │ │PrimitiveArray │ │ Primitive │ │ └───────────────────┘ │ │ │ Array │ │ └─────────────────┘ └────────────┘ │ "b" │ StructArray │ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ ┌─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌───────────┐ ┌──────────┐┌─────────────────┐ │ │ ┌─────┐ │ ┌─────┐ │ │ │ ┌─────┐ │┌─────┐ ││ ┌─────┐ ┌─────┐│ │ 0 │ │ │ ?? │ │ │ 1 │ ││ 1 │ ││ │ 0 │ │ ?? ││ │ │ ├─────┤ │ ├─────┤ │ │ │ ├─────┤ │├─────┤ ││ ├─────┤ ├─────┤│ │ 1 │ │ │ 6 │ │ │ 1 │ ││ 2 │ ││ │ 1 │ │ 1 ││ │ │ ├─────┤ │ ├─────┤ │ │ │ ├─────┤ │├─────┤ ││ ├─────┤ ├─────┤│ │ 1 │ │ │ 7 │ │ │ 0 │ ││ ?? │ ││ │ ?? │ │ ?? ││ │ │ └─────┘ │ └─────┘ │ │ │ └─────┘ │└─────┘ ││ └─────┘ └─────┘│ Validity │ Values │ Validity │ Values ││ Validity Values│ │ │ │ │ │ │ │ ││ │ │ "c.c1" │ │"d.d1" ││ "d.d2" │ │ │ │ Primitive │ │ │ │Primitive ││ PrimitiveArray │ │ Array │ │Array ││ │ │ │ └───────────┘ │ │ └──────────┘└─────────────────┘ "c" "d" │ │ StructArray │ │ StructArray ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ More technical detail is available in the StructArray format specification. Definition Levels Unlike Arrow, Parquet does not encode validity in a structured fashion, instead only storing definition levels for each of the primitive columns, i.e. those that don’t contain other columns. The definition level of a given element is the depth in the schema at which it is fully defined. For example consider the case of d.d2, which contains two nullable levels d and d2. A definition level of 0 would imply a null at the level of d: { } A definition level of 1 would imply a null at the level of d { "d": { null } } A definition level of 2 would imply a defined value for d.d2: { "d": { "d2": .. } } Going back to the three JSON documents above, they could be stored in Parquet with this schema message schema { optional int32 a; required group b { optional int32 b1; required int32 b2; } optional group c { required int32 c1; } optional group d { required int32 d1; optional int32 d2; } } The Parquet encoding of the example would be: ┌────────────────────────┐ ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ ┌─────┐ ┌─────┐ │ ┌──────────────────────┐ ┌───────────┐ │ │ │ 1 │ │ 1 │ │ │ │ ┌─────┐ ┌─────┐ │ │ ┌─────┐ │ │ ├─────┤ ├─────┤ │ │ │ 1 │ │ 1 │ │ │ │ 3 │ │ │ │ │ 1 │ │ 2 │ │ │ │ ├─────┤ ├─────┤ │ │ ├─────┤ │ │ ├─────┤ └─────┘ │ │ │ 0 │ │ 5 │ │ │ │ 4 │ │ │ │ │ 0 │ │ │ │ ├─────┤ └─────┘ │ │ ├─────┤ │ │ └─────┘ │ │ │ 1 │ │ │ │ 6 │ │ │ │ │ │ │ └─────┘ │ │ └─────┘ │ │ Definition Data │ │ │ │ │ │ │ Levels │ │ │ Definition Data │ │ Data │ │ │ │ Levels │ │ │ │ │ "a" │ │ │ │ │ │ └────────────────────────┘ │ "b.b1" │ │ "b.b2" │ │ │ └──────────────────────┘ └───────────┘ "b" │ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ─ ── ─ ─ ─ ─ ─ ┌ ─ ─ ─ ─ ── ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌────────────────────┐ │ ┌────────────────────┐ ┌──────────────────┐ │ │ │ ┌─────┐ ┌─────┐ │ │ │ ┌─────┐ ┌─────┐ │ │ ┌─────┐ ┌─────┐ │ │ │ 0 │ │ 6 │ │ │ │ │ 1 │ │ 1 │ │ │ │ 1 │ │ 1 │ │ │ │ │ ├─────┤ ├─────┤ │ │ │ ├─────┤ ├─────┤ │ │ ├─────┤ └─────┘ │ │ │ 1 │ │ 7 │ │ │ │ │ 1 │ │ 2 │ │ │ │ 2 │ │ │ │ │ ├─────┤ └─────┘ │ │ │ ├─────┤ └─────┘ │ │ ├─────┤ │ │ │ 1 │ │ │ │ │ 0 │ │ │ │ 0 │ │ │ │ │ └─────┘ │ │ │ └─────┘ │ │ └─────┘ │ │ │ │ │ │ │ │ │ │ │ Definition Data │ │ │ Definition Data │ │ Definition Data │ │ Levels │ │ │ Levels │ │ Levels │ │ │ │ │ │ │ │ │ │ │ "c.1" │ │ │ "d.1" │ │ "d.d2" │ │ │ └────────────────────┘ │ └────────────────────┘ └──────────────────┘ "c" │ "d" │ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ List / Repeated Columns Closing out support for nested types are lists, which contain a variable number of other values. For example, the following four documents each have a (nullable) field a containing a list of integers { # &lt;-- First record "a": [1], # &lt;-- top-level field a containing list of integers } { # &lt;-- "a" is not provided (is null) } { # &lt;-- "a" is non-null but empty "a": [] } { "a": [null, 2], # &lt;-- "a" has a null and non-null elements } Documents of this format could be stored in this Arrow schema Field(name: "a", nullable: true, datatype: List( Field(name: "element", nullable: true, datatype: Int32), ) As before, Arrow chooses to represent this in a hierarchical fashion as a ListArray. A ListArray contains a list of monotonically increasing integers called offsets, a validity mask if the list is nullable, and a child array containing the list elements. Each consecutive pair of elements in the offset array identifies a slice of the child array for that index in the ListArray For example, a list with offsets [0, 2, 3, 3] contains 3 pairs of offsets, (0,2), (2,3), and (3,3), and therefore represents a ListArray of length 3 with the following values: 0: [child[0], child[1]] 1: [] 2: [child[2]] For the example above with 4 JSON documents, this would be encoded in Arrow as ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌──────────────────┐ │ │ ┌─────┐ ┌─────┐ │ ┌─────┐ ┌─────┐│ │ 1 │ │ 0 │ │ │ 1 │ │ 1 ││ │ │ ├─────┤ ├─────┤ │ ├─────┤ ├─────┤│ │ 0 │ │ 1 │ │ │ 0 │ │ ?? ││ │ │ ├─────┤ ├─────┤ │ ├─────┤ ├─────┤│ │ 1 │ │ 1 │ │ │ 1 │ │ 2 ││ │ │ ├─────┤ ├─────┤ │ └─────┘ └─────┘│ │ 1 │ │ 1 │ │ Validity Values│ │ │ └─────┘ ├─────┤ │ │ │ 3 │ │ child[0] │ │ │ Validity └─────┘ │ PrimitiveArray │ │ │ │ │ Offsets └──────────────────┘ "a" │ │ ListArray ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ More technical detail is available in the ListArray format specification. Parquet Repetition Levels The example above with 4 JSON documents can be stored in this Parquet schema message schema { optional group a (LIST) { repeated group list { optional int32 element; } } } In order to encode lists, Parquet stores an integer repetition level in addition to a definition level. A repetition level identifies where in the hierarchy of repeated fields the current value is to be inserted. A value of 0 means a new list in the top-most repeated list, a value of 1 means a new element within the top-most repeated list, a value of 2 means a new element within the second top-most repeated list, and so on. A consequence of this encoding is that the number of zeros in the repetition levels is the total number of rows in the column, and the first level in a column must be 0. Each repeated field also has a corresponding definition level, however, in this case rather than indicating a null value, they indicate an empty array. The example above would therefore be encoded as ┌─────────────────────────────────────┐ │ ┌─────┐ ┌─────┐ │ │ │ 3 │ │ 0 │ │ │ ├─────┤ ├─────┤ │ │ │ 0 │ │ 0 │ │ │ ├─────┤ ├─────┤ ┌─────┐ │ │ │ 1 │ │ 0 │ │ 1 │ │ │ ├─────┤ ├─────┤ ├─────┤ │ │ │ 2 │ │ 0 │ │ 2 │ │ │ ├─────┤ ├─────┤ └─────┘ │ │ │ 3 │ │ 1 │ │ │ └─────┘ └─────┘ │ │ │ │ Definition Repetition Values │ │ Levels Levels │ │ "a" │ │ │ └─────────────────────────────────────┘ Next up: Arbitrary Nesting: Lists of Structs and Structs of Lists In our final blog post, we explain how Parquet and Arrow combine these concepts to support arbitrary nesting of potentially nullable data structures. If you want to store and process structured types, you will be pleased to hear that the Rust parquet implementation fully supports reading and writing directly into Arrow, as simply as any other type. All the complex record shredding and reconstruction is handled automatically. With this and other exciting features such as reading asynchronously from object storage, and advanced row filter pushdown, it is the fastest and most feature complete Rust parquet implementation. We look forward to seeing what you build with it!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Arrow and Parquet Part 1: Primitive Types and Nullability</title><link href="https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/" rel="alternate" type="text/html" title="Arrow and Parquet Part 1: Primitive Types and Nullability" /><published>2022-10-05T00:00:00-04:00</published><updated>2022-10-05T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/"><![CDATA[<!--

-->

<h2 id="introduction">Introduction</h2>

<p>We recently completed a long-running project within <a href="https://github.com/apache/arrow-rs">Rust Apache Arrow</a> to complete support for reading and writing arbitrarily nested Parquet and Arrow schemas. This is a complex topic, and we encountered a lack of approachable technical information, and thus wrote this blog to share our learnings with the community.</p>

<p><a href="https://arrow.apache.org/">Apache Arrow</a> is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. <a href="https://parquet.apache.org/">Apache Parquet</a> is an open, column-oriented data file format designed for very efficient data encoding and retrieval.</p>

<p>It is increasingly common for analytic systems to use Arrow to process data stored in Parquet files, and therefore fast, efficient, and correct translation between them is a key building block.</p>

<p>Historically analytic processing primarily focused on querying data with a tabular schema, where there are a fixed number of columns, and each row contains a single value for each column. However, with the increasing adoption of structured document formats such as XML, JSON, etc…, only supporting tabular schema can be frustrating for users, as it necessitates often non-trivial data transformation to first flatten the document data.</p>

<p>As of version <a href="https://crates.io/crates/arrow/20.0.0">20.0.0</a>, released in August 2022, the Rust Arrow implementation for reading structured types is feature complete. Instructions for getting started can be found <a href="https://docs.rs/parquet/latest/parquet/arrow/index.html">here</a> and feel free to raise any issues on our <a href="https://github.com/apache/arrow-rs/issues">bugtracker</a>.</p>

<p>In this series we will explain how Parquet and Arrow represent nested data, highlighting the similarities and differences between them, and give a flavor of the practicalities of converting between the formats.</p>

<h2 id="columnar-vs-record-oriented">Columnar vs Record-Oriented</h2>

<p>First, it is necessary to take a step back and discuss the difference between columnar and record-oriented data formats. In a record oriented data format, such as newline-delimited JSON (NDJSON), all the values for a given record are stored contiguously.</p>

<p>For example</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="s">"Column1"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">"Column2"</span><span class="p">:</span> <span class="mi">2</span><span class="p">}</span>
<span class="p">{</span><span class="s">"Column1"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s">"Column2"</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s">"Column3"</span><span class="p">:</span> <span class="mi">5</span><span class="p">}</span>
<span class="p">{</span><span class="s">"Column1"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s">"Column2"</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s">"Column3"</span><span class="p">:</span> <span class="mi">5</span><span class="p">}</span>
</code></pre></div></div>

<p>In a columnar representation, the data for a given column is instead stored contiguously</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Column1</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">Column2</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">Column3</span><span class="p">:</span> <span class="p">[</span><span class="n">null</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
</code></pre></div></div>

<p>Aside from potentially yielding better data compression, a columnar layout can dramatically improve performance of certain queries. This is because laying data out contiguously in memory allows both the compiler and CPU to better exploit opportunities for parallelism. The specifics of <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data">SIMD</a> and <a href="https://en.wikipedia.org/wiki/Instruction-level_parallelism">ILP</a> are well beyond the scope of this post, but the important takeaway is that processing large blocks of data without intervening conditional branches has substantial performance benefits.</p>

<h2 id="parquet-vs-arrow">Parquet vs Arrow</h2>
<p>Parquet and Arrow are complementary technologies, and they make some different design tradeoffs. In particular, Parquet is a storage format designed for maximum space efficiency, whereas Arrow is an in-memory format intended for operation by vectorized computational kernels.</p>

<p>The major distinction is that Arrow provides <code class="language-plaintext highlighter-rouge">O(1)</code> random access lookups to any array index, whilst Parquet does not. In particular, Parquet uses <a href="https://akshays-blog.medium.com/wrapping-head-around-repetition-and-definition-levels-in-dremel-powering-bigquery-c1a33c9695da">dremel record shredding</a>, <a href="https://github.com/apache/parquet-format/blob/master/Encodings.md">variable length encoding schemes</a>, and <a href="https://github.com/apache/parquet-format/blob/master/Compression.md">block compression</a> to drastically reduce the data size, but these techniques come at the loss of performant random access lookups.</p>

<p>A common pattern that plays to each technologies strengths, is to stream data from a compressed representation, such as Parquet, in thousand row batches in the Arrow format, process these batches individually, and accumulate the results in a more compressed representation. This benefits from the ability to efficiently perform computations on Arrow data, whilst keeping memory requirements in check, and allowing the computation kernels to be agnostic to the encodings of the source and destination.</p>

<p><strong>Arrow is primarily an in-memory format, whereas Parquet is a storage format.</strong></p>

<h2 id="non-nullable-primitive-column">Non-Nullable Primitive Column</h2>

<p>Let us start with the simplest case of a non-nullable list of 32-bit signed integers.</p>

<p>In Arrow this would be represented as a <code class="language-plaintext highlighter-rouge">PrimitiveArray</code>, which would store them contiguously in memory</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────┐
│  1  │
├─────┤
│  2  │
├─────┤
│  3  │
├─────┤
│  4  │
└─────┘
Values
</code></pre></div></div>

<p>Parquet has multiple <a href="https://parquet.apache.org/docs/file-format/data-pages/encodings/">different encodings</a> that may be used for integer types, the exact details of which are beyond the scope of this post. Broadly speaking the data will be stored in one or more <a href="https://parquet.apache.org/docs/file-format/data-pages/"><em>DataPage</em></a>s containing the integers in an encoded form</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────┐
│  1  │
├─────┤
|  2  │
├─────┤
│  3  │
├─────┤
│  4  │
└─────┘
Values
</code></pre></div></div>

<h1 id="nullable-primitive-column">Nullable Primitive Column</h1>

<p>Now let us consider the case of a nullable column, where some of the values might have the special sentinel value <code class="language-plaintext highlighter-rouge">NULL</code> that designates “this value is unknown”.</p>

<p>In Arrow, nulls are stored separately from the values in the form of a <a href="https://arrow.apache.org/docs/format/Columnar.html#validity-bitmaps">validity bitmask</a>, with arbitrary data in the corresponding positions in the values buffer. This space efficient encoding means that the entire validity mask for the following example is stored using 5 bits</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────┐   ┌─────┐
│  1  │   │  1  │
├─────┤   ├─────┤
│  0  │   │ ??  │
├─────┤   ├─────┤
│  1  │   │  3  │
├─────┤   ├─────┤
│  1  │   │  4  │
├─────┤   ├─────┤
│  0  │   │ ??  │
└─────┘   └─────┘
Validity   Values
</code></pre></div></div>

<p>In Parquet the validity information is also stored separately from the values, however, instead of being encoded as a validity bitmask it is encoded as a list of 16-bit integers called <em>definition levels</em>. Like other data in Parquet, these integer definition levels are stored using high efficiency encoding, and will be expanded upon in the next post, but for now a definition level of <code class="language-plaintext highlighter-rouge">1</code> indicates a valid value, and <code class="language-plaintext highlighter-rouge">0</code> a null value. Unlike Arrow, nulls are not encoded in the list of values</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────┐    ┌─────┐
│  1  │    │  1  │
├─────┤    ├─────┤
│  0  │    │  3  │
├─────┤    ├─────┤
│  1  │    │  4  │
├─────┤    └─────┘
│  1  │
├─────┤
│  0  │
└─────┘
Definition  Values
 Levels
</code></pre></div></div>

<h2 id="next-up-nested-and-hierarchical-data">Next up: Nested and Hierarchical Data</h2>

<p>Armed with the foundational understanding of how Arrow and Parquet store nullability / definition differently we are ready to move on to more complex nested types, which you can read about in our <a href="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/">next blog post on the topic</a>.</p>]]></content><author><name>tustvold and alamb</name></author><category term="parquet" /><category term="arrow" /><summary type="html"><![CDATA[Introduction We recently completed a long-running project within Rust Apache Arrow to complete support for reading and writing arbitrarily nested Parquet and Arrow schemas. This is a complex topic, and we encountered a lack of approachable technical information, and thus wrote this blog to share our learnings with the community. Apache Arrow is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. Apache Parquet is an open, column-oriented data file format designed for very efficient data encoding and retrieval. It is increasingly common for analytic systems to use Arrow to process data stored in Parquet files, and therefore fast, efficient, and correct translation between them is a key building block. Historically analytic processing primarily focused on querying data with a tabular schema, where there are a fixed number of columns, and each row contains a single value for each column. However, with the increasing adoption of structured document formats such as XML, JSON, etc…, only supporting tabular schema can be frustrating for users, as it necessitates often non-trivial data transformation to first flatten the document data. As of version 20.0.0, released in August 2022, the Rust Arrow implementation for reading structured types is feature complete. Instructions for getting started can be found here and feel free to raise any issues on our bugtracker. In this series we will explain how Parquet and Arrow represent nested data, highlighting the similarities and differences between them, and give a flavor of the practicalities of converting between the formats. Columnar vs Record-Oriented First, it is necessary to take a step back and discuss the difference between columnar and record-oriented data formats. In a record oriented data format, such as newline-delimited JSON (NDJSON), all the values for a given record are stored contiguously. For example {"Column1": 1, "Column2": 2} {"Column1": 3, "Column2": 4, "Column3": 5} {"Column1": 5, "Column2": 4, "Column3": 5} In a columnar representation, the data for a given column is instead stored contiguously Column1: [1, 3, 5] Column2: [2, 4, 4] Column3: [null, 5, 5] Aside from potentially yielding better data compression, a columnar layout can dramatically improve performance of certain queries. This is because laying data out contiguously in memory allows both the compiler and CPU to better exploit opportunities for parallelism. The specifics of SIMD and ILP are well beyond the scope of this post, but the important takeaway is that processing large blocks of data without intervening conditional branches has substantial performance benefits. Parquet vs Arrow Parquet and Arrow are complementary technologies, and they make some different design tradeoffs. In particular, Parquet is a storage format designed for maximum space efficiency, whereas Arrow is an in-memory format intended for operation by vectorized computational kernels. The major distinction is that Arrow provides O(1) random access lookups to any array index, whilst Parquet does not. In particular, Parquet uses dremel record shredding, variable length encoding schemes, and block compression to drastically reduce the data size, but these techniques come at the loss of performant random access lookups. A common pattern that plays to each technologies strengths, is to stream data from a compressed representation, such as Parquet, in thousand row batches in the Arrow format, process these batches individually, and accumulate the results in a more compressed representation. This benefits from the ability to efficiently perform computations on Arrow data, whilst keeping memory requirements in check, and allowing the computation kernels to be agnostic to the encodings of the source and destination. Arrow is primarily an in-memory format, whereas Parquet is a storage format. Non-Nullable Primitive Column Let us start with the simplest case of a non-nullable list of 32-bit signed integers. In Arrow this would be represented as a PrimitiveArray, which would store them contiguously in memory ┌─────┐ │ 1 │ ├─────┤ │ 2 │ ├─────┤ │ 3 │ ├─────┤ │ 4 │ └─────┘ Values Parquet has multiple different encodings that may be used for integer types, the exact details of which are beyond the scope of this post. Broadly speaking the data will be stored in one or more DataPages containing the integers in an encoded form ┌─────┐ │ 1 │ ├─────┤ | 2 │ ├─────┤ │ 3 │ ├─────┤ │ 4 │ └─────┘ Values Nullable Primitive Column Now let us consider the case of a nullable column, where some of the values might have the special sentinel value NULL that designates “this value is unknown”. In Arrow, nulls are stored separately from the values in the form of a validity bitmask, with arbitrary data in the corresponding positions in the values buffer. This space efficient encoding means that the entire validity mask for the following example is stored using 5 bits ┌─────┐ ┌─────┐ │ 1 │ │ 1 │ ├─────┤ ├─────┤ │ 0 │ │ ?? │ ├─────┤ ├─────┤ │ 1 │ │ 3 │ ├─────┤ ├─────┤ │ 1 │ │ 4 │ ├─────┤ ├─────┤ │ 0 │ │ ?? │ └─────┘ └─────┘ Validity Values In Parquet the validity information is also stored separately from the values, however, instead of being encoded as a validity bitmask it is encoded as a list of 16-bit integers called definition levels. Like other data in Parquet, these integer definition levels are stored using high efficiency encoding, and will be expanded upon in the next post, but for now a definition level of 1 indicates a valid value, and 0 a null value. Unlike Arrow, nulls are not encoded in the list of values ┌─────┐ ┌─────┐ │ 1 │ │ 1 │ ├─────┤ ├─────┤ │ 0 │ │ 3 │ ├─────┤ ├─────┤ │ 1 │ │ 4 │ ├─────┤ └─────┘ │ 1 │ ├─────┤ │ 0 │ └─────┘ Definition Values Levels Next up: Nested and Hierarchical Data Armed with the foundational understanding of how Arrow and Parquet store nullability / definition differently we are ready to move on to more complex nested types, which you can read about in our next blog post on the topic.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 9.0.0 Release</title><link href="https://arrow.apache.org/blog/2022/08/16/9.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 9.0.0 Release" /><published>2022-08-16T00:00:00-04:00</published><updated>2022-08-16T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/08/16/9.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/08/16/9.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 9.0.0 release. This covers
over 3 months of development work and includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%209.0.0"><strong>509 resolved issues</strong></a>
from <a href="/release/9.0.0.html#contributors"><strong>114 distinct contributors</strong></a>. See the Install Page to learn how to
get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bug fixes and improvements have been made: we refer
you to the <a href="/release/9.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 8.0.0 release, Dewey Dunnington, Alenka Frim and Rok Mihevc
have been invited to be committers.
Thanks for your contributions and participation in the project!</p>

<h2 id="columnar-format-notes">Columnar Format Notes</h2>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>

<p>Arrow Flight is now available in MacOS M1 Python wheels (<a href="https://issues.apache.org/jira/browse/ARROW-16779">ARROW-16779</a>).
Arrow Flight SQL is now buildable on Windows (<a href="https://issues.apache.org/jira/browse/ARROW-16902">ARROW-16902</a>).
Ruby now exposes more of the Flight and Flight SQL APIs (various JIRAs).</p>

<h2 id="linux-packages-notes">Linux packages notes</h2>

<p>AlmaLinux 9 is now supported. (<a href="https://issues.apache.org/jira/browse/ARROW-16745">ARROW-16745</a>)</p>

<p>AmazonLinux 2 aarch64 is now supported. (<a href="https://issues.apache.org/jira/browse/ARROW-16477">ARROW-16477</a>)</p>

<h2 id="c-notes">C++ notes</h2>

<p>STL-like iteration is now provided over chunked arrays (<a href="https://issues.apache.org/jira/browse/ARROW-602">ARROW-602</a>).</p>

<h3 id="compute">Compute</h3>

<p>The C++ compute and execution engine is now officially named “Acero”, though
its C++ namespaces have not changed.</p>

<p>New light-weight data holder abstractions have been introduced in order
to reduce the overhead of invoking compute functions and kernels, especially
at the small data sizes desirable for efficient parallelization (typically
L1- or L2-sized).  Specifically, the non-owning <code class="language-plaintext highlighter-rouge">ArraySpan</code> and <code class="language-plaintext highlighter-rouge">ExecSpan</code>
structures have internally superseded the much heavier <code class="language-plaintext highlighter-rouge">ExecBatch</code>, which
is still supported for compatibility at the API level
(<a href="https://issues.apache.org/jira/browse/ARROW-16756">ARROW-16756</a>, <a href="https://issues.apache.org/jira/browse/ARROW-16824">ARROW-16824</a>, <a href="https://issues.apache.org/jira/browse/ARROW-16852">ARROW-16852</a>).</p>

<p>In a similar vein, the <code class="language-plaintext highlighter-rouge">ValueDescr</code> class was removed and <code class="language-plaintext highlighter-rouge">ScalarKernel</code>
implementations now always receive at least one non-scalar input, removing
the special case where a <code class="language-plaintext highlighter-rouge">ScalarKernel</code> needs to output a scalar rather than
an array. The higher-level compute APIs still allow executing a scalar function
over all-scalar inputs; but those scalars are internally broadcasted to
1-element arrays so as to simplify kernel implementation (<a href="https://issues.apache.org/jira/browse/ARROW-16757">ARROW-16757</a>).</p>

<p>Some performance improvements were made to the hash join node.  These changes
do not require additional configuration.  The hash join exec node has been
improved to more efficiently use CPU cache and make better use of available
vectorization hardware (<a href="https://issues.apache.org/jira/browse/ARROW-14182">ARROW-14182</a>).</p>

<p>Some plans containing a sequence of hash join operators will now use bloom
filters to eliminate rows earlier in the plan, reducing the overall CPU
cost of the plan (<a href="https://issues.apache.org/jira/browse/ARROW-15498">ARROW-15498</a>).</p>

<p>Timestamp comparison is now supported (<a href="https://issues.apache.org/jira/browse/ARROW-16425">ARROW-16425</a>).</p>

<p>A cumulative sum function is implemented over numeric inputs (<a href="https://issues.apache.org/jira/browse/ARROW-13530">ARROW-13530</a>). Note that this is a vector
function so cannot be used in an Acero ExecPlan.</p>

<p>A rank vector kernel has been added (<a href="https://issues.apache.org/jira/browse/ARROW-16234">ARROW-16234</a>).</p>

<p>Temporal rounding functions received additional options to control how
rounding is done (<a href="https://issues.apache.org/jira/browse/ARROW-14821">ARROW-14821</a>).</p>

<p>Improper computation of the “mode” function on boolean input was fixed
(<a href="https://issues.apache.org/jira/browse/ARROW-17096">ARROW-17096</a>).</p>

<p>Function registries can now be nested (<a href="https://issues.apache.org/jira/browse/ARROW-16677">ARROW-16677</a>).</p>

<h3 id="dataset">Dataset</h3>

<p>The <code class="language-plaintext highlighter-rouge">autogenerate_column_names</code> option for CSV reading is now handled correctly
(<a href="https://issues.apache.org/jira/browse/ARROW-16436">ARROW-16436</a>).</p>

<p>Fix <code class="language-plaintext highlighter-rouge">InMemoryDataset::ReplaceSchema</code> to actually replace the schema
(<a href="https://issues.apache.org/jira/browse/ARROW-16085">ARROW-16085</a>).</p>

<p>Fix <code class="language-plaintext highlighter-rouge">FilenamePartitioning</code> to properly support null values (<a href="https://issues.apache.org/jira/browse/ARROW-16302">ARROW-16302</a>).</p>

<h3 id="filesystem">Filesystem</h3>

<p>A number of bug fixes and improvements were made to the Google Cloud Storage
filesystem implementation (<a href="https://issues.apache.org/jira/browse/ARROW-14892">ARROW-14892</a>).</p>

<p>By default, the S3 filesystem implementation does not create or drop buckets
anymore (<a href="https://issues.apache.org/jira/browse/ARROW-15906">ARROW-15906</a>). This is a compatibility-breaking change intended
to prevent user errors from having potentially catastrophic consequences.
Options have been added to restore the previous behavior if necessary.</p>

<h3 id="parquet">Parquet</h3>

<p>The default Parquet version is now 2.4 for writing, enabling use of
more recent logical types by default (<a href="https://issues.apache.org/jira/browse/ARROW-12203">ARROW-12203</a>).</p>

<p>Non-nullable fields are now handled correctly by the Parquet reader
(<a href="https://issues.apache.org/jira/browse/ARROW-16116">ARROW-16116</a>).</p>

<p>Reading encrypted files should now be thread-safe (<a href="https://issues.apache.org/jira/browse/ARROW-14114">ARROW-14114</a>).</p>

<p>Statistics equality now works correctly with minmax (<a href="https://issues.apache.org/jira/browse/ARROW-16487">ARROW-16487</a>).</p>

<p>The minimum Thrift version required for building is now 0.13 (<a href="https://issues.apache.org/jira/browse/ARROW-16721">ARROW-16721</a>).</p>

<p>The Thrift deserialization limits can now be configured to accommodate for
data files with very large metadata (<a href="https://issues.apache.org/jira/browse/ARROW-16546">ARROW-16546</a>).</p>

<h3 id="substrait">Substrait</h3>

<p>The Substrait spec has been updated to 0.6.0 (<a href="https://issues.apache.org/jira/browse/ARROW-16816">ARROW-16816</a>). In addition, a
larger subset of the Substrait specification is now supported (<a href="https://issues.apache.org/jira/browse/ARROW-15587">ARROW-15587</a>,
<a href="https://issues.apache.org/jira/browse/ARROW-15590">ARROW-15590</a>,
<a href="https://issues.apache.org/jira/browse/ARROW-15901">ARROW-15901</a>,
<a href="https://issues.apache.org/jira/browse/ARROW-16657">ARROW-16657</a>,
<a href="https://issues.apache.org/jira/browse/ARROW-15591">ARROW-15591</a>).</p>

<h2 id="c-notes-1">C# notes</h2>

<h4 id="new-features">New Features</h4>

<ul>
  <li>Added support for Time32Array and Time64Array (<a href="https://github.com/apache/arrow/pull/13279">ARROW-16660</a>)</li>
</ul>

<h4 id="bug-fixes">Bug Fixes</h4>

<ul>
  <li>When using TableFromRecordBatches, the resulting table columns have no data array. (<a href="https://github.com/apache/arrow/pull/10562">ARROW-13129</a>)</li>
  <li>Fix intermittent test failures due to async memory management bug. (<a href="https://github.com/apache/arrow/pull/13573">ARROW-16978</a>)</li>
</ul>

<h2 id="go-notes">Go notes</h2>

<h3 id="security">Security</h3>

<ul>
  <li>Updated testify dependency to address CVE-2022-28948. (<a href="https://issues.apache.org/jira/browse/ARROW-16759">ARROW-16759</a>) (This was also backported to previous versions and released as patch versions v6.0.2, v7.0.1, and v8.0.1)</li>
</ul>

<h3 id="arrow">Arrow</h3>

<h4 id="new-features-1">New Features</h4>

<ul>
  <li>Dictionary Scalars are now available (<a href="https://issues.apache.org/jira/browse/ARROW-16323">ARROW-16323</a>)</li>
  <li>Introduced a DictionaryUnifier object along with functions for unifying Chunked Arrays and Tables (<a href="https://issues.apache.org/jira/browse/ARROW-16324">ARROW-16324</a>)</li>
  <li>New CSV examples added to documentation to demonstrate error handling (<a href="https://issues.apache.org/jira/browse/ARROW-16450">ARROW-16450</a>)</li>
  <li>CSV Reader now supports arrow.TimestampType (<a href="https://issues.apache.org/jira/browse/ARROW-16504">ARROW-16504</a>)</li>
  <li>JSON parsing for Temporal Types now allow passing numeric values in addition to strings for parsing. Timezones will be properly parsed if they exist in the string and a function was added to retrieve a time.Location object from a TimestampType (<a href="https://issues.apache.org/jira/browse/ARROW-16551">ARROW-16551</a>)</li>
  <li>New utilities added to decimal128 for rescaling and easy conversion to and from float32/float64 (<a href="https://issues.apache.org/jira/browse/ARROW-16552">ARROW-16552</a>)</li>
  <li>Arrow DataType interface now has a LayoutMethod which returns the physical layout of the given datatype such as the number of buffers, types, etc. This matches the behavior of the layout() methods in C++ for data types. (<a href="https://issues.apache.org/jira/browse/ARROW-16556">ARROW-16556</a>)</li>
  <li>Added a SliceBuffer function to the memory package to allow better re-using of memory across buffer objects (<a href="https://issues.apache.org/jira/browse/ARROW-16557">ARROW-16557</a>)</li>
  <li>Dictionary Arrays can now be concatenated using array.Concatenate (<a href="https://issues.apache.org/jira/browse/ARROW-17095">ARROW-17095</a>)</li>
</ul>

<h4 id="bug-fixes-1">Bug Fixes</h4>

<ul>
  <li>ipc.FileReader now properly uses the memory.Allocator interface (<a href="https://issues.apache.org/jira/browse/ARROW-16002">ARROW-16002</a>)</li>
  <li>Addressed issue with Integration tests between Go and Java (<a href="https://issues.apache.org/jira/browse/ARROW-16441">ARROW-16441</a>)</li>
  <li>RecordBuilder.UnmarshalJSON now properly ignores extra unknown fields rather than panicking (<a href="https://issues.apache.org/jira/browse/ARROW-16456">ARROW-16456</a>)</li>
  <li>StructBuilder.UnmarshalJSON will no longer fail and panic when Nullable fields are missing (<a href="https://issues.apache.org/jira/browse/ARROW-16502">ARROW-16502</a>)</li>
  <li>ipc.Reader no longer silently accepts string columns with invalid offsets, preventing unexpected panics later when writing or accessing the resulting arrays. (<a href="https://issues.apache.org/jira/browse/ARROW-16831">ARROW-16831</a>)</li>
  <li>Arrow CSV reader no longer clobbers its reported errors and properly surfaces them (<a href="https://issues.apache.org/jira/browse/ARROW-16926">ARROW-16926</a>)</li>
</ul>

<h3 id="parquet-1">Parquet</h3>

<h4 id="new-features-2">New Features</h4>

<ul>
  <li>The CreatedBy version string for the Parquet writer will now correctly reflect the library version, and will be updated by the release scripts (<a href="https://issues.apache.org/jira/browse/ARROW-16484">ARROW-16484</a>)</li>
  <li>Parquet bit_packing functions now have ARM64 NEON implementations for performance (<a href="https://issues.apache.org/jira/browse/ARROW-16486">ARROW-16486</a>)</li>
  <li>It is now possible to customize the root node in the Parquet writer instead of hardcoding it to be named “schema” with a repetition type of Repeated. This was needed to allow producing files similar to Apache Spark where the root node has a repetition type of Required. It still defaults to the spec definition of Repeated. (<a href="https://issues.apache.org/jira/browse/ARROW-16561">ARROW-16561</a>)</li>
  <li>parquet_reader CLI mainprog has been enhanced to dump values out as JSON and CSV along with setting an output file instead of just dumping to the terminal. (<a href="https://issues.apache.org/jira/browse/ARROW-16934">ARROW-16934</a>)</li>
</ul>

<h4 id="bug-fixes-2">Bug Fixes</h4>

<ul>
  <li>Fixed a memory leak with Parquet page reading (<a href="https://issues.apache.org/jira/browse/ARROW-16473">ARROW-16473</a>)</li>
  <li>Parquet Reader properly parallelizes column reads when the parallel option is set to true. (<a href="https://issues.apache.org/jira/browse/ARROW-16530">ARROW-16530</a>)</li>
  <li>Fixed bug in the Bool decoder for plain encoding (<a href="https://issues.apache.org/jira/browse/ARROW-16563">ARROW-16563</a>)</li>
  <li>Fixed a bug in the Parquet bool column reader where it failed to properly skip rows (<a href="https://issues.apache.org/jira/browse/ARROW-16638">ARROW-16638</a>)</li>
  <li>Fixed the flakey travis ARM64 builds by reducing the size of a test case in the pqarrow unit tests to reduce the memory usage for the tests. (<a href="https://issues.apache.org/jira/browse/ARROW-16669">ARROW-16669</a>)</li>
  <li>Parquet writer now properly handles writing arrow.NULL type arrays (<a href="https://issues.apache.org/jira/browse/ARROW-16749">ARROW-16749</a>)</li>
  <li>Column level dictionary encoding configuration for Parquet writing now correctly respects the input value (<a href="https://issues.apache.org/jira/browse/ARROW-16813">ARROW-16813</a>)</li>
  <li>Memory leak in DeltaByteArray encoding fixed (<a href="https://issues.apache.org/jira/browse/ARROW-16983">ARROW-16983</a>)</li>
</ul>

<h2 id="java-notes">Java notes</h2>
<h4 id="new-features-3">New Features</h4>
<ul>
  <li>Allow overriding column nullability in arrow-jdbc (<a href="https://github.com/apache/arrow/pull/13558">#13558</a>)</li>
  <li>Enable skip BOUNDS_CHECKING with setBytes and getBytes of ArrowBuf (<a href="https://github.com/apache/arrow/pull/13161">#13161</a>)</li>
  <li>Initialize JNI components on use instead of statically (<a href="https://github.com/apache/arrow/pull/13146">#13146</a>)</li>
  <li>Provide explicit JDBC column type mapping (<a href="https://github.com/apache/arrow/pull/13166">#13166</a>)</li>
  <li>Allow duplicated field names in Java C data interface (<a href="https://github.com/apache/arrow/pull/13247">#13247</a>)</li>
  <li>Improve and document StackTrace (<a href="https://github.com/apache/arrow/pull/12656">#12656</a>)</li>
  <li>Keep more context when marshaling errors through JNI (<a href="https://github.com/apache/arrow/pull/13246">#13246</a>)</li>
  <li>Make RoundingMode configurable to handle inconsistent scale in BigDecimals (<a href="https://github.com/apache/arrow/pull/13433">#13433</a>)</li>
  <li>Improve Java dev experience with IntelliJ (<a href="https://github.com/apache/arrow/pull/13017">#13017</a>)</li>
  <li>Implement ArrowArrayStream (<a href="https://github.com/apache/arrow/pull/13465">#13465</a>))</li>
</ul>

<h4 id="bug-fixes-3">Bug Fixes</h4>
<ul>
  <li>Fix variable-width vectors in integration JSON writer (<a href="https://github.com/apache/arrow/pull/13676">#13676</a>)</li>
  <li>Handle empty JDBC ResultSet (<a href="https://github.com/apache/arrow/pull/13049">#13049</a>)</li>
  <li>Fix hasNext() in ArrowVectorIterator (<a href="https://github.com/apache/arrow/pull/13107">#13107</a>)</li>
  <li>Fix ArrayConsumer when using ArrowVectorIterator (<a href="https://github.com/apache/arrow/pull/12692">#12692</a>)</li>
  <li>Update Gandiva Protobuf library to enable builds on Apple M1 (<a href="https://github.com/apache/arrow/pull/13121">#13121</a>)</li>
  <li>Patch dataset module testing failure with JSE11+ (<a href="https://github.com/apache/arrow/pull/13200">#13200</a>)</li>
  <li>Don’t duplicate generated Protobuf classes between flight-core and flight-sql (<a href="https://github.com/apache/arrow/pull/13596">#13596</a>)</li>
</ul>

<h2 id="javascript-notes">JavaScript notes</h2>

<ul>
  <li>Fix error iterating tables with no batches (<a href="https://issues.apache.org/jira/browse/ARROW-16371">ARROW-16371</a>)</li>
  <li>Handle case where <code class="language-plaintext highlighter-rouge">tableFromIPC</code> input is an async <code class="language-plaintext highlighter-rouge">RecordBatchReader</code> (<a href="https://issues.apache.org/jira/browse/ARROW-16704">ARROW-16704</a>)</li>
</ul>

<h2 id="python-notes">Python notes</h2>

<p>Compatibility notes:</p>

<ul>
  <li>
    <p>PyArrow now requires Python &gt;= 3.7 (<a href="https://issues.apache.org/jira/browse/ARROW-16474">ARROW-16474</a>).</p>
  </li>
  <li>
    <p>The default behaviour regarding memory mapping has changed in several APIs (reading of Feather or Parquet files, IPC RecordBatchFileReader and RecordBatchStreamReader) to disable memory mapping by default (<a href="https://issues.apache.org/jira/browse/ARROW-16382">ARROW-16382</a>).</p>
  </li>
  <li>
    <p>The default Parquet version is now 2.4 for writing, enabling use of
more recent logical types by default such as unsigned integers (<a href="https://issues.apache.org/jira/browse/ARROW-12203">ARROW-12203</a>). One can specify <code class="language-plaintext highlighter-rouge">version="2.6"</code> to also enable support for nanosecond timestamps. Use <code class="language-plaintext highlighter-rouge">version="1.0"</code> to restore the old behaviour and maximizes file compatibility.</p>
  </li>
  <li>
    <p>Some deprecated APIs (deprecated at least since pyarrow 1.0.0) have been removed: IPC methods in the top-level namespace, the <code class="language-plaintext highlighter-rouge">Value</code> scalar classes and the <code class="language-plaintext highlighter-rouge">pyarrow.compat</code> module (<a href="https://issues.apache.org/jira/browse/ARROW-17010">ARROW-17010</a>).</p>
  </li>
</ul>

<p>New features:</p>

<ul>
  <li>
    <p>Google Cloud Storage (GCS) File System support is now available in the Python bindings (<a href="https://issues.apache.org/jira/browse/ARROW-14892">ARROW-14892</a>).</p>
  </li>
  <li>
    <p>The <code class="language-plaintext highlighter-rouge">Table.filter()</code> method now supports passing an expression in addition to a boolean array (<a href="https://issues.apache.org/jira/browse/ARROW-16469">ARROW-16469</a>).</p>
  </li>
  <li>
    <p>When implementing extension types in Python, it is now possible to also customize which Python scalar gets returned (in <code class="language-plaintext highlighter-rouge">Array.to_pylist()</code> or <code class="language-plaintext highlighter-rouge">Scalar.as_py()</code>) by subclassing <code class="language-plaintext highlighter-rouge">ExtensionScalar</code> (<a href="https://issues.apache.org/jira/browse/ARROW-13612">ARROW-13612</a>, (<a href="https://issues.apache.org/jira/browse/ARROW-17065">ARROW-17065</a>)).</p>
  </li>
  <li>
    <p>It is now possible to register User Defined Functions (UDF) for scalar functions using <code class="language-plaintext highlighter-rouge">register_scalar_function</code> (<a href="https://issues.apache.org/jira/browse/ARROW-15639">ARROW-15639</a>).</p>
  </li>
  <li>
    <p>Basic support for consuming a Substrait plan has been exposed in Python as <code class="language-plaintext highlighter-rouge">pyarrow.substrait.run_query</code> (<a href="https://issues.apache.org/jira/browse/ARROW-15779">ARROW-15779</a>).</p>
  </li>
  <li>
    <p>The <code class="language-plaintext highlighter-rouge">cast</code> method and compute kernel now exposes the fine grained options in addition to safe/unsafe casting (<a href="https://issues.apache.org/jira/browse/ARROW-15365">ARROW-15365</a>).</p>
  </li>
</ul>

<p>In addition, this release includes several bug fixes and documention improvements (such as expanded examples in docstrings (<a href="https://issues.apache.org/jira/browse/ARROW-16091">ARROW-16091</a>)).</p>

<p>Further, the Python bindings benefit from improvements in the C++ library
(e.g. new compute functions); see the C++ notes above for additional details.</p>

<h2 id="r-notes">R notes</h2>

<p>Highlights include several new <code class="language-plaintext highlighter-rouge">dplyr</code> verbs, including <code class="language-plaintext highlighter-rouge">glimpse()</code> and <code class="language-plaintext highlighter-rouge">union_all()</code>, as well as many more datetime functions from <code class="language-plaintext highlighter-rouge">lubridate</code>. There is also experimental support for user-defined scalar functions in the query engine, and most packages include native support for datasets in Google Cloud Storage (opt-in in the Linux full source build).</p>

<p>For more on what’s in the 9.0.0 R package, see the <a href="/docs/r/news/">R changelog</a>.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<p>FlightSQL is now supported but there are minimum features for now.</p>

<p>More Flight features are now supported.</p>

<h3 id="ruby">Ruby</h3>

<p><code class="language-plaintext highlighter-rouge">Enumerable</code> compatible methods such as <code class="language-plaintext highlighter-rouge">#min</code> and <code class="language-plaintext highlighter-rouge">#max</code> on <code class="language-plaintext highlighter-rouge">Arrow::Array</code>, <code class="language-plaintext highlighter-rouge">Arrow::ChunkedArray</code> and <code class="language-plaintext highlighter-rouge">Arrow::Column</code> are implemented by C++’s <a href="/docs/cpp/compute.html">compute functions</a>. This improves performance. (<a href="https://issues.apache.org/jira/browse/ARROW-15222">ARROW-15222</a>)</p>

<p>This release fixed some memory leaks. (<a href="https://issues.apache.org/jira/browse/ARROW-14790">ARROW-14790</a>)</p>

<p>This release improved support for interval type arrays such as <code class="language-plaintext highlighter-rouge">Arrow::MonthIntervalArray</code>. (<a href="https://issues.apache.org/jira/browse/ARROW-16206">ARROW-16206</a>)</p>

<p>This release improved auto data type conversion. (<a href="https://issues.apache.org/jira/browse/ARROW-16874">ARROW-16874</a>)</p>

<h3 id="c-glib">C GLib</h3>

<p>Vala is now supported. (<a href="https://issues.apache.org/jira/browse/ARROW-15671">ARROW-15671</a>). See <a href="https://github.com/apache/arrow/tree/apache-arrow-9.0.0/c_glib/example/vala"><code class="language-plaintext highlighter-rouge">c_glib/example/vala/</code></a> for examples.</p>

<p><code class="language-plaintext highlighter-rouge">GArrowQuantil
eOptions</code> is added. (<a href="https://issues.apache.org/jira/browse/ARROW-16623">ARROW-16623</a>)</p>

<h2 id="rust-notes">Rust notes</h2>

<p>The Rust projects have moved to separate repositories outside the
main Arrow monorepo. For notes on the 19.0.0 release of the Rust
implementation, see the <a href="https://github.com/apache/arrow-rs/blob/19.0.0/CHANGELOG.md">Arrow Rust changelog</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 9.0.0 release. This covers over 3 months of development work and includes 509 resolved issues from 114 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bug fixes and improvements have been made: we refer you to the complete changelog. Community Since the 8.0.0 release, Dewey Dunnington, Alenka Frim and Rok Mihevc have been invited to be committers. Thanks for your contributions and participation in the project! Columnar Format Notes Arrow Flight RPC notes Arrow Flight is now available in MacOS M1 Python wheels (ARROW-16779). Arrow Flight SQL is now buildable on Windows (ARROW-16902). Ruby now exposes more of the Flight and Flight SQL APIs (various JIRAs). Linux packages notes AlmaLinux 9 is now supported. (ARROW-16745) AmazonLinux 2 aarch64 is now supported. (ARROW-16477) C++ notes STL-like iteration is now provided over chunked arrays (ARROW-602). Compute The C++ compute and execution engine is now officially named “Acero”, though its C++ namespaces have not changed. New light-weight data holder abstractions have been introduced in order to reduce the overhead of invoking compute functions and kernels, especially at the small data sizes desirable for efficient parallelization (typically L1- or L2-sized). Specifically, the non-owning ArraySpan and ExecSpan structures have internally superseded the much heavier ExecBatch, which is still supported for compatibility at the API level (ARROW-16756, ARROW-16824, ARROW-16852). In a similar vein, the ValueDescr class was removed and ScalarKernel implementations now always receive at least one non-scalar input, removing the special case where a ScalarKernel needs to output a scalar rather than an array. The higher-level compute APIs still allow executing a scalar function over all-scalar inputs; but those scalars are internally broadcasted to 1-element arrays so as to simplify kernel implementation (ARROW-16757). Some performance improvements were made to the hash join node. These changes do not require additional configuration. The hash join exec node has been improved to more efficiently use CPU cache and make better use of available vectorization hardware (ARROW-14182). Some plans containing a sequence of hash join operators will now use bloom filters to eliminate rows earlier in the plan, reducing the overall CPU cost of the plan (ARROW-15498). Timestamp comparison is now supported (ARROW-16425). A cumulative sum function is implemented over numeric inputs (ARROW-13530). Note that this is a vector function so cannot be used in an Acero ExecPlan. A rank vector kernel has been added (ARROW-16234). Temporal rounding functions received additional options to control how rounding is done (ARROW-14821). Improper computation of the “mode” function on boolean input was fixed (ARROW-17096). Function registries can now be nested (ARROW-16677). Dataset The autogenerate_column_names option for CSV reading is now handled correctly (ARROW-16436). Fix InMemoryDataset::ReplaceSchema to actually replace the schema (ARROW-16085). Fix FilenamePartitioning to properly support null values (ARROW-16302). Filesystem A number of bug fixes and improvements were made to the Google Cloud Storage filesystem implementation (ARROW-14892). By default, the S3 filesystem implementation does not create or drop buckets anymore (ARROW-15906). This is a compatibility-breaking change intended to prevent user errors from having potentially catastrophic consequences. Options have been added to restore the previous behavior if necessary. Parquet The default Parquet version is now 2.4 for writing, enabling use of more recent logical types by default (ARROW-12203). Non-nullable fields are now handled correctly by the Parquet reader (ARROW-16116). Reading encrypted files should now be thread-safe (ARROW-14114). Statistics equality now works correctly with minmax (ARROW-16487). The minimum Thrift version required for building is now 0.13 (ARROW-16721). The Thrift deserialization limits can now be configured to accommodate for data files with very large metadata (ARROW-16546). Substrait The Substrait spec has been updated to 0.6.0 (ARROW-16816). In addition, a larger subset of the Substrait specification is now supported (ARROW-15587, ARROW-15590, ARROW-15901, ARROW-16657, ARROW-15591). C# notes New Features Added support for Time32Array and Time64Array (ARROW-16660) Bug Fixes When using TableFromRecordBatches, the resulting table columns have no data array. (ARROW-13129) Fix intermittent test failures due to async memory management bug. (ARROW-16978) Go notes Security Updated testify dependency to address CVE-2022-28948. (ARROW-16759) (This was also backported to previous versions and released as patch versions v6.0.2, v7.0.1, and v8.0.1) Arrow New Features Dictionary Scalars are now available (ARROW-16323) Introduced a DictionaryUnifier object along with functions for unifying Chunked Arrays and Tables (ARROW-16324) New CSV examples added to documentation to demonstrate error handling (ARROW-16450) CSV Reader now supports arrow.TimestampType (ARROW-16504) JSON parsing for Temporal Types now allow passing numeric values in addition to strings for parsing. Timezones will be properly parsed if they exist in the string and a function was added to retrieve a time.Location object from a TimestampType (ARROW-16551) New utilities added to decimal128 for rescaling and easy conversion to and from float32/float64 (ARROW-16552) Arrow DataType interface now has a LayoutMethod which returns the physical layout of the given datatype such as the number of buffers, types, etc. This matches the behavior of the layout() methods in C++ for data types. (ARROW-16556) Added a SliceBuffer function to the memory package to allow better re-using of memory across buffer objects (ARROW-16557) Dictionary Arrays can now be concatenated using array.Concatenate (ARROW-17095) Bug Fixes ipc.FileReader now properly uses the memory.Allocator interface (ARROW-16002) Addressed issue with Integration tests between Go and Java (ARROW-16441) RecordBuilder.UnmarshalJSON now properly ignores extra unknown fields rather than panicking (ARROW-16456) StructBuilder.UnmarshalJSON will no longer fail and panic when Nullable fields are missing (ARROW-16502) ipc.Reader no longer silently accepts string columns with invalid offsets, preventing unexpected panics later when writing or accessing the resulting arrays. (ARROW-16831) Arrow CSV reader no longer clobbers its reported errors and properly surfaces them (ARROW-16926) Parquet New Features The CreatedBy version string for the Parquet writer will now correctly reflect the library version, and will be updated by the release scripts (ARROW-16484) Parquet bit_packing functions now have ARM64 NEON implementations for performance (ARROW-16486) It is now possible to customize the root node in the Parquet writer instead of hardcoding it to be named “schema” with a repetition type of Repeated. This was needed to allow producing files similar to Apache Spark where the root node has a repetition type of Required. It still defaults to the spec definition of Repeated. (ARROW-16561) parquet_reader CLI mainprog has been enhanced to dump values out as JSON and CSV along with setting an output file instead of just dumping to the terminal. (ARROW-16934) Bug Fixes Fixed a memory leak with Parquet page reading (ARROW-16473) Parquet Reader properly parallelizes column reads when the parallel option is set to true. (ARROW-16530) Fixed bug in the Bool decoder for plain encoding (ARROW-16563) Fixed a bug in the Parquet bool column reader where it failed to properly skip rows (ARROW-16638) Fixed the flakey travis ARM64 builds by reducing the size of a test case in the pqarrow unit tests to reduce the memory usage for the tests. (ARROW-16669) Parquet writer now properly handles writing arrow.NULL type arrays (ARROW-16749) Column level dictionary encoding configuration for Parquet writing now correctly respects the input value (ARROW-16813) Memory leak in DeltaByteArray encoding fixed (ARROW-16983) Java notes New Features Allow overriding column nullability in arrow-jdbc (#13558) Enable skip BOUNDS_CHECKING with setBytes and getBytes of ArrowBuf (#13161) Initialize JNI components on use instead of statically (#13146) Provide explicit JDBC column type mapping (#13166) Allow duplicated field names in Java C data interface (#13247) Improve and document StackTrace (#12656) Keep more context when marshaling errors through JNI (#13246) Make RoundingMode configurable to handle inconsistent scale in BigDecimals (#13433) Improve Java dev experience with IntelliJ (#13017) Implement ArrowArrayStream (#13465)) Bug Fixes Fix variable-width vectors in integration JSON writer (#13676) Handle empty JDBC ResultSet (#13049) Fix hasNext() in ArrowVectorIterator (#13107) Fix ArrayConsumer when using ArrowVectorIterator (#12692) Update Gandiva Protobuf library to enable builds on Apple M1 (#13121) Patch dataset module testing failure with JSE11+ (#13200) Don’t duplicate generated Protobuf classes between flight-core and flight-sql (#13596) JavaScript notes Fix error iterating tables with no batches (ARROW-16371) Handle case where tableFromIPC input is an async RecordBatchReader (ARROW-16704) Python notes Compatibility notes: PyArrow now requires Python &gt;= 3.7 (ARROW-16474). The default behaviour regarding memory mapping has changed in several APIs (reading of Feather or Parquet files, IPC RecordBatchFileReader and RecordBatchStreamReader) to disable memory mapping by default (ARROW-16382). The default Parquet version is now 2.4 for writing, enabling use of more recent logical types by default such as unsigned integers (ARROW-12203). One can specify version="2.6" to also enable support for nanosecond timestamps. Use version="1.0" to restore the old behaviour and maximizes file compatibility. Some deprecated APIs (deprecated at least since pyarrow 1.0.0) have been removed: IPC methods in the top-level namespace, the Value scalar classes and the pyarrow.compat module (ARROW-17010). New features: Google Cloud Storage (GCS) File System support is now available in the Python bindings (ARROW-14892). The Table.filter() method now supports passing an expression in addition to a boolean array (ARROW-16469). When implementing extension types in Python, it is now possible to also customize which Python scalar gets returned (in Array.to_pylist() or Scalar.as_py()) by subclassing ExtensionScalar (ARROW-13612, (ARROW-17065)). It is now possible to register User Defined Functions (UDF) for scalar functions using register_scalar_function (ARROW-15639). Basic support for consuming a Substrait plan has been exposed in Python as pyarrow.substrait.run_query (ARROW-15779). The cast method and compute kernel now exposes the fine grained options in addition to safe/unsafe casting (ARROW-15365). In addition, this release includes several bug fixes and documention improvements (such as expanded examples in docstrings (ARROW-16091)). Further, the Python bindings benefit from improvements in the C++ library (e.g. new compute functions); see the C++ notes above for additional details. R notes Highlights include several new dplyr verbs, including glimpse() and union_all(), as well as many more datetime functions from lubridate. There is also experimental support for user-defined scalar functions in the query engine, and most packages include native support for datasets in Google Cloud Storage (opt-in in the Linux full source build). For more on what’s in the 9.0.0 R package, see the R changelog. Ruby and C GLib notes FlightSQL is now supported but there are minimum features for now. More Flight features are now supported. Ruby Enumerable compatible methods such as #min and #max on Arrow::Array, Arrow::ChunkedArray and Arrow::Column are implemented by C++’s compute functions. This improves performance. (ARROW-15222) This release fixed some memory leaks. (ARROW-14790) This release improved support for interval type arrays such as Arrow::MonthIntervalArray. (ARROW-16206) This release improved auto data type conversion. (ARROW-16874) C GLib Vala is now supported. (ARROW-15671). See c_glib/example/vala/ for examples. GArrowQuantil eOptions is added. (ARROW-16623) Rust notes The Rust projects have moved to separate repositories outside the main Arrow monorepo. For notes on the 19.0.0 release of the Rust implementation, see the Arrow Rust changelog.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">June 2022 Rust Apache Arrow and Parquet 16.0.0 Highlights</title><link href="https://arrow.apache.org/blog/2022/06/16/rust-16.0.0/" rel="alternate" type="text/html" title="June 2022 Rust Apache Arrow and Parquet 16.0.0 Highlights" /><published>2022-06-16T02:00:00-04:00</published><updated>2022-06-16T02:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/06/16/rust-16.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/06/16/rust-16.0.0/"><![CDATA[<!--

-->

<h1 id="introduction">Introduction</h1>

<p>We recently celebrated releasing version 16.0.0 of the Rust implementation of <a href="https://arrow.apache.org/">Apache Arrow</a>. While we still get a few comments on “most rust libraries use versions 0.x.0, why are you at 16.0.0?”, our versioning scheme appears to be working well, and permits quick releases of new features and API evolution in a semver compatible way without breaking downstream projects.</p>

<p>This post contains highlights from the last four months (versions
10.0.0 to 16.0.0) of
<a href="https://github.com/apache/arrow-rs/arrow">arrow-rs</a> and
<a href="https://github.com/apache/arrow-rs/parquet">parquet-rs</a> development
as well as a roadmap of future work. The full list of awesomeness can
be found in the
<a href="https://github.com/apache/arrow-rs/blob/master/CHANGELOG.md">CHANGELOG</a>.</p>

<p>As you <a href="https://github.com/apache/arrow-rs/issues/1715">may remember</a>, the arrow and parquet implementations are in the same crate, on the same release schedule, and in this same blog. This is not for technical reasons, but helps to keep the maintenance burden for delivering great Apache software reasonable, and allows easier development of optimized conversion between Arrow &lt;–&gt; Parquet formats.</p>

<h1 id="parquet">Parquet</h1>
<p>The <a href="https://crates.io/crates/parquet">parquet crate</a> has seen a return to substantial improvements after being relatively dormant for several years. The current major areas of focus are</p>

<ol>
  <li><strong>Performance</strong>: Improving the raw performance for reading and writing mirroring the efforts that went into the C++ version a few years ago.</li>
  <li><strong>API Ease of Use</strong>: Improving the API so it is easy to use efficiently with modern Rust for two preeminent use cases: 1) reading from local disk and 2) reading <code class="language-plaintext highlighter-rouge">async</code>hronously from remote object stores.</li>
</ol>

<p>Some Major Highlights:</p>

<ul>
  <li><strong>Advanced Metadata Access</strong>: API access to advanced parquet metadata, such as <a href="https://github.com/apache/arrow-rs/pull/1322">PageEncoding</a>, <a href="https://github.com/apache/arrow-rs/pull/1309">BloomFilters</a> and <a href="https://github.com/apache/arrow-rs/pull/1762">PageIndex</a>.</li>
  <li><strong>Improved API Usability</strong>: For example, the <a href="https://github.com/apache/arrow-rs/pull/1719">parquet writer now uses <code class="language-plaintext highlighter-rouge">std:io::Write</code></a> rather than a custom <code class="language-plaintext highlighter-rouge">ParquetWriter</code> trait, making it more interoperable with the rest of the Rust ecosystem and the <a href="https://github.com/apache/arrow-rs/pull/1716">projection API is easier to use with nested types</a>.</li>
  <li><strong>Rewritten support for nested types (e.g. struct, lists)</strong> : @tustvold has revamped / rewritten support for <a href="https://github.com/apache/arrow-rs/pull/1682">reading</a> and <a href="https://github.com/apache/arrow-rs/pull/1746">writing</a> structured types, which both improved the support for arbitrary nested schemas, and is 30% faster.</li>
</ul>

<p>Looking Forward:</p>

<ul>
  <li><strong>Even Faster</strong>: We are actively working to make <a href="https://github.com/apache/arrow-rs/issues/1764">writing even faster</a> and expect to see some major improvements over the next few releases.</li>
  <li><strong>Object Store Integration</strong>: Support for easily and efficiently reading/writing to/from object storage is improving, and we expect it will soon work well out of the box, fetching the minimal bytes, etc… More on this to follow in a separate blog post.</li>
  <li><strong>Parallel Decode</strong>: We intend to transparently support high performance parallel decoding of parquet to arrow arrays, when invoked from a <a href="https://crates.io/crates/rayon">rayon</a> threadpool.</li>
</ul>

<h1 id="arrow">Arrow</h1>

<p>The Rust arrow implementation has also had substantial improvements, in addition to bug fixes and performance improvements.</p>

<p>Some Major Highlights:</p>

<ul>
  <li><strong>Ecosystem Compatibility</strong>: <a href="https://github.com/viirya">@viriya</a> has put in a massive effort to improve (and prove) compatibility with other Arrow implementations via the Rust IPC integration tests. There have been major improvements for corner cases involving nested structures, nullability, nested dictionaries, etc.</li>
  <li><strong>Safety</strong>: We continue to improve the safety of arrow, and it is not possible to trigger undefined behavior using <code class="language-plaintext highlighter-rouge">safe</code> apis – checkout the <a href="https://github.com/apache/arrow-rs/tree/master/arrow#safety">README</a> and the <a href="https://docs.rs/arrow/16.0.0/arrow/#safety-and-security">module level rustdocs</a> for more details. Among other things, we have added additional validation checking to <a href="https://github.com/apache/arrow-rs/issues/1575">string kernels</a> and <a href="https://github.com/apache/arrow-rs/pull/1767"><code class="language-plaintext highlighter-rouge">DecimalArrays</code></a> and <a href="https://github.com/apache/arrow-rs/pull/1819">sealed some sensitive traits</a>.</li>
  <li><strong>Performance</strong>: There have been several major performance improvements such as <a href="https://github.com/apache/arrow-rs/issues/1288">much faster filter kernels</a>, thanks to <a href="https://github.com/tustvold">@tustvold</a>.</li>
  <li><strong>Easier to Use APIs</strong>: Several of the APIs are now easier to use (e.g. <a href="https://github.com/apache/arrow-rs/pull/1645">#1645</a> and <a href="https://github.com/apache/arrow-rs/pull/1739">#1739</a> which lowers the barrier to entry of using <code class="language-plaintext highlighter-rouge">arrow-rs</code>, thanks to <a href="https://github.com/HaoYang670">@HaoYang670</a>.</li>
  <li><strong>DataType::Null support</strong>: is much improved, such as <a href="https://github.com/apache/arrow-rs/pull/1572">in the cast kernels</a>, thanks to <a href="https://github.com/WinkerDu">@WinkerDu</a>.</li>
  <li><strong>Improved JSON reader</strong>: The JSON reader is <a href="https://github.com/apache/arrow-rs/pull/1451">now easier to use</a> thanks to <a href="https://github.com/sum12">@sum12</a>.</li>
</ul>

<p>Looking Forward:</p>
<ul>
  <li><strong>Make ArrayData Easier to use Safely</strong>: Some amount of <code class="language-plaintext highlighter-rouge">unsafe</code> will likely always be required in arrow (for fast IPC, for example), but we are also working to improve the underlying <code class="language-plaintext highlighter-rouge">ArrayData</code> structure to make it more compatible with the ecosystem (e.g. use <code class="language-plaintext highlighter-rouge">Bytes</code>), support faster to decode from parquet, and to avoid bugs related to offsets (slicing) which are a <a href="https://github.com/apache/arrow-rs/issues/1799">frequent pain point</a>.</li>
  <li><strong>FlightSQL</strong> – we have some <a href="https://github.com/apache/arrow-rs/pulls?q=flightsql">initial support for Flight SQL</a> thank to <a href="https://github.com/wangfenjin">@wangfenjin</a> and <a href="https://github.com/timvw">@timvw</a>, though we would love to see some additional contributors. Such help can include a basic FlightSQL server, and starting work on clients.</li>
</ul>

<p>Some areas looking for help include:</p>

<ul>
  <li><strong>Decimal 256 support</strong>: <a href="https://github.com/apache/arrow-rs/issues/131">https://github.com/apache/arrow-rs/issues/131</a></li>
  <li><strong>Support for negative Decimal scale</strong>: <a href="https://github.com/apache/arrow-rs/issues/1785">https://github.com/apache/arrow-rs/issues/1785</a></li>
  <li><strong>Support IPC file compression</strong>: <a href="https://github.com/apache/arrow-rs/issues/1709">https://github.com/apache/arrow-rs/issues/1709</a></li>
  <li><strong>Zero-copy bitmap slicing</strong>: <a href="https://github.com/apache/arrow-rs/issues/1802">https://github.com/apache/arrow-rs/issues/1802</a></li>
</ul>

<h1 id="contributors">Contributors:</h1>

<p>While some open source software can be created mostly by a single contributor, we believe the greatest software with the largest impact and reach is built around its community. Thus, Arrow is part of the <a href="https://www.apache.org/">Apache Software Foundation</a> and our releases both past and present are a result of our amazing community’s effort.</p>

<p>We would like to thank everyone who has contributed to the arrow-rs repository since the <code class="language-plaintext highlighter-rouge">9.0.2</code> release. Keep up the great work and we look forward to continued improvements:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">git shortlog -sn 9.0.0..16.0.0
    47  Liang-Chi Hsieh
    45  Raphael Taylor-Davies
    43  Andrew Lamb
    40  Remzi Yang
     8  Sergey Glushchenko
     7  Jörn Horstmann
     6  Shani Solomon
     6  dependabot[bot]
     5  Yang Jiang
     4  jakevin
     4  Chao Sun
     4  Yijie Shen
     3  kazuhiko kikuchi
     2  Sumit
     2  Ismail-Maj
     2  Kamil Konior
     2  tfeda
     2  Matthew Turner
     1  iyupeng
     1  ryan-jacobs1
     1  Alex Qyoun-ae
     1  tjwilson90
     1  Andy Grove
     1  Atef Sawaed
     1  Daniël Heres
     1  DuRipeng
     1  Helgi Kristvin Sigurbjarnarson
     1  Kun Liu
     1  Kyle Barron
     1  Marc Garcia
     1  Peter C. Jentsch
     1  Remco Verhoef
     1  Sven Cattell
     1  Thomas Peiselt
     1  Tiphaine Ruy
     1  Trent Feda
     1  Wang Fenjin
     1  Ze'ev Maor
     1  diana
</span></code></pre></div></div>

<h1 id="join-the-community">Join the community</h1>

<p>If you are interested in contributing to the Rust subproject in Apache Arrow, you can find a list of open issues
suitable for beginners <a href="https://github.com/apache/arrow-rs/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a>
and the full list <a href="https://github.com/apache/arrow-rs/issues">here</a>.</p>

<p>Other ways to get involved include trying out Arrow on some of your data and filing bug reports, and helping to
improve the documentation.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[Introduction We recently celebrated releasing version 16.0.0 of the Rust implementation of Apache Arrow. While we still get a few comments on “most rust libraries use versions 0.x.0, why are you at 16.0.0?”, our versioning scheme appears to be working well, and permits quick releases of new features and API evolution in a semver compatible way without breaking downstream projects. This post contains highlights from the last four months (versions 10.0.0 to 16.0.0) of arrow-rs and parquet-rs development as well as a roadmap of future work. The full list of awesomeness can be found in the CHANGELOG. As you may remember, the arrow and parquet implementations are in the same crate, on the same release schedule, and in this same blog. This is not for technical reasons, but helps to keep the maintenance burden for delivering great Apache software reasonable, and allows easier development of optimized conversion between Arrow &lt;–&gt; Parquet formats. Parquet The parquet crate has seen a return to substantial improvements after being relatively dormant for several years. The current major areas of focus are Performance: Improving the raw performance for reading and writing mirroring the efforts that went into the C++ version a few years ago. API Ease of Use: Improving the API so it is easy to use efficiently with modern Rust for two preeminent use cases: 1) reading from local disk and 2) reading asynchronously from remote object stores. Some Major Highlights: Advanced Metadata Access: API access to advanced parquet metadata, such as PageEncoding, BloomFilters and PageIndex. Improved API Usability: For example, the parquet writer now uses std:io::Write rather than a custom ParquetWriter trait, making it more interoperable with the rest of the Rust ecosystem and the projection API is easier to use with nested types. Rewritten support for nested types (e.g. struct, lists) : @tustvold has revamped / rewritten support for reading and writing structured types, which both improved the support for arbitrary nested schemas, and is 30% faster. Looking Forward: Even Faster: We are actively working to make writing even faster and expect to see some major improvements over the next few releases. Object Store Integration: Support for easily and efficiently reading/writing to/from object storage is improving, and we expect it will soon work well out of the box, fetching the minimal bytes, etc… More on this to follow in a separate blog post. Parallel Decode: We intend to transparently support high performance parallel decoding of parquet to arrow arrays, when invoked from a rayon threadpool. Arrow The Rust arrow implementation has also had substantial improvements, in addition to bug fixes and performance improvements. Some Major Highlights: Ecosystem Compatibility: @viriya has put in a massive effort to improve (and prove) compatibility with other Arrow implementations via the Rust IPC integration tests. There have been major improvements for corner cases involving nested structures, nullability, nested dictionaries, etc. Safety: We continue to improve the safety of arrow, and it is not possible to trigger undefined behavior using safe apis – checkout the README and the module level rustdocs for more details. Among other things, we have added additional validation checking to string kernels and DecimalArrays and sealed some sensitive traits. Performance: There have been several major performance improvements such as much faster filter kernels, thanks to @tustvold. Easier to Use APIs: Several of the APIs are now easier to use (e.g. #1645 and #1739 which lowers the barrier to entry of using arrow-rs, thanks to @HaoYang670. DataType::Null support: is much improved, such as in the cast kernels, thanks to @WinkerDu. Improved JSON reader: The JSON reader is now easier to use thanks to @sum12. Looking Forward: Make ArrayData Easier to use Safely: Some amount of unsafe will likely always be required in arrow (for fast IPC, for example), but we are also working to improve the underlying ArrayData structure to make it more compatible with the ecosystem (e.g. use Bytes), support faster to decode from parquet, and to avoid bugs related to offsets (slicing) which are a frequent pain point. FlightSQL – we have some initial support for Flight SQL thank to @wangfenjin and @timvw, though we would love to see some additional contributors. Such help can include a basic FlightSQL server, and starting work on clients. Some areas looking for help include: Decimal 256 support: https://github.com/apache/arrow-rs/issues/131 Support for negative Decimal scale: https://github.com/apache/arrow-rs/issues/1785 Support IPC file compression: https://github.com/apache/arrow-rs/issues/1709 Zero-copy bitmap slicing: https://github.com/apache/arrow-rs/issues/1802 Contributors: While some open source software can be created mostly by a single contributor, we believe the greatest software with the largest impact and reach is built around its community. Thus, Arrow is part of the Apache Software Foundation and our releases both past and present are a result of our amazing community’s effort. We would like to thank everyone who has contributed to the arrow-rs repository since the 9.0.2 release. Keep up the great work and we look forward to continued improvements: git shortlog -sn 9.0.0..16.0.0 47 Liang-Chi Hsieh 45 Raphael Taylor-Davies 43 Andrew Lamb 40 Remzi Yang 8 Sergey Glushchenko 7 Jörn Horstmann 6 Shani Solomon 6 dependabot[bot] 5 Yang Jiang 4 jakevin 4 Chao Sun 4 Yijie Shen 3 kazuhiko kikuchi 2 Sumit 2 Ismail-Maj 2 Kamil Konior 2 tfeda 2 Matthew Turner 1 iyupeng 1 ryan-jacobs1 1 Alex Qyoun-ae 1 tjwilson90 1 Andy Grove 1 Atef Sawaed 1 Daniël Heres 1 DuRipeng 1 Helgi Kristvin Sigurbjarnarson 1 Kun Liu 1 Kyle Barron 1 Marc Garcia 1 Peter C. Jentsch 1 Remco Verhoef 1 Sven Cattell 1 Thomas Peiselt 1 Tiphaine Ruy 1 Trent Feda 1 Wang Fenjin 1 Ze'ev Maor 1 diana Join the community If you are interested in contributing to the Rust subproject in Apache Arrow, you can find a list of open issues suitable for beginners here and the full list here. Other ways to get involved include trying out Arrow on some of your data and filing bug reports, and helping to improve the documentation.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow DataFusion 8.0.0 Release</title><link href="https://arrow.apache.org/blog/2022/05/16/datafusion-8.0.0/" rel="alternate" type="text/html" title="Apache Arrow DataFusion 8.0.0 Release" /><published>2022-05-16T00:00:00-04:00</published><updated>2022-05-16T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/05/16/datafusion-8.0.0</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/05/16/datafusion-8.0.0/"><![CDATA[<!--

-->

<h1 id="introduction">Introduction</h1>

<p><a href="https://arrow.apache.org/datafusion/">DataFusion</a> is an extensible query execution framework, written in Rust, that
uses Apache Arrow as its in-memory format.</p>

<p>When you want to extend your Rust project with <a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html">SQL support</a>,
a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is definitely worth
checking out.</p>

<p>DataFusion’s SQL, <code class="language-plaintext highlighter-rouge">DataFrame</code>, and manual <code class="language-plaintext highlighter-rouge">PlanBuilder</code> API let users access a sophisticated query optimizer and
execution engine capable of fast, resource efficient, and parallel execution that takes optimal advantage of
today’s multicore hardware. Being written in Rust means DataFusion can offer <em>both</em> the safety of a dynamic language and
the resource efficiency of a compiled language.</p>

<p>The Apache Arrow team is pleased to announce the DataFusion 8.0.0 release (and also the release of version 0.7.0 of
the Ballista subproject). This covers 3 months of development work and includes 279 commits from the following 49
distinct contributors.</p>

<!--
$ git log --pretty=oneline 7.0.0..8.0.0 datafusion datafusion-cli datafusion-examples ballista ballista-cli ballista-examples | wc -l
279

$ git shortlog -sn 7.0.0..8.0.0 datafusion datafusion-cli datafusion-examples ballista ballista-cli ballista-examples | wc -l
49

(feynman han, feynman.h, Feynman Han were assumed to be the same person)
-->

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    39  Andy Grove
    33  Andrew Lamb
    21  DuRipeng
    20  Yijie Shen
    19  Yang Jiang
    17  Raphael Taylor-Davies
    11  Dan Harris
    11  Matthew Turner
    11  yahoNanJing
     9  dependabot[bot]
     8  jakevin
     6  Kun Liu
     5  Jiayu Liu
     4  Daniël Heres
     4  mingmwang
     4  xudong.w
     3  Carol (Nichols || Goulding)
     3  Dmitry Patsura
     3  Eduard Karacharov
     3  Jeremy Dyer
     3  Kaushik
     3  Rich
     3  comphead
     3  gaojun2048
     3  Feynman Han
     2  Jie Han
     2  Jon Mease
     2  Tim Van Wassenhove
     2  Yt
     2  Zhang Li
     2  silence-coding
     1  Alexander Spies
     1  George Andronchik
     1  Guillaume Balaine
     1  Hao Xin
     1  Jiacai Liu
     1  Jörn Horstmann
     1  Liang-Chi Hsieh
     1  Max Burke
     1  NaincyKumariKnoldus
     1  Nga Tran
     1  Patrick More
     1  Pierre Zemb
     1  Remzi Yang
     1  Sergey Melnychuk
     1  Stephen Carman
     1  doki
</code></pre></div></div>

<p>The following sections highlight some of the changes in this release. Of course, many other bug fixes and
improvements have been made and we encourage you to check out the
<a href="https://github.com/apache/arrow-datafusion/blob/8.0.0/datafusion/CHANGELOG.md">changelog</a> for full details.</p>

<h1 id="summary">Summary</h1>

<h2 id="ddl-support">DDL Support</h2>

<p>DDL support has been expanded to include the following commands for creating databases, schemas, and views. This
allows DataFusion to be used more effectively from the CLI.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">CREATE DATABASE</code></li>
  <li><code class="language-plaintext highlighter-rouge">CREATE VIEW</code></li>
  <li><code class="language-plaintext highlighter-rouge">CREATE SCHEMA</code></li>
  <li><code class="language-plaintext highlighter-rouge">CREATE EXTERNAL TABLE</code> now supports JSON files, <code class="language-plaintext highlighter-rouge">IF NOT EXISTS</code>, and partition columns</li>
</ul>

<h2 id="sql-support">SQL Support</h2>

<p>The SQL query planner now supports a number of new SQL features, including:</p>

<ul>
  <li><em>Subqueries</em>: when used via <code class="language-plaintext highlighter-rouge">IN</code>, <code class="language-plaintext highlighter-rouge">EXISTS</code>, and as scalars</li>
  <li><em>Grouping Sets</em>: <code class="language-plaintext highlighter-rouge">CUBE</code> and <code class="language-plaintext highlighter-rouge">ROLLUP</code> grouping sets.</li>
  <li><em>Aggregate functions</em>: <code class="language-plaintext highlighter-rouge">approx_percentile</code>, <code class="language-plaintext highlighter-rouge">approx_percentile_cont</code>, <code class="language-plaintext highlighter-rouge">approx_percentile_cont_with_weight</code>, <code class="language-plaintext highlighter-rouge">approx_distinct</code>, <code class="language-plaintext highlighter-rouge">approx_median</code> and <code class="language-plaintext highlighter-rouge">array</code></li>
  <li><em><code class="language-plaintext highlighter-rouge">null</code> literals</em></li>
  <li><em>bitwise operations</em>: for example ‘<code class="language-plaintext highlighter-rouge">|</code>’</li>
</ul>

<p>There are also many bug fixes and improvements around normalizing identifiers consistently.</p>

<p>We continue our tradition of incrementally releasing support for new
features as they are developed. Thus, while the physical plan may not yet
support all new features, it gets more complete each release. These
changes also make DataFusion an increasingly compelling choice for
projects looking for a SQL parser and query planner that can produce
optimized logical plans that can be translated to
their own execution engine.</p>

<h2 id="query-execution--internals">Query Execution &amp; Internals</h2>

<p>There are several notable improvements and new features in the query execution engine:</p>

<ul>
  <li>The <code class="language-plaintext highlighter-rouge">ExecutionContext</code> has been renamed to <code class="language-plaintext highlighter-rouge">SessionContext</code> and now supports multi-tenancy</li>
  <li>The <code class="language-plaintext highlighter-rouge">ExecutionPlan</code> trait is no longer <code class="language-plaintext highlighter-rouge">async</code></li>
  <li>A new serialization API for serializing plans to bytes (based on protobuf)</li>
</ul>

<p>In addition, we have added several foundational features to drive even
more advanced query processing into DataFusion, focusing on running
arbitrary queries larger than available memory, and pushing the
envelope for performance of sorting, grouping, and joining even
further:</p>

<ul>
  <li>Morsel-Driven Scheduler based on <a href="https://15721.courses.cs.cmu.edu/spring2016/papers/p743-leis.pdf">“Morsel-Driven Parallelism: A NUMA-Aware Query
Evaluation Framework for the Many-Core Age”</a></li>
  <li>Consolidated object store implementation and integration with parquet decoding</li>
  <li>Memory Limited Spilling sort operator</li>
  <li>Memory Limited Sort-Merge join operator</li>
  <li>High performance JIT code generation for tuple comparisons</li>
  <li>Memory efficient Row Format</li>
</ul>

<h2 id="improved-file-support">Improved file support</h2>

<p>DataFusion now supports JSON, both for reading and writing. There are also new DataFrame methods for writing query
results to files in CSV, Parquet, and JSON format.</p>

<h2 id="ballista">Ballista</h2>

<p>Ballista continues to mature and now supports a wider range of operators and expressions. There are also improvements
to the scheduler to support UDFs, and there are some robustness improvements, such as cleaning up work directories
and persisting session configs to allow schedulers to restart and continue processing in-flight jobs.</p>

<h2 id="upcoming-work">Upcoming Work</h2>

<p>Here are some of the initiatives that the community plans on working on prior to the next release.</p>

<ul>
  <li>There is a <a href="https://docs.google.com/document/d/1jNRbadyStSrV5kifwn0khufAwq6OnzGczG4z8oTQJP4/edit?usp=sharing">proposal to move Ballista to its own top-level arrow-ballista repository</a>
 to decouple DataFusion and Ballista releases and to allow each project to have documentation better targeted at
its particular audience.</li>
  <li>We plan on increasing the frequency of DataFusion releases, with monthly releases now instead of quarterly. This
is driven by requests from the increasing number of projects that now depend on DataFusion.</li>
  <li>There is ongoing work to implement new optimizer rules to rewrite queries containing subquery expressions as
joins, to support a wider range of queries.</li>
  <li>The new scheduler based on morsel-driven execution will continue to evolve in this next release, with work to
refine IO abstractions to improve performance and integration with the new scheduler.</li>
  <li>Improved performance for Sort, Grouping and Joins</li>
</ul>

<h1 id="how-to-get-involved">How to Get Involved</h1>

<p>If you are interested in contributing to DataFusion, and learning about state-of-the-art query processing, we would
love to have you join us on the journey! You can help by trying out DataFusion on some of your own data and projects
and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable
for beginners is <a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a></p>

<p>Check out our new <a href="https://arrow.apache.org/datafusion/community/communication.html">Communication Doc</a> on more
ways to engage with the community.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[Introduction DataFusion is an extensible query execution framework, written in Rust, that uses Apache Arrow as its in-memory format. When you want to extend your Rust project with SQL support, a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is definitely worth checking out. DataFusion’s SQL, DataFrame, and manual PlanBuilder API let users access a sophisticated query optimizer and execution engine capable of fast, resource efficient, and parallel execution that takes optimal advantage of today’s multicore hardware. Being written in Rust means DataFusion can offer both the safety of a dynamic language and the resource efficiency of a compiled language. The Apache Arrow team is pleased to announce the DataFusion 8.0.0 release (and also the release of version 0.7.0 of the Ballista subproject). This covers 3 months of development work and includes 279 commits from the following 49 distinct contributors. 39 Andy Grove 33 Andrew Lamb 21 DuRipeng 20 Yijie Shen 19 Yang Jiang 17 Raphael Taylor-Davies 11 Dan Harris 11 Matthew Turner 11 yahoNanJing 9 dependabot[bot] 8 jakevin 6 Kun Liu 5 Jiayu Liu 4 Daniël Heres 4 mingmwang 4 xudong.w 3 Carol (Nichols || Goulding) 3 Dmitry Patsura 3 Eduard Karacharov 3 Jeremy Dyer 3 Kaushik 3 Rich 3 comphead 3 gaojun2048 3 Feynman Han 2 Jie Han 2 Jon Mease 2 Tim Van Wassenhove 2 Yt 2 Zhang Li 2 silence-coding 1 Alexander Spies 1 George Andronchik 1 Guillaume Balaine 1 Hao Xin 1 Jiacai Liu 1 Jörn Horstmann 1 Liang-Chi Hsieh 1 Max Burke 1 NaincyKumariKnoldus 1 Nga Tran 1 Patrick More 1 Pierre Zemb 1 Remzi Yang 1 Sergey Melnychuk 1 Stephen Carman 1 doki The following sections highlight some of the changes in this release. Of course, many other bug fixes and improvements have been made and we encourage you to check out the changelog for full details. Summary DDL Support DDL support has been expanded to include the following commands for creating databases, schemas, and views. This allows DataFusion to be used more effectively from the CLI. CREATE DATABASE CREATE VIEW CREATE SCHEMA CREATE EXTERNAL TABLE now supports JSON files, IF NOT EXISTS, and partition columns SQL Support The SQL query planner now supports a number of new SQL features, including: Subqueries: when used via IN, EXISTS, and as scalars Grouping Sets: CUBE and ROLLUP grouping sets. Aggregate functions: approx_percentile, approx_percentile_cont, approx_percentile_cont_with_weight, approx_distinct, approx_median and array null literals bitwise operations: for example ‘|’ There are also many bug fixes and improvements around normalizing identifiers consistently. We continue our tradition of incrementally releasing support for new features as they are developed. Thus, while the physical plan may not yet support all new features, it gets more complete each release. These changes also make DataFusion an increasingly compelling choice for projects looking for a SQL parser and query planner that can produce optimized logical plans that can be translated to their own execution engine. Query Execution &amp; Internals There are several notable improvements and new features in the query execution engine: The ExecutionContext has been renamed to SessionContext and now supports multi-tenancy The ExecutionPlan trait is no longer async A new serialization API for serializing plans to bytes (based on protobuf) In addition, we have added several foundational features to drive even more advanced query processing into DataFusion, focusing on running arbitrary queries larger than available memory, and pushing the envelope for performance of sorting, grouping, and joining even further: Morsel-Driven Scheduler based on “Morsel-Driven Parallelism: A NUMA-Aware Query Evaluation Framework for the Many-Core Age” Consolidated object store implementation and integration with parquet decoding Memory Limited Spilling sort operator Memory Limited Sort-Merge join operator High performance JIT code generation for tuple comparisons Memory efficient Row Format Improved file support DataFusion now supports JSON, both for reading and writing. There are also new DataFrame methods for writing query results to files in CSV, Parquet, and JSON format. Ballista Ballista continues to mature and now supports a wider range of operators and expressions. There are also improvements to the scheduler to support UDFs, and there are some robustness improvements, such as cleaning up work directories and persisting session configs to allow schedulers to restart and continue processing in-flight jobs. Upcoming Work Here are some of the initiatives that the community plans on working on prior to the next release. There is a proposal to move Ballista to its own top-level arrow-ballista repository to decouple DataFusion and Ballista releases and to allow each project to have documentation better targeted at its particular audience. We plan on increasing the frequency of DataFusion releases, with monthly releases now instead of quarterly. This is driven by requests from the increasing number of projects that now depend on DataFusion. There is ongoing work to implement new optimizer rules to rewrite queries containing subquery expressions as joins, to support a wider range of queries. The new scheduler based on morsel-driven execution will continue to evolve in this next release, with work to refine IO abstractions to improve performance and integration with the new scheduler. Improved performance for Sort, Grouping and Joins How to Get Involved If you are interested in contributing to DataFusion, and learning about state-of-the-art query processing, we would love to have you join us on the journey! You can help by trying out DataFusion on some of your own data and projects and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable for beginners is here Check out our new Communication Doc on more ways to engage with the community.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow 8.0.0 Release</title><link href="https://arrow.apache.org/blog/2022/05/15/8.0.0-release/" rel="alternate" type="text/html" title="Apache Arrow 8.0.0 Release" /><published>2022-05-15T00:00:00-04:00</published><updated>2022-05-15T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/05/15/8.0.0-release</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/05/15/8.0.0-release/"><![CDATA[<!--

-->

<p>The Apache Arrow team is pleased to announce the 8.0.0 release. This covers
over 3 months of development work and includes <a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20status%20%3D%20Resolved%20AND%20fixVersion%20%3D%208.0.0"><strong>586 resolved issues</strong></a>
from <a href="/release/8.0.0.html#contributors"><strong>127 distinct contributors</strong></a>. See the Install Page to learn how to
get the libraries for your platform.</p>

<p>The release notes below are not exhaustive and only expose selected highlights
of the release. Many other bugfixes and improvements have been made: we refer
you to the <a href="/release/8.0.0.html#changelog">complete changelog</a>.</p>

<h2 id="community">Community</h2>

<p>Since the 7.0.0 release, Kun Liu, Raphael Taylor-Davies Xudong Wang, Yijie Shen
and Liang-Chi Hsieh have been invited to be committers.
Thanks for your contributions and participation in the project!</p>

<h2 id="arrow-flight-rpc-notes">Arrow Flight RPC notes</h2>

<p>Flight SQL has been extended with a method to get type metadata (<a href="https://issues.apache.org/jira/browse/ARROW-15313">ARROW-15313</a>) and column metadata in returned schemas (<a href="https://issues.apache.org/jira/browse/ARROW-15314">ARROW-15314</a>, <a href="https://issues.apache.org/jira/browse/ARROW-16064">ARROW-16064</a>) New documentation is available describing Flight and Flight SQL, along with several Cookbook recipes (<a href="https://issues.apache.org/jira/browse/ARROW-14698">ARROW-14698</a>, <a href="https://issues.apache.org/jira/browse/ARROW-16065">ARROW-16065</a>).</p>

<p>The C++ libraries now support UCX as a network transport (<a href="https://issues.apache.org/jira/browse/ARROW-15706">ARROW-15706</a>), and the APIs have been refactored to allow other transports to be implemented (<a href="https://issues.apache.org/jira/browse/ARROW-15282">ARROW-15282</a>). UCX support is experimental and still subject to change. Many of the APIs have been refactored to use the <code class="language-plaintext highlighter-rouge">arrow::Result</code> type, and the original variants have been deprecated (<a href="https://issues.apache.org/jira/browse/ARROW-16032">ARROW-16032</a>). Support for gRPC &gt;= 1.43 has been added (<a href="https://issues.apache.org/jira/browse/ARROW-15551">ARROW-15551</a>).</p>

<h2 id="c-notes">C++ notes</h2>

<h3 id="compute">Compute</h3>

<p>Arrow C++ can now optionally build with support for the experimental
<a href="https://substrait.io/">Substrait</a> query representation format (<a href="https://issues.apache.org/jira/browse/ARROW-15238">ARROW-15238</a>).</p>

<p>A number of compute kernels operating on temporal data have been added:</p>

<ul>
  <li>addition, subtraction and multiplication between various temporal types;</li>
  <li>a new “is_dst” function to compute whether the input timestamps fall within
daylight saving time (DST);</li>
  <li>a new “is_leap_year” function to compute whether the input timestamps fall
within a leap year.</li>
</ul>

<p>It is possible to enable a timezone database on Windows at runtime
by calling the <code class="language-plaintext highlighter-rouge">arrow::Initialize()</code> function (<a href="https://issues.apache.org/jira/browse/ARROW-13168">ARROW-13168</a>).</p>

<p>New hash aggregations are available: “hash_one” to return one value from each
group (<a href="https://issues.apache.org/jira/browse/ARROW-13993">ARROW-13993</a>), and “hash_list” to return all values from each group
(<a href="https://issues.apache.org/jira/browse/ARROW-15152">ARROW-15152</a>).  Null columns are now supported on the sum, mean and product
hash aggregates (<a href="https://issues.apache.org/jira/browse/ARROW-15506">ARROW-15506</a>).  Also, it is now possible to execute hash
“aggregations” with only key columns (<a href="https://issues.apache.org/jira/browse/ARROW-15609">ARROW-15609</a>).</p>

<p>A new compute function “map_lookup” allows looking up a given key in a map
array (<a href="https://issues.apache.org/jira/browse/ARROW-15089">ARROW-15089</a>).</p>

<p>New compute functions “sqrt” and “sqrt_checked” allow extracting the square
root of their input (<a href="https://issues.apache.org/jira/browse/ARROW-15614">ARROW-15614</a>).</p>

<p>Casting between two struct types is now possible, assuming the destination field
names all exist in the source struct type (<a href="https://issues.apache.org/jira/browse/ARROW-1888">ARROW-1888</a>, <a href="https://issues.apache.org/jira/browse/ARROW-15643">ARROW-15643</a>).</p>

<p>Optional OpenTelemetry tracing has been added to kernel functions and execution
plan nodes (<a href="https://issues.apache.org/jira/browse/ARROW-15061">ARROW-15061</a>).</p>

<p>The CMake build option <code class="language-plaintext highlighter-rouge">ARROW_ENGINE</code> has been renamed to <code class="language-plaintext highlighter-rouge">ARROW_SUBSTRAIT</code>,
to better reflect its actual effect (<a href="https://issues.apache.org/jira/browse/ARROW-16158">ARROW-16158</a>).</p>

<h3 id="csv">CSV</h3>

<p>It is now possible to change the field delimiter when writing a CSV file
(<a href="https://issues.apache.org/jira/browse/ARROW-15672">ARROW-15672</a>).</p>

<h3 id="dataset">Dataset</h3>

<p>The ORC dataset scanner now observes the batch size parameter (<a href="https://issues.apache.org/jira/browse/ARROW-14153">ARROW-14153</a>).</p>

<p>The dataset layer now supports filename-based partitioning, where the data
files are all laid out in the dataset’s base directory, their names prefixed
with the partition values separated by underscore characters (<a href="https://issues.apache.org/jira/browse/ARROW-14612">ARROW-14612</a>).</p>

<p>Optional OpenTelemetry tracing has been added to the dataset scanner (<a href="https://issues.apache.org/jira/browse/ARROW-15067">ARROW-15067</a>).</p>

<h3 id="filesystem">Filesystem</h3>

<p>It is possible to instantiate a Google Cloud Storage (GCS) filesystem
from a URI, making GCS implicitly usable in the datasets layer (<a href="https://issues.apache.org/jira/browse/ARROW-14893">ARROW-14893</a>).
Recognized URI schemes are <code class="language-plaintext highlighter-rouge">gs</code> and <code class="language-plaintext highlighter-rouge">gcs</code>.</p>

<p><code class="language-plaintext highlighter-rouge">FileSystem::DeleteDirContents</code> can now optionally succeed when the directory
doesn’t exist (<a href="https://issues.apache.org/jira/browse/ARROW-16159">ARROW-16159</a>).</p>

<h3 id="io">IO</h3>

<p>It is possible to override the number of IO threads using the
environment variable <code class="language-plaintext highlighter-rouge">ARROW_IO_THREADS</code> (<a href="https://issues.apache.org/jira/browse/ARROW-15941">ARROW-15941</a>).</p>

<h3 id="ipc">IPC</h3>

<p>The IPC file reader and writer now allow accessing the custom metadata
associated with record batches (<a href="https://issues.apache.org/jira/browse/ARROW-16131">ARROW-16131</a>).</p>

<h3 id="miscellaneous">Miscellaneous</h3>

<p>It is possible to enable lightweight memory checks on the standard memory pools
using a dedicated environment variable <code class="language-plaintext highlighter-rouge">ARROW_DEBUG_MEMORY_POOL</code> (<a href="https://issues.apache.org/jira/browse/ARROW-15550">ARROW-15550</a>).
These checks are not a replacement for sophisticated checkers such as Address
Sanitizer or Valgrind, but might come up useful if those tools are not
available.</p>

<p>Temporal data is now validated when doing full array validation (<a href="https://issues.apache.org/jira/browse/ARROW-10924">ARROW-10924</a>).
The validation catches values not matching the specification (for example,
a time value being outside of the span of one day).</p>

<p>The GDB plugin now attempts to print the data of an array, in addition to its
metadata (<a href="https://issues.apache.org/jira/browse/ARROW-15389">ARROW-15389</a>).  This only works for primitive datatypes.</p>

<p>Pretty-printing is now shorter and more customizable for nested datatypes
(<a href="https://issues.apache.org/jira/browse/ARROW-14798">ARROW-14798</a>).</p>

<h2 id="c-notes-1">C# notes</h2>

<p>With <a href="https://devblogs.microsoft.com/dotnet/net-core-2-1-will-reach-end-of-support-on-august-21-2021">.NET Core 2.1 reaching end-of-life</a> in August 2021, the Apache.Arrow library has been updated to target <code class="language-plaintext highlighter-rouge">netcoreapp3.1</code> and higher. It still supports <code class="language-plaintext highlighter-rouge">netstandard1.3</code>, so the library works on .NET Framework. But to get the best performance, using .NET Core 3.1, .NET 5, or later is recommended.</p>

<h2 id="go-notes">Go notes</h2>

<h3 id="bug-fixes">Bug fixes</h3>

<ul>
  <li>parquet_reader / parquet_schema no longer crash (<a href="https://github.com/apache/arrow/pull/12303">ARROW-15509</a>)</li>
  <li>Base64 encoding of origin Arrow schema properly uses padding and decodes both with or without the padding in pqarrow.getOriginSchema (<a href="https://github.com/apache/arrow/pull/12679">ARROW-15544</a>)</li>
  <li>ipc.Writer no longer includes unnecessary offsets when encoding sliced arrays (<a href="https://github.com/apache/arrow/pull/12453">ARROW-15715</a>)</li>
  <li>Use base64.StdEncoding for Arrow Flight Basic Auth middleware for proper encoding/decoding (<a href="https://github.com/apache/arrow/pull/12503">ARROW-15772</a>)</li>
  <li>Fix panic during concurrent compression of ipc body buffers due to negative WaitGroup counter (<a href="https://github.com/apache/arrow/pull/12518">ARROW-15792</a>)</li>
  <li>Fix memory leak in pqarrow.NewColumnWriter with nested structures (<a href="https://github.com/apache/arrow/pull/12641">ARROW-15946</a>)</li>
  <li>ipc.FileReader no longer leaks memory when using ZSTD compression (<a href="https://github.com/apache/arrow/pull/12857">ARROW-16163</a>)</li>
</ul>

<h3 id="enhancements">Enhancements</h3>

<h4 id="flight-rpc">Flight RPC</h4>

<ul>
  <li><em>Breaking Change</em> gRPC version is updated and Flight Server creation has been simplified. Flight servers must now embed flight.BaseFlightServer (<a href="https://github.com/apache/arrow/pull/12233">ARROW-15418</a>)</li>
  <li>You can now provide a full net.Listener as an alternative to just providing an address to bind to when creating a Flight server (<a href="https://github.com/apache/arrow/pull/12768">ARROW-16082</a>)</li>
</ul>

<h4 id="parquet">Parquet</h4>

<ul>
  <li>‘unpack_bool’, sum_float64, and bitmap handling functions have been given optimized assembly implementation for Arm64 NEON (<a href="https://github.com/apache/arrow/pull/12398">ARROW-15440</a>, <a href="https://github.com/apache/arrow/pull/12502">ARROW-15742</a>, <a href="https://github.com/apache/arrow/pull/12687">ARROW-15995</a>)</li>
  <li>Go Parquet handling has been simplified to only need io.ReadSeeker instead of a ReadAtSeeker interface (<a href="https://github.com/apache/arrow/pull/12658">ARROW-15963</a>)</li>
  <li>BitSetRunReader and helper functions have been lifted to internal/bitutils package to share between arrow and parquet implementations (<a href="https://github.com/apache/arrow/pull/12926">ARROW-15950</a>)</li>
  <li>Parquet Reader now properly obeys the buffer size read property for buffered streams (<a href="https://github.com/apache/arrow/pull/12876">ARROW-16187</a>)</li>
  <li>parquet NewBufferedReader no longer panics (<a href="https://github.com/apache/arrow/pull/12960">ARROW-16283</a>)</li>
</ul>

<h4 id="arrow">Arrow</h4>

<ul>
  <li>Go Arrow library now supports Dictionary Arrays (<a href="https://issues.apache.org/jira/browse/ARROW-3039">ARROW-3039</a>, <a href="https://issues.apache.org/jira/browse/ARROW-9378">ARROW-9378</a>, <a href="https://github.com/apache/arrow/pull/12158">GH-12158</a>)</li>
  <li>array.ArrayEqual, array.ArrayApproxEqual have been renamed to array.Equal and array.ApproxEqual. Aliases are provided to avoid breaking existing code which will be removed in v9. (<a href="https://github.com/apache/arrow/pull/12877">ARROW-5598</a>)</li>
  <li>Custom cpu discovery package replaced with using golang.org/x/sys/cpu (<a href="https://github.com/apache/arrow/pull/12764">ARROW-16193</a>)</li>
  <li><em>Breaking Change</em> array.Interface, array.Record, array.Table, etc. were deprecated in v7 in favor of arrow.Array, arrow.Record, etc. The deprecated aliases have been removed in v8 (<a href="https://github.com/apache/arrow/pull/12960">ARROW-16192</a>)</li>
</ul>

<h4 id="ci">CI</h4>

<ul>
  <li>staticcheck is now run as part of CI to lint the Go code (<a href="https://github.com/apache/arrow/pull/12540">ARROW-15296</a>)</li>
  <li>Travis builds on Arm64 for Go are no longer allowed to fail (<a href="https://github.com/apache/arrow/pull/12214">ARROW-15400</a>)</li>
</ul>

<h2 id="java-notes">Java notes</h2>

<ul>
  <li>Java 17 is now supported as a target and has been added to tested platforms</li>
  <li>When scanning datasets an <code class="language-plaintext highlighter-rouge">ArrowReader</code> is now returned, which makes easier to create <code class="language-plaintext highlighter-rouge">VectorSchemaRoot</code> from it.</li>
  <li>Java Documentation had a overall improvement, with few sections added and most sections rewritten as more clear tutorials</li>
  <li>Overall improvements to FlightSQL support in Java</li>
  <li><a href="https://arrow.apache.org/cookbook/java/index.html">Java Cookbook</a> is now available</li>
</ul>

<h2 id="javascript-notes">JavaScript notes</h2>

<ul>
  <li>Tables now allow setting arbitrary symbols, which enables support for passing Arrow tables to Vega. <a href="https://github.com/apache/arrow/pull/12907">ARROW-16209</a></li>
  <li>Arrow now supports <code class="language-plaintext highlighter-rouge">tableFromJSON</code> and struct vectors in <code class="language-plaintext highlighter-rouge">vectorFromArray</code>. <a href="https://github.com/apache/arrow/pull/12908">ARROW-16210</a></li>
  <li>Fixed support for appending null children in a StructBuilder. <a href="https://github.com/apache/arrow/pull/12451">ARROW-15705</a></li>
</ul>

<h2 id="python-notes">Python notes</h2>

<p>In general, the Python bindings benefit from improvements in the C++ library
(e.g. new compute functions); see the C++ notes above for additional details.
In addition:</p>

<ul>
  <li>Tables and Datasets now support the <code class="language-plaintext highlighter-rouge">join</code> operation to perform <code class="language-plaintext highlighter-rouge">left</code>, <code class="language-plaintext highlighter-rouge">right</code>, <code class="language-plaintext highlighter-rouge">full</code> joins of <code class="language-plaintext highlighter-rouge">inner</code> or <code class="language-plaintext highlighter-rouge">outer</code> types. The result of the join operation will be a new table (<a href="https://issues.apache.org/jira/browse/ARROW-14293">ARROW-14293</a>). See https://arrow.apache.org/docs/dev/python/compute.html#table-and-dataset-joins for examples.</li>
  <li>Additional legacy keywords and properties of the <code class="language-plaintext highlighter-rouge">ParquetDataset</code> class have been deprecated and will issue a warning, in favor of functionality based on the <code class="language-plaintext highlighter-rouge">pyarrow.dataset</code> functionality (<a href="https://issues.apache.org/jira/browse/ARROW-16119">ARROW-16119</a>).</li>
  <li>It is now possible to create references to nested fields in a Table or Dataset using <code class="language-plaintext highlighter-rouge">py.field("a", "b")</code> (<a href="https://issues.apache.org/jira/browse/ARROW-11259">ARROW-11259</a>).</li>
  <li>Docstrings in <code class="language-plaintext highlighter-rouge">Schema</code>, <code class="language-plaintext highlighter-rouge">ChunkedArray</code>, <code class="language-plaintext highlighter-rouge">Tensor</code>, <code class="language-plaintext highlighter-rouge">RecordBatch</code>, <code class="language-plaintext highlighter-rouge">parquet</code> and <code class="language-plaintext highlighter-rouge">Table</code> now include examples on how to use the methods and classes (<a href="https://issues.apache.org/jira/browse/ARROW-15367">ARROW-15367</a>).</li>
  <li>Reading and writing Parquet files now supports encryption (<a href="https://issues.apache.org/jira/browse/ARROW-9947">ARROW-9947</a>). See the <a href="https://arrow.apache.org/docs/python/parquet.html#parquet-modular-encryption-columnar-encryption">docs</a> for more details.</li>
  <li>Support for <code class="language-plaintext highlighter-rouge">zoneinfo</code> (Python 3.9+) and <code class="language-plaintext highlighter-rouge">dateutil</code> timezones in conversion to Arrow data structures (<a href="https://issues.apache.org/jira/browse/">ARROW-5248</a>).</li>
  <li>Multiple bugfixes and segfaults resolved.</li>
</ul>

<h2 id="r-notes">R notes</h2>

<p>This release includes:</p>

<ul>
  <li>Support for over 20 additional <code class="language-plaintext highlighter-rouge">lubridate</code> and <code class="language-plaintext highlighter-rouge">base</code> date and time functions in Arrow dpylr queries,</li>
  <li>An API to allow external packages to define custom extension array types and custom conversions into Arrow types,</li>
  <li>Support for concatenating Arrays, RecordBatches, and Tables, including with <code class="language-plaintext highlighter-rouge">c()</code>, <code class="language-plaintext highlighter-rouge">rbind()</code> and <code class="language-plaintext highlighter-rouge">cbind()</code>.</li>
</ul>

<p>For more on what’s in the 8.0.0 R package, see the <a href="/docs/r/news/">R changelog</a>.</p>

<h2 id="ruby-and-c-glib-notes">Ruby and C GLib notes</h2>

<h3 id="ruby">Ruby</h3>

<ul>
  <li>Add support for <code class="language-plaintext highlighter-rouge">#values</code> of <code class="language-plaintext highlighter-rouge">MonthInterval</code> Type (<a href="https://issues.apache.org/jira/browse/ARROW-15749">ARROW-15749</a>)</li>
  <li>Add support for <code class="language-plaintext highlighter-rouge">#raw_records</code> of <code class="language-plaintext highlighter-rouge">MonthInterval</code> type (<a href="https://issues.apache.org/jira/browse/ARROW-15750">ARROW-15750</a>)</li>
  <li>Add support for <code class="language-plaintext highlighter-rouge">#values</code> of <code class="language-plaintext highlighter-rouge">DayTimeInterval</code> type (<a href="https://issues.apache.org/jira/browse/ARROW-15885">ARROW-15885</a>)</li>
  <li>Add <code class="language-plaintext highlighter-rouge">DayTimeIntervalArrayBuilder</code> to support to make <code class="language-plaintext highlighter-rouge">DayTimeIntervalArray</code> by a Hash with <code class="language-plaintext highlighter-rouge">:day</code> and <code class="language-plaintext highlighter-rouge">:millisecond</code> keys (<a href="https://issues.apache.org/jira/browse/ARROW-15918">ARROW-15918</a>)</li>
  <li>Add support for <code class="language-plaintext highlighter-rouge">#raw_records</code> of <code class="language-plaintext highlighter-rouge">DayTimeInterval</code> type (<a href="https://issues.apache.org/jira/browse/ARROW-15886">ARROW-15886</a>)</li>
  <li>Add support for <code class="language-plaintext highlighter-rouge">#values</code> of <code class="language-plaintext highlighter-rouge">MonthDayNanoInterval</code> type (<a href="https://issues.apache.org/jira/browse/ARROW-15924">ARROW-15924</a>)
    <ul>
      <li>Also add <code class="language-plaintext highlighter-rouge">MonthDayNanoIntervalArrayBuilder</code> to support to make <code class="language-plaintext highlighter-rouge">MonthDayNanoIntervalArray</code> by a Hash with <code class="language-plaintext highlighter-rouge">:month</code>, <code class="language-plaintext highlighter-rouge">:day</code>, and <code class="language-plaintext highlighter-rouge">:nanosecond</code> keys</li>
    </ul>
  </li>
  <li>Add support for <code class="language-plaintext highlighter-rouge">#raw_records</code> of <code class="language-plaintext highlighter-rouge">MonthDayNanoInterval</code> type (<a href="https://issues.apache.org/jira/browse/ARROW-15925">ARROW-15925</a>)</li>
  <li>Add Ruby-ish interfaces for <code class="language-plaintext highlighter-rouge">Parquet::BooleanStatistics</code> (<a href="https://issues.apache.org/jira/browse/ARROW-16251">ARROW-16251</a>)</li>
</ul>

<h3 id="c-glib">C GLib</h3>

<ul>
  <li>Add <code class="language-plaintext highlighter-rouge">gaflight_client_close</code> (<a href="https://issues.apache.org/jira/browse/ARROW-15487">ARROW-15487</a>)</li>
  <li>Add <code class="language-plaintext highlighter-rouge">GParquetFileMetadata</code> and <code class="language-plaintext highlighter-rouge">gparquet_arrow_file_reader_get_metadata</code> (<a href="https://issues.apache.org/jira/browse/ARROW-16214">ARROW-16214</a>)</li>
  <li>Fix <code class="language-plaintext highlighter-rouge">GArrowGIOInputStream</code> so that all the data is completely read (<a href="https://issues.apache.org/jira/browse/ARROW-15626">ARROW-15626</a>)</li>
  <li>Add <code class="language-plaintext highlighter-rouge">garrow_string_array_builder_append_string_len</code> and <code class="language-plaintext highlighter-rouge">garrow_large_string_array_builder_append_string_len</code> (<a href="https://issues.apache.org/jira/browse/ARROW-15629">ARROW-15629</a>)</li>
  <li>Add <code class="language-plaintext highlighter-rouge">GParquetRowGroupMetadata</code> (<a href="https://issues.apache.org/jira/browse/ARROW-16245">ARROW-16245</a>)</li>
  <li>Add <code class="language-plaintext highlighter-rouge">GParquetColumnChunkMetadata</code> (<a href="https://issues.apache.org/jira/browse/ARROW-16250">ARROW-16250</a>)</li>
  <li>Add <code class="language-plaintext highlighter-rouge">GArrowGCSFileSystem</code> (<a href="https://issues.apache.org/jira/browse/ARROW-16247">ARROW-16247</a>)</li>
  <li>Add <code class="language-plaintext highlighter-rouge">GParquetStatistics</code> and its family (<a href="https://issues.apache.org/jira/browse/ARROW-16251">ARROW-16251</a>)
    <ul>
      <li><code class="language-plaintext highlighter-rouge">GParquetBooleanStatistics</code></li>
      <li><code class="language-plaintext highlighter-rouge">GParquetInt32Statistics</code></li>
      <li><code class="language-plaintext highlighter-rouge">GParquetInt64Statistics</code></li>
      <li><code class="language-plaintext highlighter-rouge">GParquetFloatStatistics</code></li>
      <li><code class="language-plaintext highlighter-rouge">GParquetDoubleStatistics</code></li>
      <li><code class="language-plaintext highlighter-rouge">GParquetByteArrayStatistics</code></li>
      <li><code class="language-plaintext highlighter-rouge">GParquetFixedLengthByteArrayStatistics</code></li>
    </ul>
  </li>
  <li>Add missing casts for <code class="language-plaintext highlighter-rouge">GArrowRoundMode</code> (<a href="https://issues.apache.org/jira/browse/ARROW-16296">ARROW-16296</a>)</li>
</ul>

<h2 id="rust-notes">Rust notes</h2>

<p>The Rust projects have moved to separate repositories outside the
main Arrow monorepo. For notes on the 13.0.0 release of the Rust
implementation, see the <a href="https://github.com/apache/arrow-rs/blob/13.0.0/CHANGELOG.md">Arrow Rust changelog</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[The Apache Arrow team is pleased to announce the 8.0.0 release. This covers over 3 months of development work and includes 586 resolved issues from 127 distinct contributors. See the Install Page to learn how to get the libraries for your platform. The release notes below are not exhaustive and only expose selected highlights of the release. Many other bugfixes and improvements have been made: we refer you to the complete changelog. Community Since the 7.0.0 release, Kun Liu, Raphael Taylor-Davies Xudong Wang, Yijie Shen and Liang-Chi Hsieh have been invited to be committers. Thanks for your contributions and participation in the project! Arrow Flight RPC notes Flight SQL has been extended with a method to get type metadata (ARROW-15313) and column metadata in returned schemas (ARROW-15314, ARROW-16064) New documentation is available describing Flight and Flight SQL, along with several Cookbook recipes (ARROW-14698, ARROW-16065). The C++ libraries now support UCX as a network transport (ARROW-15706), and the APIs have been refactored to allow other transports to be implemented (ARROW-15282). UCX support is experimental and still subject to change. Many of the APIs have been refactored to use the arrow::Result type, and the original variants have been deprecated (ARROW-16032). Support for gRPC &gt;= 1.43 has been added (ARROW-15551). C++ notes Compute Arrow C++ can now optionally build with support for the experimental Substrait query representation format (ARROW-15238). A number of compute kernels operating on temporal data have been added: addition, subtraction and multiplication between various temporal types; a new “is_dst” function to compute whether the input timestamps fall within daylight saving time (DST); a new “is_leap_year” function to compute whether the input timestamps fall within a leap year. It is possible to enable a timezone database on Windows at runtime by calling the arrow::Initialize() function (ARROW-13168). New hash aggregations are available: “hash_one” to return one value from each group (ARROW-13993), and “hash_list” to return all values from each group (ARROW-15152). Null columns are now supported on the sum, mean and product hash aggregates (ARROW-15506). Also, it is now possible to execute hash “aggregations” with only key columns (ARROW-15609). A new compute function “map_lookup” allows looking up a given key in a map array (ARROW-15089). New compute functions “sqrt” and “sqrt_checked” allow extracting the square root of their input (ARROW-15614). Casting between two struct types is now possible, assuming the destination field names all exist in the source struct type (ARROW-1888, ARROW-15643). Optional OpenTelemetry tracing has been added to kernel functions and execution plan nodes (ARROW-15061). The CMake build option ARROW_ENGINE has been renamed to ARROW_SUBSTRAIT, to better reflect its actual effect (ARROW-16158). CSV It is now possible to change the field delimiter when writing a CSV file (ARROW-15672). Dataset The ORC dataset scanner now observes the batch size parameter (ARROW-14153). The dataset layer now supports filename-based partitioning, where the data files are all laid out in the dataset’s base directory, their names prefixed with the partition values separated by underscore characters (ARROW-14612). Optional OpenTelemetry tracing has been added to the dataset scanner (ARROW-15067). Filesystem It is possible to instantiate a Google Cloud Storage (GCS) filesystem from a URI, making GCS implicitly usable in the datasets layer (ARROW-14893). Recognized URI schemes are gs and gcs. FileSystem::DeleteDirContents can now optionally succeed when the directory doesn’t exist (ARROW-16159). IO It is possible to override the number of IO threads using the environment variable ARROW_IO_THREADS (ARROW-15941). IPC The IPC file reader and writer now allow accessing the custom metadata associated with record batches (ARROW-16131). Miscellaneous It is possible to enable lightweight memory checks on the standard memory pools using a dedicated environment variable ARROW_DEBUG_MEMORY_POOL (ARROW-15550). These checks are not a replacement for sophisticated checkers such as Address Sanitizer or Valgrind, but might come up useful if those tools are not available. Temporal data is now validated when doing full array validation (ARROW-10924). The validation catches values not matching the specification (for example, a time value being outside of the span of one day). The GDB plugin now attempts to print the data of an array, in addition to its metadata (ARROW-15389). This only works for primitive datatypes. Pretty-printing is now shorter and more customizable for nested datatypes (ARROW-14798). C# notes With .NET Core 2.1 reaching end-of-life in August 2021, the Apache.Arrow library has been updated to target netcoreapp3.1 and higher. It still supports netstandard1.3, so the library works on .NET Framework. But to get the best performance, using .NET Core 3.1, .NET 5, or later is recommended. Go notes Bug fixes parquet_reader / parquet_schema no longer crash (ARROW-15509) Base64 encoding of origin Arrow schema properly uses padding and decodes both with or without the padding in pqarrow.getOriginSchema (ARROW-15544) ipc.Writer no longer includes unnecessary offsets when encoding sliced arrays (ARROW-15715) Use base64.StdEncoding for Arrow Flight Basic Auth middleware for proper encoding/decoding (ARROW-15772) Fix panic during concurrent compression of ipc body buffers due to negative WaitGroup counter (ARROW-15792) Fix memory leak in pqarrow.NewColumnWriter with nested structures (ARROW-15946) ipc.FileReader no longer leaks memory when using ZSTD compression (ARROW-16163) Enhancements Flight RPC Breaking Change gRPC version is updated and Flight Server creation has been simplified. Flight servers must now embed flight.BaseFlightServer (ARROW-15418) You can now provide a full net.Listener as an alternative to just providing an address to bind to when creating a Flight server (ARROW-16082) Parquet ‘unpack_bool’, sum_float64, and bitmap handling functions have been given optimized assembly implementation for Arm64 NEON (ARROW-15440, ARROW-15742, ARROW-15995) Go Parquet handling has been simplified to only need io.ReadSeeker instead of a ReadAtSeeker interface (ARROW-15963) BitSetRunReader and helper functions have been lifted to internal/bitutils package to share between arrow and parquet implementations (ARROW-15950) Parquet Reader now properly obeys the buffer size read property for buffered streams (ARROW-16187) parquet NewBufferedReader no longer panics (ARROW-16283) Arrow Go Arrow library now supports Dictionary Arrays (ARROW-3039, ARROW-9378, GH-12158) array.ArrayEqual, array.ArrayApproxEqual have been renamed to array.Equal and array.ApproxEqual. Aliases are provided to avoid breaking existing code which will be removed in v9. (ARROW-5598) Custom cpu discovery package replaced with using golang.org/x/sys/cpu (ARROW-16193) Breaking Change array.Interface, array.Record, array.Table, etc. were deprecated in v7 in favor of arrow.Array, arrow.Record, etc. The deprecated aliases have been removed in v8 (ARROW-16192) CI staticcheck is now run as part of CI to lint the Go code (ARROW-15296) Travis builds on Arm64 for Go are no longer allowed to fail (ARROW-15400) Java notes Java 17 is now supported as a target and has been added to tested platforms When scanning datasets an ArrowReader is now returned, which makes easier to create VectorSchemaRoot from it. Java Documentation had a overall improvement, with few sections added and most sections rewritten as more clear tutorials Overall improvements to FlightSQL support in Java Java Cookbook is now available JavaScript notes Tables now allow setting arbitrary symbols, which enables support for passing Arrow tables to Vega. ARROW-16209 Arrow now supports tableFromJSON and struct vectors in vectorFromArray. ARROW-16210 Fixed support for appending null children in a StructBuilder. ARROW-15705 Python notes In general, the Python bindings benefit from improvements in the C++ library (e.g. new compute functions); see the C++ notes above for additional details. In addition: Tables and Datasets now support the join operation to perform left, right, full joins of inner or outer types. The result of the join operation will be a new table (ARROW-14293). See https://arrow.apache.org/docs/dev/python/compute.html#table-and-dataset-joins for examples. Additional legacy keywords and properties of the ParquetDataset class have been deprecated and will issue a warning, in favor of functionality based on the pyarrow.dataset functionality (ARROW-16119). It is now possible to create references to nested fields in a Table or Dataset using py.field("a", "b") (ARROW-11259). Docstrings in Schema, ChunkedArray, Tensor, RecordBatch, parquet and Table now include examples on how to use the methods and classes (ARROW-15367). Reading and writing Parquet files now supports encryption (ARROW-9947). See the docs for more details. Support for zoneinfo (Python 3.9+) and dateutil timezones in conversion to Arrow data structures (ARROW-5248). Multiple bugfixes and segfaults resolved. R notes This release includes: Support for over 20 additional lubridate and base date and time functions in Arrow dpylr queries, An API to allow external packages to define custom extension array types and custom conversions into Arrow types, Support for concatenating Arrays, RecordBatches, and Tables, including with c(), rbind() and cbind(). For more on what’s in the 8.0.0 R package, see the R changelog. Ruby and C GLib notes Ruby Add support for #values of MonthInterval Type (ARROW-15749) Add support for #raw_records of MonthInterval type (ARROW-15750) Add support for #values of DayTimeInterval type (ARROW-15885) Add DayTimeIntervalArrayBuilder to support to make DayTimeIntervalArray by a Hash with :day and :millisecond keys (ARROW-15918) Add support for #raw_records of DayTimeInterval type (ARROW-15886) Add support for #values of MonthDayNanoInterval type (ARROW-15924) Also add MonthDayNanoIntervalArrayBuilder to support to make MonthDayNanoIntervalArray by a Hash with :month, :day, and :nanosecond keys Add support for #raw_records of MonthDayNanoInterval type (ARROW-15925) Add Ruby-ish interfaces for Parquet::BooleanStatistics (ARROW-16251) C GLib Add gaflight_client_close (ARROW-15487) Add GParquetFileMetadata and gparquet_arrow_file_reader_get_metadata (ARROW-16214) Fix GArrowGIOInputStream so that all the data is completely read (ARROW-15626) Add garrow_string_array_builder_append_string_len and garrow_large_string_array_builder_append_string_len (ARROW-15629) Add GParquetRowGroupMetadata (ARROW-16245) Add GParquetColumnChunkMetadata (ARROW-16250) Add GArrowGCSFileSystem (ARROW-16247) Add GParquetStatistics and its family (ARROW-16251) GParquetBooleanStatistics GParquetInt32Statistics GParquetInt64Statistics GParquetFloatStatistics GParquetDoubleStatistics GParquetByteArrayStatistics GParquetFixedLengthByteArrayStatistics Add missing casts for GArrowRoundMode (ARROW-16296) Rust notes The Rust projects have moved to separate repositories outside the main Arrow monorepo. For notes on the 13.0.0 release of the Rust implementation, see the Arrow Rust changelog.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Apache Arrow for R Cheatsheet</title><link href="https://arrow.apache.org/blog/2022/04/27/arrow-r-cheatsheet/" rel="alternate" type="text/html" title="Apache Arrow for R Cheatsheet" /><published>2022-04-27T00:00:00-04:00</published><updated>2022-04-27T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/04/27/arrow-r-cheatsheet</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/04/27/arrow-r-cheatsheet/"><![CDATA[<!--

-->

<p>We are excited to introduce the new <a href="https://github.com/apache/arrow/blob/master/r/cheatsheet/arrow-cheatsheet.pdf">Apache Arrow for R Cheatsheet</a>.</p>

<div align="center">
<a href="https://github.com/apache/arrow/blob/master/r/cheatsheet/arrow-cheatsheet.pdf">
<img src="/img/20220427-arrow-r-cheatsheet-thumbnail.png" alt="Thumbnail image of the first page of the Arrow for R cheatsheet." width="70%" height="70%" />
</a>
</div>

<h2 id="helping-not-cheating">Helping (Not Cheating)</h2>

<p>While <a href="https://en.wikipedia.org/wiki/Cheat_sheet">cheatsheets</a> may have started as a set of notes used without an instructor’s knowledge—so, ummm, cheating—using the Arrow for R cheatsheet is definitely not cheating! Today, cheatsheets are a common tool to provide users an introduction to software’s functionality and a quick reference guide to help users get started.</p>

<p>The Arrow for R cheatsheet is intended to be an easy-to-scan introduction to the Arrow R package and Arrow data structures, with getting started sections on some of the package’s main functionality. The cheatsheet includes introductory snippets on using Arrow to read and work with larger-than-memory multi-file data sets, sending and receiving data with Flight, reading data from cloud storage without downloading the data first, and more. The Arrow for R cheatsheet also directs users to the full <a href="https://arrow.apache.org/docs/r/index.html">Arrow for R package documentation and articles</a> and the <a href="https://arrow.apache.org/cookbook/r/">Arrow Cookbook</a>, both full of code examples and recipes to support users build their Arrow-based data workflows. Finally, the cheatsheet debuts one of the first uses of the hot-off-the-presses Arrow hex sticker, recently made available as part of the <a href="https://arrow.apache.org/visual_identity/">Apache Arrow visual identity guidance</a>.</p>

<h2 id="cheatsheet-maintenance">Cheatsheet Maintenance</h2>

<p>See something that needs updating? Or want to suggest a change? Like software itself, a package cheatsheet needs maintenance to keep pace with new features or user-facing changes. Contributions can be made by downloading and making changes to the <a href="https://github.com/apache/arrow/tree/master/r/cheatsheet"><code class="language-plaintext highlighter-rouge">arrow-cheatsheet.pptx</code> file</a> (in Microsoft PowerPoint or Google Slides), and offering the revised <code class="language-plaintext highlighter-rouge">.pptx</code> and rendered PDF back to the project following the <em>new</em> <a href="https://arrow.apache.org/docs/developers/guide/step_by_step/set_up.html">New Contributors Guide</a>. Since a cheatsheet contribution does not touch the Arrow codebase, cheatsheet contributors don’t need to build the package or worry about running (or writing!) code tests. The New Contributors Guide will walk you through how to get set up with git, fork the Arrow GitHub repository, make a branch, replace the <code class="language-plaintext highlighter-rouge">.pptx</code> and <code class="language-plaintext highlighter-rouge">.pdf</code> files with your editions, and contribute the changes with a <a href="https://arrow.apache.org/docs/developers/guide/step_by_step/pr_and_github.html">Pull Request</a>. Questions and support are always available through the <a href="https://arrow.apache.org/community/">community mailing list</a>.</p>

<h2 id="by-the-community-for-the-community">By the Community For the Community</h2>

<p>The Arrow for R cheatsheet was initiated by Mauricio (Pachá) Vargas Sepúlveda (<a href="https://issues.apache.org/jira/browse/ARROW-13616">ARROW-13616</a>) and was co-developed and reviewed by many Apache Arrow community members. The cheatsheet was created by the community for the community, and anyone in the Arrow community is welcome and encouraged to help with maintenance and offer improvements. Thank you for your support!</p>]]></content><author><name>stephhazlitt</name></author><category term="application" /><summary type="html"><![CDATA[We are excited to introduce the new Apache Arrow for R Cheatsheet. Helping (Not Cheating) While cheatsheets may have started as a set of notes used without an instructor’s knowledge—so, ummm, cheating—using the Arrow for R cheatsheet is definitely not cheating! Today, cheatsheets are a common tool to provide users an introduction to software’s functionality and a quick reference guide to help users get started. The Arrow for R cheatsheet is intended to be an easy-to-scan introduction to the Arrow R package and Arrow data structures, with getting started sections on some of the package’s main functionality. The cheatsheet includes introductory snippets on using Arrow to read and work with larger-than-memory multi-file data sets, sending and receiving data with Flight, reading data from cloud storage without downloading the data first, and more. The Arrow for R cheatsheet also directs users to the full Arrow for R package documentation and articles and the Arrow Cookbook, both full of code examples and recipes to support users build their Arrow-based data workflows. Finally, the cheatsheet debuts one of the first uses of the hot-off-the-presses Arrow hex sticker, recently made available as part of the Apache Arrow visual identity guidance. Cheatsheet Maintenance See something that needs updating? Or want to suggest a change? Like software itself, a package cheatsheet needs maintenance to keep pace with new features or user-facing changes. Contributions can be made by downloading and making changes to the arrow-cheatsheet.pptx file (in Microsoft PowerPoint or Google Slides), and offering the revised .pptx and rendered PDF back to the project following the new New Contributors Guide. Since a cheatsheet contribution does not touch the Arrow codebase, cheatsheet contributors don’t need to build the package or worry about running (or writing!) code tests. The New Contributors Guide will walk you through how to get set up with git, fork the Arrow GitHub repository, make a branch, replace the .pptx and .pdf files with your editions, and contribute the changes with a Pull Request. Questions and support are always available through the community mailing list. By the Community For the Community The Arrow for R cheatsheet was initiated by Mauricio (Pachá) Vargas Sepúlveda (ARROW-13616) and was co-developed and reviewed by many Apache Arrow community members. The cheatsheet was created by the community for the community, and anyone in the Arrow community is welcome and encouraged to help with maintenance and offer improvements. Thank you for your support!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Introducing Apache Arrow DataFusion Contrib</title><link href="https://arrow.apache.org/blog/2022/03/21/datafusion-contrib/" rel="alternate" type="text/html" title="Introducing Apache Arrow DataFusion Contrib" /><published>2022-03-21T00:00:00-04:00</published><updated>2022-03-21T00:00:00-04:00</updated><id>https://arrow.apache.org/blog/2022/03/21/datafusion-contrib</id><content type="html" xml:base="https://arrow.apache.org/blog/2022/03/21/datafusion-contrib/"><![CDATA[<!--

-->

<h1 id="introduction">Introduction</h1>

<p>Apache Arrow <a href="https://arrow.apache.org/datafusion/">DataFusion</a> is an extensible query execution framework, written in Rust, that uses <a href="https://arrow.apache.org">Apache Arrow</a> as its in-memory format.</p>

<p>When you want to extend your Rust project with <a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html">SQL support</a>, a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is definitely worth checking out. DataFusion’s pluggable design makes creating extensions at various points particular easy to build.</p>

<p>DataFusion’s  SQL, <code class="language-plaintext highlighter-rouge">DataFrame</code>, and manual <code class="language-plaintext highlighter-rouge">PlanBuilder</code> API let users access a sophisticated query optimizer and execution engine capable of fast, resource efficient, and parallel execution that takes optimal advantage of todays multicore hardware. Being written in Rust means DataFusion can offer <em>both</em> the safety of dynamic languages as well as the resource efficiency of a compiled language.</p>

<p>The DataFusion team is pleased to announce the creation of the <a href="https://github.com/datafusion-contrib">DataFusion-Contrib</a> GitHub organization to support and accelerate other projects.  While the core DataFusion library remains under Apache governance, the contrib organization provides a more flexible testing ground for new DataFusion features and a home for DataFusion extensions.  With this announcement, we are pleased to introduce the following inaugural DataFusion-Contrib repositories.</p>

<h2 id="datafusion-python">DataFusion-Python</h2>

<p>This <a href="https://github.com/datafusion-contrib/datafusion-python">project</a> provides Python bindings to the core Rust implementation of DataFusion, which allows users to:</p>

<ul>
  <li>Work with familiar SQL or DataFrame APIs to run queries in a safe, multi-threaded environment, returning results in Python</li>
  <li>Create User Defined Functions and User Defined Aggregate Functions for complex operations</li>
  <li>Pay no overhead to copy between Python and underlying Rust execution engine (by way of Apache Arrow arrays)</li>
</ul>

<h3 id="upcoming-enhancements">Upcoming enhancements</h3>

<p>The team is focusing on exposing more features from the underlying Rust implementation of DataFusion and improving documentation.</p>

<h3 id="how-to-install">How to install</h3>

<p>From <code class="language-plaintext highlighter-rouge">pip</code></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>datafusion
</code></pre></div></div>

<p>Or</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-m</span> pip <span class="nb">install </span>datafusion
</code></pre></div></div>

<h2 id="datafusion-objectstore-s3">DataFusion-ObjectStore-S3</h2>

<p>This <a href="https://github.com/datafusion-contrib/datafusion-objectstore-s3">crate</a> provides an <code class="language-plaintext highlighter-rouge">ObjectStore</code> implementation for querying data stored in S3 or S3 compatible storage. This makes it almost as easy to query data that lives on S3 as lives in local files</p>

<ul>
  <li>Ability to create <code class="language-plaintext highlighter-rouge">S3FileSystem</code> to register as part of DataFusion <code class="language-plaintext highlighter-rouge">ExecutionContext</code></li>
  <li>Register files or directories stored on S3 with <code class="language-plaintext highlighter-rouge">ctx.register_listing_table</code></li>
</ul>

<h3 id="upcoming-enhancements-1">Upcoming enhancements</h3>

<p>The current priority is adding python bindings for <code class="language-plaintext highlighter-rouge">S3FileSystem</code>.  After that there will be async improvements as DataFusion adopts more of that functionality and we are looking into S3 Select functionality.</p>

<h3 id="how-to-install-1">How to Install</h3>

<p>Add the below to your <code class="language-plaintext highlighter-rouge">Cargo.toml</code> in your Rust Project with DataFusion.</p>

<div class="language-toml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="py">datafusion-objectstore-s3</span> <span class="p">=</span> <span class="s">"0.1.0"</span>
</code></pre></div></div>

<h2 id="datafusion-substrait">DataFusion-Substrait</h2>

<p><a href="https://substrait.io/">Substrait</a> is an emerging standard that provides a cross-language serialization format for relational algebra (e.g. expressions and query plans).</p>

<p>This <a href="https://github.com/datafusion-contrib/datafusion-substrait">crate</a> provides a Substrait producer and consumer for DataFusion.  A producer converts a DataFusion logical plan into a Substrait protobuf and a consumer does the reverse.</p>

<p>Examples of how to use this crate can be found <a href="https://github.com/datafusion-contrib/datafusion-substrait/blob/main/src/lib.rs">here</a>.</p>

<h3 id="potential-use-cases">Potential Use Cases</h3>

<ul>
  <li>Replace custom DataFusion protobuf serialization.</li>
  <li>Make it easier to pass query plans over FFI boundaries, such as from Python to Rust</li>
  <li>Allow Apache Calcite query plans to be executed in DataFusion</li>
</ul>

<h2 id="datafusion-bigtable">DataFusion-BigTable</h2>

<p>This <a href="https://github.com/datafusion-contrib/datafusion-bigtable">crate</a> implements <a href="https://cloud.google.com/bigtable">Bigtable</a> as a data source and physical executor for DataFusion queries.  It currently supports both UTF-8 string and 64-bit big-endian signed integers in Bigtable.  From a SQL perspective it supports both simple and composite row keys with <code class="language-plaintext highlighter-rouge">=</code>, <code class="language-plaintext highlighter-rouge">IN</code>, and <code class="language-plaintext highlighter-rouge">BETWEEN</code> operators as well as projection pushdown.  The physical execution for queries is handled by this crate while any subsequent aggregation, group bys, or joins are handled in DataFusion.</p>

<h3 id="upcoming-enhancements-2">Upcoming Enhancements</h3>

<ul>
  <li>Predicate pushdown
    <ul>
      <li>Value range</li>
      <li>Value Regex</li>
      <li>Timestamp range</li>
    </ul>
  </li>
  <li>Multithreaded</li>
  <li>Partition aware execution</li>
  <li>Production ready</li>
</ul>

<h3 id="how-to-install-2">How to Install</h3>

<p>Add the below to your <code class="language-plaintext highlighter-rouge">Cargo.toml</code> in your Rust Project with DataFusion.</p>

<div class="language-toml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="py">datafusion-bigtable</span> <span class="p">=</span> <span class="s">"0.1.0"</span>
</code></pre></div></div>

<h2 id="datafusion-hdfs">DataFusion-HDFS</h2>

<p>This <a href="https://github.com/datafusion-contrib/datafusion-objectstore-hdfs">crate</a> introduces <code class="language-plaintext highlighter-rouge">HadoopFileSystem</code> as a remote <code class="language-plaintext highlighter-rouge">ObjectStore</code> which provides the ability to query HDFS files.  For HDFS access the <a href="https://github.com/yahoNanJing/fs-hdfs">fs-hdfs</a> library is used.</p>

<h2 id="datafusion-tokomak">DataFusion-Tokomak</h2>

<p>This <a href="https://github.com/datafusion-contrib/datafusion-tokomak">crate</a> provides an e-graph based DataFusion optimization framework based on the Rust <a href="https://egraphs-good.github.io">egg</a> library.  An e-graph is a data structure that powers the equality saturation optimization technique.</p>

<p>As context, the optimizer framework within DataFusion is currently <a href="https://github.com/apache/arrow-datafusion/issues/1972">under review</a> with the objective of implementing a more strategic long term solution that is more efficient and simpler to develop.</p>

<p>Some of the benefits of using <code class="language-plaintext highlighter-rouge">egg</code> within DataFusion are:</p>

<ul>
  <li>Implements optimized algorithms that are hard to match with manually written optimization passes</li>
  <li>Makes it easy and less verbose to add optimization rules</li>
  <li>Plugin framework to add more complex optimizations</li>
  <li>Egg does not depend on rule order and can lead to a higher level of optimization by being able to apply multiple rules at the same time until it converges</li>
  <li>Allows for cost-based optimizations</li>
</ul>

<p>This is an exciting new area for DataFusion with lots of opportunity for community involvement!</p>

<h2 id="datafusion-tui">DataFusion-Tui</h2>

<p><a href="https://github.com/datafusion-contrib/datafusion-tui">DataFusion-tui</a> aka <code class="language-plaintext highlighter-rouge">dft</code> provides a feature rich terminal application for using DataFusion.  It has drawn inspiration and several features from <code class="language-plaintext highlighter-rouge">datafusion-cli</code>.  In contrast to <code class="language-plaintext highlighter-rouge">datafusion-cli</code> the objective of this tool is to provide a light SQL IDE experience for querying data with DataFusion.  This includes features such as the following which are currently implemented:</p>

<ul>
  <li>Tab Management to provide clean and structured organization of DataFusion queries, results, <code class="language-plaintext highlighter-rouge">ExecutionContext</code> information, and logs
    <ul>
      <li>SQL Editor
        <ul>
          <li>Text editor for writing SQL queries</li>
        </ul>
      </li>
      <li>Query History
        <ul>
          <li>History of executed queries, their execution time, and the number of returned rows</li>
        </ul>
      </li>
      <li><code class="language-plaintext highlighter-rouge">ExecutionContext</code> information
        <ul>
          <li>Expose information on which physical optimizers are used and which <code class="language-plaintext highlighter-rouge">ExecutionConfig</code> settings are set</li>
        </ul>
      </li>
      <li>Logs
        <ul>
          <li>Logs from <code class="language-plaintext highlighter-rouge">dft</code>, DataFusion, and any dependent libraries</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Support for custom <code class="language-plaintext highlighter-rouge">ObjectStore</code>s
    <ul>
      <li>S3</li>
    </ul>
  </li>
  <li>Preload DDL from <code class="language-plaintext highlighter-rouge">~/.datafusionrc</code> to enable having local “database” available at startup</li>
</ul>

<h3 id="upcoming-enhancements-3">Upcoming Enhancements</h3>

<ul>
  <li>SQL Editor
    <ul>
      <li>Command to write query results to file</li>
      <li>Multiple SQL editor tabs</li>
    </ul>
  </li>
  <li>Expose more information from <code class="language-plaintext highlighter-rouge">ExecutionContext</code></li>
  <li>A help tab that provides information on functions</li>
  <li>Query custom <code class="language-plaintext highlighter-rouge">TableProvider</code>s such as <a href="https://github.com/delta-io/delta-rs">DeltaTable</a> or <a href="https://github.com/datafusion-contrib/datafusion-bigtable">BigTable</a></li>
</ul>

<h2 id="datafusion-streams">DataFusion-Streams</h2>

<p><a href="https://github.com/datafusion-contrib/datafusion-streams">DataFusion-Stream</a> is a new testing ground for creating a <code class="language-plaintext highlighter-rouge">StreamProvider</code> in DataFusion that will enable querying streaming data sources such as Apache Kafka.  The implementation for this feature is currently being designed and is under active review.  Once the design is finalized the trait and attendant data structures will be added back to the core DataFusion crate.</p>

<h2 id="datafusion-java">DataFusion-Java</h2>

<p>This <a href="https://github.com/datafusion-contrib/datafusion-java">project</a> created an initial set of Java bindings to DataFusion.  The project is currently in maintenance mode and is looking for maintainers to drive future development.</p>

<h1 id="how-to-get-involved">How to Get Involved</h1>

<p>If you are interested in contributing to DataFusion, and learning about state of
the art query processing, we would love to have you join us on the journey! You
can help by trying out DataFusion on some of your own data and projects and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable for beginners is <a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a></p>

<p>The best way to find out about creating new extensions within DataFusion-Contrib is reaching out on the <code class="language-plaintext highlighter-rouge">#arrow-rust</code> channel of the Apache Software Foundation <a href="https://join.slack.com/t/the-asf/shared_invite/zt-vlfbf7ch-HkbNHiU_uDlcH_RvaHv9gQ">Slack</a> workspace.</p>

<p>You can also check out our new <a href="https://arrow.apache.org/datafusion/community/communication.html">Communication Doc</a> on more ways to engage with the community.</p>

<p>Links for each DataFusion-Contrib repository are provided above if you would like to contribute to those.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[Introduction Apache Arrow DataFusion is an extensible query execution framework, written in Rust, that uses Apache Arrow as its in-memory format. When you want to extend your Rust project with SQL support, a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is definitely worth checking out. DataFusion’s pluggable design makes creating extensions at various points particular easy to build. DataFusion’s SQL, DataFrame, and manual PlanBuilder API let users access a sophisticated query optimizer and execution engine capable of fast, resource efficient, and parallel execution that takes optimal advantage of todays multicore hardware. Being written in Rust means DataFusion can offer both the safety of dynamic languages as well as the resource efficiency of a compiled language. The DataFusion team is pleased to announce the creation of the DataFusion-Contrib GitHub organization to support and accelerate other projects. While the core DataFusion library remains under Apache governance, the contrib organization provides a more flexible testing ground for new DataFusion features and a home for DataFusion extensions. With this announcement, we are pleased to introduce the following inaugural DataFusion-Contrib repositories. DataFusion-Python This project provides Python bindings to the core Rust implementation of DataFusion, which allows users to: Work with familiar SQL or DataFrame APIs to run queries in a safe, multi-threaded environment, returning results in Python Create User Defined Functions and User Defined Aggregate Functions for complex operations Pay no overhead to copy between Python and underlying Rust execution engine (by way of Apache Arrow arrays) Upcoming enhancements The team is focusing on exposing more features from the underlying Rust implementation of DataFusion and improving documentation. How to install From pip pip install datafusion Or python -m pip install datafusion DataFusion-ObjectStore-S3 This crate provides an ObjectStore implementation for querying data stored in S3 or S3 compatible storage. This makes it almost as easy to query data that lives on S3 as lives in local files Ability to create S3FileSystem to register as part of DataFusion ExecutionContext Register files or directories stored on S3 with ctx.register_listing_table Upcoming enhancements The current priority is adding python bindings for S3FileSystem. After that there will be async improvements as DataFusion adopts more of that functionality and we are looking into S3 Select functionality. How to Install Add the below to your Cargo.toml in your Rust Project with DataFusion. datafusion-objectstore-s3 = "0.1.0" DataFusion-Substrait Substrait is an emerging standard that provides a cross-language serialization format for relational algebra (e.g. expressions and query plans). This crate provides a Substrait producer and consumer for DataFusion. A producer converts a DataFusion logical plan into a Substrait protobuf and a consumer does the reverse. Examples of how to use this crate can be found here. Potential Use Cases Replace custom DataFusion protobuf serialization. Make it easier to pass query plans over FFI boundaries, such as from Python to Rust Allow Apache Calcite query plans to be executed in DataFusion DataFusion-BigTable This crate implements Bigtable as a data source and physical executor for DataFusion queries. It currently supports both UTF-8 string and 64-bit big-endian signed integers in Bigtable. From a SQL perspective it supports both simple and composite row keys with =, IN, and BETWEEN operators as well as projection pushdown. The physical execution for queries is handled by this crate while any subsequent aggregation, group bys, or joins are handled in DataFusion. Upcoming Enhancements Predicate pushdown Value range Value Regex Timestamp range Multithreaded Partition aware execution Production ready How to Install Add the below to your Cargo.toml in your Rust Project with DataFusion. datafusion-bigtable = "0.1.0" DataFusion-HDFS This crate introduces HadoopFileSystem as a remote ObjectStore which provides the ability to query HDFS files. For HDFS access the fs-hdfs library is used. DataFusion-Tokomak This crate provides an e-graph based DataFusion optimization framework based on the Rust egg library. An e-graph is a data structure that powers the equality saturation optimization technique. As context, the optimizer framework within DataFusion is currently under review with the objective of implementing a more strategic long term solution that is more efficient and simpler to develop. Some of the benefits of using egg within DataFusion are: Implements optimized algorithms that are hard to match with manually written optimization passes Makes it easy and less verbose to add optimization rules Plugin framework to add more complex optimizations Egg does not depend on rule order and can lead to a higher level of optimization by being able to apply multiple rules at the same time until it converges Allows for cost-based optimizations This is an exciting new area for DataFusion with lots of opportunity for community involvement! DataFusion-Tui DataFusion-tui aka dft provides a feature rich terminal application for using DataFusion. It has drawn inspiration and several features from datafusion-cli. In contrast to datafusion-cli the objective of this tool is to provide a light SQL IDE experience for querying data with DataFusion. This includes features such as the following which are currently implemented: Tab Management to provide clean and structured organization of DataFusion queries, results, ExecutionContext information, and logs SQL Editor Text editor for writing SQL queries Query History History of executed queries, their execution time, and the number of returned rows ExecutionContext information Expose information on which physical optimizers are used and which ExecutionConfig settings are set Logs Logs from dft, DataFusion, and any dependent libraries Support for custom ObjectStores S3 Preload DDL from ~/.datafusionrc to enable having local “database” available at startup Upcoming Enhancements SQL Editor Command to write query results to file Multiple SQL editor tabs Expose more information from ExecutionContext A help tab that provides information on functions Query custom TableProviders such as DeltaTable or BigTable DataFusion-Streams DataFusion-Stream is a new testing ground for creating a StreamProvider in DataFusion that will enable querying streaming data sources such as Apache Kafka. The implementation for this feature is currently being designed and is under active review. Once the design is finalized the trait and attendant data structures will be added back to the core DataFusion crate. DataFusion-Java This project created an initial set of Java bindings to DataFusion. The project is currently in maintenance mode and is looking for maintainers to drive future development. How to Get Involved If you are interested in contributing to DataFusion, and learning about state of the art query processing, we would love to have you join us on the journey! You can help by trying out DataFusion on some of your own data and projects and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable for beginners is here The best way to find out about creating new extensions within DataFusion-Contrib is reaching out on the #arrow-rust channel of the Apache Software Foundation Slack workspace. You can also check out our new Communication Doc on more ways to engage with the community. Links for each DataFusion-Contrib repository are provided above if you would like to contribute to those.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" /><media:content medium="image" url="https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>